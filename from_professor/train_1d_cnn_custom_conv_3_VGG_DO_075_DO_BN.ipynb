{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_075_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3669 - acc: 0.2556\n",
      "Epoch 00001: val_loss improved from inf to 2.15177, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_1_conv_checkpoint/001-2.1518.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.3669 - acc: 0.2556 - val_loss: 2.1518 - val_acc: 0.3557\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9174 - acc: 0.4260\n",
      "Epoch 00002: val_loss improved from 2.15177 to 1.97358, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_1_conv_checkpoint/002-1.9736.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.9173 - acc: 0.4261 - val_loss: 1.9736 - val_acc: 0.4151\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5962 - acc: 0.5312\n",
      "Epoch 00003: val_loss improved from 1.97358 to 1.96512, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_1_conv_checkpoint/003-1.9651.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.5962 - acc: 0.5312 - val_loss: 1.9651 - val_acc: 0.3920\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3330 - acc: 0.6104\n",
      "Epoch 00004: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.3329 - acc: 0.6104 - val_loss: 1.9767 - val_acc: 0.4009\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1194 - acc: 0.6783\n",
      "Epoch 00005: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1193 - acc: 0.6783 - val_loss: 2.0125 - val_acc: 0.3986\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9480 - acc: 0.7305\n",
      "Epoch 00006: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9480 - acc: 0.7306 - val_loss: 2.0838 - val_acc: 0.3913\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7947 - acc: 0.7784\n",
      "Epoch 00007: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7948 - acc: 0.7783 - val_loss: 2.1903 - val_acc: 0.3823\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8111\n",
      "Epoch 00008: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6809 - acc: 0.8111 - val_loss: 2.2328 - val_acc: 0.3934\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5753 - acc: 0.8392\n",
      "Epoch 00009: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5753 - acc: 0.8392 - val_loss: 2.3118 - val_acc: 0.3834\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.8665\n",
      "Epoch 00010: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4931 - acc: 0.8665 - val_loss: 2.4532 - val_acc: 0.3806\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8863\n",
      "Epoch 00011: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4245 - acc: 0.8863 - val_loss: 2.5649 - val_acc: 0.3816\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.9001\n",
      "Epoch 00012: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3724 - acc: 0.9001 - val_loss: 2.6569 - val_acc: 0.3729\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9150\n",
      "Epoch 00013: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3236 - acc: 0.9150 - val_loss: 2.7421 - val_acc: 0.3883\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9259\n",
      "Epoch 00014: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2849 - acc: 0.9259 - val_loss: 2.8201 - val_acc: 0.3850\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9358\n",
      "Epoch 00015: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2515 - acc: 0.9358 - val_loss: 2.9205 - val_acc: 0.3685\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9432\n",
      "Epoch 00016: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2245 - acc: 0.9432 - val_loss: 2.9806 - val_acc: 0.3769\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9498\n",
      "Epoch 00017: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1995 - acc: 0.9498 - val_loss: 3.0762 - val_acc: 0.3820\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9545\n",
      "Epoch 00018: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1840 - acc: 0.9545 - val_loss: 3.1292 - val_acc: 0.3976\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9630\n",
      "Epoch 00019: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1603 - acc: 0.9630 - val_loss: 3.2557 - val_acc: 0.3883\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9630\n",
      "Epoch 00020: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1526 - acc: 0.9630 - val_loss: 3.2486 - val_acc: 0.3823\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9660\n",
      "Epoch 00021: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1388 - acc: 0.9660 - val_loss: 3.3795 - val_acc: 0.3776\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9688\n",
      "Epoch 00022: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1309 - acc: 0.9688 - val_loss: 3.4880 - val_acc: 0.3825\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9746\n",
      "Epoch 00023: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1142 - acc: 0.9746 - val_loss: 3.5079 - val_acc: 0.3811\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9743\n",
      "Epoch 00024: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1089 - acc: 0.9743 - val_loss: 3.5508 - val_acc: 0.3797\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9767\n",
      "Epoch 00025: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1017 - acc: 0.9767 - val_loss: 3.6212 - val_acc: 0.3874\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9789\n",
      "Epoch 00026: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0948 - acc: 0.9789 - val_loss: 3.6773 - val_acc: 0.3841\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9778\n",
      "Epoch 00027: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0951 - acc: 0.9778 - val_loss: 3.7694 - val_acc: 0.3781\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9796\n",
      "Epoch 00028: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0898 - acc: 0.9796 - val_loss: 3.7676 - val_acc: 0.3883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9787\n",
      "Epoch 00029: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0891 - acc: 0.9788 - val_loss: 3.8391 - val_acc: 0.3771\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9828\n",
      "Epoch 00030: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0771 - acc: 0.9828 - val_loss: 3.8289 - val_acc: 0.3839\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9831\n",
      "Epoch 00031: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0740 - acc: 0.9831 - val_loss: 3.9091 - val_acc: 0.3869\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9849\n",
      "Epoch 00032: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0705 - acc: 0.9849 - val_loss: 3.8680 - val_acc: 0.3799\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9831\n",
      "Epoch 00033: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0714 - acc: 0.9830 - val_loss: 4.0162 - val_acc: 0.3827\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9840\n",
      "Epoch 00034: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0689 - acc: 0.9840 - val_loss: 3.9797 - val_acc: 0.3806\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9864\n",
      "Epoch 00035: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0624 - acc: 0.9864 - val_loss: 3.9976 - val_acc: 0.3878\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9867\n",
      "Epoch 00036: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0622 - acc: 0.9867 - val_loss: 4.0518 - val_acc: 0.3897\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9871\n",
      "Epoch 00037: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0597 - acc: 0.9871 - val_loss: 4.1429 - val_acc: 0.3795\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9883\n",
      "Epoch 00038: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0559 - acc: 0.9883 - val_loss: 4.1449 - val_acc: 0.3809\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9886\n",
      "Epoch 00039: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0553 - acc: 0.9886 - val_loss: 4.1879 - val_acc: 0.3804\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9872\n",
      "Epoch 00040: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0559 - acc: 0.9872 - val_loss: 4.1888 - val_acc: 0.3818\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9888\n",
      "Epoch 00041: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0534 - acc: 0.9888 - val_loss: 4.2016 - val_acc: 0.3883\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9888\n",
      "Epoch 00042: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0537 - acc: 0.9888 - val_loss: 4.1989 - val_acc: 0.3906\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9895\n",
      "Epoch 00043: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0507 - acc: 0.9895 - val_loss: 4.1895 - val_acc: 0.3925\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9896\n",
      "Epoch 00044: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0493 - acc: 0.9896 - val_loss: 4.1985 - val_acc: 0.3911\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9900\n",
      "Epoch 00045: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0474 - acc: 0.9900 - val_loss: 4.3168 - val_acc: 0.3948\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9908\n",
      "Epoch 00046: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0452 - acc: 0.9908 - val_loss: 4.2611 - val_acc: 0.3827\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9908\n",
      "Epoch 00047: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0455 - acc: 0.9908 - val_loss: 4.3651 - val_acc: 0.3932\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9898\n",
      "Epoch 00048: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0466 - acc: 0.9898 - val_loss: 4.4006 - val_acc: 0.3774\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9899\n",
      "Epoch 00049: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0460 - acc: 0.9899 - val_loss: 4.3425 - val_acc: 0.3892\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9906\n",
      "Epoch 00050: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0449 - acc: 0.9906 - val_loss: 4.4373 - val_acc: 0.3839\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9919\n",
      "Epoch 00051: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0399 - acc: 0.9919 - val_loss: 4.4127 - val_acc: 0.3867\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9915\n",
      "Epoch 00052: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0402 - acc: 0.9915 - val_loss: 4.4010 - val_acc: 0.3869\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9914\n",
      "Epoch 00053: val_loss did not improve from 1.96512\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0394 - acc: 0.9914 - val_loss: 4.4324 - val_acc: 0.3860\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VNX5+PHPmS2TPSEQCGsAFVnCDtKiuFAUN6oiUsW6tGL9SS1qtcUd2691bUtpbRFbFKugFqVuKHUBolZUVkHAIrJD9n2bycyc3x9nZpJAgACZzEzmeb9e93Unkzv3Pncyee6Zc899rtJaI4QQov2zhDsAIYQQbUMSvhBCxAhJ+EIIESMk4QshRIyQhC+EEDFCEr4QQsQISfhCCBEjJOELIUSMkIQvhBAxwhbuABrr2LGjzs7ODncYQggRNdauXVukte7UkmUjKuFnZ2ezZs2acIchhBBRQym1u6XLSpeOEELECEn4QggRIyThCyFEjIioPvzm1NfXs2/fPurq6sIdSlRyOp10794du90e7lCEEGEW8Ql/3759JCcnk52djVIq3OFEFa01xcXF7Nu3j969e4c7HCFEmEV8l05dXR0ZGRmS7E+AUoqMjAz5diSEAKIg4QOS7E+CvHdCiICI79IRQoiIVF0Nzz0HHTrAsGFw2mlgtR7fOrSG7dth50644ILQxNmIJPxjKCsrY9GiRdx6663H/dqLLrqIRYsWkZaW1qLlZ8+eTVJSEnfddddxb0sI0YbWr4err4Zvvml4Lj4eBg+GoUPNvFs36Ny5YUpIAK8XNm+G3Fwzffwx5OdDejoUFYEltJ0ukvCPoaysjL/+9a/NJnyPx4PNduS3cNmyZaEMTQjRGurr4fPP4T//gY8+gh494I47YPTow5fVGubOhV/9Cjp2hPffN8l8/XrYsMHMX3kFnnnm8NcmJYFSUFlpfu7ZEyZMgHHjzNQG3a+S8I9h1qxZ7Nixg6FDhzJhwgQuvvhiHnjgAdLT09m2bRv/+9//uOyyy9i7dy91dXXMnDmTm2++GWgoFVFVVcWFF17ImWeeyX//+1+6devGG2+8QXx8/BG3u2HDBm655RZqamro27cvCxYsID09nblz5zJv3jxsNhsDBgzg5ZdfZtWqVcycORMwffa5ubkkJye3yfsjRFTKy4PXXjNJfsUKk4QtFhgxAt59F15+GcaOhTvvhB/+0HTVFBbCjTfCO+/ApZfCggUm6QPk5MB115nHWsOBA2Yb+flNJ7cbvvc9OOss6NWrzXc7qhL+9u23U1W1oVXXmZQ0lFNPnXPE3z/22GNs3ryZDRvMdleuXMm6devYvHlzcKjjggUL6NChA7W1tYwaNYrJkyeTkZFxSOzbWbx4Mc8++yxXXXUVr732Gtdee+0Rt3vdddfx5z//mbPPPpsHH3yQhx9+mDlz5vDYY4+xc+dO4uLiKCsrA+Cpp57i6aefZuzYsVRVVeF0Ok/2bRGi/frwQ5g6FYqLITsbrrkGzj8fzjsP0tJM8l+wAObMgcmToU8fk8yfeca8Zu5c+PnPj9wiV8p053Tr1qa71RJRMUon0owePbrJuPa5c+cyZMgQxowZw969e9m+ffthr+nduzdDhw4FYMSIEezateuI6y8vL6esrIyzzz4bgOuvv57c3FwABg8ezLRp03jxxReD3Uljx47lzjvvZO7cuZSVlR21m0mIdutYw4+1hj/8wST3zp1h40b47juYNw+uuMIke4DkZJg505xMXbLELDt7NqSkmK6f225rk+6XUIiqzHC0lnhbSkxMDD5euXIlH3zwAZ999hkJCQmcc845zY57j4uLCz62Wq3U1tae0LbfeecdcnNzeeutt3jkkUfYtGkTs2bN4uKLL2bZsmWMHTuW5cuXc/rpp5/Q+oWIOl4vzJgB//iHaZH/4hem26RxUq6pgZtugsWLTXJ//nmT2I/GZjPrmzzZHBiyssyJ2SgmLfxjSE5OpjJwkqUZ5eXlpKenk5CQwLZt21i9evVJbzM1NZX09HQ+/vhjAP75z39y9tln4/P52Lt3L+eeey6PP/445eXlVFVVsWPHDnJycvj1r3/NqFGj2LZt20nHIERUqKuDKVNMd8sFF8B775m+95EjYeFC8/udO+H73zf98o88Ylrtx3uOq0+fqE/2EGUt/HDIyMhg7NixDBo0iAsvvJCLL764ye8nTpzIvHnz6N+/P/369WPMmDGtst2FCxcGT9r26dOH5557Dq/Xy7XXXkt5eTlaa37xi1+QlpbGAw88wIoVK7BYLAwcOJALL7ywVWIQIqKVlZkTqrm58Kc/mZZ9dTW8+KLpZ7/hBrjrLvD5zPTOOxDj/xtKax3uGIJGjhypD70BytatW+nfv3+YImof5D0U7c7BgzBxImzdCi+8AD/6UdPfa21G38ydC1VVpp/+lFPCE2uIKaXWaq1HtmRZaeELIaLL9u2m+6agwLTaJ0w4fBmlzKib885r+/gimCR8IcSJ2b/fXGzk9ZouE60buk8qK00r/MCBpnOXCxwOsNubzlNSTImCjIyGeWqqOdlaUgKlpQ3z1avNmPkVK2DUqHC/C1FFEr4QouX27jUnPZcsgf/+99jLp6eb0S1ZWeZq0vh4c2Vr48nlgooK0z1TXGwmj6fpetLSzIEgPR3OPBOefNLUrhHHRRK+EMLw+UwSr6gwCbfxtH+/uTL188/NskOHwv/9H5x7LjidpsVtsZiuFIvF1I3JyjK/O15am373sjJITDQt/eMtSiaaJQlfCAH79plRLR9+eORlRoyARx+FK68M7QlQpcywSSkP0uok4QsR6155BW65xXSv/OUvpl/cZms6paRAly7hjlScJEn4IZCUlERVVVWLnxciLMrKTE2Yl16CM84w49fb6dBFYYT8SlullFUptV4p9XaotyWEaKEVK0zN9pdfNnViPvlEkn0MaIvSCjOBrW2wnZCYNWsWTz/9dPDn2bNn89RTT1FVVcX48eMZPnw4OTk5vPHGGy1ep9aau+++m0GDBpGTk8Mrr7wCwMGDBxk3bhxDhw5l0KBBfPzxx3i9Xm644Ybgsn/84x9bfR9FjHC7TS2ZM88049OdTvj0U3joIdNtI9q9kP6VlVLdgYuBR4A7T3qFt99uxv22pqFDTRnUI5g6dSq33347M2bMAODVV19l+fLlOJ1Oli5dSkpKCkVFRYwZM4ZJkya16B6yr7/+Ohs2bGDjxo0UFRUxatQoxo0bx6JFi7jgggu477778Hq91NTUsGHDBvbv38/mzZsBgiWRRYz68EMzDn3CBHMStSWjV/bsgfnz4dlnzcVKffvCU0+ZfvtGhQBF+xfqw/oc4FdA1J5uHzZsGAUFBRw4cIDCwkLS09Pp0aMH9fX13HvvveTm5mKxWNi/fz/5+fl0acGJrU8++YSrr74aq9VK586dOfvss/nyyy8ZNWoUP/nJT6ivr+eyyy5j6NCh9OnTh++++47bbruNiy++mPPPP78N9lpEpE8/hYsvNuPW77/fjEs//3xz1ekFF5gx6t99Bzt2NExbt5ruG63hkktMVckJE0J+Kz0RmUKW8JVSlwAFWuu1SqlzjrLczcDNAD179jz6So/SEg+lKVOmsGTJEvLy8pg6dSoAL730EoWFhaxduxa73U52dnazZZGPx7hx48jNzeWdd97hhhtu4M477+S6665j48aNLF++nHnz5vHqq6+yYMGC1tgtEU2++QYmTTK3xXvrLXMrvffeM9PLL5tllDKJPSAlxbTm777btOazs8MSuogcoWzhjwUmKaUuApxAilLqRa11k9s8aa3nA/PBFE8LYTwnbOrUqUyfPp2ioiJWrVoFmLLImZmZ2O12VqxYwe7du1u8vrPOOotnnnmG66+/npKSEnJzc3nyySfZvXs33bt3Z/r06bhcLtatW8dFF12Ew+Fg8uTJ9OvX76h3yRLtVH6+qfJotZrb7/XtC/36mYJhPh989ZW5VV9dnfld377mBGxGRtTeqEOERsgSvtb6HuAeAH8L/65Dk320GDhwIJWVlXTr1o2srCwApk2bxqWXXkpOTg4jR448rhuOXH755Xz22WcMGTIEpRRPPPEEXbp0YeHChTz55JPY7XaSkpJ44YUX2L9/PzfeeCM+nw+ARx99NCT7KCJUdbXpisnLg5UrTTJvzGIx56H8d1MT4mjapDxyo4R/ydGWk/LIoSHvYYTS2oyaefVVM3Lm8subJnSPxzy3bBksXWq6dIQ4RMSVR9ZarwRWtsW2hIgK33xjTqB++CFkZsIbb5i+9kGDTJK//HIzsubtt+Gvf5VkL1qFnKoXoi3V1sKDD5qLntasMcn8wAEzuuaPfzQjbx55BIYPNzft+PWv4f/9v3BHLdoJudpCiNa0dy/8+c+mLk3Hjk2nkhLTit+xA6ZNM2PhA8N4e/c215ncfjsUFpqROGVl5mchWokkfCFaQ3k5PPaYGTrs9UJcnCnxe6jTToMPPoDx44+8rk6d4Cc/CV2sImZJwhfiZLjdpuvlN78xN+649lpTJ75XL3OBVHExFBWZqbYWfvADczAQIgwk4QtxLMuXm/52m82MhQ/M3W74299MF8348fDEE6bvPSAuDrp2NZMQEUAS/jGUlZWxaNEibr311uN+7UUXXcSiRYtIS0sLQWQi5KqqYOZMONqVzTk55mKoCy6Qi5xExJNROsdQVlbGX//612Z/5zn0vpuHWLZsmST7aPX55+Zipuefh/vuMxdA1dSY2/+Vlpoumvx8U8xv4kRJ9iIqSMI/hlmzZrFjxw6GDh3K3XffzcqVKznrrLOYNGkSAwYMAOCyyy5jxIgRDBw4kPnz5wdfm52dTVFREbt27aJ///5Mnz6dgQMHcv7551NbW3vYtt566y3OOOMMhg0bxg9+8APy8/MBqKqq4sYbbyQnJ4fBgwfz2muvAfDee+8xfPhwhgwZwvijnQQULefxmP74sWPN45UrTZ98QoK5AXdysrmhdkaGGT8vRchEFImqLp0wVEfmscceY/PmzWzwb3jlypWsW7eOzZs307t3bwAWLFhAhw4dqK2tZdSoUUyePJmMjIwm69m+fTuLFy/m2Wef5aqrruK11147rC7OmWeeyerVq1FK8fe//50nnniC3//+9/z2t78lNTWVTZs2AVBaWkphYSHTp08nNzeX3r17U1JS0orvSozauhVuusncyPvaa83t/lJTwx2VEK0mqhJ+pBg9enQw2QPMnTuXpUuXArB37162b99+WMLv3bs3Q/31TkaMGMGuXbsOW+++ffuYOnUqBw8exO12B7fxwQcf8HKgIiKQnp7OW2+9xbhx44LLdOjQoVX3MWb4fOak7Ny5pvJkaqopd/CjH4U7MiFaXVQl/DBVRz5MYqObRqxcuZIPPviAzz77jISEBM4555xmyyTHNRqKZ7Vam+3Sue2227jzzjuZNGkSK1euZPbs2SGJXwCVlbBwoblI6n//g6ws05Xzs5+Zrhoh2iHpgDyG5ORkKisrj/j78vJy0tPTSUhIYNu2baxevfqEt1VeXk63bt0AWLhwYfD5CRMmNLnNYmlpKWPGjCE3N5edO3cCSJdOS5WXm5uHdOsGt91mbhqyaBHs2gUPPCDJXrRrkvCPISMjg7FjxzJo0CDuvvvuw34/ceJEPB4P/fv3Z9asWYwZM+aEtzV79mymTJnCiBEj6NixY/D5+++/n9LSUgYNGsSQIUNYsWIFnTp1Yv78+VxxxRUMGTIkeGMWcQQuF/zpT6Ya5SOPmJE1q1eb6eqrweEId4RChFyblEduKSmPHBox/R76fKZP/v77TSt+/Hh4/HFzP1gh2oGIK48sRJsrKDA15OfNM0O7hg41J2cnTJAx8yJmScIX7UdeHrz+OixZAqtWmdZ9v37w4oum20bGzIsYJwlfRB+fz5Qh/uYb2LbNzDdsgM8+M3eROv10c3XslVea0gfSohcCkIQvosl335mywV98YSpPBqSkQP/+8NBDJskPHBi+GIWIYJLwRXRYtQomTzYt+FtuMa34fv3M1LmztOKFaAFJ+CLy/f3v5jZ/p5xi7gR1yinhjkiIqCRnsUIgKSkp3CG0Dx4P3HEHTJ9uhlOuXi3JXoiTIAlfRKbycrj0UlNPY+ZMePttKWQmxEmShH8Ms2bNalLWYPbs2Tz11FNUVVUxfvx4hg8fTk5ODm+88cYx13WkMsrNlTk+UknkmFBQAGeeae79On++Sfo26X0U4mRF1X/R7e/dzoa81q2PPLTLUOZMPHJVtqlTp3L77bczY8YMAF599VWWL1+O0+lk6dKlpKSkUFRUxJgxY5g0aRLqKCcPmyuj7PP5mi1z3FxJ5JhQUADnnWdG5Lz33tFv9i2EOC5RlfDDYdiwYRQUFHDgwAEKCwtJT0+nR48e1NfXc++995Kbm4vFYmH//v3k5+fTpUuXI66ruTLKhYWFzZY5bq4kcrtXWGgS/HffwTvvwLnnhjsiIdqVqEr4R2uJh9KUKVNYsmQJeXl5wSJlL730EoWFhaxduxa73U52dnazZZEDWlpGOWYVFpqW/Y4dpr9ekr0QrU768Ftg6tSpvPzyyyxZsoQpU6YAppRxZmYmdrudFStWsHv37qOu40hllI9U5ri5ksjtVqBl/+23ZtjleeeFOyIh2qWoauGHy8CBA6msrKRbt25kZWUBMG3aNC699FJycnIYOXIkp59++lHXMXHiRObNm0f//v3p169fsIxy4zLHPp+PzMxM3n//fe6//35mzJjBoEGDsFqtPPTQQ1xxxRUh39eQ2bsXVqyAxERzZWxgsljgqqtg+3aT7KXPXoiQkfLIMSDs7+HmzSaRFxQ0/3unE95801SyFEIcFymPLCLH+vUmkcfFmZuDJyVBRUXDVF4OZ5wBQ4aEO1Ih2j1J+CJ0vvwSzj/fdN189JG525QQImyi4qRtJHU7RZuwvXeffmq6cdLTITdXkr0QESDiE77T6aS4uFiS/gnQWlNcXIzT6WzbDa9cCRdcAF26mGTfq1fbbl8I0ayI79Lp3r07+/bto7CwMNyhRCWn00n37t3bZmPFxfDcc/Dgg9C7tymN4B/VJIQIv4hP+Ha7PXgVqohAWpuTsfPmwb/+BS6XGUe/eDFkZoY7OiFEIxGf8EWEcrtNnfp582DTJkhOhptugp/9zNxWUAgRcUKW8JVSTiAXiPNvZ4nW+qFQbU+0oW+/hR/9CNauhWHDTEXLq682Qy6FEBErlC18F3Ce1rpKKWUHPlFKvau1Xh3CbYpQW7TItOLtdnj9dbjsMrm9oBBRImSjdLRR5f/R7p9kqE20qq42NxCfNs1cJLVhA1x+uSR7IaJISIdlKqWsSqkNQAHwvtb681BuT4TIxo0wciQ8/zzcf78ZdtmzZ7ijEkIcp5AmfK21V2s9FOgOjFZKDTp0GaXUzUqpNUqpNTL0MsJ88425n+zo0VBWZoZZ/va3cvcpIaJUm1x4pbUuA1YAE5v53Xyt9Uit9chOnTq1RTjiWD77zHTX9O8PL75ounI2bpSyxUJEuZAlfKVUJ6VUmv9xPDAB2Baq7YlWsHw5jBsH3/8+rFoF990Hu3fD3/4mY+qFaAdC+d08C1iolLJiDiyvaq3fDuH2xIlyu+GXv4S//MX0zc+ZAz/9qQyzFKKdCVnC11p/BQwL1fpFK9m7F6ZMgc8/hzvugMceA4cj3FEJIUJAzr7Fsv/8B665xrTw//UvuPLKcEckhAihiK+WKULA54Pf/AYmTjTFzdaskWQvRAyQFn6s2b4dbr3VDLG89lpTCycxMdxRCSHagLTwY0VNjbloatAg01//zDPwwguS7IWIIdLCb++0hqVLzQnZPXtMq/6JJ6ROvRAxSBJ+e7Z9O9x2mxlfn5NjxtaPGxfuqIQQYSJdOu2RxwNPPgmDB5urZufMgXXrJNkLEeOkhd/ebNpkSiGsWWNKFz/9NHTtGu6ohBARQFr47YXbbYZajhhhyiG88oqpVy/JXgjhJy389mD9erjhBvjqK3Mh1Z/+BB07hjsqIUSEkRZ+NPN64fHH4YwzoKgI3nwTXnpJkr0QolnSwo9We/bAddeZkTdTppgLqDp0CHdUQogIJi38aLR4sRmBs3atuQvVK69IshdCHJMk/GhSUWEunLrmGhgwwNyU5Prr5b6yQogWkYQfLXbvhrFj4eWX4eGHITcX+vQJd1RCiCgiffjR4Msv4dJLoa7OXDU7fny4IxJCRCFp4Ue6pUvh7LMhPh7++19J9kKIEyYJP1JpDb//PUyeDEOGmAqXAwaEOyohRBSThB+JPB5Ts/6uu8yNST76SG4iLoQ4aZLwI43PBzfdZMbVz5plTtLGx4c7KiFEOyAJP5JoDb/6FSxcCLNnw6OPgkX+REKI1iHZJJI88YTpt//5z+HBB8MdjRCinWlRwldKzVRKpSjjH0qpdUqp80MdXEv4fC4OHHiWsrJPwh3KyXn2WdOFc/XVpviZXEwlhGhlLW3h/0RrXQGcD6QDPwYeC1lUx0EpG999N4uDB58Ndygn7rXX4JZbYOJEUypBunGEECHQ0swSaG5eBPxTa/11o+fCSikrHTpMpKTkXbT2hTuc4/fRR6ZUwhlnwJIl4HCEOyIhRDvV0oS/Vin1H0zCX66USgYiJrtmZFxEfX0hlZVrwx1Ky3k88Oc/w6RJcOqp8PbbkJgY7qiEEO1YSxP+T4FZwCitdQ1gB24MWVTHKT39AkBRUrIs3KG0zOrVMGoU/OIX8P3vw3/+I9UuhRAh19KE/z3gG611mVLqWuB+oDx0YR0fh6MjKSlnUFz8TrhDObriYpg+Hb73PSgshFdfNbVx5DaEQog20NKE/zegRik1BPglsAN4IWRRnYAOHS6msvJL3O78cIdyuJISczPxfv3guefgl7+ErVvNjUtkNI4Qoo20NOF7tNYa+CHwF63100By6MI6fhkZFwFQUrI8zJH4FRaaoZYXXACdO5ux9QMGmPvPPvUUJEfU2yeEiAEtTfiVSql7MMMx31FKWTD9+BEjKWkoDkcXiovD3I//yiumomWXLnDzzbBjh2nRf/GFuR1hTk544xNCxKyW1sOfClyDGY+fp5TqCTwZurCOk8+HUooOHS6kqGgpPp8Hi6WNS/3X18Mdd5ium9NOg3vvNYXPBg+WbhshRERoUQtfa50HvASkKqUuAeq01pHRh19WBhMmwMKFdOhwER5PGRUVq9s2hqIi03Xz9NOmwuWWLfDb35qyxpLshRARoqWlFa4CvgCmAFcBnyulrgxlYC2WnGwqTM6YQYfCbJSyte3wzE2bYPRoc3OShQvhySfBam277QshRAu1tA//PswY/Ou11tcBo4EHQhfWcbBa4cUXIT4e27SbSHN+r+2GZy5daoZY1tWZ/vnrrmub7QohxAloacK3aK0LGv1cfByvDb1u3UwNmo0b6TPPR3X1V9TV7Qvd9srL4e674YorzMibNWtMaQQhhIhgLU3a7ymlliulblBK3QC8Axy130Qp1UMptUIptUUp9bVSaubJBntUl1wCM2eS/PynZHwKJSXvtv423G6YOxf69jVDK2+6ybTs5cIpIUQUaOlJ27uB+cBg/zRfa/3rY7zMA/xSaz0AGAPMUEqF9qasjz+OHjaM05+wULHltdZbr9bmqtj+/WHmTHMydu1aM85e7kYlhIgSLR67qLV+DWhxFtVaHwQO+h9XKqW2At2ALccbZIvFxaFefhnrsIF0uft9fF/UYLEnnPj6Cgrggw9MffovvjBj6N9914zIkdE3Qogoc9QWvlKqUilV0cxUqZSqaOlGlFLZwDDg85MLtwVOO43qx2eQtsGH68Fbj++1LhesWAH33APDh5srZKdNgwMHYMECc5XsxImS7IUQUUmZigkh3IBSScAq4BGt9evN/P5m4GaAnj17jti9e/dJb9PrqaLowlQyP9KoCedDerqZ0tLMPDHR1LcpKID8/IZp926orQWbzVSxPP9805ofNkyGWgohIpJSaq3WemRLlg3p5ahKKTumG+il5pI9gNZ6Pub8ACNHjmyVo4/VlkTBw+ditX5Bx5ISU96gtNRcpOX1NiyYmmpa8Z07w8CBpvV+7rlmklo3Qoh2JmQJXymlgH8AW7XWfwjVdo4kveckNs/6kDPOWEx8fF/zpNZQVWWm9HRwOts6LCGECJtQjqUfiym2dp5SaoN/uiiE22uiQwezqcLCRl8slDIt96wsSfZCiJgTsha+1voTwnjf24SEU0hJGcvBg8/So8cvMQU+hRAidrXrLNi16y3U1m6nrGxFuEMRQoiwa9cJv1OnK7HZOnDgwDPhDkUIIcKuXSd8q9VJly43UlS0FJcrL9zhCCFEWLXrhA/QtevNaO0hL29BuEMRQoiwavcJPyHhNNLSxnPgwHy09h77BUII0U61+4QP5uSty7U7cm5wLoQQYRATCb9jxx9it3eWk7dCiJgWEwnfYrGTlfVTiovfpq5ub7jDEUKIsIiJhA+QlTUd0Bw8+PdwhyKEEGERMwk/Pj6bDh0u5ODBZ/H56sMdjhBCtLmYSfhgTt663QcpLn473KEIIUSbi6mEn5FxEXFxPThwYF64QxFCiDYXUwlfKStZWdMpLf0PNTXfhjscIYRoUzGV8MGcvFUqjj17Hgt3KEII0aZiLuHHxXWha9ebyc9fSG3trnCHI4QQbSbmEj5Az56/Bizs2fNouEMRQog2E5MJPy6uG1lZ08nLe466uj3hDkcIIdpETCZ8CLTykVa+ECJmxGzCdzp7kJX1Uw4e/IeUWxBCxISYTfgAPXvOAmDPnsfDHIkQQoReTCd8p7MXXbrcwMGDz+Jy7Q93OEIIEVIxnfABeva8B/CxZ88T4Q5FCCFCKuYTfnx8bzp3vo6DB+fjch0MdzhCCBEyMZ/wAXr1uhefr569e6WVL4RovyThA/Hxfenc+VoOHJhHXd2+cIcjhBAhIQnfLzt7NqDYseOOcIcihBAhIQnfLz4+m1697qewcAnFxe+FOxwhhGh1kvAb6dHjl8TH92P79p/j9daFOxwhhGhVkvAbsVjiOO20p6mr28HevXIxlhCifZGEf4j09PFkZl7N7t2Pyk1ShBDtiiT8ZvTt+3ssFgfbt/8crXW4wxFCiFYhCb8bgBBGAAAZ30lEQVQZcXFZ9O79f5SWLqeo6PVwhyOEEK1CEv4RdO16K0lJQ9m+fSYeT2W4wxFCiJMmCf8ILBYbp576N9zu/eza9XC4wxFCiJMmCf8oUlPHkJV1M/v2/ZHS0hXhDkcIIU6KJPxj6Nv39yQknMaWLVfjcuWFOxwhhDhhIUv4SqkFSqkCpdTmUG2jLdhsSQwcuASvt4KtW6/G5/OEOyQhhDghoWzhPw9MDOH620xi4kBOO20eZWUr2bXroXCHI4QQJyRkCV9rnQuUhGr9ba1Ll+vIyrqJPXt+R3Hxu+EORwghjpst3AFEk1NOmUtFxRds3XotI0eux+nsGe6QxEnSGrxeqK8Hjwd8PrBYwGo188AUWM7jaZh7/L17Fgso1TBXyqyvrq7p5HKZ9Qe2G7imT+uG9Tbehs8HNpuJxWZreGyxmN8dOgX2I7Avjffp0O01ng59rvH6Gs8DyzWeQ0NMVmvDBIfvj9d75L9B4203juFoyzf3uPHrA/Ojafw3DkzHirG5WAP7F3ivAtsNfB6UavpzQOBxairMmXP0WFtD2BO+Uupm4GaAnj0jO4FarfEMHLiEtWtH8PXXVzFsWC4WiyPcYUUcraG6GsrKoLTUTGVlUFPT/LJeL7jdZqqvb3h86HOBeU0NVFWZbVRXm8c1NQ3/+I3/uQLJNJAAD02GR0pCom01PmAGEu+hyfHQ5ZtLoo1fe6x1HHqACyTuo22z8Tobb6fxQS/w/KHbCTxu/HxAx47Hfo9aQ9gTvtZ6PjAfYOTIkRFfxyAh4VT69fsHW7ZcxY4dd3HqqXPDHVKrqK2F4mKTmCsqoLy86byy8vApkGhra80UeFxd3dD6PVlWKzgcYLebyeGAhARITISkJEhOhqws81ygJQ5N/5lsNvPawLzx40DLOfCzUs23nJVqvrUd2Fbj5bU2cTqdTae4uIYW5KEtv8brDsyVav5bhc93+DeQQHyH7mPjOJtLks0913jdjR83Xi4wP1ILV+um+9NcIhRtL+wJPxplZk6houJ29u2bQ2LiQLp2/Vm4QzqMywX79kFeHhQUQGFh03lRkUnwgXlzre9DBRJsYEpKgk6dID6+6ZSYCOnpkJZm5oEpIaH5f3iLxSRDh6NhCiT3I33FFpEh0MK1SSaJCiH7MymlFgPnAB2VUvuAh7TW/wjV9tpanz5PUlPzDf/73wyczj506DChzbZdVwf79zdM+/aZae/ehik/v/nXpqSYJN2pE3TrBoMHm6+TGRlmSk83y6SmNp0nJkryFSLahSzha62vDtW6I4HFYmPAgJdZv34sX399JcOHf0Zi4oBWW39dHWzfDt9803TascO0yA+VlAQ9ekDPnjB0qHncowd07QqZmWbq2NG0pIUQsUm+iJ0Emy2FnJy3Wbv2DDZtuoThw1fjcGQe93rq62HzZvjii4Zpy5amIwy6dYN+/eDKK6F7dzN169YwpaRI/6gQ4ugk4Z8kp7MXOTlvsmHD2WzefBlDhnyE1eo84vJaw+7dsHo1fP65mdavNy16MN0qo0fD5ZfDgAEmyZ96qmnBCyHEyZCE3wpSUkZz+un/ZMuWKXzzzU/o3/8llL+5XV8Pa9bAypXw2WcmwRcUmNc5nTBiBNx6K4waZRJ9797SUhdChIYk/FaSmXkltbW/49tvH2DLljFs334bq1YpPvnEDFME01qfOBHOOAPGjIGcHDMaRQgh2oIk/FagNXz5JSxcOItFi26jrMz0vwwc6OOGGyycey6MG2dGxgghRLhIwj8Je/bAiy/CCy+YETRxcYrLLkvkrLOW0rXrz+jdeyADBy7Fbk8Ld6hCCCEJ/3i53fDvf8P8+fDhh+a5cePg7rvNCJrUVAVcTn5+Ddu23cj69WMZPHgZTmevsMYthBCS8Ftoxw549ll47jlz0rVXL3j4Yfjxj82J1kN17jwNh6Mrmzdfzrp1Y8jJeYfk5OFtH7gQQvjJtZNHoTW89x5MmACnnAJPPQXf/z68+645ADz4YPPJPiA9/VyGD/8UpRysXz+OoqK32y54IYQ4hCT8I/j4Yzj7bLjwQti2DX7zGzN+fulSM9ImUJDqWBITBzJ8+GoSEk5n8+ZJ7N79O7SO+BpxQoh2SBL+IdauNUl+3Dj49lt4+mnTmn/gAXNF64mIi8ti2LBcMjOvZufO+9iy5Ud4vdWtG7gQQhyDJHy/b781J11HjjSlDZ54wjx3662mauPJsloT6N//Rfr0eYLCwn+xbt1Yamt3nfyKhRCihWI+4dfWwuzZMGgQLF9u+uW/+86MuklIaN1tKaXo2fNucnKWUVe3i7VrR1JaurJ1NyKEEEcQ0wn/3XdNon/4YVO75ptvzOPU1NBuNyNjIiNGfIHD0YmNG3/Azp0P4fO5QrtRIUTMi8mEv3cvTJ4MF11kSht88AEsXmxKCbeVhITTGD78czIzf8Tu3b/hyy+HUFa2qu0CEELEnJhL+IsXQ//+pnX/u9/Bxo0wfnx4YrHZUhgw4EUGD34Prd1s2HAO27bdRH19SXgCEkK0azGT8L1e+NWv4JprYPhwU2/+nnsi44YgHTpcwKhRm+nR41fk5T3PF1/0Jz9/sQzfFEK0qphI+KWlpvvmySdhxgxTEiE7O9xRNWW1JtC37+OMGLEGp7MXW7dew7p136O09KNwhyaEaCfafcL/+mtTa37FClMa4S9/ieySxMnJQxk+/DP69fs7bvd+Nm4cz8aNE6io+DLcoQkholy7Tvj//repO19dbW5ActNN4Y6oZZSykpX1U0aP3k7fvn+gqmoD69aNZvPmyVRXbw13eEKIKNUui6dprXns1Q+499XnyZySxsxrcqDHYCpcg0iJS2n2NS6Piyp3FTaLDZvFht1qx2axYVHhOyZarU569LiDrKyfsm/fH9m79/cUFb1OevoPyMr6GR07TsJiaYWrwoQQMUFF0onBkSNH6jVr1pzw6+u99bzy9Ss8uuoptpRsxOrKICHZTaW7MrhMr9Re9OvYjzpPHSW1JZTWllJaV0pNfU2z67QoC3HWOFLiUkiJSyHVmRp8nOZMI92ZTof4Dk0mt9dNQXVBk6mwphCnzUlGfAYdEzoGpw7xHYi3xWO32nFYHdgt/rnVjtYan/YFJ5e7hOLi16gre5M430HS4jvRLesndO06nfj4vvi0j4OVB9lTvic4VbmriLPF4bA6glOcNY5OiZ3ondab7LRsEh2JR31ffdpHnaeOmvoaauprqHZXU1NfQ4Wrgr0Ve9lTvofdZbvZXb6bPeV7KKktIc2ZRkZCRvA9yYjPIM2ZRpIjiSRHEon2xODjBHsCCfYE4u3xxNvig4+9Pi819TXUemqD266tr0WjsSorVov1sLlFWZo8p9FUuCooryun3FUenNfU1wTfC4fVEXyP4m3xJDoaYgtM9d56CmsKKaopajI5bU4yEzObTB0TOlLvrafSXUmlq5JKdyUVrgqq3FW4PC7cXneTyau9JDuSSY5LDn62UuJScNqc1HvrqffV4/a6qfeauUYHGyY2iw27xTROXF4XFa6K4FReV06FqwK3143H5wlOXu3F6/OS6kylY0JHOiV0Cn4e0+PTg39zr88bXFZj8oRCoZQKzptbNvB5qXRXUuWuCr4Hla5K6jx1eHwe6n31Zu41c4/2NI3R58Xj82BRFvM39f89Az+7vW7qPHW4vC5cHhd1HnNT6MzETDondqZLUpfgFG+PD/69CqvN37CwphCrxUqv1F5mSmuY+7SP/Kp88qvzya/KJ68qj8KaQhSKBHsCiY5EM7cnBj+nh/6NPD5P8D07NMdalAWlVHBfLMpCoj2R64defwJZD5RSa7XWI1u0bHtI+BWuCp5d+yxzPp/Dvop9xFcOQH96F18uuIaBpzvYXb6bTfmb2FSwia/yv+Lbkm9JciSRHp8eTNjpznSS45KDH7TGH8jAh7fcVd7kHypwsKhyVx0xtiRHUjAJuDyu4AfP5W2dC60SrZBsB4vFSWFdPfU+73Gvo1NCJ7LTsslOy8arvZTWllJWV0ZpnZmX15UHP7xH0jmxM73SetEztScZ8RmUu8oprimmpLaEktoSimuLqXBVnOhuipMUODhYlTX4zbXcVY5P+9o0DofV0eQgZbfasSpr8Bt14xitFita6yYHEq82c4fVgdPmJM4aR5wtjjirGW5XUF1AXlUeBdUFePXh/wsOqyN4gPP4POwu333U/9/GrwNwe92t+4b4dU7sTN5deSf02uNJ+FHfpVPhqqD3n3pTUlvCOdnnMHz/PN78w4X861ULg/qbZQLJ7NJ+l4YkBrfXTWltaTCxxVnjyEzMpFNiJxLsh9dn0FpTU19DUU0RxbXF1Hnqgi2DQEvB4/OgaNoKsCgLXu01ydh/sCmq2suB0rXU1H5LRkYdWQlxnNb5HHJ6TmNAt0mkxKUE1xloXbq8LvKq8thVtoudpTvNvGwnG/M3YrfYSXOm0S2lGwMzB5LuTCc1LtW0yv0tm8CU5EiiR0oPeqT2wGlzHvN98mkfNfU1VLmrglOlqzLYiq+trw3Oa+prsFqsDa1/f8vfaXOilGqSAAKty+Zamhod/DaWGpdKqjOV1LhUEuwJwRa2y+sKvj+1nlqq3dVNYgx09XVKbGgJd0rodNRvcw6ro0mrPdmRTJIjCafNGfymFfhWZ1EWqtxVVLgqqHRVBhsUdZ66Zr/5KRRe7Q22hgPfAhp/Ew18G012JOOwOoKt8UP/HmV1ZU2+sZTUlqBQh31bsigLWms0uskcaPYbVpwtLrj/gXmiPRGrpYVlZk+ST/sorikmryqPWk9t8G+W5Ehq8l5orSmtKw1+Q91VtgurstI5yXxLCHxbSIlLQSlFvbc++BkJfOu0WqyH/Y0O7Q5WmG0G3rfAZzPw7b2ttIsW/pzVcziz55ns+u9IpkyBmTNhzpwQBBjBtPZRXv4xeXnPU1DwL3y+auLjTyEz8xoyMi4lOXk4KoznI4QQoRFzXToA//ufqXQ5cCCsWtU6FS6jlcdTRVHRa+TlPU9ZWS7gw+HIIiPjYjIyLiE9/QdYrUfvtxdCRIeYS/g1NWb45YEDsH499OgRguCilNtdREnJuxQXv01JyXt4vRUoFUda2lmkpZ1LWtp5JCePxGKJ+t49IWJSTPXhA/z857B5MyxbJsn+UA5HR7p0+TFduvwYn6+e8vJPKC5+i9LSD9m58z4ArNZkUlPPIj39PFJSvkdS0jCs1vgwRy6EaG1Rn/BLSiA319yRauLEcEcT2SwWO+np55Kefi4AbnchZWUrKStbQWnpR5SULPMvaSUxcRApKaNITjZTYuJAGfMvRJRrF106FRWQmNjy+8yK5rlcB6mo+JzKyi/90xo8nlIAlHKQmDiIpKRhJCcPJylpGElJg+VcgBBhFnN9+CI0tNbU1u6gsvJLqqrWU1W1nsrK9Xg8xcFl7PbOOJ09iYvridPZC6ezJ05nbxITc3A6s5sdDiiEaD0x14cvQkMpRULCKSQknELnzlcD5iDgcu2jqmo91dWbqKvbTV3dbmpqvqakZBk+X23w9VZrCklJg0lMHExS0hDi40/FZkvDZkvFZkvFak2Vk8VCtCH5bxPHRSmF09kDp7MHHTtOavI7rTX19cXU1n5LdfVXVFVtpKpqI/n5/+TAgb82uz6LJRG7vSNxcd0aTd1xOLpht3dsdIAwczmPIMSJk4QvWo1SCoejIw5HR1JTxwSf11pTV7eLurqdeDzleDzleL3lwcf19QW4XPupqtpIcfE7+HzN1zUCsFjiiYvrjtPZG6czG6ezN/Hx5rHVmoLVmojFkuCfO6VLSYhGJOGLkFNKER9vEvOxaK3xeMpxu/dTX1/sPyiUNZqX4nLtpa5uJ4WFa5ucT2hmy1gsCdhsKVitKf5vCoHHKVityVityY0ep2CxBEpE6CZzpexYrQlYLIlYrYEDSiJK2fwHFTOZq5kt2GwpKCWjCERkkYQvIopSCrs9Dbs9rUXLezyV1NXtwuXag8dTic9Xjddbg9db7X9cjddbicdTgddbgcdTgdud5/+WUYnHUwkcf8G5Y7PgcGRit3fG4ejinzKxWOKxWBwo5ThkHofF0jCZn+1o7QN8jebmAGSx2FGqYbJY7Fgs8f4DV5J8uxHNCmnCV0pNBP4EWIG/a60fC+X2ROyx2ZJJSsohKSnnhF6vtcbnqw0eFHy+OvAXumpImAqfz43PFziQ1PgPJNVo7cF8CwgUE/OhtRePpwS3Ox+3Ow+3O4+amq3U1xf4198WrFitSdhsySgVuMWbajJXytbowGFrdPBw+qe4Ro8d/tdZmnyjMZOvyf6b90M1ORg1HJwsjcoFN4wQDGy/6UHQjlJW/zclq/+xBaVs/p9tzU5mew0/m/TTeJuBssWHHky9/seBv73Fvx+B/bY0OzcH2vio+EYXsoSvzN4/DUwA9gFfKqXe1FpvCdU2hTheSil/F00CDkfnkG9Pa43WHrR24/O5/XNXcNK68WPPIcklkIQ0WtejdT0+X32jx7V4PJV4vVV4vZXBSWsvHFabPRBH4PUe/7rceDxl+Hx1/ljqgvGY1wQSesPjxrE1HAx0k9g4Rnnt9kCpOH+3X0KjrkFofJDxL3nYe2a3d2LYsNyQxxjKFv5o4Fut9XcASqmXgR8CkvBFzFJK+Vvc9pi6aE1rLz5f48Tf8C3KHIS8/gNgfaMDoRvw+lveXhpa4Z5m5p7ggavpVO9/bcMNWxp/Owl8a2jcYvdHTOODWsOBrvE3Ap//QFnn/9ZX65/XNBqe3PjboqLhm1BgXWZus6WG5H0/VCgTfjdgb6Of9wFnhHB7QogIpZQV6zEuhY+lA2C4hL1AulLqZqXUGqXUmsLCwnCHI4QQ7VYoE/5+oHHtyu7+55rQWs/XWo/UWo/s1KlTCMMRQojYFsqE/yVwqlKqt1LKAfwIeDOE2xNCCHEUIevD11p7lFI/B5ZjxkUt0Fp/HartCSGEOLqQjsPXWi8Dlh1zQSGEECEX9pO2Qggh2oYkfCGEiBGS8IUQIkZE1B2vlFKFwO4TfHlHoKgVw4lUsbKfEDv7Giv7CbGzr225n7201i0a0x5RCf9kKKXWtPQ2X9EsVvYTYmdfY2U/IXb2NVL3U7p0hBAiRkjCF0KIGNGeEv78cAfQRmJlPyF29jVW9hNiZ18jcj/bTR++EEKIo2tPLXwhhBBHEfUJXyk1USn1jVLqW6XUrHDH05qUUguUUgVKqc2NnuuglHpfKbXdP08PZ4ytQSnVQym1Qim1RSn1tVJqpv/59rivTqXUF0qpjf59fdj/fG+l1Of+z/Er/oKDUU8pZVVKrVdKve3/ub3u5y6l1Cal1Aal1Br/cxH3+Y3qhN/oNooXAgOAq5VSA8IbVat6Hph4yHOzgA+11qcCH/p/jnYe4Jda6wHAGGCG/+/YHvfVBZyntR4CDAUmKqXGAI8Df9RanwKUAj8NY4ytaSawtdHP7XU/Ac7VWg9tNBwz4j6/UZ3waXQbRa21GwjcRrFd0FrnAiWHPP1DYKH/8ULgsjYNKgS01ge11uv8jysxCaIb7XNftda6yv+j3T9p4Dxgif/5drGvSqnuwMXA3/0/K9rhfh5FxH1+oz3hN3cbxW5hiqWtdNZaH/Q/zgNCf+ftNqSUygaGAZ/TTvfV382xASgA3gd2AGVaa49/kfbyOZ4D/Arw+X/OoH3uJ5iD9n+UUmuVUjf7n4u4z29IyyOL0NJaa6VUuxlmpZRKAl4DbtdaVzTcdLp97as2d9UeqpRKA5YCp4c5pFanlLoEKNBar1VKnRPueNrAmVrr/UqpTOB9pdS2xr+MlM9vtLfwW3QbxXYmXymVBeCfF4Q5nlahlLJjkv1LWuvX/U+3y30N0FqXASuA7wFpSqlAA6w9fI7HApOUUrswXa3nAX+i/e0nAFrr/f55AeYgPpoI/PxGe8KPxdsovglc7398PfBGGGNpFf6+3X8AW7XWf2j0q/a4r538LXuUUvHABMw5ixXAlf7Fon5ftdb3aK27a62zMf+XH2mtp9HO9hNAKZWolEoOPAbOBzYTgZ/fqL/wSil1EaavMHAbxUfCHFKrUUotBs7BVN7LBx4C/g28CvTEVBa9Smt96IndqKKUOhP4GNhEQ3/vvZh+/Pa2r4MxJ/CsmAbXq1rr3yil+mBawh2A9cC1WmtX+CJtPf4unbu01pe0x/3079NS/482YJHW+hGlVAYR9vmN+oQvhBCiZaK9S0cIIUQLScIXQogYIQlfCCFihCR8IYSIEZLwhRAiRkjCF6IVKKXOCVSEFCJSScIXQogYIQlfxBSl1LX+evQblFLP+AuZVSml/uivT/+hUqqTf9mhSqnVSqmvlFJLA/XMlVKnKKU+8Ne0X6eU6utffZJSaolSaptS6iXVuBiQEBFAEr6IGUqp/sBUYKzWeijgBaYBicAarfVAYBXmimaAF4Bfa60HY64CDjz/EvC0v6b994FARcRhwO2YezP0wdSTESJiSLVMEUvGAyOAL/2N73hMQSsf8Ip/mReB15VSqUCa1nqV//mFwL/8NVO6aa2XAmit6wD86/tCa73P//MGIBv4JPS7JUTLSMIXsUQBC7XW9zR5UqkHDlnuROuNNK4J40X+v0SEkS4dEUs+BK701ywP3HO0F+b/IFDB8RrgE611OVCqlDrL//yPgVX+O3LtU0pd5l9HnFIqoU33QogTJC0QETO01luUUvdj7kxkAeqBGUA1MNr/uwJMPz+Ykrbz/An9O+BG//M/Bp5RSv3Gv44pbbgbQpwwqZYpYp5SqkprnRTuOIQINenSEUKIGCEtfCGEiBHSwhdCiBghCV8IIWKEJHwhhIgRkvCFECJGSMIXQogYIQlfCCFixP8HbQDpwTIpx7QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 484us/sample - loss: 1.9901 - acc: 0.3761\n",
      "Loss: 1.9900841282659354 Accuracy: 0.3761163\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2963 - acc: 0.2737\n",
      "Epoch 00001: val_loss improved from inf to 2.04367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv_checkpoint/001-2.0437.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.2962 - acc: 0.2737 - val_loss: 2.0437 - val_acc: 0.3823\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8893 - acc: 0.4253\n",
      "Epoch 00002: val_loss improved from 2.04367 to 1.81009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv_checkpoint/002-1.8101.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.8891 - acc: 0.4253 - val_loss: 1.8101 - val_acc: 0.4302\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5981 - acc: 0.5186\n",
      "Epoch 00003: val_loss improved from 1.81009 to 1.69023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv_checkpoint/003-1.6902.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.5980 - acc: 0.5186 - val_loss: 1.6902 - val_acc: 0.4750\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4005 - acc: 0.5780\n",
      "Epoch 00004: val_loss improved from 1.69023 to 1.64045, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv_checkpoint/004-1.6404.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.4004 - acc: 0.5780 - val_loss: 1.6404 - val_acc: 0.4931\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2460 - acc: 0.6239\n",
      "Epoch 00005: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.2459 - acc: 0.6239 - val_loss: 1.6960 - val_acc: 0.4789\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1206 - acc: 0.6623\n",
      "Epoch 00006: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.1205 - acc: 0.6623 - val_loss: 1.7010 - val_acc: 0.4948\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0049 - acc: 0.6977\n",
      "Epoch 00007: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.0048 - acc: 0.6978 - val_loss: 1.6923 - val_acc: 0.5034\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7262\n",
      "Epoch 00008: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.9027 - acc: 0.7262 - val_loss: 1.7568 - val_acc: 0.5048\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8153 - acc: 0.7487\n",
      "Epoch 00009: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.8154 - acc: 0.7487 - val_loss: 1.8041 - val_acc: 0.4894\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7292 - acc: 0.7764\n",
      "Epoch 00010: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.7294 - acc: 0.7764 - val_loss: 1.8412 - val_acc: 0.4990\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.7960\n",
      "Epoch 00011: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.6562 - acc: 0.7960 - val_loss: 1.8881 - val_acc: 0.5001\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5881 - acc: 0.8149\n",
      "Epoch 00012: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5884 - acc: 0.8149 - val_loss: 1.9150 - val_acc: 0.5127\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.8297\n",
      "Epoch 00013: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5413 - acc: 0.8297 - val_loss: 1.9732 - val_acc: 0.5085\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.8472\n",
      "Epoch 00014: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4845 - acc: 0.8472 - val_loss: 2.0279 - val_acc: 0.5076\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8642\n",
      "Epoch 00015: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4347 - acc: 0.8642 - val_loss: 2.0637 - val_acc: 0.5020\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8731\n",
      "Epoch 00016: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3977 - acc: 0.8731 - val_loss: 2.1489 - val_acc: 0.5127\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8853\n",
      "Epoch 00017: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3642 - acc: 0.8853 - val_loss: 2.1897 - val_acc: 0.5055\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8954\n",
      "Epoch 00018: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3390 - acc: 0.8954 - val_loss: 2.2734 - val_acc: 0.5080\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9005\n",
      "Epoch 00019: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3142 - acc: 0.9006 - val_loss: 2.2745 - val_acc: 0.5104\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9104\n",
      "Epoch 00020: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2894 - acc: 0.9104 - val_loss: 2.3685 - val_acc: 0.4966\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9162\n",
      "Epoch 00021: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2688 - acc: 0.9162 - val_loss: 2.3763 - val_acc: 0.5099\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9199\n",
      "Epoch 00022: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2549 - acc: 0.9200 - val_loss: 2.4729 - val_acc: 0.5087\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9269\n",
      "Epoch 00023: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 0.2343 - acc: 0.9269 - val_loss: 2.4758 - val_acc: 0.5143\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9318\n",
      "Epoch 00024: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2243 - acc: 0.9318 - val_loss: 2.5169 - val_acc: 0.5043\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9348\n",
      "Epoch 00025: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2122 - acc: 0.9347 - val_loss: 2.5256 - val_acc: 0.5097\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9398\n",
      "Epoch 00026: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2013 - acc: 0.9397 - val_loss: 2.5590 - val_acc: 0.5167\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9414\n",
      "Epoch 00027: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1950 - acc: 0.9414 - val_loss: 2.5985 - val_acc: 0.5183\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9449\n",
      "Epoch 00028: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1818 - acc: 0.9448 - val_loss: 2.5150 - val_acc: 0.5190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9465\n",
      "Epoch 00029: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1750 - acc: 0.9466 - val_loss: 2.6729 - val_acc: 0.5155\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9493\n",
      "Epoch 00030: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1668 - acc: 0.9494 - val_loss: 2.6255 - val_acc: 0.5174\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9520\n",
      "Epoch 00031: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1565 - acc: 0.9520 - val_loss: 2.6832 - val_acc: 0.5239\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9543\n",
      "Epoch 00032: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1526 - acc: 0.9544 - val_loss: 2.6407 - val_acc: 0.5215\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9571\n",
      "Epoch 00033: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1471 - acc: 0.9570 - val_loss: 2.7087 - val_acc: 0.5208\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9580\n",
      "Epoch 00034: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1437 - acc: 0.9580 - val_loss: 2.7228 - val_acc: 0.5220\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9582\n",
      "Epoch 00035: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1451 - acc: 0.9582 - val_loss: 2.6902 - val_acc: 0.5304\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9603\n",
      "Epoch 00036: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1379 - acc: 0.9603 - val_loss: 2.7467 - val_acc: 0.5250\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9609\n",
      "Epoch 00037: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1317 - acc: 0.9609 - val_loss: 2.7957 - val_acc: 0.5236\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9648\n",
      "Epoch 00038: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1216 - acc: 0.9648 - val_loss: 2.7943 - val_acc: 0.5241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9632\n",
      "Epoch 00039: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1243 - acc: 0.9632 - val_loss: 2.8202 - val_acc: 0.5211\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9651\n",
      "Epoch 00040: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1193 - acc: 0.9651 - val_loss: 2.8297 - val_acc: 0.5215\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9668\n",
      "Epoch 00041: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1167 - acc: 0.9668 - val_loss: 2.8004 - val_acc: 0.5292\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9661\n",
      "Epoch 00042: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1197 - acc: 0.9661 - val_loss: 2.8134 - val_acc: 0.5278\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9688\n",
      "Epoch 00043: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1102 - acc: 0.9688 - val_loss: 2.9052 - val_acc: 0.5222\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9681\n",
      "Epoch 00044: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1099 - acc: 0.9681 - val_loss: 2.8643 - val_acc: 0.5260\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9695\n",
      "Epoch 00045: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1048 - acc: 0.9694 - val_loss: 2.8710 - val_acc: 0.5344\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9685\n",
      "Epoch 00046: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1082 - acc: 0.9685 - val_loss: 2.8361 - val_acc: 0.5222\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9716\n",
      "Epoch 00047: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1026 - acc: 0.9716 - val_loss: 2.8799 - val_acc: 0.5227\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9713\n",
      "Epoch 00048: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1004 - acc: 0.9713 - val_loss: 2.8499 - val_acc: 0.5327\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9712\n",
      "Epoch 00049: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1027 - acc: 0.9713 - val_loss: 2.9538 - val_acc: 0.5269\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9712\n",
      "Epoch 00050: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0987 - acc: 0.9712 - val_loss: 2.8816 - val_acc: 0.5318\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9723\n",
      "Epoch 00051: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0962 - acc: 0.9723 - val_loss: 2.8990 - val_acc: 0.5255\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9737\n",
      "Epoch 00052: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0914 - acc: 0.9737 - val_loss: 2.9143 - val_acc: 0.5379\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9740\n",
      "Epoch 00053: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0911 - acc: 0.9740 - val_loss: 3.0716 - val_acc: 0.5197\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9740\n",
      "Epoch 00054: val_loss did not improve from 1.64045\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0936 - acc: 0.9740 - val_loss: 2.9852 - val_acc: 0.5313\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 779us/sample - loss: 1.7036 - acc: 0.4652\n",
      "Loss: 1.7035639770553368 Accuracy: 0.46521288\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3208 - acc: 0.2594\n",
      "Epoch 00001: val_loss improved from inf to 1.92375, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/001-1.9237.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.3209 - acc: 0.2594 - val_loss: 1.9237 - val_acc: 0.4235\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7208 - acc: 0.4714\n",
      "Epoch 00002: val_loss improved from 1.92375 to 1.49081, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/002-1.4908.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.7208 - acc: 0.4714 - val_loss: 1.4908 - val_acc: 0.5341\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4245 - acc: 0.5644\n",
      "Epoch 00003: val_loss improved from 1.49081 to 1.33591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/003-1.3359.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.4244 - acc: 0.5644 - val_loss: 1.3359 - val_acc: 0.5935\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2489 - acc: 0.6208\n",
      "Epoch 00004: val_loss improved from 1.33591 to 1.27723, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/004-1.2772.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.2489 - acc: 0.6208 - val_loss: 1.2772 - val_acc: 0.6161\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1160 - acc: 0.6592\n",
      "Epoch 00005: val_loss improved from 1.27723 to 1.24761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/005-1.2476.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1160 - acc: 0.6592 - val_loss: 1.2476 - val_acc: 0.6310\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0065 - acc: 0.6922\n",
      "Epoch 00006: val_loss improved from 1.24761 to 1.22716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/006-1.2272.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0065 - acc: 0.6922 - val_loss: 1.2272 - val_acc: 0.6406\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9079 - acc: 0.7211\n",
      "Epoch 00007: val_loss improved from 1.22716 to 1.18870, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv_checkpoint/007-1.1887.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9080 - acc: 0.7211 - val_loss: 1.1887 - val_acc: 0.6513\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8319 - acc: 0.7445\n",
      "Epoch 00008: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8318 - acc: 0.7446 - val_loss: 1.1912 - val_acc: 0.6478\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7669 - acc: 0.7605\n",
      "Epoch 00009: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7669 - acc: 0.7604 - val_loss: 1.2331 - val_acc: 0.6466\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.7807\n",
      "Epoch 00010: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7015 - acc: 0.7807 - val_loss: 1.2127 - val_acc: 0.6532\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.7961\n",
      "Epoch 00011: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6508 - acc: 0.7961 - val_loss: 1.2362 - val_acc: 0.6525\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8128\n",
      "Epoch 00012: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5939 - acc: 0.8127 - val_loss: 1.2419 - val_acc: 0.6534\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8234\n",
      "Epoch 00013: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5509 - acc: 0.8234 - val_loss: 1.2259 - val_acc: 0.6702\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8358\n",
      "Epoch 00014: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5187 - acc: 0.8358 - val_loss: 1.2608 - val_acc: 0.6636\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8475\n",
      "Epoch 00015: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4780 - acc: 0.8475 - val_loss: 1.2926 - val_acc: 0.6597\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8603\n",
      "Epoch 00016: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4388 - acc: 0.8603 - val_loss: 1.2797 - val_acc: 0.6662\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8656\n",
      "Epoch 00017: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4115 - acc: 0.8656 - val_loss: 1.3340 - val_acc: 0.6620\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8744\n",
      "Epoch 00018: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3875 - acc: 0.8744 - val_loss: 1.3241 - val_acc: 0.6646\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8820\n",
      "Epoch 00019: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3681 - acc: 0.8820 - val_loss: 1.2938 - val_acc: 0.6781\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8894\n",
      "Epoch 00020: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3480 - acc: 0.8894 - val_loss: 1.3252 - val_acc: 0.6802\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8913\n",
      "Epoch 00021: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3308 - acc: 0.8913 - val_loss: 1.3060 - val_acc: 0.6846\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8966\n",
      "Epoch 00022: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3135 - acc: 0.8966 - val_loss: 1.3371 - val_acc: 0.6830\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9036\n",
      "Epoch 00023: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2976 - acc: 0.9036 - val_loss: 1.3884 - val_acc: 0.6774\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9085\n",
      "Epoch 00024: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2842 - acc: 0.9085 - val_loss: 1.4085 - val_acc: 0.6720\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9121\n",
      "Epoch 00025: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2708 - acc: 0.9121 - val_loss: 1.3570 - val_acc: 0.6886\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9161\n",
      "Epoch 00026: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2608 - acc: 0.9161 - val_loss: 1.3998 - val_acc: 0.6820\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9209\n",
      "Epoch 00027: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2464 - acc: 0.9209 - val_loss: 1.3834 - val_acc: 0.6883\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9238\n",
      "Epoch 00028: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2376 - acc: 0.9238 - val_loss: 1.4334 - val_acc: 0.6830\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9253\n",
      "Epoch 00029: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2327 - acc: 0.9253 - val_loss: 1.4122 - val_acc: 0.6925\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9315\n",
      "Epoch 00030: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2179 - acc: 0.9315 - val_loss: 1.3956 - val_acc: 0.6953\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9305\n",
      "Epoch 00031: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2167 - acc: 0.9305 - val_loss: 1.4065 - val_acc: 0.6942\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9329\n",
      "Epoch 00032: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2117 - acc: 0.9329 - val_loss: 1.4753 - val_acc: 0.6844\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9373\n",
      "Epoch 00033: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1971 - acc: 0.9373 - val_loss: 1.4709 - val_acc: 0.6949\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9361\n",
      "Epoch 00034: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1992 - acc: 0.9361 - val_loss: 1.4300 - val_acc: 0.6962\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9384\n",
      "Epoch 00035: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1942 - acc: 0.9384 - val_loss: 1.4722 - val_acc: 0.6879\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9417\n",
      "Epoch 00036: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1827 - acc: 0.9417 - val_loss: 1.4790 - val_acc: 0.6962\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9437\n",
      "Epoch 00037: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1786 - acc: 0.9437 - val_loss: 1.4778 - val_acc: 0.6939\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9449\n",
      "Epoch 00038: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1721 - acc: 0.9449 - val_loss: 1.5110 - val_acc: 0.6893\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9453\n",
      "Epoch 00039: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1749 - acc: 0.9453 - val_loss: 1.4850 - val_acc: 0.6953\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9463\n",
      "Epoch 00040: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1686 - acc: 0.9463 - val_loss: 1.5311 - val_acc: 0.6839\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9478\n",
      "Epoch 00041: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1667 - acc: 0.9478 - val_loss: 1.4410 - val_acc: 0.7025\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9485\n",
      "Epoch 00042: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1619 - acc: 0.9485 - val_loss: 1.4776 - val_acc: 0.7028\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9500\n",
      "Epoch 00043: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1576 - acc: 0.9500 - val_loss: 1.4914 - val_acc: 0.7002\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9529\n",
      "Epoch 00044: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1504 - acc: 0.9529 - val_loss: 1.5457 - val_acc: 0.6939\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9521\n",
      "Epoch 00045: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1531 - acc: 0.9520 - val_loss: 1.5079 - val_acc: 0.7044\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9522\n",
      "Epoch 00046: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1540 - acc: 0.9522 - val_loss: 1.4749 - val_acc: 0.7049\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9529\n",
      "Epoch 00047: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1490 - acc: 0.9529 - val_loss: 1.4556 - val_acc: 0.7135\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9559\n",
      "Epoch 00048: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1428 - acc: 0.9559 - val_loss: 1.5349 - val_acc: 0.7100\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9568\n",
      "Epoch 00049: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1370 - acc: 0.9568 - val_loss: 1.5254 - val_acc: 0.7167\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9584\n",
      "Epoch 00050: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1370 - acc: 0.9584 - val_loss: 1.5935 - val_acc: 0.7039\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9564\n",
      "Epoch 00051: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1413 - acc: 0.9563 - val_loss: 1.4742 - val_acc: 0.7172\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9586\n",
      "Epoch 00052: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1329 - acc: 0.9586 - val_loss: 1.5371 - val_acc: 0.7058\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9585\n",
      "Epoch 00053: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1329 - acc: 0.9585 - val_loss: 1.5194 - val_acc: 0.7107\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9606\n",
      "Epoch 00054: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1303 - acc: 0.9606 - val_loss: 1.5279 - val_acc: 0.7102\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9606\n",
      "Epoch 00055: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1270 - acc: 0.9606 - val_loss: 1.4896 - val_acc: 0.7112\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9630\n",
      "Epoch 00056: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1211 - acc: 0.9630 - val_loss: 1.5360 - val_acc: 0.7116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9620\n",
      "Epoch 00057: val_loss did not improve from 1.18870\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1223 - acc: 0.9620 - val_loss: 1.4949 - val_acc: 0.7100\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 875us/sample - loss: 1.2764 - acc: 0.6183\n",
      "Loss: 1.2764445521254653 Accuracy: 0.61827624\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2644 - acc: 0.2636\n",
      "Epoch 00001: val_loss improved from inf to 1.60072, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/001-1.6007.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.2643 - acc: 0.2636 - val_loss: 1.6007 - val_acc: 0.5041\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5548 - acc: 0.4996\n",
      "Epoch 00002: val_loss improved from 1.60072 to 1.34620, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/002-1.3462.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.5548 - acc: 0.4996 - val_loss: 1.3462 - val_acc: 0.5768\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3400 - acc: 0.5753\n",
      "Epoch 00003: val_loss improved from 1.34620 to 1.29198, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/003-1.2920.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.3399 - acc: 0.5754 - val_loss: 1.2920 - val_acc: 0.6005\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1884 - acc: 0.6273\n",
      "Epoch 00004: val_loss improved from 1.29198 to 1.11688, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/004-1.1169.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1885 - acc: 0.6272 - val_loss: 1.1169 - val_acc: 0.6550\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0790 - acc: 0.6648\n",
      "Epoch 00005: val_loss improved from 1.11688 to 1.03627, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/005-1.0363.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0789 - acc: 0.6648 - val_loss: 1.0363 - val_acc: 0.6834\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9887 - acc: 0.6939\n",
      "Epoch 00006: val_loss improved from 1.03627 to 0.96875, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/006-0.9688.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9888 - acc: 0.6938 - val_loss: 0.9688 - val_acc: 0.7037\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9143 - acc: 0.7188\n",
      "Epoch 00007: val_loss did not improve from 0.96875\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9144 - acc: 0.7188 - val_loss: 1.0246 - val_acc: 0.6902\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8463 - acc: 0.7403\n",
      "Epoch 00008: val_loss improved from 0.96875 to 0.89808, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/008-0.8981.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8463 - acc: 0.7403 - val_loss: 0.8981 - val_acc: 0.7275\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7936 - acc: 0.7548\n",
      "Epoch 00009: val_loss did not improve from 0.89808\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7935 - acc: 0.7548 - val_loss: 0.9019 - val_acc: 0.7291\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7492 - acc: 0.7660\n",
      "Epoch 00010: val_loss improved from 0.89808 to 0.84353, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/010-0.8435.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7492 - acc: 0.7660 - val_loss: 0.8435 - val_acc: 0.7461\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7011 - acc: 0.7829\n",
      "Epoch 00011: val_loss did not improve from 0.84353\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7011 - acc: 0.7829 - val_loss: 0.8550 - val_acc: 0.7552\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.7954\n",
      "Epoch 00012: val_loss improved from 0.84353 to 0.83255, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/012-0.8326.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6581 - acc: 0.7954 - val_loss: 0.8326 - val_acc: 0.7508\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.8048\n",
      "Epoch 00013: val_loss improved from 0.83255 to 0.81972, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/013-0.8197.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6253 - acc: 0.8048 - val_loss: 0.8197 - val_acc: 0.7633\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.8109\n",
      "Epoch 00014: val_loss did not improve from 0.81972\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5934 - acc: 0.8109 - val_loss: 0.8414 - val_acc: 0.7575\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8233\n",
      "Epoch 00015: val_loss did not improve from 0.81972\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5581 - acc: 0.8233 - val_loss: 0.8335 - val_acc: 0.7631\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8331\n",
      "Epoch 00016: val_loss improved from 0.81972 to 0.80288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/016-0.8029.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5338 - acc: 0.8331 - val_loss: 0.8029 - val_acc: 0.7761\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8397\n",
      "Epoch 00017: val_loss improved from 0.80288 to 0.78139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/017-0.7814.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5065 - acc: 0.8397 - val_loss: 0.7814 - val_acc: 0.7785\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8424\n",
      "Epoch 00018: val_loss did not improve from 0.78139\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4912 - acc: 0.8424 - val_loss: 0.8091 - val_acc: 0.7682\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.8489\n",
      "Epoch 00019: val_loss did not improve from 0.78139\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4711 - acc: 0.8489 - val_loss: 0.8360 - val_acc: 0.7608\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.8557\n",
      "Epoch 00020: val_loss did not improve from 0.78139\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4485 - acc: 0.8557 - val_loss: 0.7839 - val_acc: 0.7727\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8621\n",
      "Epoch 00021: val_loss did not improve from 0.78139\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4308 - acc: 0.8621 - val_loss: 0.7862 - val_acc: 0.7831\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8642\n",
      "Epoch 00022: val_loss improved from 0.78139 to 0.78036, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv_checkpoint/022-0.7804.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4193 - acc: 0.8642 - val_loss: 0.7804 - val_acc: 0.7820\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8717\n",
      "Epoch 00023: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3941 - acc: 0.8716 - val_loss: 0.8050 - val_acc: 0.7768\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8711\n",
      "Epoch 00024: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3890 - acc: 0.8712 - val_loss: 0.8159 - val_acc: 0.7857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8755\n",
      "Epoch 00025: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3780 - acc: 0.8755 - val_loss: 0.8090 - val_acc: 0.7831\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8806\n",
      "Epoch 00026: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3607 - acc: 0.8806 - val_loss: 0.8470 - val_acc: 0.7780\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8843\n",
      "Epoch 00027: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3537 - acc: 0.8843 - val_loss: 0.9135 - val_acc: 0.7619\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8865\n",
      "Epoch 00028: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3431 - acc: 0.8866 - val_loss: 0.8135 - val_acc: 0.7859\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8926\n",
      "Epoch 00029: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3369 - acc: 0.8927 - val_loss: 0.8404 - val_acc: 0.7899\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8911\n",
      "Epoch 00030: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3284 - acc: 0.8911 - val_loss: 0.8387 - val_acc: 0.7866\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8948\n",
      "Epoch 00031: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3149 - acc: 0.8948 - val_loss: 0.8232 - val_acc: 0.7920\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8955\n",
      "Epoch 00032: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3131 - acc: 0.8955 - val_loss: 0.8445 - val_acc: 0.7901\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.8986\n",
      "Epoch 00033: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3053 - acc: 0.8987 - val_loss: 0.8212 - val_acc: 0.7962\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9045\n",
      "Epoch 00034: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2903 - acc: 0.9045 - val_loss: 0.8583 - val_acc: 0.7922\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9027\n",
      "Epoch 00035: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2928 - acc: 0.9027 - val_loss: 0.8710 - val_acc: 0.7822\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9032\n",
      "Epoch 00036: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2847 - acc: 0.9032 - val_loss: 0.8700 - val_acc: 0.7911\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9075\n",
      "Epoch 00037: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2776 - acc: 0.9075 - val_loss: 0.8655 - val_acc: 0.7962\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9108\n",
      "Epoch 00038: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2732 - acc: 0.9108 - val_loss: 0.8283 - val_acc: 0.7959\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9111\n",
      "Epoch 00039: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2701 - acc: 0.9111 - val_loss: 0.8584 - val_acc: 0.7925\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9131\n",
      "Epoch 00040: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2599 - acc: 0.9131 - val_loss: 0.8639 - val_acc: 0.7950\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9145\n",
      "Epoch 00041: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2592 - acc: 0.9145 - val_loss: 0.8543 - val_acc: 0.7992\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9155\n",
      "Epoch 00042: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2563 - acc: 0.9155 - val_loss: 0.8247 - val_acc: 0.7908\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9176\n",
      "Epoch 00043: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2488 - acc: 0.9176 - val_loss: 0.8795 - val_acc: 0.7906\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9188\n",
      "Epoch 00044: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2431 - acc: 0.9188 - val_loss: 0.8553 - val_acc: 0.7948\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9199\n",
      "Epoch 00045: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2426 - acc: 0.9199 - val_loss: 0.8377 - val_acc: 0.7994\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9193\n",
      "Epoch 00046: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2406 - acc: 0.9193 - val_loss: 0.8782 - val_acc: 0.7934\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9218\n",
      "Epoch 00047: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2373 - acc: 0.9218 - val_loss: 0.8369 - val_acc: 0.8008\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9223\n",
      "Epoch 00048: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2328 - acc: 0.9222 - val_loss: 0.8560 - val_acc: 0.8036\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9228\n",
      "Epoch 00049: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2306 - acc: 0.9228 - val_loss: 0.8227 - val_acc: 0.8006\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9252\n",
      "Epoch 00050: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2256 - acc: 0.9252 - val_loss: 0.8234 - val_acc: 0.7999\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9220\n",
      "Epoch 00051: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2307 - acc: 0.9220 - val_loss: 0.8475 - val_acc: 0.8043\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9241\n",
      "Epoch 00052: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2241 - acc: 0.9241 - val_loss: 0.8368 - val_acc: 0.8020\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9249\n",
      "Epoch 00053: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2286 - acc: 0.9249 - val_loss: 0.8281 - val_acc: 0.8076\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9266\n",
      "Epoch 00054: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2197 - acc: 0.9266 - val_loss: 0.8199 - val_acc: 0.8055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9307\n",
      "Epoch 00055: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2095 - acc: 0.9306 - val_loss: 0.8505 - val_acc: 0.8078\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9319\n",
      "Epoch 00056: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2113 - acc: 0.9319 - val_loss: 0.8325 - val_acc: 0.8041\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9297\n",
      "Epoch 00057: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2101 - acc: 0.9297 - val_loss: 0.8891 - val_acc: 0.8099\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9296\n",
      "Epoch 00058: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2102 - acc: 0.9296 - val_loss: 0.8408 - val_acc: 0.8085\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9320\n",
      "Epoch 00059: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2061 - acc: 0.9320 - val_loss: 0.8312 - val_acc: 0.8120\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9333\n",
      "Epoch 00060: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2008 - acc: 0.9333 - val_loss: 0.8764 - val_acc: 0.7990\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9325\n",
      "Epoch 00061: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2060 - acc: 0.9325 - val_loss: 0.8579 - val_acc: 0.8032\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9332\n",
      "Epoch 00062: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2018 - acc: 0.9332 - val_loss: 0.8157 - val_acc: 0.8116\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9360\n",
      "Epoch 00063: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1966 - acc: 0.9360 - val_loss: 0.8407 - val_acc: 0.8092\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9362\n",
      "Epoch 00064: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1959 - acc: 0.9362 - val_loss: 0.8398 - val_acc: 0.8148\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9355\n",
      "Epoch 00065: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1957 - acc: 0.9354 - val_loss: 0.8772 - val_acc: 0.8123\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9360\n",
      "Epoch 00066: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1931 - acc: 0.9360 - val_loss: 0.8467 - val_acc: 0.8160\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9356\n",
      "Epoch 00067: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1967 - acc: 0.9356 - val_loss: 0.9038 - val_acc: 0.8092\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9364\n",
      "Epoch 00068: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1884 - acc: 0.9364 - val_loss: 0.8842 - val_acc: 0.8074\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9374\n",
      "Epoch 00069: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1845 - acc: 0.9375 - val_loss: 0.8577 - val_acc: 0.8150\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9372\n",
      "Epoch 00070: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1860 - acc: 0.9372 - val_loss: 0.8864 - val_acc: 0.8046\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9396\n",
      "Epoch 00071: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1866 - acc: 0.9396 - val_loss: 0.8160 - val_acc: 0.8153\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9404\n",
      "Epoch 00072: val_loss did not improve from 0.78036\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1793 - acc: 0.9404 - val_loss: 0.8643 - val_acc: 0.8130\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 912us/sample - loss: 0.8678 - acc: 0.7526\n",
      "Loss: 0.8677615195667508 Accuracy: 0.752648\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3058 - acc: 0.2465\n",
      "Epoch 00001: val_loss improved from inf to 1.64343, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/001-1.6434.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 2.3057 - acc: 0.2464 - val_loss: 1.6434 - val_acc: 0.4780\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5702 - acc: 0.4797\n",
      "Epoch 00002: val_loss improved from 1.64343 to 1.42025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/002-1.4202.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5704 - acc: 0.4797 - val_loss: 1.4202 - val_acc: 0.5497\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3770 - acc: 0.5520\n",
      "Epoch 00003: val_loss improved from 1.42025 to 1.30373, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/003-1.3037.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3772 - acc: 0.5520 - val_loss: 1.3037 - val_acc: 0.5798\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2480 - acc: 0.6016\n",
      "Epoch 00004: val_loss improved from 1.30373 to 1.06367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/004-1.0637.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2479 - acc: 0.6016 - val_loss: 1.0637 - val_acc: 0.6667\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1203 - acc: 0.6497\n",
      "Epoch 00005: val_loss improved from 1.06367 to 0.98750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/005-0.9875.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1202 - acc: 0.6497 - val_loss: 0.9875 - val_acc: 0.7011\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0308 - acc: 0.6812\n",
      "Epoch 00006: val_loss improved from 0.98750 to 0.92659, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/006-0.9266.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0307 - acc: 0.6812 - val_loss: 0.9266 - val_acc: 0.7207\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9312 - acc: 0.7106\n",
      "Epoch 00007: val_loss improved from 0.92659 to 0.83591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/007-0.8359.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9312 - acc: 0.7106 - val_loss: 0.8359 - val_acc: 0.7494\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7355\n",
      "Epoch 00008: val_loss improved from 0.83591 to 0.76297, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/008-0.7630.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8651 - acc: 0.7355 - val_loss: 0.7630 - val_acc: 0.7738\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.7533\n",
      "Epoch 00009: val_loss did not improve from 0.76297\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8025 - acc: 0.7533 - val_loss: 0.8493 - val_acc: 0.7510\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7519 - acc: 0.7734\n",
      "Epoch 00010: val_loss improved from 0.76297 to 0.70624, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/010-0.7062.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7519 - acc: 0.7734 - val_loss: 0.7062 - val_acc: 0.8046\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7084 - acc: 0.7847\n",
      "Epoch 00011: val_loss improved from 0.70624 to 0.70596, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/011-0.7060.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7085 - acc: 0.7846 - val_loss: 0.7060 - val_acc: 0.8020\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6718 - acc: 0.7988\n",
      "Epoch 00012: val_loss did not improve from 0.70596\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6718 - acc: 0.7988 - val_loss: 0.7248 - val_acc: 0.7838\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.8051\n",
      "Epoch 00013: val_loss improved from 0.70596 to 0.65789, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/013-0.6579.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6416 - acc: 0.8051 - val_loss: 0.6579 - val_acc: 0.8071\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.8178\n",
      "Epoch 00014: val_loss improved from 0.65789 to 0.63073, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/014-0.6307.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6096 - acc: 0.8178 - val_loss: 0.6307 - val_acc: 0.8234\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8270\n",
      "Epoch 00015: val_loss improved from 0.63073 to 0.61911, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/015-0.6191.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5797 - acc: 0.8270 - val_loss: 0.6191 - val_acc: 0.8265\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8303\n",
      "Epoch 00016: val_loss did not improve from 0.61911\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5580 - acc: 0.8302 - val_loss: 0.6198 - val_acc: 0.8213\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8402\n",
      "Epoch 00017: val_loss improved from 0.61911 to 0.59809, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/017-0.5981.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5277 - acc: 0.8402 - val_loss: 0.5981 - val_acc: 0.8279\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8454\n",
      "Epoch 00018: val_loss improved from 0.59809 to 0.56337, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/018-0.5634.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5142 - acc: 0.8454 - val_loss: 0.5634 - val_acc: 0.8465\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8554\n",
      "Epoch 00019: val_loss did not improve from 0.56337\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4876 - acc: 0.8554 - val_loss: 0.5685 - val_acc: 0.8463\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8557\n",
      "Epoch 00020: val_loss did not improve from 0.56337\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4672 - acc: 0.8557 - val_loss: 0.5661 - val_acc: 0.8481\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.8620\n",
      "Epoch 00021: val_loss did not improve from 0.56337\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4583 - acc: 0.8620 - val_loss: 0.5726 - val_acc: 0.8465\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8649\n",
      "Epoch 00022: val_loss improved from 0.56337 to 0.54691, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/022-0.5469.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4349 - acc: 0.8649 - val_loss: 0.5469 - val_acc: 0.8553\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8714\n",
      "Epoch 00023: val_loss improved from 0.54691 to 0.54084, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/023-0.5408.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4217 - acc: 0.8714 - val_loss: 0.5408 - val_acc: 0.8570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8767\n",
      "Epoch 00024: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4003 - acc: 0.8767 - val_loss: 0.5423 - val_acc: 0.8546\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8793\n",
      "Epoch 00025: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3863 - acc: 0.8793 - val_loss: 0.5758 - val_acc: 0.8493\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8852\n",
      "Epoch 00026: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3752 - acc: 0.8852 - val_loss: 0.5744 - val_acc: 0.8539\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8871\n",
      "Epoch 00027: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3631 - acc: 0.8871 - val_loss: 0.5437 - val_acc: 0.8472\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8918\n",
      "Epoch 00028: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3485 - acc: 0.8919 - val_loss: 0.5535 - val_acc: 0.8421\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8944\n",
      "Epoch 00029: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3344 - acc: 0.8944 - val_loss: 0.5520 - val_acc: 0.8565\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8970\n",
      "Epoch 00030: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3231 - acc: 0.8970 - val_loss: 0.5601 - val_acc: 0.8616\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9010\n",
      "Epoch 00031: val_loss did not improve from 0.54084\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3134 - acc: 0.9010 - val_loss: 0.5473 - val_acc: 0.8626\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9027\n",
      "Epoch 00032: val_loss improved from 0.54084 to 0.53761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/032-0.5376.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3101 - acc: 0.9027 - val_loss: 0.5376 - val_acc: 0.8595\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9045\n",
      "Epoch 00033: val_loss improved from 0.53761 to 0.53447, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/033-0.5345.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2999 - acc: 0.9045 - val_loss: 0.5345 - val_acc: 0.8598\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9081\n",
      "Epoch 00034: val_loss did not improve from 0.53447\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2913 - acc: 0.9081 - val_loss: 0.5362 - val_acc: 0.8633\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9109\n",
      "Epoch 00035: val_loss did not improve from 0.53447\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2815 - acc: 0.9109 - val_loss: 0.5611 - val_acc: 0.8647\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9120\n",
      "Epoch 00036: val_loss did not improve from 0.53447\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2763 - acc: 0.9120 - val_loss: 0.5510 - val_acc: 0.8609\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9149\n",
      "Epoch 00037: val_loss did not improve from 0.53447\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2684 - acc: 0.9149 - val_loss: 0.5358 - val_acc: 0.8691\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9162\n",
      "Epoch 00038: val_loss did not improve from 0.53447\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2617 - acc: 0.9163 - val_loss: 0.5555 - val_acc: 0.8654\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9173\n",
      "Epoch 00039: val_loss improved from 0.53447 to 0.52496, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv_checkpoint/039-0.5250.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2581 - acc: 0.9173 - val_loss: 0.5250 - val_acc: 0.8670\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9210\n",
      "Epoch 00040: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2512 - acc: 0.9210 - val_loss: 0.5315 - val_acc: 0.8668\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9215\n",
      "Epoch 00041: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2433 - acc: 0.9215 - val_loss: 0.5486 - val_acc: 0.8654\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9249\n",
      "Epoch 00042: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2351 - acc: 0.9250 - val_loss: 0.5731 - val_acc: 0.8614\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9234\n",
      "Epoch 00043: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2347 - acc: 0.9234 - val_loss: 0.5284 - val_acc: 0.8705\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9261\n",
      "Epoch 00044: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2298 - acc: 0.9261 - val_loss: 0.6010 - val_acc: 0.8549\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9251\n",
      "Epoch 00045: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2272 - acc: 0.9251 - val_loss: 0.5412 - val_acc: 0.8684\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9292\n",
      "Epoch 00046: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2217 - acc: 0.9292 - val_loss: 0.5493 - val_acc: 0.8682\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9298\n",
      "Epoch 00047: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2192 - acc: 0.9298 - val_loss: 0.5470 - val_acc: 0.8712\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9324\n",
      "Epoch 00048: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2054 - acc: 0.9324 - val_loss: 0.5408 - val_acc: 0.8782\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9341\n",
      "Epoch 00049: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2047 - acc: 0.9341 - val_loss: 0.5723 - val_acc: 0.8640\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9332\n",
      "Epoch 00050: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2048 - acc: 0.9332 - val_loss: 0.5468 - val_acc: 0.8698\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9362\n",
      "Epoch 00051: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1956 - acc: 0.9362 - val_loss: 0.5532 - val_acc: 0.8707\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9355\n",
      "Epoch 00052: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2013 - acc: 0.9355 - val_loss: 0.5583 - val_acc: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9362\n",
      "Epoch 00053: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1983 - acc: 0.9363 - val_loss: 0.5760 - val_acc: 0.8737\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9382\n",
      "Epoch 00054: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1891 - acc: 0.9382 - val_loss: 0.5293 - val_acc: 0.8786\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9383\n",
      "Epoch 00055: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1885 - acc: 0.9383 - val_loss: 0.5535 - val_acc: 0.8751\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9411\n",
      "Epoch 00056: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1809 - acc: 0.9411 - val_loss: 0.6009 - val_acc: 0.8730\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9417\n",
      "Epoch 00057: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1772 - acc: 0.9417 - val_loss: 0.5408 - val_acc: 0.8751\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9424\n",
      "Epoch 00058: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1812 - acc: 0.9424 - val_loss: 0.5331 - val_acc: 0.8719\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9432\n",
      "Epoch 00059: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1753 - acc: 0.9432 - val_loss: 0.5644 - val_acc: 0.8689\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9449\n",
      "Epoch 00060: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1708 - acc: 0.9449 - val_loss: 0.5741 - val_acc: 0.8733\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9451\n",
      "Epoch 00061: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1680 - acc: 0.9451 - val_loss: 0.5500 - val_acc: 0.8758\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9457\n",
      "Epoch 00062: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1676 - acc: 0.9457 - val_loss: 0.5843 - val_acc: 0.8633\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9360\n",
      "Epoch 00063: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1926 - acc: 0.9360 - val_loss: 0.5704 - val_acc: 0.8744\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9478\n",
      "Epoch 00064: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1642 - acc: 0.9478 - val_loss: 0.5721 - val_acc: 0.8791\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9490\n",
      "Epoch 00065: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1597 - acc: 0.9489 - val_loss: 0.5411 - val_acc: 0.8768\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9475\n",
      "Epoch 00066: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1603 - acc: 0.9475 - val_loss: 0.5613 - val_acc: 0.8730\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9493\n",
      "Epoch 00067: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1570 - acc: 0.9493 - val_loss: 0.5569 - val_acc: 0.8756\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9489\n",
      "Epoch 00068: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1539 - acc: 0.9489 - val_loss: 0.5578 - val_acc: 0.8705\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9485\n",
      "Epoch 00069: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1579 - acc: 0.9485 - val_loss: 0.5655 - val_acc: 0.8772\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9507\n",
      "Epoch 00070: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1562 - acc: 0.9507 - val_loss: 0.5345 - val_acc: 0.8831\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9506\n",
      "Epoch 00071: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1529 - acc: 0.9506 - val_loss: 0.5796 - val_acc: 0.8758\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9535\n",
      "Epoch 00072: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1469 - acc: 0.9535 - val_loss: 0.5685 - val_acc: 0.8814\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9514\n",
      "Epoch 00073: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1494 - acc: 0.9514 - val_loss: 0.5924 - val_acc: 0.8775\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9529\n",
      "Epoch 00074: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1450 - acc: 0.9529 - val_loss: 0.5306 - val_acc: 0.8793\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9546\n",
      "Epoch 00075: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1405 - acc: 0.9547 - val_loss: 0.5635 - val_acc: 0.8800\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9551\n",
      "Epoch 00076: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1405 - acc: 0.9551 - val_loss: 0.5622 - val_acc: 0.8784\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9562\n",
      "Epoch 00077: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1405 - acc: 0.9562 - val_loss: 0.5787 - val_acc: 0.8717\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9538\n",
      "Epoch 00078: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1441 - acc: 0.9538 - val_loss: 0.5496 - val_acc: 0.8749\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9563\n",
      "Epoch 00079: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1378 - acc: 0.9562 - val_loss: 0.5762 - val_acc: 0.8777\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9571\n",
      "Epoch 00080: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1360 - acc: 0.9571 - val_loss: 0.5582 - val_acc: 0.8821\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9585\n",
      "Epoch 00081: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1278 - acc: 0.9585 - val_loss: 0.5708 - val_acc: 0.8763\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9533\n",
      "Epoch 00082: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1431 - acc: 0.9533 - val_loss: 0.5611 - val_acc: 0.8810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9596\n",
      "Epoch 00083: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1275 - acc: 0.9596 - val_loss: 0.5344 - val_acc: 0.8828\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9565\n",
      "Epoch 00084: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1331 - acc: 0.9566 - val_loss: 0.5342 - val_acc: 0.8884\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9570\n",
      "Epoch 00085: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1338 - acc: 0.9570 - val_loss: 0.5352 - val_acc: 0.8821\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9597\n",
      "Epoch 00086: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1263 - acc: 0.9597 - val_loss: 0.6142 - val_acc: 0.8742\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9584\n",
      "Epoch 00087: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1291 - acc: 0.9584 - val_loss: 0.5609 - val_acc: 0.8768\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9590\n",
      "Epoch 00088: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1265 - acc: 0.9590 - val_loss: 0.5370 - val_acc: 0.8819\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9617\n",
      "Epoch 00089: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1218 - acc: 0.9617 - val_loss: 0.5955 - val_acc: 0.8810\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 973us/sample - loss: 0.6535 - acc: 0.8226\n",
      "Loss: 0.6535081007275131 Accuracy: 0.8226376\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4320 - acc: 0.1985\n",
      "Epoch 00001: val_loss improved from inf to 1.68660, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/001-1.6866.hdf5\n",
      "36805/36805 [==============================] - 96s 3ms/sample - loss: 2.4319 - acc: 0.1985 - val_loss: 1.6866 - val_acc: 0.4582\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6466 - acc: 0.4561\n",
      "Epoch 00002: val_loss improved from 1.68660 to 1.39801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/002-1.3980.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.6465 - acc: 0.4561 - val_loss: 1.3980 - val_acc: 0.5821\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4406 - acc: 0.5289\n",
      "Epoch 00003: val_loss improved from 1.39801 to 1.27350, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/003-1.2735.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4406 - acc: 0.5289 - val_loss: 1.2735 - val_acc: 0.6131\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3302 - acc: 0.5667\n",
      "Epoch 00004: val_loss improved from 1.27350 to 1.16459, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/004-1.1646.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3302 - acc: 0.5667 - val_loss: 1.1646 - val_acc: 0.6352\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2385 - acc: 0.5990\n",
      "Epoch 00005: val_loss improved from 1.16459 to 1.06272, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/005-1.0627.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2384 - acc: 0.5990 - val_loss: 1.0627 - val_acc: 0.6706\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.6393\n",
      "Epoch 00006: val_loss improved from 1.06272 to 0.98552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/006-0.9855.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1320 - acc: 0.6393 - val_loss: 0.9855 - val_acc: 0.6988\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0239 - acc: 0.6791\n",
      "Epoch 00007: val_loss improved from 0.98552 to 0.84142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/007-0.8414.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0238 - acc: 0.6791 - val_loss: 0.8414 - val_acc: 0.7498\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9277 - acc: 0.7137\n",
      "Epoch 00008: val_loss improved from 0.84142 to 0.76603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/008-0.7660.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9276 - acc: 0.7137 - val_loss: 0.7660 - val_acc: 0.7743\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8512 - acc: 0.7408\n",
      "Epoch 00009: val_loss improved from 0.76603 to 0.69812, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/009-0.6981.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8511 - acc: 0.7409 - val_loss: 0.6981 - val_acc: 0.8032\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7626\n",
      "Epoch 00010: val_loss improved from 0.69812 to 0.66770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/010-0.6677.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7851 - acc: 0.7626 - val_loss: 0.6677 - val_acc: 0.8216\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7387 - acc: 0.7790\n",
      "Epoch 00011: val_loss improved from 0.66770 to 0.59041, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/011-0.5904.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7387 - acc: 0.7791 - val_loss: 0.5904 - val_acc: 0.8365\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6791 - acc: 0.7951\n",
      "Epoch 00012: val_loss did not improve from 0.59041\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6792 - acc: 0.7951 - val_loss: 0.6359 - val_acc: 0.8274\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.8080\n",
      "Epoch 00013: val_loss improved from 0.59041 to 0.53970, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/013-0.5397.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6479 - acc: 0.8080 - val_loss: 0.5397 - val_acc: 0.8435\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6168 - acc: 0.8169\n",
      "Epoch 00014: val_loss did not improve from 0.53970\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6168 - acc: 0.8169 - val_loss: 0.5835 - val_acc: 0.8360\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.8285\n",
      "Epoch 00015: val_loss improved from 0.53970 to 0.50104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/015-0.5010.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5769 - acc: 0.8284 - val_loss: 0.5010 - val_acc: 0.8574\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.8370\n",
      "Epoch 00016: val_loss did not improve from 0.50104\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5454 - acc: 0.8370 - val_loss: 0.5063 - val_acc: 0.8553\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8432\n",
      "Epoch 00017: val_loss improved from 0.50104 to 0.44142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/017-0.4414.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5251 - acc: 0.8432 - val_loss: 0.4414 - val_acc: 0.8765\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8526\n",
      "Epoch 00018: val_loss improved from 0.44142 to 0.41911, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/018-0.4191.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4919 - acc: 0.8527 - val_loss: 0.4191 - val_acc: 0.8840\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8597\n",
      "Epoch 00019: val_loss improved from 0.41911 to 0.41476, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/019-0.4148.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4715 - acc: 0.8597 - val_loss: 0.4148 - val_acc: 0.8838\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4591 - acc: 0.8635\n",
      "Epoch 00020: val_loss improved from 0.41476 to 0.38621, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/020-0.3862.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4592 - acc: 0.8634 - val_loss: 0.3862 - val_acc: 0.8910\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8702\n",
      "Epoch 00021: val_loss improved from 0.38621 to 0.37333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/021-0.3733.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4326 - acc: 0.8702 - val_loss: 0.3733 - val_acc: 0.9003\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8727\n",
      "Epoch 00022: val_loss did not improve from 0.37333\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4228 - acc: 0.8727 - val_loss: 0.3846 - val_acc: 0.8882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8768\n",
      "Epoch 00023: val_loss improved from 0.37333 to 0.36242, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/023-0.3624.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4116 - acc: 0.8768 - val_loss: 0.3624 - val_acc: 0.8968\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8835\n",
      "Epoch 00024: val_loss did not improve from 0.36242\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3886 - acc: 0.8835 - val_loss: 0.3759 - val_acc: 0.8968\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8865\n",
      "Epoch 00025: val_loss improved from 0.36242 to 0.35454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/025-0.3545.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3761 - acc: 0.8865 - val_loss: 0.3545 - val_acc: 0.9052\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8887\n",
      "Epoch 00026: val_loss improved from 0.35454 to 0.32692, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/026-0.3269.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3680 - acc: 0.8887 - val_loss: 0.3269 - val_acc: 0.9159\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8945\n",
      "Epoch 00027: val_loss did not improve from 0.32692\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3535 - acc: 0.8945 - val_loss: 0.3407 - val_acc: 0.9024\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8967\n",
      "Epoch 00028: val_loss improved from 0.32692 to 0.32465, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/028-0.3246.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3481 - acc: 0.8966 - val_loss: 0.3246 - val_acc: 0.9126\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8979\n",
      "Epoch 00029: val_loss did not improve from 0.32465\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3380 - acc: 0.8979 - val_loss: 0.3477 - val_acc: 0.9040\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9000\n",
      "Epoch 00030: val_loss improved from 0.32465 to 0.32249, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/030-0.3225.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3240 - acc: 0.9000 - val_loss: 0.3225 - val_acc: 0.9110\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9038\n",
      "Epoch 00031: val_loss improved from 0.32249 to 0.32207, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/031-0.3221.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3151 - acc: 0.9038 - val_loss: 0.3221 - val_acc: 0.9173\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9086\n",
      "Epoch 00032: val_loss improved from 0.32207 to 0.29367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/032-0.2937.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3012 - acc: 0.9087 - val_loss: 0.2937 - val_acc: 0.9194\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9093\n",
      "Epoch 00033: val_loss improved from 0.29367 to 0.28699, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/033-0.2870.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2976 - acc: 0.9093 - val_loss: 0.2870 - val_acc: 0.9231\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9111\n",
      "Epoch 00034: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2931 - acc: 0.9111 - val_loss: 0.3338 - val_acc: 0.9068\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9105\n",
      "Epoch 00035: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2879 - acc: 0.9105 - val_loss: 0.3082 - val_acc: 0.9180\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9138\n",
      "Epoch 00036: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2747 - acc: 0.9138 - val_loss: 0.3092 - val_acc: 0.9138\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9182\n",
      "Epoch 00037: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2608 - acc: 0.9182 - val_loss: 0.2963 - val_acc: 0.9201\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9203\n",
      "Epoch 00038: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2570 - acc: 0.9203 - val_loss: 0.3039 - val_acc: 0.9262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9199\n",
      "Epoch 00039: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2539 - acc: 0.9199 - val_loss: 0.3043 - val_acc: 0.9189\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9235\n",
      "Epoch 00040: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2511 - acc: 0.9235 - val_loss: 0.3012 - val_acc: 0.9215\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9244\n",
      "Epoch 00041: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2396 - acc: 0.9244 - val_loss: 0.3004 - val_acc: 0.9229\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9269\n",
      "Epoch 00042: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2319 - acc: 0.9269 - val_loss: 0.3253 - val_acc: 0.9243\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9278\n",
      "Epoch 00043: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2262 - acc: 0.9278 - val_loss: 0.2886 - val_acc: 0.9255\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9291\n",
      "Epoch 00044: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2260 - acc: 0.9291 - val_loss: 0.3012 - val_acc: 0.9241\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9304\n",
      "Epoch 00045: val_loss did not improve from 0.28699\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2188 - acc: 0.9304 - val_loss: 0.2879 - val_acc: 0.9287\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9305\n",
      "Epoch 00046: val_loss improved from 0.28699 to 0.28333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/046-0.2833.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2194 - acc: 0.9306 - val_loss: 0.2833 - val_acc: 0.9294\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9332\n",
      "Epoch 00047: val_loss did not improve from 0.28333\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2137 - acc: 0.9332 - val_loss: 0.2865 - val_acc: 0.9248\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9327\n",
      "Epoch 00048: val_loss did not improve from 0.28333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2104 - acc: 0.9326 - val_loss: 0.2838 - val_acc: 0.9322\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9342\n",
      "Epoch 00049: val_loss improved from 0.28333 to 0.28055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/049-0.2806.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2075 - acc: 0.9342 - val_loss: 0.2806 - val_acc: 0.9294\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9353\n",
      "Epoch 00050: val_loss did not improve from 0.28055\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2020 - acc: 0.9353 - val_loss: 0.3261 - val_acc: 0.9222\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9364\n",
      "Epoch 00051: val_loss improved from 0.28055 to 0.27129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/051-0.2713.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1971 - acc: 0.9364 - val_loss: 0.2713 - val_acc: 0.9315\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9396\n",
      "Epoch 00052: val_loss did not improve from 0.27129\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1908 - acc: 0.9396 - val_loss: 0.3134 - val_acc: 0.9273\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9403\n",
      "Epoch 00053: val_loss improved from 0.27129 to 0.26824, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/053-0.2682.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1861 - acc: 0.9403 - val_loss: 0.2682 - val_acc: 0.9336\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9408\n",
      "Epoch 00054: val_loss did not improve from 0.26824\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1834 - acc: 0.9408 - val_loss: 0.2945 - val_acc: 0.9290\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9422\n",
      "Epoch 00055: val_loss did not improve from 0.26824\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1804 - acc: 0.9422 - val_loss: 0.3088 - val_acc: 0.9245\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9414\n",
      "Epoch 00056: val_loss did not improve from 0.26824\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1807 - acc: 0.9414 - val_loss: 0.2858 - val_acc: 0.9315\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9412\n",
      "Epoch 00057: val_loss did not improve from 0.26824\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1823 - acc: 0.9412 - val_loss: 0.2793 - val_acc: 0.9343\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9452\n",
      "Epoch 00058: val_loss improved from 0.26824 to 0.25886, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv_checkpoint/058-0.2589.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1718 - acc: 0.9452 - val_loss: 0.2589 - val_acc: 0.9371\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9467\n",
      "Epoch 00059: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1648 - acc: 0.9467 - val_loss: 0.2784 - val_acc: 0.9373\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9457\n",
      "Epoch 00060: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1663 - acc: 0.9457 - val_loss: 0.2962 - val_acc: 0.9280\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9477\n",
      "Epoch 00061: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1630 - acc: 0.9477 - val_loss: 0.2655 - val_acc: 0.9362\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9452\n",
      "Epoch 00062: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1680 - acc: 0.9452 - val_loss: 0.2953 - val_acc: 0.9317\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9483\n",
      "Epoch 00063: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1610 - acc: 0.9483 - val_loss: 0.2764 - val_acc: 0.9343\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9493\n",
      "Epoch 00064: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1537 - acc: 0.9493 - val_loss: 0.2594 - val_acc: 0.9369\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9510\n",
      "Epoch 00065: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1514 - acc: 0.9510 - val_loss: 0.3233 - val_acc: 0.9208\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9496\n",
      "Epoch 00066: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1529 - acc: 0.9497 - val_loss: 0.2884 - val_acc: 0.9336\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9510\n",
      "Epoch 00067: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1500 - acc: 0.9510 - val_loss: 0.2868 - val_acc: 0.9338\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9522\n",
      "Epoch 00068: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1442 - acc: 0.9522 - val_loss: 0.2882 - val_acc: 0.9327\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9537\n",
      "Epoch 00069: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1436 - acc: 0.9537 - val_loss: 0.2829 - val_acc: 0.9357\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9545\n",
      "Epoch 00070: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1408 - acc: 0.9545 - val_loss: 0.2956 - val_acc: 0.9338\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9526\n",
      "Epoch 00071: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1440 - acc: 0.9525 - val_loss: 0.2819 - val_acc: 0.9348\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9525\n",
      "Epoch 00072: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1434 - acc: 0.9525 - val_loss: 0.3008 - val_acc: 0.9369\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9552\n",
      "Epoch 00073: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1328 - acc: 0.9553 - val_loss: 0.2673 - val_acc: 0.9387\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9557\n",
      "Epoch 00074: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1339 - acc: 0.9557 - val_loss: 0.2845 - val_acc: 0.9385\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9562\n",
      "Epoch 00075: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1352 - acc: 0.9562 - val_loss: 0.2843 - val_acc: 0.9352\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9582\n",
      "Epoch 00076: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1302 - acc: 0.9582 - val_loss: 0.2731 - val_acc: 0.9355\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9561\n",
      "Epoch 00077: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1316 - acc: 0.9561 - val_loss: 0.2795 - val_acc: 0.9315\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9585\n",
      "Epoch 00078: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1259 - acc: 0.9585 - val_loss: 0.2664 - val_acc: 0.9369\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9585\n",
      "Epoch 00079: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1237 - acc: 0.9585 - val_loss: 0.2878 - val_acc: 0.9331\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9604\n",
      "Epoch 00080: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1226 - acc: 0.9604 - val_loss: 0.2929 - val_acc: 0.9348\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9593\n",
      "Epoch 00081: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1215 - acc: 0.9593 - val_loss: 0.2913 - val_acc: 0.9311\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9583\n",
      "Epoch 00082: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1240 - acc: 0.9583 - val_loss: 0.2837 - val_acc: 0.9345\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9627\n",
      "Epoch 00083: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1119 - acc: 0.9627 - val_loss: 0.2763 - val_acc: 0.9348\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9626\n",
      "Epoch 00084: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1138 - acc: 0.9626 - val_loss: 0.2678 - val_acc: 0.9373\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9610\n",
      "Epoch 00085: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1173 - acc: 0.9610 - val_loss: 0.2935 - val_acc: 0.9376\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9626\n",
      "Epoch 00086: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1122 - acc: 0.9626 - val_loss: 0.2949 - val_acc: 0.9338\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9625\n",
      "Epoch 00087: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1113 - acc: 0.9625 - val_loss: 0.2843 - val_acc: 0.9385\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9613\n",
      "Epoch 00088: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1155 - acc: 0.9613 - val_loss: 0.2756 - val_acc: 0.9394\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9638\n",
      "Epoch 00089: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1131 - acc: 0.9638 - val_loss: 0.2833 - val_acc: 0.9376\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9650\n",
      "Epoch 00090: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1059 - acc: 0.9650 - val_loss: 0.3024 - val_acc: 0.9376\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9647\n",
      "Epoch 00091: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1065 - acc: 0.9647 - val_loss: 0.3145 - val_acc: 0.9352\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9618\n",
      "Epoch 00092: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1133 - acc: 0.9618 - val_loss: 0.3190 - val_acc: 0.9269\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9633\n",
      "Epoch 00093: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1114 - acc: 0.9633 - val_loss: 0.2653 - val_acc: 0.9373\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9647\n",
      "Epoch 00094: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1093 - acc: 0.9647 - val_loss: 0.2797 - val_acc: 0.9364\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9671\n",
      "Epoch 00095: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1011 - acc: 0.9671 - val_loss: 0.2776 - val_acc: 0.9390\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9656\n",
      "Epoch 00096: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1033 - acc: 0.9656 - val_loss: 0.2976 - val_acc: 0.9415\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9680\n",
      "Epoch 00097: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0972 - acc: 0.9679 - val_loss: 0.3156 - val_acc: 0.9380\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9662\n",
      "Epoch 00098: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1025 - acc: 0.9662 - val_loss: 0.2887 - val_acc: 0.9411\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9672\n",
      "Epoch 00099: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0967 - acc: 0.9672 - val_loss: 0.2767 - val_acc: 0.9408\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9678\n",
      "Epoch 00100: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0965 - acc: 0.9678 - val_loss: 0.2834 - val_acc: 0.9369\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9671\n",
      "Epoch 00101: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0980 - acc: 0.9672 - val_loss: 0.2770 - val_acc: 0.9364\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9688\n",
      "Epoch 00102: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0943 - acc: 0.9688 - val_loss: 0.3182 - val_acc: 0.9343\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9677\n",
      "Epoch 00103: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0998 - acc: 0.9677 - val_loss: 0.2749 - val_acc: 0.9376\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9688\n",
      "Epoch 00104: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0960 - acc: 0.9688 - val_loss: 0.2759 - val_acc: 0.9420\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9688\n",
      "Epoch 00105: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0923 - acc: 0.9688 - val_loss: 0.2849 - val_acc: 0.9394\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9693\n",
      "Epoch 00106: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0919 - acc: 0.9693 - val_loss: 0.2977 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9704\n",
      "Epoch 00107: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0893 - acc: 0.9704 - val_loss: 0.2818 - val_acc: 0.9399\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9694\n",
      "Epoch 00108: val_loss did not improve from 0.25886\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0915 - acc: 0.9694 - val_loss: 0.3284 - val_acc: 0.9362\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 991us/sample - loss: 0.3041 - acc: 0.9107\n",
      "Loss: 0.30411705678620937 Accuracy: 0.91069573\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6640 - acc: 0.1106\n",
      "Epoch 00001: val_loss improved from inf to 2.32180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/001-2.3218.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 2.6640 - acc: 0.1106 - val_loss: 2.3218 - val_acc: 0.2457\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9710 - acc: 0.3494\n",
      "Epoch 00002: val_loss improved from 2.32180 to 1.44424, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/002-1.4442.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.9710 - acc: 0.3494 - val_loss: 1.4442 - val_acc: 0.5646\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5322 - acc: 0.4900\n",
      "Epoch 00003: val_loss improved from 1.44424 to 1.23958, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/003-1.2396.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.5321 - acc: 0.4900 - val_loss: 1.2396 - val_acc: 0.6021\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3602 - acc: 0.5471\n",
      "Epoch 00004: val_loss improved from 1.23958 to 1.08115, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/004-1.0812.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.3601 - acc: 0.5472 - val_loss: 1.0812 - val_acc: 0.6515\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2205 - acc: 0.5957\n",
      "Epoch 00005: val_loss improved from 1.08115 to 1.07477, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/005-1.0748.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.2205 - acc: 0.5957 - val_loss: 1.0748 - val_acc: 0.6639\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1046 - acc: 0.6393\n",
      "Epoch 00006: val_loss improved from 1.07477 to 0.84427, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/006-0.8443.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.1047 - acc: 0.6393 - val_loss: 0.8443 - val_acc: 0.7582\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.6777\n",
      "Epoch 00007: val_loss improved from 0.84427 to 0.72807, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/007-0.7281.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9990 - acc: 0.6777 - val_loss: 0.7281 - val_acc: 0.7852\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9071 - acc: 0.7098\n",
      "Epoch 00008: val_loss improved from 0.72807 to 0.69968, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/008-0.6997.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9071 - acc: 0.7098 - val_loss: 0.6997 - val_acc: 0.7992\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8193 - acc: 0.7422\n",
      "Epoch 00009: val_loss improved from 0.69968 to 0.65269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/009-0.6527.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8192 - acc: 0.7422 - val_loss: 0.6527 - val_acc: 0.8067\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7626\n",
      "Epoch 00010: val_loss improved from 0.65269 to 0.51816, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/010-0.5182.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7496 - acc: 0.7626 - val_loss: 0.5182 - val_acc: 0.8530\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6755 - acc: 0.7852\n",
      "Epoch 00011: val_loss improved from 0.51816 to 0.46672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/011-0.4667.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6755 - acc: 0.7852 - val_loss: 0.4667 - val_acc: 0.8654\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6293 - acc: 0.8036\n",
      "Epoch 00012: val_loss improved from 0.46672 to 0.43598, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/012-0.4360.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6293 - acc: 0.8036 - val_loss: 0.4360 - val_acc: 0.8786\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.8176\n",
      "Epoch 00013: val_loss improved from 0.43598 to 0.40309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/013-0.4031.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5854 - acc: 0.8176 - val_loss: 0.4031 - val_acc: 0.8847\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8322\n",
      "Epoch 00014: val_loss improved from 0.40309 to 0.36201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/014-0.3620.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5380 - acc: 0.8322 - val_loss: 0.3620 - val_acc: 0.8996\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.8429\n",
      "Epoch 00015: val_loss improved from 0.36201 to 0.34370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/015-0.3437.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5015 - acc: 0.8429 - val_loss: 0.3437 - val_acc: 0.9087\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8580\n",
      "Epoch 00016: val_loss improved from 0.34370 to 0.31708, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/016-0.3171.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4632 - acc: 0.8580 - val_loss: 0.3171 - val_acc: 0.9068\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.8643\n",
      "Epoch 00017: val_loss improved from 0.31708 to 0.30065, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/017-0.3006.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4433 - acc: 0.8643 - val_loss: 0.3006 - val_acc: 0.9173\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8688\n",
      "Epoch 00018: val_loss improved from 0.30065 to 0.28380, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/018-0.2838.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4219 - acc: 0.8688 - val_loss: 0.2838 - val_acc: 0.9262\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8764\n",
      "Epoch 00019: val_loss did not improve from 0.28380\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3939 - acc: 0.8764 - val_loss: 0.2998 - val_acc: 0.9140\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8810\n",
      "Epoch 00020: val_loss improved from 0.28380 to 0.28333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/020-0.2833.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3814 - acc: 0.8810 - val_loss: 0.2833 - val_acc: 0.9217\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8892\n",
      "Epoch 00021: val_loss improved from 0.28333 to 0.23577, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/021-0.2358.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3529 - acc: 0.8893 - val_loss: 0.2358 - val_acc: 0.9350\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8923\n",
      "Epoch 00022: val_loss did not improve from 0.23577\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3466 - acc: 0.8923 - val_loss: 0.2466 - val_acc: 0.9304\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8981\n",
      "Epoch 00023: val_loss improved from 0.23577 to 0.23430, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/023-0.2343.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3269 - acc: 0.8981 - val_loss: 0.2343 - val_acc: 0.9324\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9026\n",
      "Epoch 00024: val_loss improved from 0.23430 to 0.21775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/024-0.2177.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3133 - acc: 0.9025 - val_loss: 0.2177 - val_acc: 0.9385\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9065\n",
      "Epoch 00025: val_loss did not improve from 0.21775\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3014 - acc: 0.9066 - val_loss: 0.2590 - val_acc: 0.9217\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9087\n",
      "Epoch 00026: val_loss improved from 0.21775 to 0.21322, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/026-0.2132.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2938 - acc: 0.9087 - val_loss: 0.2132 - val_acc: 0.9392\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9129\n",
      "Epoch 00027: val_loss improved from 0.21322 to 0.19460, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/027-0.1946.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2793 - acc: 0.9129 - val_loss: 0.1946 - val_acc: 0.9467\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9127\n",
      "Epoch 00028: val_loss improved from 0.19460 to 0.18308, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/028-0.1831.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2773 - acc: 0.9128 - val_loss: 0.1831 - val_acc: 0.9490\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9194\n",
      "Epoch 00029: val_loss did not improve from 0.18308\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2608 - acc: 0.9194 - val_loss: 0.1936 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9210\n",
      "Epoch 00030: val_loss did not improve from 0.18308\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2537 - acc: 0.9210 - val_loss: 0.1867 - val_acc: 0.9502\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9223\n",
      "Epoch 00031: val_loss improved from 0.18308 to 0.18135, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/031-0.1813.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2518 - acc: 0.9223 - val_loss: 0.1813 - val_acc: 0.9490\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9235\n",
      "Epoch 00032: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2399 - acc: 0.9235 - val_loss: 0.2003 - val_acc: 0.9394\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9270\n",
      "Epoch 00033: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2296 - acc: 0.9270 - val_loss: 0.1908 - val_acc: 0.9478\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9270\n",
      "Epoch 00034: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2301 - acc: 0.9270 - val_loss: 0.1851 - val_acc: 0.9432\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9283\n",
      "Epoch 00035: val_loss improved from 0.18135 to 0.17235, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/035-0.1723.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2253 - acc: 0.9284 - val_loss: 0.1723 - val_acc: 0.9520\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9321\n",
      "Epoch 00036: val_loss did not improve from 0.17235\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2167 - acc: 0.9321 - val_loss: 0.1733 - val_acc: 0.9518\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9335\n",
      "Epoch 00037: val_loss did not improve from 0.17235\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2070 - acc: 0.9335 - val_loss: 0.1738 - val_acc: 0.9492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9334\n",
      "Epoch 00038: val_loss did not improve from 0.17235\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2067 - acc: 0.9334 - val_loss: 0.1820 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9367\n",
      "Epoch 00039: val_loss did not improve from 0.17235\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1968 - acc: 0.9367 - val_loss: 0.1829 - val_acc: 0.9520\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9378\n",
      "Epoch 00040: val_loss did not improve from 0.17235\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1935 - acc: 0.9378 - val_loss: 0.1745 - val_acc: 0.9492\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9410\n",
      "Epoch 00041: val_loss improved from 0.17235 to 0.17114, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/041-0.1711.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1888 - acc: 0.9410 - val_loss: 0.1711 - val_acc: 0.9497\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9418\n",
      "Epoch 00042: val_loss improved from 0.17114 to 0.16669, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/042-0.1667.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1832 - acc: 0.9418 - val_loss: 0.1667 - val_acc: 0.9511\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9423\n",
      "Epoch 00043: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1812 - acc: 0.9423 - val_loss: 0.1751 - val_acc: 0.9529\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9426\n",
      "Epoch 00044: val_loss improved from 0.16669 to 0.15181, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/044-0.1518.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1794 - acc: 0.9426 - val_loss: 0.1518 - val_acc: 0.9571\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9449\n",
      "Epoch 00045: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1715 - acc: 0.9449 - val_loss: 0.1608 - val_acc: 0.9553\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9442\n",
      "Epoch 00046: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1686 - acc: 0.9442 - val_loss: 0.1656 - val_acc: 0.9578\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9465\n",
      "Epoch 00047: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1676 - acc: 0.9466 - val_loss: 0.1628 - val_acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9474\n",
      "Epoch 00048: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1642 - acc: 0.9474 - val_loss: 0.1774 - val_acc: 0.9522\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9486\n",
      "Epoch 00049: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1590 - acc: 0.9486 - val_loss: 0.1662 - val_acc: 0.9529\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9488\n",
      "Epoch 00050: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1576 - acc: 0.9488 - val_loss: 0.1571 - val_acc: 0.9590\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9492\n",
      "Epoch 00051: val_loss did not improve from 0.15181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1560 - acc: 0.9492 - val_loss: 0.1629 - val_acc: 0.9534\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9514\n",
      "Epoch 00052: val_loss improved from 0.15181 to 0.14442, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/052-0.1444.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1504 - acc: 0.9514 - val_loss: 0.1444 - val_acc: 0.9602\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9530\n",
      "Epoch 00053: val_loss did not improve from 0.14442\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1465 - acc: 0.9530 - val_loss: 0.1654 - val_acc: 0.9522\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9541\n",
      "Epoch 00054: val_loss did not improve from 0.14442\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1411 - acc: 0.9541 - val_loss: 0.1562 - val_acc: 0.9557\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9519\n",
      "Epoch 00055: val_loss did not improve from 0.14442\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1472 - acc: 0.9519 - val_loss: 0.1622 - val_acc: 0.9543\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9555\n",
      "Epoch 00056: val_loss did not improve from 0.14442\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1367 - acc: 0.9555 - val_loss: 0.1528 - val_acc: 0.9557\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9564\n",
      "Epoch 00057: val_loss did not improve from 0.14442\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1367 - acc: 0.9564 - val_loss: 0.1467 - val_acc: 0.9560\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9566\n",
      "Epoch 00058: val_loss improved from 0.14442 to 0.14146, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/058-0.1415.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1311 - acc: 0.9566 - val_loss: 0.1415 - val_acc: 0.9588\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9577\n",
      "Epoch 00059: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1300 - acc: 0.9578 - val_loss: 0.1548 - val_acc: 0.9599\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9590\n",
      "Epoch 00060: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1259 - acc: 0.9590 - val_loss: 0.1550 - val_acc: 0.9618\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9573\n",
      "Epoch 00061: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1292 - acc: 0.9573 - val_loss: 0.1634 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9608\n",
      "Epoch 00062: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1202 - acc: 0.9608 - val_loss: 0.1689 - val_acc: 0.9567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9595\n",
      "Epoch 00063: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1210 - acc: 0.9595 - val_loss: 0.1700 - val_acc: 0.9585\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9590\n",
      "Epoch 00064: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1213 - acc: 0.9590 - val_loss: 0.1583 - val_acc: 0.9576\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 00065: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1144 - acc: 0.9622 - val_loss: 0.1648 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9619\n",
      "Epoch 00066: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1155 - acc: 0.9619 - val_loss: 0.1473 - val_acc: 0.9583\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9628\n",
      "Epoch 00067: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1126 - acc: 0.9628 - val_loss: 0.1659 - val_acc: 0.9576\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9622\n",
      "Epoch 00068: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1135 - acc: 0.9622 - val_loss: 0.1726 - val_acc: 0.9546\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9636\n",
      "Epoch 00069: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1106 - acc: 0.9636 - val_loss: 0.1591 - val_acc: 0.9611\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9640\n",
      "Epoch 00070: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1050 - acc: 0.9641 - val_loss: 0.1523 - val_acc: 0.9599\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9636\n",
      "Epoch 00071: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1107 - acc: 0.9636 - val_loss: 0.1669 - val_acc: 0.9550\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9667\n",
      "Epoch 00072: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1016 - acc: 0.9667 - val_loss: 0.1675 - val_acc: 0.9604\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9667\n",
      "Epoch 00073: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1027 - acc: 0.9667 - val_loss: 0.1444 - val_acc: 0.9616\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9660\n",
      "Epoch 00074: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0998 - acc: 0.9660 - val_loss: 0.1554 - val_acc: 0.9595\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9668\n",
      "Epoch 00075: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1003 - acc: 0.9668 - val_loss: 0.1517 - val_acc: 0.9613\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9677\n",
      "Epoch 00076: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0983 - acc: 0.9677 - val_loss: 0.1501 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9690\n",
      "Epoch 00077: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0963 - acc: 0.9691 - val_loss: 0.1570 - val_acc: 0.9588\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9706\n",
      "Epoch 00078: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0899 - acc: 0.9706 - val_loss: 0.1565 - val_acc: 0.9597\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9695\n",
      "Epoch 00079: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0900 - acc: 0.9695 - val_loss: 0.1579 - val_acc: 0.9592\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9690\n",
      "Epoch 00080: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0924 - acc: 0.9691 - val_loss: 0.1718 - val_acc: 0.9606\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9693\n",
      "Epoch 00081: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0915 - acc: 0.9694 - val_loss: 0.1617 - val_acc: 0.9618\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9698\n",
      "Epoch 00082: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0915 - acc: 0.9698 - val_loss: 0.1519 - val_acc: 0.9634\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9727\n",
      "Epoch 00083: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0818 - acc: 0.9727 - val_loss: 0.1719 - val_acc: 0.9611\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9709\n",
      "Epoch 00084: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0867 - acc: 0.9709 - val_loss: 0.1533 - val_acc: 0.9641\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9721\n",
      "Epoch 00085: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0838 - acc: 0.9721 - val_loss: 0.1625 - val_acc: 0.9609\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9714\n",
      "Epoch 00086: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0837 - acc: 0.9714 - val_loss: 0.1948 - val_acc: 0.9560\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9720\n",
      "Epoch 00087: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0860 - acc: 0.9720 - val_loss: 0.1551 - val_acc: 0.9637\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9732\n",
      "Epoch 00088: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0822 - acc: 0.9732 - val_loss: 0.1610 - val_acc: 0.9581\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9721\n",
      "Epoch 00089: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0792 - acc: 0.9722 - val_loss: 0.1624 - val_acc: 0.9627\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9737\n",
      "Epoch 00090: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0807 - acc: 0.9737 - val_loss: 0.1695 - val_acc: 0.9602\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9729\n",
      "Epoch 00091: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0799 - acc: 0.9729 - val_loss: 0.1581 - val_acc: 0.9604\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9734\n",
      "Epoch 00092: val_loss did not improve from 0.14146\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0783 - acc: 0.9734 - val_loss: 0.1652 - val_acc: 0.9620\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9748\n",
      "Epoch 00093: val_loss improved from 0.14146 to 0.14096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv_checkpoint/093-0.1410.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0737 - acc: 0.9748 - val_loss: 0.1410 - val_acc: 0.9648\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9750\n",
      "Epoch 00094: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0761 - acc: 0.9750 - val_loss: 0.1466 - val_acc: 0.9632\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9751\n",
      "Epoch 00095: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0736 - acc: 0.9751 - val_loss: 0.1835 - val_acc: 0.9630\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9745\n",
      "Epoch 00096: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0740 - acc: 0.9745 - val_loss: 0.1710 - val_acc: 0.9592\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9763\n",
      "Epoch 00097: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0710 - acc: 0.9763 - val_loss: 0.1518 - val_acc: 0.9632\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9757\n",
      "Epoch 00098: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0735 - acc: 0.9757 - val_loss: 0.1650 - val_acc: 0.9599\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9753\n",
      "Epoch 00099: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0734 - acc: 0.9753 - val_loss: 0.1589 - val_acc: 0.9595\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9783\n",
      "Epoch 00100: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0669 - acc: 0.9783 - val_loss: 0.1643 - val_acc: 0.9611\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9775\n",
      "Epoch 00101: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0670 - acc: 0.9775 - val_loss: 0.1438 - val_acc: 0.9620\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9793\n",
      "Epoch 00102: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0622 - acc: 0.9793 - val_loss: 0.1610 - val_acc: 0.9667\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9768\n",
      "Epoch 00103: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0703 - acc: 0.9768 - val_loss: 0.1749 - val_acc: 0.9627\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9785\n",
      "Epoch 00104: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0662 - acc: 0.9785 - val_loss: 0.1547 - val_acc: 0.9651\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9789\n",
      "Epoch 00105: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0620 - acc: 0.9789 - val_loss: 0.1765 - val_acc: 0.9606\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9786\n",
      "Epoch 00106: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0638 - acc: 0.9786 - val_loss: 0.1741 - val_acc: 0.9639\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9793\n",
      "Epoch 00107: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0626 - acc: 0.9792 - val_loss: 0.1805 - val_acc: 0.9602\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9798\n",
      "Epoch 00108: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0627 - acc: 0.9798 - val_loss: 0.1703 - val_acc: 0.9618\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9803\n",
      "Epoch 00109: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0564 - acc: 0.9803 - val_loss: 0.1726 - val_acc: 0.9588\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9790\n",
      "Epoch 00110: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0615 - acc: 0.9790 - val_loss: 0.1566 - val_acc: 0.9634\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9800\n",
      "Epoch 00111: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0589 - acc: 0.9800 - val_loss: 0.1562 - val_acc: 0.9576\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9802\n",
      "Epoch 00112: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0605 - acc: 0.9802 - val_loss: 0.1756 - val_acc: 0.9520\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9805\n",
      "Epoch 00113: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0587 - acc: 0.9805 - val_loss: 0.1562 - val_acc: 0.9651\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9825\n",
      "Epoch 00114: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0511 - acc: 0.9825 - val_loss: 0.1856 - val_acc: 0.9618\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9789\n",
      "Epoch 00115: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0613 - acc: 0.9789 - val_loss: 0.1671 - val_acc: 0.9625\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9811\n",
      "Epoch 00116: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0571 - acc: 0.9811 - val_loss: 0.1609 - val_acc: 0.9613\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9812\n",
      "Epoch 00117: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0564 - acc: 0.9812 - val_loss: 0.1717 - val_acc: 0.9595\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9817\n",
      "Epoch 00118: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0566 - acc: 0.9816 - val_loss: 0.1910 - val_acc: 0.9585\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9810\n",
      "Epoch 00119: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0556 - acc: 0.9810 - val_loss: 0.1664 - val_acc: 0.9644\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9807\n",
      "Epoch 00120: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0558 - acc: 0.9807 - val_loss: 0.1733 - val_acc: 0.9616\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9821\n",
      "Epoch 00121: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0536 - acc: 0.9821 - val_loss: 0.1878 - val_acc: 0.9604\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9830\n",
      "Epoch 00122: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0527 - acc: 0.9829 - val_loss: 0.1684 - val_acc: 0.9604\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9805\n",
      "Epoch 00123: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0588 - acc: 0.9805 - val_loss: 0.1760 - val_acc: 0.9641\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9831\n",
      "Epoch 00124: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0517 - acc: 0.9831 - val_loss: 0.1711 - val_acc: 0.9630\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9837\n",
      "Epoch 00125: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0489 - acc: 0.9837 - val_loss: 0.1744 - val_acc: 0.9632\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9827\n",
      "Epoch 00126: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0542 - acc: 0.9827 - val_loss: 0.1772 - val_acc: 0.9632\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9820\n",
      "Epoch 00127: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0505 - acc: 0.9820 - val_loss: 0.1970 - val_acc: 0.9576\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9826\n",
      "Epoch 00128: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0495 - acc: 0.9826 - val_loss: 0.1829 - val_acc: 0.9588\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9839\n",
      "Epoch 00129: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0491 - acc: 0.9839 - val_loss: 0.1971 - val_acc: 0.9590\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9832\n",
      "Epoch 00130: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0499 - acc: 0.9832 - val_loss: 0.1571 - val_acc: 0.9648\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9849\n",
      "Epoch 00131: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0444 - acc: 0.9849 - val_loss: 0.1871 - val_acc: 0.9625\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9846\n",
      "Epoch 00132: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0455 - acc: 0.9846 - val_loss: 0.1778 - val_acc: 0.9606\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9832\n",
      "Epoch 00133: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0492 - acc: 0.9832 - val_loss: 0.1855 - val_acc: 0.9623\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9840\n",
      "Epoch 00134: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0475 - acc: 0.9840 - val_loss: 0.1939 - val_acc: 0.9571\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9829\n",
      "Epoch 00135: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0473 - acc: 0.9829 - val_loss: 0.1707 - val_acc: 0.9655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9847\n",
      "Epoch 00136: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0457 - acc: 0.9847 - val_loss: 0.1991 - val_acc: 0.9669\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9857\n",
      "Epoch 00137: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0453 - acc: 0.9857 - val_loss: 0.1868 - val_acc: 0.9646\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9850\n",
      "Epoch 00138: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0438 - acc: 0.9850 - val_loss: 0.1997 - val_acc: 0.9604\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9864\n",
      "Epoch 00139: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0408 - acc: 0.9864 - val_loss: 0.1874 - val_acc: 0.9634\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9855\n",
      "Epoch 00140: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0433 - acc: 0.9855 - val_loss: 0.1778 - val_acc: 0.9653\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9846\n",
      "Epoch 00141: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0458 - acc: 0.9846 - val_loss: 0.1920 - val_acc: 0.9623\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9851\n",
      "Epoch 00142: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0467 - acc: 0.9851 - val_loss: 0.1794 - val_acc: 0.9625\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9865\n",
      "Epoch 00143: val_loss did not improve from 0.14096\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0400 - acc: 0.9865 - val_loss: 0.2033 - val_acc: 0.9609\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmX2y7wkESNiFsCcgFhXrrlhcEa22rthau1itv1Jtra3aWrWtuxZbWmyxal1qrViqFYpWUQKCsu9LWEL2ZJLZ5/z+OJOQQBICZAgw7+d57jPbXd57k7nvnHPPOVdprRFCCCEALD0dgBBCiGOHJAUhhBAtJCkIIYRoIUlBCCFEC0kKQgghWkhSEEII0UKSghBCiBaSFIQQQrSQpCCEEKKFracDOFRZWVm6sLCwp8MQQojjytKlSyu11tkHm++4SwqFhYWUlpb2dBhCCHFcUUpt68p8Un0khBCihSQFIYQQLSQpCCGEaHHcXVNoTzAYpKysDJ/P19OhHLdcLhd9+vTBbrf3dChCiB50QiSFsrIykpOTKSwsRCnV0+Ecd7TWVFVVUVZWRv/+/Xs6HCFEDzohqo98Ph+ZmZmSEA6TUorMzEwpaQkhToykAEhCOEJy/IQQcAIlhYMJh734/TuJRII9HYoQQhyz4iYpRCI+AoHdaN39SaG2tpZnnnnmsJa98MILqa2t7fL89913H48++uhhbUsIIQ4mbpKCUmZXtY50+7o7SwqhUKjTZefNm0daWlq3xySEEIcjbpLCvl3t/qQwc+ZMNm3axJgxY7jrrrtYuHAhp512GlOnTmX48OEAXHLJJRQXF1NUVMSsWbNali0sLKSyspKtW7cybNgwZsyYQVFREeeeey5er7fT7S5fvpyJEycyatQoLr30UmpqagB44oknGD58OKNGjeKqq64C4L///S9jxoxhzJgxjB07loaGhm4/DkKI498J0SS1tQ0bbsfjWd7OJ2HC4SYsFjdKHdpuJyWNYfDgxzr8/KGHHmLlypUsX262u3DhQpYtW8bKlStbmnjOnj2bjIwMvF4v48eP5/LLLyczM3O/2Dfw17/+leeff54rr7yS1157jWuvvbbD7X7961/nySefZPLkydx777387Gc/47HHHuOhhx5iy5YtOJ3OlqqpRx99lKeffppJkybh8XhwuVyHdAyEEPEhjkoKR7d1zYQJE9q0+X/iiScYPXo0EydOZMeOHWzYsOGAZfr378+YMWMAKC4uZuvWrR2uv66ujtraWiZPngzAddddx6JFiwAYNWoU11xzDX/5y1+w2UwCnDRpEnfccQdPPPEEtbW1Le8LIURrJ9yZoaNf9JGIn8bGL3A6C3E4smIeR2JiYsvzhQsX8t577/Hxxx+TkJDAGWec0W6fAKfT2fLcarUetPqoI2+//TaLFi3irbfe4sEHH+SLL75g5syZTJkyhXnz5jFp0iTmz5/PSSeddFjrF0KcuOKopNC8q+FuX3NycnKndfR1dXWkp6eTkJDA2rVrWbx48RFvMzU1lfT0dD744AMA/vznPzN58mQikQg7duzgy1/+Mr/61a+oq6vD4/GwadMmRo4cyQ9/+EPGjx/P2rVrjzgGIcSJ54QrKXQklq2PMjMzmTRpEiNGjOCCCy5gypQpbT4///zzee655xg2bBhDhw5l4sSJ3bLdOXPm8M1vfpOmpiYGDBjAH//4R8LhMNdeey11dXVorfnud79LWloaP/nJT1iwYAEWi4WioiIuuOCCbolBCHFiUVrrno7hkJSUlOj9b7KzZs0ahg0b1ulyWms8nqU4HL1wOvNjGeJxqyvHUQhxfFJKLdValxxsvphVHyml+iqlFiilViulVimlvtfOPGcopeqUUsuj070xjAewxKSkIIQQJ4pYVh+FgDu11suUUsnAUqXUu1rr1fvN94HW+qIYxtHCVCFJUhBCiI7ErKSgtd6ttV4Wfd4ArAF6uN5GSgpCCNGZo9L6SClVCIwFPmnn41OUUiuUUu8opYpiG4eUFIQQojMxb32klEoCXgNu11rX7/fxMqBAa+1RSl0I/B0Y3M46bgFuAejXr98RRCMlBSGE6ExMSwpKKTsmIczVWr++/+da63qttSf6fB5gV0od0LNMaz1La12itS7Jzs4+gnisxKKfghBCnChi2fpIAX8A1mitf9PBPHnR+VBKTYjGUxWrmI6lkkJSUtIhvS+EEEdDLKuPJgFfA75QSjWPUHc30A9Aa/0ccAVwq1IqBHiBq3QMO04odewkBSGEOBbFsvXRh1prpbUepbUeE53maa2fiyYEtNZPaa2LtNajtdYTtdYfxSoeIzZJYebMmTz99NMtr5tvhOPxeDjrrLMYN24cI0eO5M033+zyOrXW3HXXXYwYMYKRI0fy8ssvA7B7925OP/10xowZw4gRI/jggw8Ih8Ncf/31LfP+9re/7fZ9FELEhxNvmIvbb4fl7Q2dDc6ID61DYD3EKpoxY+CxjofOnj59Orfffju33XYbAK+88grz58/H5XLxxhtvkJKSQmVlJRMnTmTq1Klduh/y66+/zvLly1mxYgWVlZWMHz+e008/nRdffJHzzjuPe+65h3A4TFNTE8uXL2fnzp2sXLkS4JDu5CaEEK2deEmhU4pY1E2NHTuWvXv3smvXLioqKkhPT6dv374Eg0HuvvtuFi1ahMViYefOnZSXl5OXl3fQdX744YdcffXVWK1WcnNzmTx5MkuWLGH8+PHceOONBINBLrnkEsaMGcOAAQPYvHkz3/nOd5gyZQrnnntuDPZSCBEPTryk0Mkv+qB/J4HAbpKSirv0a/1QTJs2jVdffZU9e/Ywffp0AObOnUtFRQVLly7FbrdTWFjY7pDZh+L0009n0aJFvP3221x//fXccccdfP3rX2fFihXMnz+f5557jldeeYXZs2d3x24JIeJMHA2dDbG8Jef06dN56aWXePXVV5k2bRpghszOycnBbrezYMECtm3b1uX1nXbaabz88suEw2EqKipYtGgREyZMYNu2beTm5jJjxgxuvvlmli1bRmVlJZFIhMsvv5wHHniAZcuWdfv+CSHiw4lXUuiE6adghs9uft5dioqKaGhoID8/n169egFwzTXX8JWvfIWRI0dSUlJySDe1ufTSS/n4448ZPXo0Sikefvhh8vLymDNnDo888gh2u52kpCReeOEFdu7cyQ033EAkYpLdL3/5y27dNyFE/IibobMBAoFK/P6tJCaOxGJxHnT+eCNDZwtx4urxobOPRbG80Y4QQpwI4icpNDRg3bwbFQQZFE8IIdoXP0khFMLS4EWFpaQghBAdiZ+kYInuqgYpKQghRPviJylE+yUoDVrLSKlCCNGe+EkKUlIQQoiDirukoCLdf02htraWZ5555rCWvfDCC2WsIiHEMSN+kkLzsBb66CaFUCjU6bLz5s0jLS2tW+MRQojDFT9JIYbVRzNnzmTTpk2MGTOGu+66i4ULF3LaaacxdepUhg8fDsAll1xCcXExRUVFzJo1q2XZwsJCKisr2bp1K8OGDWPGjBkUFRVx7rnn4vV6D9jWW2+9xcknn8zYsWM5++yzKS8vB8Dj8XDDDTcwcuRIRo0axWuvvQbAv/71L8aNG8fo0aM566yzunW/hRAnnhNumIsOR87WdvAMJeIA7I6WHNEVBxk5m4ceeoiVK1eyPLrhhQsXsmzZMlauXEn//v0BmD17NhkZGXi9XsaPH8/ll19OZmZmm/Vs2LCBv/71rzz//PNceeWVvPbaa1x77bVt5jn11FNZvHgxSil+//vf8/DDD/PrX/+a+++/n9TUVL744gsAampqqKioYMaMGSxatIj+/ftTXV3d9Z0WQsSlEy4pdKz1qKixH9pjwoQJLQkB4IknnuCNN94AYMeOHWzYsOGApNC/f3/GjBkDQHFxMVu3bj1gvWVlZUyfPp3du3cTCARatvHee+/x0ksvtcyXnp7OW2+9xemnn94yT0ZGRrfuoxDixHPCJYUOf9FHNCxbhz/bSiQnHbe7MKZxJCYmtjxfuHAh7733Hh9//DEJCQmcccYZ7Q6h7XTuG4/JarW2W330ne98hzvuuIOpU6eycOFC7rvvvpjEL4SIT/FzTaGln4ICurefQnJyMg0NDR1+XldXR3p6OgkJCaxdu5bFixcf9rbq6urIz88HYM6cOS3vn3POOW1uCVpTU8PEiRNZtGgRW7ZsAZDqIyHEQcVXUrBYop3XuvdCc2ZmJpMmTWLEiBHcddddB3x+/vnnEwqFGDZsGDNnzmTixImHva377ruPadOmUVxcTFZWVsv7P/7xj6mpqWHEiBGMHj2aBQsWkJ2dzaxZs7jssssYPXp0y81/hBCiI3E1dDaffUYwxUKwl4uEhKExivD4JUNnC3HikqGz2xOjkoIQQpwo4i4poBUyzIUQQrQvvpKCUlJSEEKITsRXUrBYZEA8IYToRFwmBRk6Wwgh2hdfSUEpVAQgwvHW6koIIY6G+EoKFgu0JIOeTQpJSUk9un0hhGhP/CWF6OUEudgshBAHillSUEr1VUotUEqtVkqtUkp9r515lFLqCaXURqXU50qpcbGKJ7pBVEtJofuSwsyZM9sMMXHffffx6KOP4vF4OOussxg3bhwjR47kzTffPOi6Ohpiu70hsDsaLlsIIQ5XLAfECwF3aq2XKaWSgaVKqXe11qtbzXMBMDg6nQw8G308bLf/63aW72lv7GzA54NQiPASjcWSiFJdy4lj8sbw2Pkdj509ffp0br/9dm677TYAXnnlFebPn4/L5eKNN94gJSWFyspKJk6cyNSpU1FKdbiu9obYjkQi7Q6B3d5w2UIIcSRilhS01ruB3dHnDUqpNUA+0DopXAy8oM1V38VKqTSlVK/ost2vk5PxkRg7dix79+5l165dVFRUkJ6eTt++fQkGg9x9990sWrQIi8XCzp07KS8vJy8vr8N1tTfEdkVFRbtDYLc3XLYQQhyJozJ0tlKqEBgLfLLfR/nAjlavy6LvtUkKSqlbgFsA+vXr1+m2OvtFT1kZem85nsEat3soNltyl+LvimnTpvHqq6+yZ8+eloHn5s6dS0VFBUuXLsVut1NYWNjukNnNujrEthBCxErMLzQrpZKA14Dbtdb1h7MOrfUsrXWJ1rokOzv7SIIx91WIQQe26dOn89JLL/Hqq68ybdo0wAxznZOTg91uZ8GCBWzbtq3TdXQ0xHZHQ2C3N1y2EEIciZgmBaWUHZMQ5mqtX29nlp1A31av+0Tfiw2Lxdx/LQZDXRQVFdHQ0EB+fj69evUC4JprrqG0tJSRI0fywgsvcNJJJ3W6jo6G2O5oCOz2hssWQogjEbOhs5W5mjoHqNZa397BPFOAbwMXYi4wP6G1ntDZeo9o6Ow9e6CsjIZB4EwoxOHIOvgycUSGzhbixNXVobNjeU1hEvA14AulVHNzoLuBfgBa6+eAeZiEsBFoAm6IYTymnwKgNJjGUUIIIVqLZeujD4FOm/tEWx3dFqsYDhBNCjL+kRBCtO+E6dHcpWqwlvs0WyUp7EfGghJCwAmSFFwuF1VVVQc/sbVUH1nQWqqPmmmtqaqqwuVy9XQoQogedlT6KcRanz59KCsro6KiovMZvV6orCSo7WhHLQ6H/+gEeBxwuVz06dOnp8MQQvSwEyIp2O32lt6+nVq4EC64gI2zxlJf7GbYsP/FPDYhhDienBDVR13mdgNgCyUQCklHLyGE2F98JYVonbk97CYUqu3hYIQQ4tgTl0nBFnJKSUEIIdoRl0nBGnQQifgIh2WwOSGEaC2+kkLzNYWgA0CqkIQQYj/xlRRaSgqm0ZVUIQkhRFvxmRQCVkBKCkIIsb/4Sgp2O1gsWAJmt6WkIIQQbcVXUlAKXC6sweakICUFIYRoLb6SAoDLhSU6uoWUFIQQoq34SwpuN5aAGThPSgpCCNFW/CUFlwvlD2CxJBAMSklBCCFai8ukgM+HzZYm1UdCCLGf+EwKXi82W7pUHwkhxH7iLym43eDzYbenS0lBCCH2E39JoU31kZQUhBCitfhMCi3VR1JSEEKI1uIvKUSrj6SkIIQQB4q/pNBSfZROKFSH1pGejkgIIY4ZcZ0UQBMK1fV0REIIccyIv6TgdkevKaQB0qtZCCFai7+kEC0p2O3pgIx/JIQQrcVtUrBZUwEpKQghRGvxmRQAWzgRkJKCEEK0FrOkoJSarZTaq5Ra2cHnZyil6pRSy6PTvbGKpY3m+zSHzKOUFIQQYh9bDNf9J+Ap4IVO5vlAa31RDGM4ULSkYA8nABAI7D2qmxdCiGNZzEoKWutFQHWs1n/Ymu/THLTgcPSmqWldDwckhBDHjp6+pnCKUmqFUuodpVTRUdlitPoIr5fExOE0Na0+KpsVQojjQU8mhWVAgdZ6NPAk8PeOZlRK3aKUKlVKlVZUVBzZVqMlBXw+EhKG09i4Wno1CyFEVI8lBa11vdbaE30+D7ArpbI6mHeW1rpEa12SnZ19ZBtulRQSE4uIRBrx+3cc2TqFEOIE0WNJQSmVp5RS0ecTorFUxXzDraqPEhKGA9DYuCrmmxVCiONBzFofKaX+CpwBZCmlyoCfAnYArfVzwBXArUqpEOAFrtJa61jF06JNSWEMAI2Nq8nMvDDmmxZCiGNdl5KCUup7wB+BBuD3wFhgptb63x0to7W+urN1aq2fwjRZPbrSzfAWVFRgt2fgcOTJxWYhhIjqavXRjVrreuBcIB34GvBQzKKKpYICsFhg0yaA6MVmqT4SQgjoelJQ0ccLgT9rrVe1eu/44nCYxBBNCs3NUo9GzZUQQhzrupoUliql/o1JCvOVUsnA8duOc+BA2LgRgISEIsJhD35/WQ8HJYQQPa+rSeEmYCYwXmvdhLlgfEPMooq1QYNakkJiorRAEkKIZl1NCqcA67TWtUqpa4EfA8fvLcsGDYLqaqipaWmWKhebhRCi60nhWaBJKTUauBPYROcD3R3bBg40j5s24XBkYbfn0Nj4Rc/GJIQQx4CuJoVQtA/BxcBTWuungeTYhRVjgwaZx+jF5uTkYhoalvZgQEIIcWzoalJoUEr9CNMU9W2llIVoR7Tj0oAB5jF6XSE5uYTGxlWEw009GJQQQvS8riaF6YAf019hD9AHeCRmUcVaQgL07t0mKUAEj2d5z8YlhBA9rEtJIZoI5gKpSqmLAJ/W+vi9pgDmukJL9VEJAA0NS3oyIiGE6HFdSgpKqSuBT4FpwJXAJ0qpK2IZWMy1apbqdPbG4ehNQ0NpDwclhBA9q6sD4t2D6aOwF0AplQ28B7waq8BibtAg2L0bGhshMZHk5BJJCkKIuNfVawqW5oQQVXUIyx6bmpulbt4MmCqkpqZ1hEL1PRiUEEL0rK6e2P+llJqvlLpeKXU98DYwL3ZhHQXNzVJbLjaPBzQez2c9F5MQQvSwrl5ovguYBYyKTrO01j+MZWAx15wUNmwATF8FQKqQhBBxrcs32dFavwa8FsNYjq7UVOjTB74wPZkdjmyczgLq66UFkhAifnWaFJRSDUB7Y0orQGutU2IS1dEyahR8/nnLy5SUidTVfYjWmuidQoUQIq50Wn2ktU7WWqe0MyUf9wkBTFJYswYCAQDS0iYTCOzE59vcw4EJIUTPOL5bEB2pUaMgGIR16wCTFABqa//bk1EJIUSPie+kMHKkeYxWISUkDMNuz5KkIISIW/GdFIYOBbu9JSkopUhNPV2SghAibsV3UrDbYfjwNheb09Im4/dvw+fb1oOBCSFEz4jvpAAHtECS6wpCiHgmSWHUKNi1CyorAUhMHInNli5JQQgRlyQpjBplHqOd2JSykJp6miQFIURckqTQnBQ+/hi06aeXnn4mPt8mvN6tPReXEEL0AEkKubmQnw/33AM5OfDb35Kefg4ANTXv9nBwQghxdElSUAr+9z/43e8gIwPmzCEhYRgOR29JCkKIuCNJAaCgAG65Bc45B7ZtQylFevo51NT8B63DPR2dEEIcNTFLCkqp2UqpvUqplR18rpRSTyilNiqlPldKjYtVLF1WWAi1tVBXR0bGOYRC1TQ0yP0VhBDxI5YlhT8B53fy+QXA4Oh0C/BsDGPpmoIC87htG+npZwNyXUEIEV9ilhS01ouA6k5muRh4QRuLgTSlVK9YxdMlhYXmcetWHI5cEhNHSVIQQsSVLt9kJwbygR2tXpdF39vdM+HQJikApKefw86dTxIKebDZknosLCHEiSMcBo8HIhFwOs1jQwP4/WCzHThZrdDUBPX1kJQE2dmxja8nk0KXKaVuwVQx0a9fv9htKCsL3G7YZsY9ysy8iLKyX1Nd/S9ycq6I3XaFOIimJmhsNDcMdDggFAKf78D5fD4zn1KQkGC63tTXg9dr3muewNxGxOczJyO/35ycdKtbajU/3/8xGIQ9e8yklDmxNTWZy3EOh2nl7Xab95qn5vU3byMSMfsQDO6b7HZz0rNaTVzNk9/f9nUgAHl55jdcKGQGI/B696239dTee4GAWWdysjnBBoNQXg51dea51uBy7Zuaj/f+8bb3Gsw+uFz73m9ej89nTv5e7+H/H8ycCb/85eEv3xU9mRR2An1bve4Tfe8AWutZmHtEU1JS0t6d4LqHUuY/LVpSSE09FZstk8rKNyQpHCX+kB+nzdnyeo9nDxnuDBxWR5eWj+gIjYFGGgIN1PsaqGqsp8rTQFZSKkV5g0i0pVJWZk5gTqfG6giSnGDHalXs2gU7d5ovd0RHUKg2d+ALBqHcU86upm3oQAL2cDoZtt6AomxXiPUVm2mK1BLUXlQ4AXswk7AO47fUQNiGw59PpDGd+sYADU1m8gb8WJ0BHK4AdrcfmzOAtgQI4SdsaSRorcPbaKGpMgsidkjeicVdT6SuN9QVQOVJEEiClDIo+C94M2DPWFBhyF4Njkbw5IE/GWw+M9m9YG+EpD2QWAFBN/jSwZsOvrR9z+1eSN8Eznqo7wOeXhB2gFZgbzLrtjeCswEyNuHovYZw0Eb4w0EQSDTrVhHs9UNwBHqhkneh3ZVYvb2xevphtYewuBtwRFJwBfMJqDrqnasJhyGpYRyJoQLcLoXLBZGMtdQVvo5bpZEUzmdHtYWlqwPYLU5S3UkkOJOwR5Kw6yRskSS0zUNjSik+92ac4WxcoVwc2oVV2cm0OXDYbFQH97DBvwXl8JIyxEahy4bNasOClVBIEQm4cNePxFF/Eg6bDZs9jLIHsDi8NLrXUO0uBauPREsmiSqTJGsmVu2izl+PPxAmRw0jxZLHNv0hWy3/IcFRQYbdg9ueQLo9mwxrP9Iig3GpZMLuvXhVBXWhvXgjHpJ0HonkocMWwmFNsiuRjMQUJo8uBHp375duPz2ZFP4BfFsp9RJwMlCnte65qqNmBQUtScFisZGVNZWKiteJRAJYLF07MfUEX8jHpupN7G3ciy/ko1dyL9JcaWyt3cqWmi3YLDaSHEmc3Odkeiebf6qd9TvxBDwMzBiIN+hl/qb5lO4qBcBmsZHqTCXdnU66K51UVyrlnnI21WzCoiz0Tu6N2+bGF/LhDXnxhXw0Bhqp9dVS0VTBxuqNbK3disvmItWVSoO/gb2Ne0lyJDMwdQg57nzclmQsOPAFAlR6K1hV+yl7/TvIcRTQ2zGM7d5VVId34NBJ9PafCcFkGizbiFi8uK1JKKx4Ag34dANhaz1hWwPa7gHVye+GoNucsABctWCN/rwL280JL2IDqx/s0Z/hIac50dbng7saMva7K58vBer6QcZGKGjnp3sMRPZ7nUQOHvYelW13JsGVRjgSpiHQ0Ob9YHQ6FA1ApjuTAb2LsSor72x8p+0M/fc9rTicYLuqF1iVFY0movc/8h3Y95sGi7IQ0RFsFhtZCVkkOZLYE2jki6YKQsHQvhn9By7ThsdMIc//cS6/Ouzd6YqYJQWl1F+BM4AspVQZ8FPADqC1fg6YB1wIbASagBtiFcshKSyEJUtaXmZlXcqePX+ktnYBGRnnHZUQPAEPqytWMyhjEBnuDCI6QlVTFUt3L+WL8i84e8DZjO01loiOMGf5HF5c+SIfbv8QX+jgJyWFYlK/STT4G1hRvgIAu8WOUopAOIDNYsOiLATDQXS7t+funCWUiMWfia1+IJa6C6giwHZbLRFfMtqTQ4Ozjt1Z6yDxY/ML0xowJ15/KuyaBFVD2Ju1jr1Za6DyFNh5O8GsDWwf9C4WSxhHYwEqmEsDHrTykWDNJMNeiCuUgiuYjNufTII1hQRbMkm2FJIcySTak6gP1lDWtBG/bS+unCaczggu0rGEEwiEQgQiAVwJflyJQdx2F06rG4BgxE9toJJybxkJ9n6U5H6LoVlDiFh81PgrWFu1mh0N2yjKO5fRuaPISsjCbXfTGGikyluFzWIjzZVGMBxkZ8NO6nx1OG1OHFZHy+S0tn3dPCU5kkhxphDRESqbKgmEA+Sn5JPsSGZXwy621m5lVcUqNtVsYkT2CM7sfyb1/no+2/MZdoudopwikh3JlDeW4wl4cNlcuGwu3DY3CfYEcpNyyUnMwRv0UuurpcZXQ423puXRaXMyKGMQKc4UyurL2OPZQygSIhwJk2BPIMmRRKIjkSRHEoVpheQm5gJQ0VSBN+glOzEbrTXrq9azx7OH3sm9yUrIYlfDLrbXbcdpc5LsSKbOX0dZfRlJjiSKsouI6AhLdy+ldFcpS3cvpbKpkntPv5dbx99KREfY1bCr5f82GAniCXgOmOwWO8W9ixmcMZgqbxV7G/fiD/kJhAMEI0GC4SDZidn0T+tPijOFUCREKBIiGAkSjpi+SfX+elaUr2BNxRosytLmbzMwYyAlvUtIdaZS5a2iqqmKKm8VvpCPVGcqGs3qitVsrd3KKX1O4awBZ5Hk2Hddsnk/1letpynYRG6i+VtkJ2bjsrmoaKygvLG8Zf7GQCP1/nr6pcaw+jxKaR272phYKCkp0aWlpbHbwK9+ZSru6ushOZlw2Mf//pdFbu61DB36XLdsQmtN6a5S3tn4Dn1T+nLViKtw291sr9vOH5b9gSc/fZIaXw0AKc4UGgONhFt1olMobhp7EysrVrK4bDHDsoZx7sBzOTn/ZHKTcnHZXOxq2M2OihrSVQHpagAej6a8roZY4c8+AAAgAElEQVSPKt7hk/o3sEdSGBCagsWby57QGvyBCKnlFxHZ9iW2brZRVaXB4QF3jflF7aqFxmyojf5ES9qDzeUjL9NNVpqLBIcLtz0Bt8OOy2XqmZsfO3re2Xsul6k/z8iAxMR99eBCiMOjlFqqtS452HzHxYXmo6pVXwVGjMBqdZGZeQFVVW+i9TModWiteBv8DSzdvZREeyJKKV5b/Rpzv5jLjvp9Da9+8O4PyE7IZl2VuVf0xUMv5qsjv8qOuh1srd1KijOF7MRsRueOZnDmYB796FGe+vQpUh0Z3D3sBU7yX8uGjxSLouXo6mr46CMoK2svovHAvQBsVeaiWEqKueimUiArA8YXQ79+irS0ZBITk7FYzK+T7Gzo2xcyMyEhoZDERLBIn3ghTiiSFPbX3Cw1mhTAVCFVVLxKff1iUlO/dNBVaK2Zv2k+Ty95mnc3vYs/vK/C0KqsnDfoPB448wGmDJ7CF3u/4NnSZ2nwN3BL8S1cNOQihmQOaZm/utqM6v35Enjxc/N8/frHCEe+T7UvjV/4UwFzcs7MNMskJsKkSTBhgjmRN5/0k5P3PU9JMa1T5KQuhGhNksL+mksK0YvNAJmZU1DKTmXlGx0mBa01m2s288nOT3iu9Dk+2P4B+cn53FpyK+cOPJewDtMUbGJywWRyk3Jbljuj8AzOKDwDrU2zuP8thIfehsWLzS/9hlbX7DIzzUjfV10FgwYVkJ9vWtH27g0DBphqFyGEOBKSFPaXm2vOrq2Sgs2WSlramVRUvMGAAQ+3aabYFGziT8v/xGOLH2ND9QYA8pLyeOqCp5hRPKPDppQ1NbB6NXzwAbz3HixdappJgqlLnzwZzj3XVNeMGGGSQV6e1K0LIWJLksL+LBZTWoh2YGuWnX0p69d/k8bGlSQljWSPZw9Pf/o0z5Q+Q7W3mgn5E3h2yrOc0ucUinKKsFnaHtraWvjjH00S+OQTcwfQZqNGwfTpUFQEY8fCxImmJ6MQQhxtcuppT6u+Cs0yMy8GbqWy8g2qw6mMfm40db46Lj7pYu485U4m9Z3UpgTRrLYWZs2Chx4ypYOBA+HMM2H0aDjpJCgpMSUAIYQ4FkhSaM+gQTB3rhmkxGoFwOnMIyXlFCor3+DBTz4jEA6w4psrGJk7st1VfPIJPP44vPGG6d5+wQXw4IOmJCCEEMcqaXvSnkmTTD+FFSvavJ2dfRn/3rqcv6/9O/eefm+7CeHTT+HCC00V0DvvwE03QWkpzJsnCUEIceyTpNCeyZPN46JFLW9VNFawsMLBExthcGo23z/l+20WWb4cpkyBk082ieGXv4QdO+Cpp6C4+GgGL4QQh0+qj9rTp49p4/nf//L51WfywKIHeHX1q2g0qXY7Px8KtmgntqYmuO8++PWvIS0NfvEL+Pa3TV8AIYQ43khS6ICefDo/KX+JB5/7O8mOZP5v0v9x6UmX0s9exro1V1Bd/TYVFRdz6aWwbh3MmAEPP2wSgxBCHK8kKbQjoiN8b8QOnirwcUPBJfx6+mzS3enms0gxWzbl88ILn/PTn15MQoLpZ3DWWT0ctBBCdANJCu34xQe/4KmG/3DnR/BI5lmoaEIA8Ptt/O53r/PCCxMYP97HG2+4yM/vwWCFEKIbyYXm/Wit+dPyP3F2/7N5ZE0fVKuLzbt2mfGEXnhhAtOm/ZY5cx6QhCCEOKFISWE/ayvXsqlmE3eecifq9P/Bu++C10t90M2FF5o+be+8A/n571NTs5RI5D4sFjmMQogTg5QU9vPW+rcAuGjIRXDttVBRQfArl3HFZWFWroRXX4Xzz4e8vJsIBHZTUzO/hyMWQojuIz9x9/OPdf9gTN4Y+qb2hfP7ov8wmxk3wrtYmT0rxHnnmUOWmTkFuz2H3bv/QGbmlB6OWgghuoeUFFqpbKrk47KPmTpkast7P91yPXO4nvv4KTdUPdryvsViJzf3a1RVvUUg0PP3xxVCiO4gSaGVeRvmEdERvjL0KwC88ALcfz/ceCPce/Hn5sX27S3z9+p1I1qH2LNnTk+FLIQQ3UqSAqbF0ftb3uc3H/+GXkm9GNdrHKtXw623whlnwHPPgXr8MdAabr+9ZbnExOGkp5/N9u2/IBAo73gDQghxnIj7pBCOhDnnz+dw1gtnsbNhJ4+c8wh+n4Xp081tLV98Eex2zHDaP/mJGfb0f/9rWX7QoCcJhxvZuPHOntsJIYToJnGfFGZ/Npv/bPkPvzjzF+z4/g6uGXUNd90FK1fCX/4CvXq1mvm228ytz95/v+WtxMST6NdvJnv3zqW6+r2jvwNCCNGN4jop1PnquOf9ezi136nMPHUmLpuL//4Xnn7a1BKde+5+C6SkwLBhZhjUVvr1uxuXayAbNnyLcNh39HZACCG6WVwnhfsX3U9lUyWPn/84SimamuDmm83d0R58sIOFJkwwSUHrlresVhdDhjyL17uB7dt/eXSCF0KIGIjbpPBJ2Sc8/snj3Dj2Rsb1GgfAT38KGzfC738PCQkdLDhhAuzd26YVEkBGxjnk5FzN9u0P0dS0LsbRCyFEbMRlUqj2VnPlq1eSn5zPw+c8DMDq1fDYY6akcMYZnSw8YYJ53K8KCWDgwN9gsbhZv/5WdKuShBBCHC/iLilorbnu79exu2E3f5v2NzLcGWgN3/seJCWZO6Z1auRIcDrbTQpOZx4DBjxEbe0Cysv/EpsdEEKIGIq7pLCifAX/XP9PHjzzQcbnjwfgzTfNPRHuvx+ysg6yAofD3Gy5naQA0Lv3LaSkTGTTpjsJBqu7OXohhIituEsKO+t3AnB6wekABINw550wYgR885tdXMmECVBaCqHQAR8pZWHIkOcIBqvZvPmH3RW2EEIcFTFNCkqp85VS65RSG5VSM9v5/HqlVIVSanl0ujmW8QCUN5qex7lJuYAZymLzZnjoIbB1dXjACRPMzZnXrGn346Sk0fTt+3127/49lZX/6I6whRDiqIhZUlBKWYGngQuA4cDVSqnh7cz6stZ6THT6faziaba30Qxel5OYQzBomp6WlMCFFx7CSk4+2Tx+8EGHsxQW3k9S0jjWrr0Or3fLEUQshBBHTyxLChOAjVrrzVrrAPAScHEMt9cl5Z5ykhxJJNgT+MtfYMsW0xRVqUNYycCBMGYMPPww+NrvrGa1uigq+htaa1atmkYk4u+eHRBCiBiKZVLIB3a0el0WfW9/lyulPldKvaqU6tveipRStyilSpVSpRUVFUcUVHljObmJuUQi8MADUFwMUw71dghKwSOPwLZt8NRTHc7mdg9g2LA5eDxL2bjxjiOKWwghjoaevtD8FlCotR4FvAu0Owa11nqW1rpEa12SnZ19RBssbywnNymXtWvNtYRvfesQSwnNzj4bLrjAZJaqqg5ny8q6mL59f8CuXc9QXv7S4QcuhBBHQSyTwk6g9S//PtH3Wmitq7TWzfUqvweKYxgPYKqPchNzWbrUvG6+PHBYHn4YGhrMQEmddFbr3/8XpKRMYt26m6mt/fAINiiEELEVy6SwBBislOqvlHIAVwFtmuIopVqPQToVaL85Tzdqrj4qLTVDWZx00hGsbMQIuPdeM5zqvfd2OJvFYqeo6GUcjjyWL5/M1q0/R+vwEWxYCCFiI2ZJQWsdAr4NzMec7F/RWq9SSv1cKdV8v8vvKqVWKaVWAN8Fro9VPAChSIiqpipyk0xSGDcOrNYjXOm995qxMR54wNyNpwNOZz4lJcvIybmarVt/yvLlZ+Lz7ehwfiGE6AldbZl/WLTW84B5+713b6vnPwJ+FMsYWqtsqkSjyXLn8tlncMst3bBSpeDZZ2HPHnO/hbw8uOSSdme12VIYPvwvZGScx4YN36K0dDTDhr1IZub53RCIEEIcuZ6+0HxUlXtMx7VgbQ5er+mf0C1sNnjpJRg/Hq6+us2d2dqTl/c1ios/w+UqYOXKqVRU/L2bAhFCiCMTX0kh2pu5YovpzdxtSQHMvTv/+U/o18+UFMo7v2dzQsIgxoxZSFLSOFavnsauXbOIRILdGJAQQhy6+EoK0ZLC9jW5JCXBkCHdvIGsLPj738HjMdcZDjJ8ts2WyujR/yYl5RTWr/8Gixf3Z+fOZ7o5KCGE6Lr4SgrRksLapbkUF4MlFns/bJgZSOmf/4Q//OGgs9tsKYwZs5CRI/+J2z2IDRtuY8uW+2IQmBBCHFx8JQVPOS6bi5VLk7u36mh/3/kOnHmmeXz00XZHU21NKQuZmVMYM+Z98vJuZNu2n7Fx4w9obFyL1pEYBiqEEG3FVVLY27SXDEcufp9i3LgYbshigb/+Fc49F+66C045BcrKDrqYUhaGDn2evLybKCv7NUuWDOOjj/IoL58rd3ITQhwVcZUUyj3lJJIDwODBMd5YTo65vvDyy7B+PZx6KmzYcNDFmhPD+PGrGTp0Nm73YNasuZbVq6+Sfg1CiJiLr6TQWI4zaFoe9et3FDaoFFx5JSxYAI2NcNpp8M47XVhMkZg4jF69bmDs2EX07/8LKivf4JNPBrJ27c00Nsa847cQIk7FV1LwlKOacnE6zQ/5o2bcOHPvhdRUc+OGr3wFNm7c9/nSpbBsWbuLKmWloOBHnHzyBnr1uoXy8r+wZMlwli8/k927/4DPt/0o7YQQIh7ETVKI6Ah7G/cSqs2lX7/DHBn1SJx0EnzxhRlEb+FCKCoy1xsuv9x0mCguNh3fdrRfReRyFTBkyFOccsoO+vf/JV7vZtatu5nFiwtYuvRkqqrekesOQogjpo63E0lJSYkuLS095OUqmyrJfiSbgjWPM7jqu7z7bgyC66rdu+FHP4I5c0ynt5kzTQulX/0K0tPNbT5TUztdhdaaxsZV1NTMZ+fOp/D5tuJyFeJw9MLtHkhBwY9JSBh6lHZICHGsU0ot1VoftN1l3JQUmjuu1e3MPTrXEzrTqxf86U+mCmnLFvjxj+G++2DRIjOG0k9/etBVqI0bSQr1oW/fO5kwYR1DhswiOXkCVmsilZVvsmTJCDZsuJ26uo+IRAIx3yUhxIkhpgPiHUua781cW5ZLwTk9HEyzgQPbvh4/Hr7xDXM3t5tugrQ0+PRTc0Of1iWHdetg7FgYORI++giL1UHv3jPo3XsGAIFAOVu2/JidO59g587HUcqJw5GH3Z5JevpZ9Op1k5QihBDtip+SQrQ3M405PV9S6MyDD5pkcOaZUFAAV1wBffqYjnC7dplqpq99DcJhkzB+97sDVuFw5DJ06PN86Ut7KSp6jT59vkNa2mRstnTKyn7Lp5+exLJlp7J795/wejfR1LQen69MrkkIIeLnmoIn4OHlf2/h5kuG8P67Tr785RgE111eegnuv99chD7tNPjzn817Dofp7zB/vun/8PzzJjGsXWuqpLrA799DefkL7N79B7ze9W0+s1pTSEoaS1bWVLKyLsXt7h+LvRNC9ICuXlOIm6QAphr/hhtMVf7+NTfHvE2b4Ac/MB3ivvpVmDvXdIYbOdLcU3TuXFOi6CKtNfX1H+H1bkQpO6FQLY2Nq6ir+x+NjSsAogniYuz2HJSykZJyCklJI2K1h0KIzqxaBf37m1tGHoauJoW4uaYAsD3apP8Qzp3HjoED4Y03zD9Gc3fswYNNaeEb3zBNXO+/32S95GTYtg0+/BBGj4bhw/eN/vf553D77SilSJ02jdQpU8wBadVG1+vdTEXF61RWvs7Wrfe1CSMxcQRpaV/G7R6E2z0Yt3sQLlchFov9KB0I0UYoZH4QTJ1qWq4J8PmgpqbLpedOrVwJs2eb43vGGeYX5VNPmWrdU0/teDmtzXdt7VqzrNsNtbXmWmF+Pnz727B3L/z2t6Zj6ze/afovNd8KMhiEFStg1ChTQ/CPf5gfg9ddB08/feT71Rmt9XE1FRcX68N1441a9+p12IsfuzZu1PrLX9YatE5K0vrkk83z5ikjQ+uzz9b6uuu0ttm0zsnResiQfZ/n5Gg9bZrWH3ygdSTSZtVBf7X2f/iODtx3p264+ct67xV5etNtDr14DnrB++gFC9ALFzr0kiXFevXq6/SGDXfqrVsf0GVlT+vy8pe117uj47i9Xq0rKmJ7bE5kkYjWN99s/oYXXnjA3+6wPPmk1r/5jdY1NUe+rmaRiNZ792rt82kdDGq9bJnWc+ZovXZt15YPhfY937ZN64su0nr27Pb398MPtR44UGunU+uXX973fl2d1j//udaDB2v9wx+aWLTW2u83MbUWDmtdXq71r3+ttcOx73sycqTWVuu+70x5uYntppu0HjfOrP/FF7W+4QZzomlerqRE65UrzaPd3nadGRla9+tnnvftq/UPfqD1009rPWCAeS8/36xPKbP8zp2HduxbAUp1F86xPX6SP9TpSJLC2Web8+UJKRLR+qOPzIm/+R90yRKt//Snff+0CQlaf/WrWldWmvlXrDAngeuuM/+cYL5Qw4aZxz59tE5JMe8rZZ5nZrb8Q4ez07W/ZJCuv3i43v2N/nrbzcm6doRF+9PQq39oEsaCBehPPhmm13zwFb197mV610tf12Xv36FrfzZNh3PSdUQpHTn/PK1feUVrj2ff/jQ1td2/rVvN1Hp/w+FDO0Yej9aLFmn92GNa33671jNmaP2975nj1LzNlSvbnoQ++8wcy/PO03r6dK0//bTtOqurtf7jH81xfOYZrauqDi2m1hYu1PrKK7W+4w6tS0v3nfRCIa0feUTra67Revv2ffP/7GfmbzFpknl88sl9n0UiJukuXWpObj//udabN5vPamq0/vzzA0+qf/7zvpNVYqI5eTb/HZqazLoaG81rn8/8GGn9NwiHtf7b38zJ65RTzLGsrdX6ssv2rddma/uDpbhY6wsu0Hr8eK1vvVXrPXvaHttrr9U6OdmcKMvKzP+lUmbZc87Rev58sz8ffbTv5Nm//74fRt/6ljmm6enm9bhx5rGoyJwQXC5zoh45UusvfUnrggLzujm+iy82iejxx7UeO1br739f63ffNUlnypR9SXnUqH1xpaWZbc6erfXcueaHGphk8NZbZh8fekjrZ581xzMY1PrVV836mrc9bpzZ57PPNq+vuGLfsT9MXU0KcXVNYcgQ05Lz5Ze7OajjhdYdd+VubDSd6d57z9xe1OEwU0ICfOlLZsTXrCwz77ZtMG+eGZpj0yYz7dhh1l9SgrZaUJ98SuCmKwgEduH49zIc5b4DNlkzBuqHQ967CmeFJuKy0DQiHecOL/byJpouLiH86/tJeK0U6z0/M1UlxcVmjJIlSyAQMEXuSy4x11VaV4PV18OLL5oienNLrQULwO83nyclmam21lQ39O9vRrINBs02fvMbeP11eOIJM/+IEWYfa2vN8TjzTFMl9/jjUFe3b6cGDoQ33zT36n7lFXPDpf79oXdv06y4psb0R1m92lQVWCxmnq1bobQUMjNN7MGgqR68+mrTA37RIvN3cbvhxhvNLV9LS011wuzZ5jj85z9wwQXm79LcUq1Z83EZPNhci9Iarr/etF5zOEw1yYQJpln0o4+a/Zo7F4YONQ0enn8eKipMzAUFpi42FIKzzoIXXoDNm02VyIoV5otWW2v2IzfXHNcf/MBUazY0mCrNoiIT79/+ZvY1JcXso8sFl10GTie8/bb5+40ebYaCcbvN9ufPN9v5v/8zx65ZQoKpnnnwQbDbzf69/LL5vzjjDPje98zoAfPmwXe/a9Z31llmW6tWQVOTqdppnoYNM5+395154gmzPjD9jO6/3xzzXbtgzBjzt2q2ahXceafZ5oUXtv/9a/lS1JhjOW7cvu3u2WOO4xEOwyAXmvejtfkf+M534JFHYhBYvPP7zZcqPd18ye+4w9S9JiTAeefBpElQVIS2WNC7dhDsk4q3JA+vdx2eus+wfPApye/tIPHzOpoKLAQTA+T9w9ye1BKCyskO/KN7k7HIh6UpQMNQC1qHyfjQh7XWC4DOzoTi8ah+/cwJubZ2X3yDB8NFF5kv+bhx++qb6+pM6675882Jqndvc5Ok3bvN59/+NvzsZ5CRYU5ozz1n1v3ZZybZXHwx3HOPOfGvXg3Tp5uTYShkklZHCgrMlzwcNr3a09JMnfHNN5sk9dprZvj1BQtM8nrqKVOHPWMGvP++OYFPn27+oe12c/L80pdMkikuNvEkJ5vtTJ4MkYi56dOnn5plvV4z5MqkSWbef//bLLts2b5j8957JgHt2GFOZlddZfrIrFtnjmdiojkBW63m5Nyvnzl2V14JlZVmX1asMMm5s/r3ZuvXm2P58cfmuPTrB888Y/5ev/udqUt/+mk4/XQzf0MDfPKJmXr3NvX8yclt11lTY45td49rozXceqs5Vvfe2wPj5hw6SQr7KS83P96efNJ8z8VRsGmT+bK63Ye8qNaawGfvwT330HhqPhWXZ9PYtBKPZwWgSEkZTyQSpKH6fySvh6R1kLweUtZbcZdpGib3ombGWCJjRpKQOBitIvh827HbM8jMvBi3u7DjjdfXm5PPaad1fDJraIDqanPSba2szPyCzc42v1r79TO91vfuNQnI5TIn4szMrh2IPXvML/mMjOYDY0p1SUldW74zL74It9xifqUXF5ue9PvffaqpyexnR60z1q41v4DHj4e77zaJorXOSqfiqJKksJ8lS8wPpDffNI0BxPGp+U50SpnWVF7vJurqPkTrCOFwI42Nn+NpWEE4Uk847MHv3wU0371OAeb/3eHoDUSIRIJoHUQpK8nJJaSkfAmLxUEk4sVqTcXpzI++9qGUE6ezd8sYU0opIpEggcAenM4+qOPx5BcO72vxIk5o0iR1P9u2mcf9f9iJ40tzMmjmdg/E7e6400kkEsDn24ZSdpzOfPz+HVRWvkFj4yqUsqGUHaXsRCJe6us/Ztu2nzVvieYE0h67PRenM5/GxlVo7cflGkBW1lSs1uRoQknB4eiF1epG6xBKObHbM7DZMrDbM7Dbc7DZuuHX/pGShCD2EzclhR07zPW6yy47sIQrRLNwuAmlrCjlIByux+/fidYhLBYXkYgXv383Xu96GhqWEQjsIjFxFE5nH2pq/k1NzXtt5u2cIiFhGMnJJVgsTrQOEwpVEwxWonU4mrDM5HINICfnSpKSxtLUtJZAYBd2ey4Oh+lUqHWEUKiWUKgKqzUFl6sAhyPvgAQq4ptUHwlxlGkdBizRaiU/gcAeIpEASlmJRPzRk341oVA1Pt826usX4/F8DpjlTAkiC6XsaB2KTkE8ni+IRBoPKRZTMuqL1ZpIONxAJBLEak3Aak3EYknEZkuJdj4cTCTiIxiswGpNxG7PiSapEFZrAg5Hb6zWJILBCkKhmui6nSQnj8XlGtBulVk47ENrPzZb58O/i6NLqo+EOMqU2lcVY7E4cbm6p64yHG6iqmoePt8mEhKG43T2JRisIBAop/l6ic2Wjt2eQShUi8+3DZ9vO37/NsJhLzZbSrSKrIlwuJFwuJFAYA+1tYtako1JRMFDistuz8JqNVVgTmdfEhKG4/dvp7Z2IZGIF7s9C7d7CG73YJzO3oTDHsLhBkzitGOxOFDKQULCYFJSTkHrCI2NnxMON+Bw5GGxNCc0HzZbCnZ7FklJ47Ba3QSDtVRV/ROLxY7bPSSaTG2tqgRt0V721pZjZLE42sSvdZjGxjVYrYm4XIXH5zWhGJCkIMQxzmpNICfnim5fr9YRAoFyrNZErNZktA4RDFZEk4OVSKQRv38n4XAjDkcONlsaoAiHPTQ0LKGhoZRIxA9ovN4tVFS8jN2eRa9eN+N09sHr3UhT03pqat4lENiDzZYS3U4Erc0F/kjE14Wqtn0sFheJiaPxeD5D60O7T4hSjmgMKVitiXi9m1uSotPZj+TkYhyOXoDG4/kcv78Mp7MPLlcBVmsCSjmwWJzRBBpG6wAORy5JSWNwOHoTiXgJBqvx+bYQDO7Fak3Cak3FanVjsbj2m9wtk82Wht2e2dJwwST8vQSDFUQiXrQOkpAwnISEk45K4opp9ZFS6nzgcUy6/r3W+qH9PncCLwDFQBUwXWu9tbN1SvWREMcfrXW7JzStNT7fZurrF6OUjcTE0dhsaQSD5YTDjVitKVgsrpbrO7W1C6ivX0xy8gRyc7+KxeLG611PKFQbbUm2r9rNPIaj11Y04bCHUKiecLieUKgBl6uA5OTxhMN11NYupLFxDYHAbrQOkZg4EperAL+/DL9/RzR5BdA60FIlqJSdcLjuwJ0FDtZQ4YC5lQOrNbGliq49NlsmBQV307fvHV1eb9tt9HD1kTJl6aeBc4AyYIlS6h9a69WtZrsJqNFaD1JKXQX8Cpgeq5iEED2jo1+4Sql2W5A5nXkHzJucPI6srK8c8H5S0sgjji8//7bDWi4UqsfjWUEwWIXV6sZqTcXtHoDdnk0k4o0mK99+k5dw2NvyPBSqxu/f1VIis9tzcDhyo9VziYDC4/mMuroPok2pYyuW1UcTgI1a680ASqmXgIuB1knhYuC+6PNXgaeUUkofb1e/hRBxyWZLIS3ttHY/Mxf2D2+Y6/0lJ4+lV68bu2VdBxPLNmv5wI5Wr8ui77U7j9Y6BNQBXezqKYQQorsdFw2ZlVK3KKVKlVKlFRUVPR2OEEKcsGKZFHYCfVu97hN9r915lFI2IBVzwbkNrfUsrXWJ1rokOzs7RuEKIYSIZVJYAgxWSvVXSjmAq4B/7DfPP4Dros+vAN6X6wlCCNFzYnahWWsdUkp9G5iPaZI6W2u9Sin1c8zNHv4B/AH4s1JqI1CNSRxCCCF6SEw7r2mt5wHz9nvv3lbPfcC0WMYghBCi646LC81CCCGODkkKQgghWhx3o6QqpSqAbYe5eBZQ2Y3hxJLEGhsSa2xIrN2vu+Ms0FoftPnmcZcUjoRSqrQrY38cCyTW2JBYY0Ni7X49FadUHwkhhGghSUEIIUSLeEsKs3o6gEMgscaGxBobEmv365E44+qaghBCiM7FW0lBCCFEJ6Alo1sAAAYxSURBVOImKSilzldKrVNKbVRKzezpeFpTSvVVSi1QSq1WSq1SSn0v+n6GUupdpdSG/2/v3mLlqqs4jn9/pqT2YjwgtmprbLlELMReNKaKGgIEKRDKg0SwgoCJL00U0kSp9RJ9MxqrJgokCC3aAAHKJSSYwpGU8NDWtvQW2mIrpB5SaB9absZS4efDf82we3pOekLS2X8z65NMzuzLmayzzuy9Zv6z57/i56ltxwqlgZKk5yQ9HsszJa2P3N4fc121TtKApAcl7ZK0U9IXK87pLfG/3yHpXkkfrCWvku6SdEDSjsa6EfOo4vcR8zZJ8yqI9VfxHNgm6WFJA41tSyPW3ZK+1nasjW1LJFnS6bHcs7z2RVFodIFbAMwCrpU0q92ojvFfYIntWcB8YHHEdyswaPtsYDCWa/B9YGdj+ZfActtnAYcoHfVq8Dvgr7bPAWZTYq4up5KmAd8DPm/7PMpcYZ1OhDXkdQVw6bB1o+VxAXB23L4L3NajGDtWcHysTwLn2f4s8AKwFCCOsWuAc+N3/hjnil5ZwfGxIumTwCXAvsbqnuW1L4oCjS5wLt2+O13gqmB7v+3Ncf8NyslrGiXGlbHbSuCqdiJ8j6TpwOXAnbEs4EJK5zyoJ84PA1+lTLqI7bdtH6bCnIZxwISYQn4isJ9K8mr7GcqElU2j5XEhcI+LdcCApI/3JtKRY7W9Jpp4AayjTOPfifU+20dsvwjsoZwrWos1LAd+wLFNnnuW134pCmPpAlcFSTOAucB6YKrt/bHpFWBqS2E1/ZbyhH03lj8CHG4cdLXkdiZwELg7hrrulDSJCnNq+2Xg15RXhvspHQg3UWdeO0bLY+3H2k3AE3G/ulglLQRetr112KaexdovReH/gqTJwEPAzbZfb26LPhOtXiom6QrggO1NbcYxRuOAecBttucCbzFsqKiGnALEePxCSiH7BDCJEYYValVLHk9E0jLKUO2qtmMZiaSJwI+An55o35OpX4rCWLrAtUrSKZSCsMr26lj9auctYvw80FZ84XzgSkkvUYbgLqSM2w/EsAfUk9shYMj2+lh+kFIkasspwMXAi7YP2j4KrKbkusa8doyWxyqPNUk3AFcAixqNvGqL9UzKC4OtcYxNBzZL+hg9jLVfisJYusC1Jsbl/wTstP2bxqZmZ7pvA4/2OrYm20ttT7c9g5LDv9leBDxN6ZwHFcQJYPsV4F+SPh2rLgKep7Kchn3AfEkT47nQibW6vDaMlsfHgOvjapn5wGuNYaZWSLqUMuR5pe1/NzY9BlwjabykmZQPcTe0ESOA7e22p9ieEcfYEDAvnsu9y6vtvrgBl1GuPNgLLGs7nmGxfZny9nsbsCVul1HG6weBfwBPAae1HWsj5guAx+P+GZSDaQ/wADC+7fgirjnAxsjrI8CpteYU+DmwC9gB/BkYX0tegXspn3UcpZyovjNaHgFRrvTbC2ynXFHVdqx7KOPxnWPr9sb+yyLW3cCCtmMdtv0l4PRe5zW/0ZxSSqmrX4aPUkopjUEWhZRSSl1ZFFJKKXVlUUgppdSVRSGllFJXFoWUekjSBYrZZVOqURaFlFJKXVkUUhqBpG9J2iBpi6Q7VHpIvClpefQ9GJT00dh3jqR1jfn6O70FzpL0lKStkjZLOjMefrLe6/OwKr7FnFIVsiikNIykzwDfAM63PQd4B1hEmahuo+1zgbXAz+JX7gF+6DJf//bG+lXAH2zPBr5E+fYqlFlwb6b09jiDMs9RSlUYd+JdUuo7FwGfA/4eL+InUCZ8exe4P/b5C7A6+jYM2F4b61cCD0j6EDDN9sMAtv8DEI+3wfZQLG8BZgDPnvw/K6UTy6KQ0vEErLS99JiV0k+G7fd+54g50rj/Dnkcpork8FFKxxsEvi5pCnT7EX+Kcrx0Zi39JvCs7deAQ5K+EuuvA9a6dNAbknRVPMb4mC8/parlK5SUhrH9vKQfA2skfYAyi+ViSqOeL8S2A5TPHaBMHX17nPT/CdwY668D7pD0i3iMq3v4Z6T0vuQsqSmNkaQ3bU9uO46UTqYcPkoppdSV7xRSSil15TuFlFJKXVkUUkopdWVRSCml1JVFIaWUUlcWhZRSSl1ZFFJKKXX9Dzy6b6I+nfoRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1917 - acc: 0.9499\n",
      "Loss: 0.19169405397580916 Accuracy: 0.9499481\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6313 - acc: 0.1308\n",
      "Epoch 00001: val_loss improved from inf to 2.23899, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/001-2.2390.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 2.6314 - acc: 0.1308 - val_loss: 2.2390 - val_acc: 0.3035\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9980 - acc: 0.3387\n",
      "Epoch 00002: val_loss improved from 2.23899 to 1.29258, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/002-1.2926.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.9979 - acc: 0.3387 - val_loss: 1.2926 - val_acc: 0.5896\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4641 - acc: 0.5107\n",
      "Epoch 00003: val_loss improved from 1.29258 to 0.97055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/003-0.9706.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4641 - acc: 0.5107 - val_loss: 0.9706 - val_acc: 0.6911\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2131 - acc: 0.5979\n",
      "Epoch 00004: val_loss improved from 0.97055 to 0.79479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/004-0.7948.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.2130 - acc: 0.5979 - val_loss: 0.7948 - val_acc: 0.7517\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0485 - acc: 0.6567\n",
      "Epoch 00005: val_loss improved from 0.79479 to 0.76030, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/005-0.7603.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0485 - acc: 0.6567 - val_loss: 0.7603 - val_acc: 0.7617\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9202 - acc: 0.6968\n",
      "Epoch 00006: val_loss improved from 0.76030 to 0.66739, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/006-0.6674.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9201 - acc: 0.6969 - val_loss: 0.6674 - val_acc: 0.7978\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8260 - acc: 0.7311\n",
      "Epoch 00007: val_loss improved from 0.66739 to 0.55000, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/007-0.5500.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8259 - acc: 0.7312 - val_loss: 0.5500 - val_acc: 0.8276\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7345 - acc: 0.7631\n",
      "Epoch 00008: val_loss improved from 0.55000 to 0.43824, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/008-0.4382.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7345 - acc: 0.7631 - val_loss: 0.4382 - val_acc: 0.8626\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.7884\n",
      "Epoch 00009: val_loss did not improve from 0.43824\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6582 - acc: 0.7884 - val_loss: 0.4439 - val_acc: 0.8635\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.8096\n",
      "Epoch 00010: val_loss improved from 0.43824 to 0.33546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/010-0.3355.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5989 - acc: 0.8096 - val_loss: 0.3355 - val_acc: 0.9033\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8258\n",
      "Epoch 00011: val_loss improved from 0.33546 to 0.30579, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/011-0.3058.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5494 - acc: 0.8258 - val_loss: 0.3058 - val_acc: 0.9122\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.8432\n",
      "Epoch 00012: val_loss did not improve from 0.30579\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5019 - acc: 0.8432 - val_loss: 0.3277 - val_acc: 0.9085\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.8534\n",
      "Epoch 00013: val_loss improved from 0.30579 to 0.26377, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/013-0.2638.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4640 - acc: 0.8534 - val_loss: 0.2638 - val_acc: 0.9259\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8652\n",
      "Epoch 00014: val_loss improved from 0.26377 to 0.23207, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/014-0.2321.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4306 - acc: 0.8653 - val_loss: 0.2321 - val_acc: 0.9355\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8743\n",
      "Epoch 00015: val_loss improved from 0.23207 to 0.22995, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/015-0.2300.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4052 - acc: 0.8743 - val_loss: 0.2300 - val_acc: 0.9371\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8817\n",
      "Epoch 00016: val_loss improved from 0.22995 to 0.21570, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/016-0.2157.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3822 - acc: 0.8817 - val_loss: 0.2157 - val_acc: 0.9399\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8871\n",
      "Epoch 00017: val_loss did not improve from 0.21570\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3634 - acc: 0.8871 - val_loss: 0.2221 - val_acc: 0.9371\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8910\n",
      "Epoch 00018: val_loss improved from 0.21570 to 0.20092, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/018-0.2009.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3462 - acc: 0.8910 - val_loss: 0.2009 - val_acc: 0.9397\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8975\n",
      "Epoch 00019: val_loss improved from 0.20092 to 0.18213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/019-0.1821.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3279 - acc: 0.8975 - val_loss: 0.1821 - val_acc: 0.9483\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9041\n",
      "Epoch 00020: val_loss did not improve from 0.18213\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3108 - acc: 0.9041 - val_loss: 0.2117 - val_acc: 0.9411\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.9089\n",
      "Epoch 00021: val_loss did not improve from 0.18213\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2941 - acc: 0.9089 - val_loss: 0.1824 - val_acc: 0.9499\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9126\n",
      "Epoch 00022: val_loss improved from 0.18213 to 0.15702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/022-0.1570.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2827 - acc: 0.9126 - val_loss: 0.1570 - val_acc: 0.9574\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9178\n",
      "Epoch 00023: val_loss improved from 0.15702 to 0.15590, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/023-0.1559.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2671 - acc: 0.9178 - val_loss: 0.1559 - val_acc: 0.9527\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9197\n",
      "Epoch 00024: val_loss did not improve from 0.15590\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2550 - acc: 0.9197 - val_loss: 0.1573 - val_acc: 0.9550\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9232\n",
      "Epoch 00025: val_loss did not improve from 0.15590\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2483 - acc: 0.9232 - val_loss: 0.1780 - val_acc: 0.9488\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9246\n",
      "Epoch 00026: val_loss improved from 0.15590 to 0.13759, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/026-0.1376.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2384 - acc: 0.9246 - val_loss: 0.1376 - val_acc: 0.9597\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9292\n",
      "Epoch 00027: val_loss improved from 0.13759 to 0.13331, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/027-0.1333.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2293 - acc: 0.9292 - val_loss: 0.1333 - val_acc: 0.9627\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9315\n",
      "Epoch 00028: val_loss did not improve from 0.13331\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2204 - acc: 0.9315 - val_loss: 0.1463 - val_acc: 0.9592\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9314\n",
      "Epoch 00029: val_loss did not improve from 0.13331\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2221 - acc: 0.9314 - val_loss: 0.1427 - val_acc: 0.9618\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9348\n",
      "Epoch 00030: val_loss improved from 0.13331 to 0.13062, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/030-0.1306.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2085 - acc: 0.9348 - val_loss: 0.1306 - val_acc: 0.9641\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9367\n",
      "Epoch 00031: val_loss improved from 0.13062 to 0.12625, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/031-0.1262.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1994 - acc: 0.9367 - val_loss: 0.1262 - val_acc: 0.9634\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9404\n",
      "Epoch 00032: val_loss did not improve from 0.12625\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1932 - acc: 0.9404 - val_loss: 0.1434 - val_acc: 0.9606\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9415\n",
      "Epoch 00033: val_loss did not improve from 0.12625\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1845 - acc: 0.9415 - val_loss: 0.1348 - val_acc: 0.9590\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9415\n",
      "Epoch 00034: val_loss did not improve from 0.12625\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1863 - acc: 0.9415 - val_loss: 0.1357 - val_acc: 0.9646\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9439\n",
      "Epoch 00035: val_loss improved from 0.12625 to 0.11982, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/035-0.1198.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1775 - acc: 0.9439 - val_loss: 0.1198 - val_acc: 0.9641\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9457\n",
      "Epoch 00036: val_loss did not improve from 0.11982\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1713 - acc: 0.9456 - val_loss: 0.1351 - val_acc: 0.9616\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9469\n",
      "Epoch 00037: val_loss did not improve from 0.11982\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1666 - acc: 0.9469 - val_loss: 0.1245 - val_acc: 0.9655\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9489\n",
      "Epoch 00038: val_loss improved from 0.11982 to 0.11806, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/038-0.1181.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1611 - acc: 0.9489 - val_loss: 0.1181 - val_acc: 0.9669\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9485\n",
      "Epoch 00039: val_loss did not improve from 0.11806\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1591 - acc: 0.9485 - val_loss: 0.1217 - val_acc: 0.9667\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9523\n",
      "Epoch 00040: val_loss did not improve from 0.11806\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1525 - acc: 0.9523 - val_loss: 0.1305 - val_acc: 0.9625\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9539\n",
      "Epoch 00041: val_loss improved from 0.11806 to 0.11554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/041-0.1155.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1461 - acc: 0.9539 - val_loss: 0.1155 - val_acc: 0.9660\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9524\n",
      "Epoch 00042: val_loss did not improve from 0.11554\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1459 - acc: 0.9524 - val_loss: 0.1209 - val_acc: 0.9674\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9548\n",
      "Epoch 00043: val_loss did not improve from 0.11554\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1392 - acc: 0.9548 - val_loss: 0.1189 - val_acc: 0.9683\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9552\n",
      "Epoch 00044: val_loss did not improve from 0.11554\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1407 - acc: 0.9552 - val_loss: 0.1583 - val_acc: 0.9553\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9558\n",
      "Epoch 00045: val_loss improved from 0.11554 to 0.11512, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/045-0.1151.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1357 - acc: 0.9558 - val_loss: 0.1151 - val_acc: 0.9679\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9563\n",
      "Epoch 00046: val_loss improved from 0.11512 to 0.11014, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/046-0.1101.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1354 - acc: 0.9563 - val_loss: 0.1101 - val_acc: 0.9690\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9601\n",
      "Epoch 00047: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1243 - acc: 0.9601 - val_loss: 0.1167 - val_acc: 0.9660\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9606\n",
      "Epoch 00048: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1212 - acc: 0.9606 - val_loss: 0.1257 - val_acc: 0.9655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9610\n",
      "Epoch 00049: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1226 - acc: 0.9610 - val_loss: 0.1173 - val_acc: 0.9655\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9622\n",
      "Epoch 00050: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1169 - acc: 0.9622 - val_loss: 0.1141 - val_acc: 0.9683\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9618\n",
      "Epoch 00051: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1161 - acc: 0.9618 - val_loss: 0.1120 - val_acc: 0.9690\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9620\n",
      "Epoch 00052: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1150 - acc: 0.9620 - val_loss: 0.1191 - val_acc: 0.9695\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9632\n",
      "Epoch 00053: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1119 - acc: 0.9632 - val_loss: 0.1245 - val_acc: 0.9681\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9669\n",
      "Epoch 00054: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1036 - acc: 0.9669 - val_loss: 0.1224 - val_acc: 0.9693\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9671\n",
      "Epoch 00055: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1001 - acc: 0.9671 - val_loss: 0.1188 - val_acc: 0.9697\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9655\n",
      "Epoch 00056: val_loss did not improve from 0.11014\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1049 - acc: 0.9654 - val_loss: 0.1226 - val_acc: 0.9683\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9678\n",
      "Epoch 00057: val_loss improved from 0.11014 to 0.10826, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/057-0.1083.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1016 - acc: 0.9678 - val_loss: 0.1083 - val_acc: 0.9672\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9685\n",
      "Epoch 00058: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0954 - acc: 0.9685 - val_loss: 0.1245 - val_acc: 0.9695\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9689\n",
      "Epoch 00059: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0969 - acc: 0.9689 - val_loss: 0.1139 - val_acc: 0.9693\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9695\n",
      "Epoch 00060: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0924 - acc: 0.9695 - val_loss: 0.1131 - val_acc: 0.9695\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9710\n",
      "Epoch 00061: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0910 - acc: 0.9710 - val_loss: 0.1136 - val_acc: 0.9716\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9699\n",
      "Epoch 00062: val_loss improved from 0.10826 to 0.10451, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/062-0.1045.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0897 - acc: 0.9699 - val_loss: 0.1045 - val_acc: 0.9716\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9704\n",
      "Epoch 00063: val_loss did not improve from 0.10451\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0890 - acc: 0.9704 - val_loss: 0.1438 - val_acc: 0.9662\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9692\n",
      "Epoch 00064: val_loss did not improve from 0.10451\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0918 - acc: 0.9692 - val_loss: 0.1123 - val_acc: 0.9676\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9740\n",
      "Epoch 00065: val_loss improved from 0.10451 to 0.10437, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/065-0.1044.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0802 - acc: 0.9741 - val_loss: 0.1044 - val_acc: 0.9739\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9729\n",
      "Epoch 00066: val_loss improved from 0.10437 to 0.09989, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv_checkpoint/066-0.0999.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0818 - acc: 0.9729 - val_loss: 0.0999 - val_acc: 0.9720\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9726\n",
      "Epoch 00067: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0846 - acc: 0.9726 - val_loss: 0.1263 - val_acc: 0.9672\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9734\n",
      "Epoch 00068: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0820 - acc: 0.9734 - val_loss: 0.1014 - val_acc: 0.9725\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9753\n",
      "Epoch 00069: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0745 - acc: 0.9753 - val_loss: 0.1108 - val_acc: 0.9718\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9750\n",
      "Epoch 00070: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0747 - acc: 0.9750 - val_loss: 0.1129 - val_acc: 0.9672\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9752\n",
      "Epoch 00071: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0735 - acc: 0.9752 - val_loss: 0.1017 - val_acc: 0.9716\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9759\n",
      "Epoch 00072: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0738 - acc: 0.9758 - val_loss: 0.1160 - val_acc: 0.9679\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9754\n",
      "Epoch 00073: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0732 - acc: 0.9754 - val_loss: 0.1052 - val_acc: 0.9734\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9758\n",
      "Epoch 00074: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0708 - acc: 0.9758 - val_loss: 0.1214 - val_acc: 0.9706\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9768\n",
      "Epoch 00075: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0704 - acc: 0.9768 - val_loss: 0.1222 - val_acc: 0.9718\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9772\n",
      "Epoch 00076: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0686 - acc: 0.9772 - val_loss: 0.1235 - val_acc: 0.9702\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9776\n",
      "Epoch 00077: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0675 - acc: 0.9776 - val_loss: 0.1180 - val_acc: 0.9697\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9793\n",
      "Epoch 00078: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0637 - acc: 0.9793 - val_loss: 0.1359 - val_acc: 0.9697\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9790\n",
      "Epoch 00079: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0641 - acc: 0.9790 - val_loss: 0.1297 - val_acc: 0.9658\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9797\n",
      "Epoch 00080: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0624 - acc: 0.9797 - val_loss: 0.1164 - val_acc: 0.9704\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9795\n",
      "Epoch 00081: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0628 - acc: 0.9795 - val_loss: 0.1015 - val_acc: 0.9713\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9803\n",
      "Epoch 00082: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0618 - acc: 0.9803 - val_loss: 0.1158 - val_acc: 0.9709\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9801\n",
      "Epoch 00083: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0602 - acc: 0.9801 - val_loss: 0.1082 - val_acc: 0.9704\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9801\n",
      "Epoch 00084: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0592 - acc: 0.9801 - val_loss: 0.1170 - val_acc: 0.9704\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9802\n",
      "Epoch 00085: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0605 - acc: 0.9802 - val_loss: 0.1280 - val_acc: 0.9688\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9806\n",
      "Epoch 00086: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0583 - acc: 0.9805 - val_loss: 0.1479 - val_acc: 0.9641\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9806\n",
      "Epoch 00087: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0580 - acc: 0.9806 - val_loss: 0.1363 - val_acc: 0.9669\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9816\n",
      "Epoch 00088: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0531 - acc: 0.9816 - val_loss: 0.1175 - val_acc: 0.9700\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9820\n",
      "Epoch 00089: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0545 - acc: 0.9820 - val_loss: 0.1236 - val_acc: 0.9704\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9827\n",
      "Epoch 00090: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0527 - acc: 0.9827 - val_loss: 0.1140 - val_acc: 0.9697\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9831\n",
      "Epoch 00091: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0519 - acc: 0.9831 - val_loss: 0.1200 - val_acc: 0.9711\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9819\n",
      "Epoch 00092: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0539 - acc: 0.9819 - val_loss: 0.1172 - val_acc: 0.9713\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9832\n",
      "Epoch 00093: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0484 - acc: 0.9831 - val_loss: 0.1338 - val_acc: 0.9681\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9834\n",
      "Epoch 00094: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0502 - acc: 0.9834 - val_loss: 0.1233 - val_acc: 0.9727\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9839\n",
      "Epoch 00095: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0482 - acc: 0.9839 - val_loss: 0.1390 - val_acc: 0.9695\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9823\n",
      "Epoch 00096: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0537 - acc: 0.9823 - val_loss: 0.1160 - val_acc: 0.9711\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9852\n",
      "Epoch 00097: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0454 - acc: 0.9852 - val_loss: 0.1267 - val_acc: 0.9718\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9842\n",
      "Epoch 00098: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0481 - acc: 0.9842 - val_loss: 0.1297 - val_acc: 0.9700\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9836\n",
      "Epoch 00099: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0498 - acc: 0.9836 - val_loss: 0.1637 - val_acc: 0.9713\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9858\n",
      "Epoch 00100: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0420 - acc: 0.9858 - val_loss: 0.1309 - val_acc: 0.9706\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9845\n",
      "Epoch 00101: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0465 - acc: 0.9845 - val_loss: 0.1262 - val_acc: 0.9702\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9864\n",
      "Epoch 00102: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0441 - acc: 0.9864 - val_loss: 0.1338 - val_acc: 0.9704\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9868\n",
      "Epoch 00103: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0424 - acc: 0.9868 - val_loss: 0.1561 - val_acc: 0.9695\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9858\n",
      "Epoch 00104: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0458 - acc: 0.9858 - val_loss: 0.1292 - val_acc: 0.9695\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9865\n",
      "Epoch 00105: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0403 - acc: 0.9866 - val_loss: 0.1449 - val_acc: 0.9674\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9871\n",
      "Epoch 00106: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0395 - acc: 0.9871 - val_loss: 0.1516 - val_acc: 0.9693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9850\n",
      "Epoch 00107: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0451 - acc: 0.9850 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9858\n",
      "Epoch 00108: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0430 - acc: 0.9858 - val_loss: 0.1575 - val_acc: 0.9706\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 00109: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0379 - acc: 0.9874 - val_loss: 0.1443 - val_acc: 0.9716\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9874\n",
      "Epoch 00110: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0391 - acc: 0.9874 - val_loss: 0.1588 - val_acc: 0.9674\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9859\n",
      "Epoch 00111: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0436 - acc: 0.9859 - val_loss: 0.1422 - val_acc: 0.9688\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9882\n",
      "Epoch 00112: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0352 - acc: 0.9882 - val_loss: 0.1562 - val_acc: 0.9688\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9867\n",
      "Epoch 00113: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0406 - acc: 0.9867 - val_loss: 0.1256 - val_acc: 0.9697\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9884\n",
      "Epoch 00114: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0364 - acc: 0.9884 - val_loss: 0.1512 - val_acc: 0.9667\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9871\n",
      "Epoch 00115: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0386 - acc: 0.9871 - val_loss: 0.1382 - val_acc: 0.9706\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9878\n",
      "Epoch 00116: val_loss did not improve from 0.09989\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0359 - acc: 0.9878 - val_loss: 0.1523 - val_acc: 0.9693\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1385 - acc: 0.9610\n",
      "Loss: 0.1385373390615296 Accuracy: 0.9609553\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5069 - acc: 0.1788\n",
      "Epoch 00001: val_loss improved from inf to 2.02420, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/001-2.0242.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 2.5069 - acc: 0.1788 - val_loss: 2.0242 - val_acc: 0.3878\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6843 - acc: 0.4560\n",
      "Epoch 00002: val_loss improved from 2.02420 to 1.12097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/002-1.1210.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.6843 - acc: 0.4560 - val_loss: 1.1210 - val_acc: 0.6385\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1517 - acc: 0.6234\n",
      "Epoch 00003: val_loss improved from 1.12097 to 0.75754, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/003-0.7575.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.1517 - acc: 0.6234 - val_loss: 0.7575 - val_acc: 0.7533\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.7150\n",
      "Epoch 00004: val_loss improved from 0.75754 to 0.58720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/004-0.5872.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.8833 - acc: 0.7150 - val_loss: 0.5872 - val_acc: 0.8202\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7696\n",
      "Epoch 00005: val_loss improved from 0.58720 to 0.46292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/005-0.4629.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7172 - acc: 0.7697 - val_loss: 0.4629 - val_acc: 0.8526\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6134 - acc: 0.8028\n",
      "Epoch 00006: val_loss improved from 0.46292 to 0.41219, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/006-0.4122.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.6134 - acc: 0.8028 - val_loss: 0.4122 - val_acc: 0.8740\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8311\n",
      "Epoch 00007: val_loss improved from 0.41219 to 0.33175, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/007-0.3318.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.5323 - acc: 0.8311 - val_loss: 0.3318 - val_acc: 0.8998\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4665 - acc: 0.8522\n",
      "Epoch 00008: val_loss improved from 0.33175 to 0.28126, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/008-0.2813.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.4665 - acc: 0.8522 - val_loss: 0.2813 - val_acc: 0.9099\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.8683\n",
      "Epoch 00009: val_loss improved from 0.28126 to 0.27117, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/009-0.2712.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.4261 - acc: 0.8683 - val_loss: 0.2712 - val_acc: 0.9201\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8799\n",
      "Epoch 00010: val_loss improved from 0.27117 to 0.23727, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/010-0.2373.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3841 - acc: 0.8799 - val_loss: 0.2373 - val_acc: 0.9245\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8919\n",
      "Epoch 00011: val_loss improved from 0.23727 to 0.23272, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/011-0.2327.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3456 - acc: 0.8919 - val_loss: 0.2327 - val_acc: 0.9271\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8991\n",
      "Epoch 00012: val_loss improved from 0.23272 to 0.21676, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/012-0.2168.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3229 - acc: 0.8991 - val_loss: 0.2168 - val_acc: 0.9290\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9089\n",
      "Epoch 00013: val_loss improved from 0.21676 to 0.21587, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/013-0.2159.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2950 - acc: 0.9090 - val_loss: 0.2159 - val_acc: 0.9350\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9147\n",
      "Epoch 00014: val_loss improved from 0.21587 to 0.17802, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/014-0.1780.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2755 - acc: 0.9147 - val_loss: 0.1780 - val_acc: 0.9457\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9188\n",
      "Epoch 00015: val_loss did not improve from 0.17802\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2609 - acc: 0.9188 - val_loss: 0.1843 - val_acc: 0.9448\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9272\n",
      "Epoch 00016: val_loss improved from 0.17802 to 0.16333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/016-0.1633.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2353 - acc: 0.9272 - val_loss: 0.1633 - val_acc: 0.9502\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9306\n",
      "Epoch 00017: val_loss did not improve from 0.16333\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2241 - acc: 0.9306 - val_loss: 0.1787 - val_acc: 0.9453\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9361\n",
      "Epoch 00018: val_loss improved from 0.16333 to 0.14778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/018-0.1478.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2068 - acc: 0.9361 - val_loss: 0.1478 - val_acc: 0.9553\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9408\n",
      "Epoch 00019: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1956 - acc: 0.9407 - val_loss: 0.1589 - val_acc: 0.9520\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9423\n",
      "Epoch 00020: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1861 - acc: 0.9422 - val_loss: 0.1558 - val_acc: 0.9546\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9443\n",
      "Epoch 00021: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1764 - acc: 0.9443 - val_loss: 0.1525 - val_acc: 0.9555\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9474\n",
      "Epoch 00022: val_loss did not improve from 0.14778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1671 - acc: 0.9474 - val_loss: 0.1880 - val_acc: 0.9478\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9500\n",
      "Epoch 00023: val_loss improved from 0.14778 to 0.12844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/023-0.1284.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1589 - acc: 0.9500 - val_loss: 0.1284 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9555\n",
      "Epoch 00024: val_loss did not improve from 0.12844\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1447 - acc: 0.9555 - val_loss: 0.1285 - val_acc: 0.9597\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9542\n",
      "Epoch 00025: val_loss did not improve from 0.12844\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1414 - acc: 0.9542 - val_loss: 0.1357 - val_acc: 0.9562\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9576\n",
      "Epoch 00026: val_loss did not improve from 0.12844\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1341 - acc: 0.9576 - val_loss: 0.1384 - val_acc: 0.9595\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9588\n",
      "Epoch 00027: val_loss improved from 0.12844 to 0.12749, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/027-0.1275.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1299 - acc: 0.9588 - val_loss: 0.1275 - val_acc: 0.9634\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9616\n",
      "Epoch 00028: val_loss did not improve from 0.12749\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1212 - acc: 0.9616 - val_loss: 0.1473 - val_acc: 0.9585\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9627\n",
      "Epoch 00029: val_loss did not improve from 0.12749\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1172 - acc: 0.9626 - val_loss: 0.1341 - val_acc: 0.9602\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9638\n",
      "Epoch 00030: val_loss did not improve from 0.12749\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1124 - acc: 0.9638 - val_loss: 0.1304 - val_acc: 0.9627\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9667\n",
      "Epoch 00031: val_loss did not improve from 0.12749\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1076 - acc: 0.9667 - val_loss: 0.1314 - val_acc: 0.9616\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9668\n",
      "Epoch 00032: val_loss improved from 0.12749 to 0.12102, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/032-0.1210.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1050 - acc: 0.9668 - val_loss: 0.1210 - val_acc: 0.9639\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9686\n",
      "Epoch 00033: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0982 - acc: 0.9686 - val_loss: 0.1306 - val_acc: 0.9630\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9692\n",
      "Epoch 00034: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0934 - acc: 0.9692 - val_loss: 0.1527 - val_acc: 0.9532\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9699\n",
      "Epoch 00035: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0926 - acc: 0.9699 - val_loss: 0.1279 - val_acc: 0.9609\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9706\n",
      "Epoch 00036: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0883 - acc: 0.9706 - val_loss: 0.1342 - val_acc: 0.9639\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9722\n",
      "Epoch 00037: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0835 - acc: 0.9722 - val_loss: 0.1280 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9724\n",
      "Epoch 00038: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0835 - acc: 0.9724 - val_loss: 0.1274 - val_acc: 0.9623\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9745\n",
      "Epoch 00039: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0806 - acc: 0.9745 - val_loss: 0.1389 - val_acc: 0.9613\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9755\n",
      "Epoch 00040: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0731 - acc: 0.9755 - val_loss: 0.1430 - val_acc: 0.9639\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9775\n",
      "Epoch 00041: val_loss improved from 0.12102 to 0.11623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv_checkpoint/041-0.1162.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0694 - acc: 0.9775 - val_loss: 0.1162 - val_acc: 0.9679\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9773\n",
      "Epoch 00042: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 0.1383 - val_acc: 0.9655\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9778\n",
      "Epoch 00043: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0680 - acc: 0.9778 - val_loss: 0.1511 - val_acc: 0.9646\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9792\n",
      "Epoch 00044: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0644 - acc: 0.9792 - val_loss: 0.1349 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9793\n",
      "Epoch 00045: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0622 - acc: 0.9793 - val_loss: 0.1299 - val_acc: 0.9632\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9796\n",
      "Epoch 00046: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0608 - acc: 0.9796 - val_loss: 0.1390 - val_acc: 0.9632\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9779\n",
      "Epoch 00047: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0657 - acc: 0.9779 - val_loss: 0.1427 - val_acc: 0.9611\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9819\n",
      "Epoch 00048: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0552 - acc: 0.9819 - val_loss: 0.1508 - val_acc: 0.9662\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9807\n",
      "Epoch 00049: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0611 - acc: 0.9807 - val_loss: 0.1414 - val_acc: 0.9630\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9825\n",
      "Epoch 00050: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0534 - acc: 0.9825 - val_loss: 0.1643 - val_acc: 0.9606\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9821\n",
      "Epoch 00051: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0515 - acc: 0.9821 - val_loss: 0.1386 - val_acc: 0.9660\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9825\n",
      "Epoch 00052: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0518 - acc: 0.9825 - val_loss: 0.1633 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9847\n",
      "Epoch 00053: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0468 - acc: 0.9847 - val_loss: 0.1714 - val_acc: 0.9604\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9830\n",
      "Epoch 00054: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0510 - acc: 0.9830 - val_loss: 0.1563 - val_acc: 0.9637\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9840\n",
      "Epoch 00055: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0487 - acc: 0.9841 - val_loss: 0.1272 - val_acc: 0.9693\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9847\n",
      "Epoch 00056: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0459 - acc: 0.9847 - val_loss: 0.1695 - val_acc: 0.9644\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9847\n",
      "Epoch 00057: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0484 - acc: 0.9847 - val_loss: 0.1492 - val_acc: 0.9646\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9862\n",
      "Epoch 00058: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0414 - acc: 0.9863 - val_loss: 0.1553 - val_acc: 0.9639\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9857\n",
      "Epoch 00059: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0434 - acc: 0.9857 - val_loss: 0.1409 - val_acc: 0.9679\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9863\n",
      "Epoch 00060: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0412 - acc: 0.9863 - val_loss: 0.1411 - val_acc: 0.9662\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9857\n",
      "Epoch 00061: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0420 - acc: 0.9857 - val_loss: 0.1331 - val_acc: 0.9683\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9882\n",
      "Epoch 00062: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0367 - acc: 0.9882 - val_loss: 0.1456 - val_acc: 0.9681\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9871\n",
      "Epoch 00063: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0404 - acc: 0.9871 - val_loss: 0.1764 - val_acc: 0.9644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9855\n",
      "Epoch 00064: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0456 - acc: 0.9855 - val_loss: 0.1455 - val_acc: 0.9693\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9877\n",
      "Epoch 00065: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0381 - acc: 0.9877 - val_loss: 0.1571 - val_acc: 0.9683\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9882\n",
      "Epoch 00066: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0370 - acc: 0.9882 - val_loss: 0.1697 - val_acc: 0.9613\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9883\n",
      "Epoch 00067: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0362 - acc: 0.9883 - val_loss: 0.1923 - val_acc: 0.9613\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9904\n",
      "Epoch 00068: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0316 - acc: 0.9904 - val_loss: 0.1701 - val_acc: 0.9693\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9884\n",
      "Epoch 00069: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0353 - acc: 0.9884 - val_loss: 0.1727 - val_acc: 0.9665\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9880\n",
      "Epoch 00070: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0370 - acc: 0.9880 - val_loss: 0.1561 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9883\n",
      "Epoch 00071: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0348 - acc: 0.9883 - val_loss: 0.1611 - val_acc: 0.9667\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9907\n",
      "Epoch 00072: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0304 - acc: 0.9907 - val_loss: 0.2072 - val_acc: 0.9637\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9892\n",
      "Epoch 00073: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0356 - acc: 0.9892 - val_loss: 0.1585 - val_acc: 0.9683\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9891\n",
      "Epoch 00074: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0338 - acc: 0.9891 - val_loss: 0.1835 - val_acc: 0.9651\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9895\n",
      "Epoch 00075: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0342 - acc: 0.9895 - val_loss: 0.2180 - val_acc: 0.9590\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9896\n",
      "Epoch 00076: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0355 - acc: 0.9896 - val_loss: 0.1966 - val_acc: 0.9634\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9915\n",
      "Epoch 00077: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0279 - acc: 0.9916 - val_loss: 0.2182 - val_acc: 0.9679\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9903\n",
      "Epoch 00078: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0309 - acc: 0.9903 - val_loss: 0.1559 - val_acc: 0.9662\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9915\n",
      "Epoch 00079: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0281 - acc: 0.9916 - val_loss: 0.1860 - val_acc: 0.9646\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 00080: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0256 - acc: 0.9917 - val_loss: 0.1888 - val_acc: 0.9686\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9906\n",
      "Epoch 00081: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0300 - acc: 0.9906 - val_loss: 0.1604 - val_acc: 0.9669\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9913\n",
      "Epoch 00082: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0275 - acc: 0.9913 - val_loss: 0.1907 - val_acc: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9912\n",
      "Epoch 00083: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0287 - acc: 0.9912 - val_loss: 0.1746 - val_acc: 0.9676\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9914\n",
      "Epoch 00084: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0276 - acc: 0.9914 - val_loss: 0.1988 - val_acc: 0.9672\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9917\n",
      "Epoch 00085: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0258 - acc: 0.9917 - val_loss: 0.1594 - val_acc: 0.9674\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9921\n",
      "Epoch 00086: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0260 - acc: 0.9921 - val_loss: 0.1655 - val_acc: 0.9697\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9929\n",
      "Epoch 00087: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0222 - acc: 0.9929 - val_loss: 0.1671 - val_acc: 0.9681\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9902\n",
      "Epoch 00088: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0320 - acc: 0.9902 - val_loss: 0.1549 - val_acc: 0.9723\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9936\n",
      "Epoch 00089: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0205 - acc: 0.9936 - val_loss: 0.1850 - val_acc: 0.9667\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9905\n",
      "Epoch 00090: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0321 - acc: 0.9905 - val_loss: 0.1568 - val_acc: 0.9683\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9939\n",
      "Epoch 00091: val_loss did not improve from 0.11623\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0183 - acc: 0.9939 - val_loss: 0.1856 - val_acc: 0.9688\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1809 - acc: 0.9502\n",
      "Loss: 0.18085583141063913 Accuracy: 0.95015574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_075_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "    \n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 615us/sample - loss: 1.9901 - acc: 0.3761\n",
      "Loss: 1.9900841282659354 Accuracy: 0.3761163\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 956us/sample - loss: 1.7036 - acc: 0.4652\n",
      "Loss: 1.7035639770553368 Accuracy: 0.46521288\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.2764 - acc: 0.6183\n",
      "Loss: 1.2764445521254653 Accuracy: 0.61827624\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8678 - acc: 0.7526\n",
      "Loss: 0.8677615195667508 Accuracy: 0.752648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6535 - acc: 0.8226\n",
      "Loss: 0.6535081007275131 Accuracy: 0.8226376\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3041 - acc: 0.9107\n",
      "Loss: 0.30411705678620937 Accuracy: 0.91069573\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1917 - acc: 0.9499\n",
      "Loss: 0.19169405397580916 Accuracy: 0.9499481\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1385 - acc: 0.9610\n",
      "Loss: 0.1385373390615296 Accuracy: 0.9609553\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1809 - acc: 0.9502\n",
      "Loss: 0.18085583141063913 Accuracy: 0.95015574\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6275 - acc: 0.6814\n",
      "Loss: 1.627498361601389 Accuracy: 0.6814123\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9758 - acc: 0.7848\n",
      "Loss: 0.9757609721903736 Accuracy: 0.78483903\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.7068 - acc: 0.8486\n",
      "Loss: 0.7067555297696083 Accuracy: 0.8485981\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3490 - acc: 0.9180\n",
      "Loss: 0.34901009484166173 Accuracy: 0.9179647\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2321 - acc: 0.9495\n",
      "Loss: 0.23209096371879953 Accuracy: 0.9495327\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2217 - acc: 0.9585\n",
      "Loss: 0.22167081626316243 Accuracy: 0.95846313\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2721 - acc: 0.9572\n",
      "Loss: 0.27214573371550027 Accuracy: 0.95721704\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO_075_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
