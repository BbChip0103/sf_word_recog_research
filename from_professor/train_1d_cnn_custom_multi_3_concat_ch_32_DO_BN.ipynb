{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 170656)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 246464)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 246464)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3943440     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,954,320\n",
      "Trainable params: 3,954,128\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 56864)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 82112)        0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 82112)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1313808     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,968\n",
      "Trainable params: 1,329,712\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 18944)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 29408)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 29408)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           470544      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 497,264\n",
      "Trainable params: 496,880\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 6304)         0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11808)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 11808)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           188944      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 236,464\n",
      "Trainable params: 235,952\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 4160)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 5952)         0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 5952)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           95248       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 163,568\n",
      "Trainable params: 162,928\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 1344)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1920)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1920)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           30736       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 119,856\n",
      "Trainable params: 119,088\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5614 - acc: 0.3341\n",
      "Epoch 00001: val_loss improved from inf to 1.99982, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv_checkpoint/001-1.9998.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 2.5614 - acc: 0.3341 - val_loss: 1.9998 - val_acc: 0.4072\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.5683\n",
      "Epoch 00002: val_loss improved from 1.99982 to 1.93119, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv_checkpoint/002-1.9312.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 1.4454 - acc: 0.5683 - val_loss: 1.9312 - val_acc: 0.4598\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0327 - acc: 0.6761\n",
      "Epoch 00003: val_loss did not improve from 1.93119\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 1.0327 - acc: 0.6761 - val_loss: 1.9923 - val_acc: 0.4736\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7375\n",
      "Epoch 00004: val_loss improved from 1.93119 to 1.86686, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv_checkpoint/004-1.8669.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.8267 - acc: 0.7375 - val_loss: 1.8669 - val_acc: 0.5134\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6848 - acc: 0.7815\n",
      "Epoch 00005: val_loss did not improve from 1.86686\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.6848 - acc: 0.7816 - val_loss: 2.0473 - val_acc: 0.5206\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8159\n",
      "Epoch 00006: val_loss improved from 1.86686 to 1.75730, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv_checkpoint/006-1.7573.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5740 - acc: 0.8159 - val_loss: 1.7573 - val_acc: 0.5579\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.8398\n",
      "Epoch 00007: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.4993 - acc: 0.8398 - val_loss: 1.8060 - val_acc: 0.5684\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8614\n",
      "Epoch 00008: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.4369 - acc: 0.8615 - val_loss: 1.8740 - val_acc: 0.5572\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8734\n",
      "Epoch 00009: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.3911 - acc: 0.8734 - val_loss: 1.9436 - val_acc: 0.5595\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8843\n",
      "Epoch 00010: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.3549 - acc: 0.8843 - val_loss: 2.2476 - val_acc: 0.5379\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8955\n",
      "Epoch 00011: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.3237 - acc: 0.8955 - val_loss: 2.2469 - val_acc: 0.5537\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9004\n",
      "Epoch 00012: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.3108 - acc: 0.9003 - val_loss: 2.2883 - val_acc: 0.5549\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9127\n",
      "Epoch 00013: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2773 - acc: 0.9127 - val_loss: 3.1231 - val_acc: 0.4743\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9152\n",
      "Epoch 00014: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2708 - acc: 0.9153 - val_loss: 2.2014 - val_acc: 0.5721\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9255\n",
      "Epoch 00015: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2335 - acc: 0.9256 - val_loss: 2.2841 - val_acc: 0.5691\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9264\n",
      "Epoch 00016: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2257 - acc: 0.9264 - val_loss: 2.2387 - val_acc: 0.5716\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9331\n",
      "Epoch 00017: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2160 - acc: 0.9331 - val_loss: 2.4945 - val_acc: 0.5816\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9345\n",
      "Epoch 00018: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2072 - acc: 0.9344 - val_loss: 2.3324 - val_acc: 0.5840\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9395\n",
      "Epoch 00019: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1947 - acc: 0.9395 - val_loss: 2.3861 - val_acc: 0.5816\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9444\n",
      "Epoch 00020: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1779 - acc: 0.9444 - val_loss: 2.2159 - val_acc: 0.5991\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9490\n",
      "Epoch 00021: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1689 - acc: 0.9490 - val_loss: 2.1651 - val_acc: 0.6177\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9452\n",
      "Epoch 00022: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1751 - acc: 0.9452 - val_loss: 2.2468 - val_acc: 0.6143\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9489\n",
      "Epoch 00023: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1634 - acc: 0.9489 - val_loss: 2.3307 - val_acc: 0.6131\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9512\n",
      "Epoch 00024: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1602 - acc: 0.9512 - val_loss: 2.1579 - val_acc: 0.6129\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9523\n",
      "Epoch 00025: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1487 - acc: 0.9523 - val_loss: 2.4303 - val_acc: 0.5884\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9566\n",
      "Epoch 00026: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1435 - acc: 0.9566 - val_loss: 2.6702 - val_acc: 0.5819\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9542\n",
      "Epoch 00027: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1504 - acc: 0.9542 - val_loss: 2.5775 - val_acc: 0.5919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9556\n",
      "Epoch 00028: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1485 - acc: 0.9556 - val_loss: 2.3700 - val_acc: 0.6129\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9570\n",
      "Epoch 00029: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1415 - acc: 0.9570 - val_loss: 2.8935 - val_acc: 0.5737\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9624\n",
      "Epoch 00030: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1262 - acc: 0.9625 - val_loss: 2.3900 - val_acc: 0.6145\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9639\n",
      "Epoch 00031: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1247 - acc: 0.9639 - val_loss: 2.3555 - val_acc: 0.6222\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9630\n",
      "Epoch 00032: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1307 - acc: 0.9630 - val_loss: 2.8392 - val_acc: 0.5791\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9645\n",
      "Epoch 00033: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1239 - acc: 0.9645 - val_loss: 3.1054 - val_acc: 0.5509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9630\n",
      "Epoch 00034: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1288 - acc: 0.9630 - val_loss: 2.7432 - val_acc: 0.5931\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9673\n",
      "Epoch 00035: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1123 - acc: 0.9673 - val_loss: 2.5842 - val_acc: 0.6105\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9669\n",
      "Epoch 00036: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1142 - acc: 0.9669 - val_loss: 3.1490 - val_acc: 0.5653\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9699\n",
      "Epoch 00037: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1032 - acc: 0.9699 - val_loss: 2.8498 - val_acc: 0.5893\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9684\n",
      "Epoch 00038: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1130 - acc: 0.9684 - val_loss: 2.6242 - val_acc: 0.6056\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9688\n",
      "Epoch 00039: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1099 - acc: 0.9688 - val_loss: 3.2377 - val_acc: 0.5672\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9733\n",
      "Epoch 00040: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0966 - acc: 0.9733 - val_loss: 2.5177 - val_acc: 0.6348\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9680\n",
      "Epoch 00041: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1135 - acc: 0.9679 - val_loss: 2.6376 - val_acc: 0.6026\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9722\n",
      "Epoch 00042: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1014 - acc: 0.9722 - val_loss: 2.7703 - val_acc: 0.6038\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9748\n",
      "Epoch 00043: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0908 - acc: 0.9748 - val_loss: 2.6237 - val_acc: 0.6157\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9711\n",
      "Epoch 00044: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1064 - acc: 0.9711 - val_loss: 2.4530 - val_acc: 0.6357\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9743\n",
      "Epoch 00045: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0975 - acc: 0.9743 - val_loss: 2.6253 - val_acc: 0.6289\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9713\n",
      "Epoch 00046: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1022 - acc: 0.9713 - val_loss: 2.9318 - val_acc: 0.5779\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9719\n",
      "Epoch 00047: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0999 - acc: 0.9719 - val_loss: 2.8615 - val_acc: 0.6129\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9741\n",
      "Epoch 00048: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0954 - acc: 0.9741 - val_loss: 2.6475 - val_acc: 0.6355\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9785\n",
      "Epoch 00049: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0806 - acc: 0.9784 - val_loss: 2.8555 - val_acc: 0.5919\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9749\n",
      "Epoch 00050: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0904 - acc: 0.9749 - val_loss: 2.8841 - val_acc: 0.6094\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9771\n",
      "Epoch 00051: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0853 - acc: 0.9771 - val_loss: 2.8527 - val_acc: 0.6038\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9787\n",
      "Epoch 00052: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0791 - acc: 0.9787 - val_loss: 2.7182 - val_acc: 0.6208\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9760\n",
      "Epoch 00053: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0872 - acc: 0.9760 - val_loss: 2.8336 - val_acc: 0.6119\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9782\n",
      "Epoch 00054: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0827 - acc: 0.9782 - val_loss: 2.7409 - val_acc: 0.6208\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9785\n",
      "Epoch 00055: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0750 - acc: 0.9785 - val_loss: 2.6826 - val_acc: 0.6303\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9770\n",
      "Epoch 00056: val_loss did not improve from 1.75730\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0875 - acc: 0.9770 - val_loss: 2.7032 - val_acc: 0.6292\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 393us/sample - loss: 1.9066 - acc: 0.5223\n",
      "Loss: 1.9066379495870287 Accuracy: 0.52232605\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4339 - acc: 0.3299\n",
      "Epoch 00001: val_loss improved from inf to 1.98249, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv_checkpoint/001-1.9825.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 2.4338 - acc: 0.3299 - val_loss: 1.9825 - val_acc: 0.3655\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5320 - acc: 0.5347\n",
      "Epoch 00002: val_loss improved from 1.98249 to 1.34411, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv_checkpoint/002-1.3441.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 1.5321 - acc: 0.5347 - val_loss: 1.3441 - val_acc: 0.5903\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2077 - acc: 0.6257\n",
      "Epoch 00003: val_loss improved from 1.34411 to 1.29591, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv_checkpoint/003-1.2959.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 1.2079 - acc: 0.6256 - val_loss: 1.2959 - val_acc: 0.6059\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0210 - acc: 0.6792\n",
      "Epoch 00004: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 1.0213 - acc: 0.6791 - val_loss: 1.5297 - val_acc: 0.5577\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8893 - acc: 0.7190\n",
      "Epoch 00005: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.8893 - acc: 0.7190 - val_loss: 1.7632 - val_acc: 0.5416\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7642 - acc: 0.7566\n",
      "Epoch 00006: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7646 - acc: 0.7565 - val_loss: 1.4050 - val_acc: 0.5966\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6957 - acc: 0.7736\n",
      "Epoch 00007: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.6959 - acc: 0.7735 - val_loss: 1.3395 - val_acc: 0.6278\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.8051\n",
      "Epoch 00008: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5983 - acc: 0.8052 - val_loss: 1.4375 - val_acc: 0.6024\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8240\n",
      "Epoch 00009: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5430 - acc: 0.8240 - val_loss: 1.5440 - val_acc: 0.6124\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.8389\n",
      "Epoch 00010: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4918 - acc: 0.8388 - val_loss: 1.5864 - val_acc: 0.6035\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8530\n",
      "Epoch 00011: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4500 - acc: 0.8529 - val_loss: 1.8581 - val_acc: 0.5551\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8654\n",
      "Epoch 00012: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4128 - acc: 0.8654 - val_loss: 1.3419 - val_acc: 0.6629\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8794\n",
      "Epoch 00013: val_loss did not improve from 1.29591\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3661 - acc: 0.8794 - val_loss: 1.3951 - val_acc: 0.6462\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8863\n",
      "Epoch 00014: val_loss improved from 1.29591 to 1.23553, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv_checkpoint/014-1.2355.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3444 - acc: 0.8863 - val_loss: 1.2355 - val_acc: 0.6925\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8918\n",
      "Epoch 00015: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3251 - acc: 0.8918 - val_loss: 1.3156 - val_acc: 0.6769\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9036\n",
      "Epoch 00016: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.2898 - acc: 0.9036 - val_loss: 1.6069 - val_acc: 0.6219\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9066\n",
      "Epoch 00017: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.2778 - acc: 0.9065 - val_loss: 1.5938 - val_acc: 0.6406\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9076\n",
      "Epoch 00018: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.2740 - acc: 0.9076 - val_loss: 1.3193 - val_acc: 0.6837\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9182\n",
      "Epoch 00019: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.2444 - acc: 0.9182 - val_loss: 1.4785 - val_acc: 0.6553\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9247\n",
      "Epoch 00020: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.2278 - acc: 0.9247 - val_loss: 1.4743 - val_acc: 0.6685\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9254\n",
      "Epoch 00021: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.2262 - acc: 0.9254 - val_loss: 1.8266 - val_acc: 0.6224\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9302\n",
      "Epoch 00022: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.2112 - acc: 0.9302 - val_loss: 1.4993 - val_acc: 0.6804\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9374\n",
      "Epoch 00023: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.1941 - acc: 0.9374 - val_loss: 1.3922 - val_acc: 0.6988\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9354\n",
      "Epoch 00024: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.1957 - acc: 0.9354 - val_loss: 2.0196 - val_acc: 0.6101\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9406\n",
      "Epoch 00025: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1824 - acc: 0.9406 - val_loss: 1.6427 - val_acc: 0.6536\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9442\n",
      "Epoch 00026: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.1698 - acc: 0.9442 - val_loss: 1.7264 - val_acc: 0.6364\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9425\n",
      "Epoch 00027: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.1733 - acc: 0.9425 - val_loss: 2.0204 - val_acc: 0.6166\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9489\n",
      "Epoch 00028: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1580 - acc: 0.9489 - val_loss: 1.5510 - val_acc: 0.6788\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9540\n",
      "Epoch 00029: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1448 - acc: 0.9541 - val_loss: 1.4796 - val_acc: 0.6897\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9520\n",
      "Epoch 00030: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1490 - acc: 0.9519 - val_loss: 1.6395 - val_acc: 0.6778\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9533\n",
      "Epoch 00031: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1434 - acc: 0.9533 - val_loss: 1.9644 - val_acc: 0.6168\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9535\n",
      "Epoch 00032: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.1426 - acc: 0.9535 - val_loss: 1.7342 - val_acc: 0.6580\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9548\n",
      "Epoch 00033: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1398 - acc: 0.9548 - val_loss: 1.4584 - val_acc: 0.6953\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9581\n",
      "Epoch 00034: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1310 - acc: 0.9580 - val_loss: 1.5648 - val_acc: 0.7014\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9579\n",
      "Epoch 00035: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.1282 - acc: 0.9579 - val_loss: 1.6074 - val_acc: 0.6781\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9611\n",
      "Epoch 00036: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1219 - acc: 0.9611 - val_loss: 1.7764 - val_acc: 0.6706\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9595\n",
      "Epoch 00037: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1228 - acc: 0.9595 - val_loss: 1.5814 - val_acc: 0.7016\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9627\n",
      "Epoch 00038: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1148 - acc: 0.9627 - val_loss: 1.6770 - val_acc: 0.6795\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9626\n",
      "Epoch 00039: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1141 - acc: 0.9626 - val_loss: 1.6077 - val_acc: 0.6993\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9659\n",
      "Epoch 00040: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.1077 - acc: 0.9659 - val_loss: 1.6485 - val_acc: 0.6874\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9664\n",
      "Epoch 00041: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1050 - acc: 0.9664 - val_loss: 1.6002 - val_acc: 0.6900\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9667\n",
      "Epoch 00042: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1063 - acc: 0.9667 - val_loss: 2.2636 - val_acc: 0.6166\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9664\n",
      "Epoch 00043: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1070 - acc: 0.9664 - val_loss: 1.6753 - val_acc: 0.6785\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9669\n",
      "Epoch 00044: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.1036 - acc: 0.9669 - val_loss: 1.5383 - val_acc: 0.7170\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9675\n",
      "Epoch 00045: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1030 - acc: 0.9675 - val_loss: 1.6538 - val_acc: 0.6888\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9692\n",
      "Epoch 00046: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0954 - acc: 0.9692 - val_loss: 1.4510 - val_acc: 0.7126\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9713\n",
      "Epoch 00047: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0920 - acc: 0.9713 - val_loss: 1.7461 - val_acc: 0.6783\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9709\n",
      "Epoch 00048: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0944 - acc: 0.9708 - val_loss: 1.8677 - val_acc: 0.6778\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9714\n",
      "Epoch 00049: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0917 - acc: 0.9714 - val_loss: 1.6410 - val_acc: 0.6923\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9725\n",
      "Epoch 00050: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 1.5927 - val_acc: 0.7149\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9717\n",
      "Epoch 00051: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0883 - acc: 0.9716 - val_loss: 1.7607 - val_acc: 0.6935\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9745\n",
      "Epoch 00052: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0824 - acc: 0.9745 - val_loss: 1.5549 - val_acc: 0.7119\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9742\n",
      "Epoch 00053: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0817 - acc: 0.9742 - val_loss: 1.4633 - val_acc: 0.7219\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9756\n",
      "Epoch 00054: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0806 - acc: 0.9756 - val_loss: 1.8366 - val_acc: 0.6851\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9731\n",
      "Epoch 00055: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0857 - acc: 0.9731 - val_loss: 1.6208 - val_acc: 0.7028\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9740\n",
      "Epoch 00056: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0820 - acc: 0.9739 - val_loss: 1.4904 - val_acc: 0.7251\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9753\n",
      "Epoch 00057: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0792 - acc: 0.9753 - val_loss: 1.5470 - val_acc: 0.7226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9774\n",
      "Epoch 00058: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0703 - acc: 0.9774 - val_loss: 1.4849 - val_acc: 0.7326\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9774\n",
      "Epoch 00059: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0713 - acc: 0.9774 - val_loss: 1.6546 - val_acc: 0.7130\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9749\n",
      "Epoch 00060: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0810 - acc: 0.9749 - val_loss: 1.5804 - val_acc: 0.7109\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9770\n",
      "Epoch 00061: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0751 - acc: 0.9770 - val_loss: 2.1597 - val_acc: 0.6636\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9797\n",
      "Epoch 00062: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0662 - acc: 0.9797 - val_loss: 1.4646 - val_acc: 0.7358\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9782\n",
      "Epoch 00063: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0689 - acc: 0.9782 - val_loss: 1.6108 - val_acc: 0.7137\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9784\n",
      "Epoch 00064: val_loss did not improve from 1.23553\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0713 - acc: 0.9783 - val_loss: 1.8476 - val_acc: 0.6974\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 405us/sample - loss: 1.2882 - acc: 0.6754\n",
      "Loss: 1.288201423696516 Accuracy: 0.6753894\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4125 - acc: 0.3157\n",
      "Epoch 00001: val_loss improved from inf to 1.64657, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/001-1.6466.hdf5\n",
      "36805/36805 [==============================] - 36s 986us/sample - loss: 2.4127 - acc: 0.3157 - val_loss: 1.6466 - val_acc: 0.4500\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5323 - acc: 0.5297\n",
      "Epoch 00002: val_loss improved from 1.64657 to 1.16508, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/002-1.1651.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 1.5317 - acc: 0.5298 - val_loss: 1.1651 - val_acc: 0.6408\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2631 - acc: 0.6114\n",
      "Epoch 00003: val_loss did not improve from 1.16508\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 1.2631 - acc: 0.6114 - val_loss: 1.2079 - val_acc: 0.6264\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1054 - acc: 0.6610\n",
      "Epoch 00004: val_loss improved from 1.16508 to 0.94599, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/004-0.9460.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.1054 - acc: 0.6609 - val_loss: 0.9460 - val_acc: 0.7254\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9941 - acc: 0.6968\n",
      "Epoch 00005: val_loss did not improve from 0.94599\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.9941 - acc: 0.6968 - val_loss: 0.9573 - val_acc: 0.7004\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9082 - acc: 0.7216\n",
      "Epoch 00006: val_loss did not improve from 0.94599\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.9086 - acc: 0.7215 - val_loss: 1.1293 - val_acc: 0.6762\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8358 - acc: 0.7451\n",
      "Epoch 00007: val_loss did not improve from 0.94599\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.8358 - acc: 0.7451 - val_loss: 0.9599 - val_acc: 0.7212\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7623\n",
      "Epoch 00008: val_loss improved from 0.94599 to 0.91139, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/008-0.9114.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.7769 - acc: 0.7623 - val_loss: 0.9114 - val_acc: 0.7440\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7241 - acc: 0.7753\n",
      "Epoch 00009: val_loss improved from 0.91139 to 0.90154, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/009-0.9015.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.7241 - acc: 0.7753 - val_loss: 0.9015 - val_acc: 0.7433\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6886 - acc: 0.7859\n",
      "Epoch 00010: val_loss improved from 0.90154 to 0.82030, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/010-0.8203.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.6887 - acc: 0.7858 - val_loss: 0.8203 - val_acc: 0.7622\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.8007\n",
      "Epoch 00011: val_loss did not improve from 0.82030\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.6438 - acc: 0.8007 - val_loss: 0.8813 - val_acc: 0.7501\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6111 - acc: 0.8096\n",
      "Epoch 00012: val_loss did not improve from 0.82030\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.6111 - acc: 0.8096 - val_loss: 1.1510 - val_acc: 0.6893\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5757 - acc: 0.8206\n",
      "Epoch 00013: val_loss did not improve from 0.82030\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.5760 - acc: 0.8205 - val_loss: 0.8638 - val_acc: 0.7619\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8297\n",
      "Epoch 00014: val_loss did not improve from 0.82030\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.5421 - acc: 0.8297 - val_loss: 0.8535 - val_acc: 0.7771\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8365\n",
      "Epoch 00015: val_loss did not improve from 0.82030\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.5186 - acc: 0.8363 - val_loss: 0.8544 - val_acc: 0.7605\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8476\n",
      "Epoch 00016: val_loss improved from 0.82030 to 0.80378, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/016-0.8038.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4879 - acc: 0.8476 - val_loss: 0.8038 - val_acc: 0.7720\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8528\n",
      "Epoch 00017: val_loss did not improve from 0.80378\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4641 - acc: 0.8529 - val_loss: 1.2111 - val_acc: 0.6755\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8609\n",
      "Epoch 00018: val_loss did not improve from 0.80378\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4427 - acc: 0.8609 - val_loss: 0.8234 - val_acc: 0.7834\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8652\n",
      "Epoch 00019: val_loss did not improve from 0.80378\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4242 - acc: 0.8652 - val_loss: 0.8051 - val_acc: 0.7827\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8690\n",
      "Epoch 00020: val_loss improved from 0.80378 to 0.69388, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/020-0.6939.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.4101 - acc: 0.8690 - val_loss: 0.6939 - val_acc: 0.8148\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8766\n",
      "Epoch 00021: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.3868 - acc: 0.8766 - val_loss: 0.7664 - val_acc: 0.7836\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8804\n",
      "Epoch 00022: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3724 - acc: 0.8804 - val_loss: 0.7083 - val_acc: 0.8064\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8889\n",
      "Epoch 00023: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.3537 - acc: 0.8889 - val_loss: 0.8207 - val_acc: 0.7913\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8920\n",
      "Epoch 00024: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.3397 - acc: 0.8920 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8966\n",
      "Epoch 00025: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.3256 - acc: 0.8965 - val_loss: 0.7031 - val_acc: 0.8148\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8987\n",
      "Epoch 00026: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.3200 - acc: 0.8986 - val_loss: 0.9677 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9014\n",
      "Epoch 00027: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.3063 - acc: 0.9014 - val_loss: 0.8223 - val_acc: 0.7813\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9078\n",
      "Epoch 00028: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2851 - acc: 0.9078 - val_loss: 0.8491 - val_acc: 0.7787\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9060\n",
      "Epoch 00029: val_loss did not improve from 0.69388\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2892 - acc: 0.9060 - val_loss: 0.8134 - val_acc: 0.7929\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9120\n",
      "Epoch 00030: val_loss improved from 0.69388 to 0.68916, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/030-0.6892.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.2728 - acc: 0.9119 - val_loss: 0.6892 - val_acc: 0.8225\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9167\n",
      "Epoch 00031: val_loss did not improve from 0.68916\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2631 - acc: 0.9167 - val_loss: 0.7097 - val_acc: 0.8132\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9183\n",
      "Epoch 00032: val_loss did not improve from 0.68916\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.2574 - acc: 0.9183 - val_loss: 0.7943 - val_acc: 0.7971\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9204\n",
      "Epoch 00033: val_loss improved from 0.68916 to 0.67883, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/033-0.6788.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.2449 - acc: 0.9204 - val_loss: 0.6788 - val_acc: 0.8225\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9226\n",
      "Epoch 00034: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2405 - acc: 0.9225 - val_loss: 0.8840 - val_acc: 0.7661\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9262\n",
      "Epoch 00035: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2311 - acc: 0.9262 - val_loss: 0.7193 - val_acc: 0.8190\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9298\n",
      "Epoch 00036: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2229 - acc: 0.9299 - val_loss: 0.7632 - val_acc: 0.8001\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9308\n",
      "Epoch 00037: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.2107 - acc: 0.9309 - val_loss: 1.0007 - val_acc: 0.7580\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9314\n",
      "Epoch 00038: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2135 - acc: 0.9313 - val_loss: 0.6957 - val_acc: 0.8248\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9334\n",
      "Epoch 00039: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.2086 - acc: 0.9333 - val_loss: 0.8410 - val_acc: 0.7941\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9363\n",
      "Epoch 00040: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1984 - acc: 0.9363 - val_loss: 0.9028 - val_acc: 0.7796\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9365\n",
      "Epoch 00041: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1979 - acc: 0.9365 - val_loss: 0.8920 - val_acc: 0.7855\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9350\n",
      "Epoch 00042: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1992 - acc: 0.9349 - val_loss: 0.8135 - val_acc: 0.8001\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9382\n",
      "Epoch 00043: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1870 - acc: 0.9382 - val_loss: 0.7981 - val_acc: 0.8078\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9412\n",
      "Epoch 00044: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1801 - acc: 0.9412 - val_loss: 0.9072 - val_acc: 0.7754\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9433\n",
      "Epoch 00045: val_loss did not improve from 0.67883\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1746 - acc: 0.9433 - val_loss: 0.7448 - val_acc: 0.8309\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9465\n",
      "Epoch 00046: val_loss improved from 0.67883 to 0.67312, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/046-0.6731.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1677 - acc: 0.9465 - val_loss: 0.6731 - val_acc: 0.8360\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9442\n",
      "Epoch 00047: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1722 - acc: 0.9442 - val_loss: 0.7060 - val_acc: 0.8253\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9460\n",
      "Epoch 00048: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1681 - acc: 0.9459 - val_loss: 0.8080 - val_acc: 0.7983\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9477\n",
      "Epoch 00049: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1610 - acc: 0.9476 - val_loss: 0.6749 - val_acc: 0.8376\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9487\n",
      "Epoch 00050: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1582 - acc: 0.9487 - val_loss: 0.8990 - val_acc: 0.7873\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9492\n",
      "Epoch 00051: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1576 - acc: 0.9492 - val_loss: 0.8535 - val_acc: 0.7990\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9514\n",
      "Epoch 00052: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1491 - acc: 0.9514 - val_loss: 0.6958 - val_acc: 0.8351\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9534\n",
      "Epoch 00053: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1478 - acc: 0.9533 - val_loss: 0.7660 - val_acc: 0.8293\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9518\n",
      "Epoch 00054: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1476 - acc: 0.9519 - val_loss: 0.7473 - val_acc: 0.8246\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9558\n",
      "Epoch 00055: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1381 - acc: 0.9557 - val_loss: 0.8328 - val_acc: 0.8046\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9526\n",
      "Epoch 00056: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1473 - acc: 0.9525 - val_loss: 0.8493 - val_acc: 0.8181\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9565\n",
      "Epoch 00057: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1369 - acc: 0.9565 - val_loss: 0.8045 - val_acc: 0.8078\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9576\n",
      "Epoch 00058: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1325 - acc: 0.9576 - val_loss: 0.7762 - val_acc: 0.8239\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9590\n",
      "Epoch 00059: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1294 - acc: 0.9590 - val_loss: 0.7378 - val_acc: 0.8258\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9587\n",
      "Epoch 00060: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1302 - acc: 0.9587 - val_loss: 0.7095 - val_acc: 0.8304\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9586\n",
      "Epoch 00061: val_loss did not improve from 0.67312\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1279 - acc: 0.9586 - val_loss: 0.8302 - val_acc: 0.8118\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9594\n",
      "Epoch 00062: val_loss improved from 0.67312 to 0.66015, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv_checkpoint/062-0.6601.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1270 - acc: 0.9594 - val_loss: 0.6601 - val_acc: 0.8423\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9609\n",
      "Epoch 00063: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1226 - acc: 0.9609 - val_loss: 0.6694 - val_acc: 0.8495\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9628\n",
      "Epoch 00064: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1195 - acc: 0.9628 - val_loss: 0.7243 - val_acc: 0.8365\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9608\n",
      "Epoch 00065: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1226 - acc: 0.9608 - val_loss: 0.7395 - val_acc: 0.8325\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9631\n",
      "Epoch 00066: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1170 - acc: 0.9630 - val_loss: 0.7438 - val_acc: 0.8290\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9627\n",
      "Epoch 00067: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1181 - acc: 0.9626 - val_loss: 0.7213 - val_acc: 0.8348\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9638\n",
      "Epoch 00068: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.1150 - acc: 0.9638 - val_loss: 0.6988 - val_acc: 0.8507\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9645\n",
      "Epoch 00069: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1120 - acc: 0.9645 - val_loss: 0.8227 - val_acc: 0.8167\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9661\n",
      "Epoch 00070: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1065 - acc: 0.9661 - val_loss: 0.7602 - val_acc: 0.8339\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9659\n",
      "Epoch 00071: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1076 - acc: 0.9658 - val_loss: 0.7527 - val_acc: 0.8372\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9662\n",
      "Epoch 00072: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1075 - acc: 0.9662 - val_loss: 0.8684 - val_acc: 0.8102\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9673\n",
      "Epoch 00073: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1047 - acc: 0.9672 - val_loss: 0.6771 - val_acc: 0.8467\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9674\n",
      "Epoch 00074: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1037 - acc: 0.9674 - val_loss: 0.7683 - val_acc: 0.8295\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9671\n",
      "Epoch 00075: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1038 - acc: 0.9670 - val_loss: 0.7431 - val_acc: 0.8372\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9695\n",
      "Epoch 00076: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0971 - acc: 0.9695 - val_loss: 0.7068 - val_acc: 0.8456\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9691\n",
      "Epoch 00077: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0988 - acc: 0.9691 - val_loss: 0.7288 - val_acc: 0.8409\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9687\n",
      "Epoch 00078: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1017 - acc: 0.9687 - val_loss: 0.8464 - val_acc: 0.8157\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9694\n",
      "Epoch 00079: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0942 - acc: 0.9694 - val_loss: 0.7432 - val_acc: 0.8339\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9699\n",
      "Epoch 00080: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0933 - acc: 0.9699 - val_loss: 0.6790 - val_acc: 0.8491\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9706\n",
      "Epoch 00081: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0941 - acc: 0.9707 - val_loss: 0.7926 - val_acc: 0.8314\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9700\n",
      "Epoch 00082: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0951 - acc: 0.9700 - val_loss: 0.9140 - val_acc: 0.8053\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9744\n",
      "Epoch 00083: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0861 - acc: 0.9744 - val_loss: 1.1287 - val_acc: 0.7615\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9708\n",
      "Epoch 00084: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0911 - acc: 0.9708 - val_loss: 0.7872 - val_acc: 0.8293\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9719\n",
      "Epoch 00085: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0899 - acc: 0.9719 - val_loss: 0.8573 - val_acc: 0.8153\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9719\n",
      "Epoch 00086: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0901 - acc: 0.9718 - val_loss: 0.7060 - val_acc: 0.8444\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9745\n",
      "Epoch 00087: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0824 - acc: 0.9744 - val_loss: 0.7691 - val_acc: 0.8344\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9724\n",
      "Epoch 00088: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0885 - acc: 0.9724 - val_loss: 0.7283 - val_acc: 0.8463\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9754\n",
      "Epoch 00089: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0797 - acc: 0.9753 - val_loss: 0.8486 - val_acc: 0.8295\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9733\n",
      "Epoch 00090: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0874 - acc: 0.9733 - val_loss: 0.9144 - val_acc: 0.8055\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9759\n",
      "Epoch 00091: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0819 - acc: 0.9759 - val_loss: 0.6879 - val_acc: 0.8523\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9745\n",
      "Epoch 00092: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0818 - acc: 0.9745 - val_loss: 0.7153 - val_acc: 0.8451\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9739\n",
      "Epoch 00093: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0833 - acc: 0.9739 - val_loss: 0.7200 - val_acc: 0.8486\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9757\n",
      "Epoch 00094: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0794 - acc: 0.9757 - val_loss: 0.7279 - val_acc: 0.8451\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9754\n",
      "Epoch 00095: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0806 - acc: 0.9754 - val_loss: 0.7353 - val_acc: 0.8351\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9752\n",
      "Epoch 00096: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0807 - acc: 0.9752 - val_loss: 0.7128 - val_acc: 0.8463\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9763\n",
      "Epoch 00097: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0780 - acc: 0.9763 - val_loss: 0.7469 - val_acc: 0.8446\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9767\n",
      "Epoch 00098: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.0761 - acc: 0.9767 - val_loss: 0.7693 - val_acc: 0.8344\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9763\n",
      "Epoch 00099: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0774 - acc: 0.9763 - val_loss: 0.7292 - val_acc: 0.8474\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9768\n",
      "Epoch 00100: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0777 - acc: 0.9767 - val_loss: 1.2256 - val_acc: 0.7708\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9740\n",
      "Epoch 00101: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0860 - acc: 0.9741 - val_loss: 0.7051 - val_acc: 0.8521\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9782\n",
      "Epoch 00102: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0726 - acc: 0.9782 - val_loss: 0.7460 - val_acc: 0.8444\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9782\n",
      "Epoch 00103: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0712 - acc: 0.9782 - val_loss: 0.8043 - val_acc: 0.8365\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9779\n",
      "Epoch 00104: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.0726 - acc: 0.9779 - val_loss: 0.7238 - val_acc: 0.8516\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9792\n",
      "Epoch 00105: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0689 - acc: 0.9792 - val_loss: 0.6933 - val_acc: 0.8514\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9775\n",
      "Epoch 00106: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0730 - acc: 0.9775 - val_loss: 0.8587 - val_acc: 0.8269\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9797\n",
      "Epoch 00107: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0666 - acc: 0.9797 - val_loss: 0.7304 - val_acc: 0.8456\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9801\n",
      "Epoch 00108: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0679 - acc: 0.9802 - val_loss: 0.7377 - val_acc: 0.8348\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9786\n",
      "Epoch 00109: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0698 - acc: 0.9786 - val_loss: 0.7791 - val_acc: 0.8358\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9788\n",
      "Epoch 00110: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0680 - acc: 0.9788 - val_loss: 0.7805 - val_acc: 0.8416\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9789\n",
      "Epoch 00111: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0691 - acc: 0.9789 - val_loss: 0.7090 - val_acc: 0.8486\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9804\n",
      "Epoch 00112: val_loss did not improve from 0.66015\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.0664 - acc: 0.9804 - val_loss: 0.6950 - val_acc: 0.8565\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 443us/sample - loss: 0.7644 - acc: 0.8154\n",
      "Loss: 0.7643917271281329 Accuracy: 0.81536865\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2857 - acc: 0.3218\n",
      "Epoch 00001: val_loss improved from inf to 1.87342, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/001-1.8734.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 2.2858 - acc: 0.3218 - val_loss: 1.8734 - val_acc: 0.4009\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4960 - acc: 0.5277\n",
      "Epoch 00002: val_loss improved from 1.87342 to 1.14352, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/002-1.1435.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 1.4960 - acc: 0.5278 - val_loss: 1.1435 - val_acc: 0.6504\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1981 - acc: 0.6253\n",
      "Epoch 00003: val_loss improved from 1.14352 to 0.99797, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/003-0.9980.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 1.1981 - acc: 0.6253 - val_loss: 0.9980 - val_acc: 0.6916\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0137 - acc: 0.6896\n",
      "Epoch 00004: val_loss improved from 0.99797 to 0.79479, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/004-0.7948.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 1.0136 - acc: 0.6896 - val_loss: 0.7948 - val_acc: 0.7694\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7312\n",
      "Epoch 00005: val_loss improved from 0.79479 to 0.74162, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/005-0.7416.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.8850 - acc: 0.7311 - val_loss: 0.7416 - val_acc: 0.7869\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7925 - acc: 0.7604\n",
      "Epoch 00006: val_loss improved from 0.74162 to 0.70031, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/006-0.7003.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.7924 - acc: 0.7604 - val_loss: 0.7003 - val_acc: 0.7952\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7865\n",
      "Epoch 00007: val_loss improved from 0.70031 to 0.60970, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/007-0.6097.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.7119 - acc: 0.7865 - val_loss: 0.6097 - val_acc: 0.8293\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.8033\n",
      "Epoch 00008: val_loss improved from 0.60970 to 0.56409, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/008-0.5641.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.6513 - acc: 0.8033 - val_loss: 0.5641 - val_acc: 0.8388\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.8204\n",
      "Epoch 00009: val_loss did not improve from 0.56409\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.6021 - acc: 0.8204 - val_loss: 0.5769 - val_acc: 0.8311\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8307\n",
      "Epoch 00010: val_loss improved from 0.56409 to 0.49238, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/010-0.4924.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.5643 - acc: 0.8306 - val_loss: 0.4924 - val_acc: 0.8593\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8419\n",
      "Epoch 00011: val_loss improved from 0.49238 to 0.47049, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/011-0.4705.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.5258 - acc: 0.8419 - val_loss: 0.4705 - val_acc: 0.8689\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8511\n",
      "Epoch 00012: val_loss improved from 0.47049 to 0.45913, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/012-0.4591.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.4930 - acc: 0.8511 - val_loss: 0.4591 - val_acc: 0.8693\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8593\n",
      "Epoch 00013: val_loss did not improve from 0.45913\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.4685 - acc: 0.8593 - val_loss: 0.6260 - val_acc: 0.8123\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8665\n",
      "Epoch 00014: val_loss did not improve from 0.45913\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.4442 - acc: 0.8666 - val_loss: 0.4641 - val_acc: 0.8661\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8753\n",
      "Epoch 00015: val_loss improved from 0.45913 to 0.42515, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/015-0.4251.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.4184 - acc: 0.8753 - val_loss: 0.4251 - val_acc: 0.8798\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8773\n",
      "Epoch 00016: val_loss improved from 0.42515 to 0.40451, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/016-0.4045.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.4010 - acc: 0.8773 - val_loss: 0.4045 - val_acc: 0.8863\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8843\n",
      "Epoch 00017: val_loss did not improve from 0.40451\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.3806 - acc: 0.8843 - val_loss: 0.4560 - val_acc: 0.8684\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8912\n",
      "Epoch 00018: val_loss did not improve from 0.40451\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.3599 - acc: 0.8912 - val_loss: 0.4412 - val_acc: 0.8742\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8917\n",
      "Epoch 00019: val_loss did not improve from 0.40451\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.3535 - acc: 0.8917 - val_loss: 0.4664 - val_acc: 0.8642\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8965\n",
      "Epoch 00020: val_loss improved from 0.40451 to 0.38850, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/020-0.3885.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3368 - acc: 0.8965 - val_loss: 0.3885 - val_acc: 0.8873\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.9018\n",
      "Epoch 00021: val_loss did not improve from 0.38850\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3254 - acc: 0.9018 - val_loss: 0.4824 - val_acc: 0.8588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9040\n",
      "Epoch 00022: val_loss did not improve from 0.38850\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3126 - acc: 0.9039 - val_loss: 0.4151 - val_acc: 0.8817\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9071\n",
      "Epoch 00023: val_loss did not improve from 0.38850\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3029 - acc: 0.9071 - val_loss: 0.3902 - val_acc: 0.8880\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9097\n",
      "Epoch 00024: val_loss improved from 0.38850 to 0.35860, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/024-0.3586.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2910 - acc: 0.9096 - val_loss: 0.3586 - val_acc: 0.9033\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9139\n",
      "Epoch 00025: val_loss did not improve from 0.35860\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2778 - acc: 0.9139 - val_loss: 0.3704 - val_acc: 0.8987\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9137\n",
      "Epoch 00026: val_loss improved from 0.35860 to 0.34856, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/026-0.3486.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2739 - acc: 0.9137 - val_loss: 0.3486 - val_acc: 0.9019\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9164\n",
      "Epoch 00027: val_loss did not improve from 0.34856\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2617 - acc: 0.9163 - val_loss: 0.3606 - val_acc: 0.9017\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9188\n",
      "Epoch 00028: val_loss did not improve from 0.34856\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2595 - acc: 0.9188 - val_loss: 0.3933 - val_acc: 0.8880\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9224- ETA\n",
      "Epoch 00029: val_loss improved from 0.34856 to 0.34491, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/029-0.3449.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2473 - acc: 0.9224 - val_loss: 0.3449 - val_acc: 0.9024\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9262\n",
      "Epoch 00030: val_loss did not improve from 0.34491\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2394 - acc: 0.9262 - val_loss: 0.3841 - val_acc: 0.8898\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9283\n",
      "Epoch 00031: val_loss did not improve from 0.34491\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2285 - acc: 0.9283 - val_loss: 0.4373 - val_acc: 0.8814\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9294\n",
      "Epoch 00032: val_loss did not improve from 0.34491\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2240 - acc: 0.9294 - val_loss: 0.3684 - val_acc: 0.8959\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9303\n",
      "Epoch 00033: val_loss improved from 0.34491 to 0.34059, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/033-0.3406.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2207 - acc: 0.9303 - val_loss: 0.3406 - val_acc: 0.9071\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9306\n",
      "Epoch 00034: val_loss did not improve from 0.34059\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2193 - acc: 0.9306 - val_loss: 0.3700 - val_acc: 0.8947\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9340\n",
      "Epoch 00035: val_loss did not improve from 0.34059\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2084 - acc: 0.9340 - val_loss: 0.3644 - val_acc: 0.8987\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9334\n",
      "Epoch 00036: val_loss improved from 0.34059 to 0.33800, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/036-0.3380.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2076 - acc: 0.9334 - val_loss: 0.3380 - val_acc: 0.9078\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9365\n",
      "Epoch 00037: val_loss did not improve from 0.33800\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1969 - acc: 0.9364 - val_loss: 0.3401 - val_acc: 0.9036\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9387\n",
      "Epoch 00038: val_loss improved from 0.33800 to 0.30867, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/038-0.3087.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1913 - acc: 0.9387 - val_loss: 0.3087 - val_acc: 0.9119\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9407\n",
      "Epoch 00039: val_loss did not improve from 0.30867\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1849 - acc: 0.9407 - val_loss: 0.3124 - val_acc: 0.9131\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9421\n",
      "Epoch 00040: val_loss did not improve from 0.30867\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1791 - acc: 0.9421 - val_loss: 0.3162 - val_acc: 0.9129\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9427\n",
      "Epoch 00041: val_loss improved from 0.30867 to 0.30684, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/041-0.3068.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1766 - acc: 0.9427 - val_loss: 0.3068 - val_acc: 0.9161\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9448\n",
      "Epoch 00042: val_loss did not improve from 0.30684\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1697 - acc: 0.9448 - val_loss: 0.3091 - val_acc: 0.9131\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9475\n",
      "Epoch 00043: val_loss did not improve from 0.30684\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1676 - acc: 0.9475 - val_loss: 0.3149 - val_acc: 0.9124\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9445\n",
      "Epoch 00044: val_loss did not improve from 0.30684\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1711 - acc: 0.9444 - val_loss: 0.3501 - val_acc: 0.9075\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9467\n",
      "Epoch 00045: val_loss did not improve from 0.30684\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1609 - acc: 0.9467 - val_loss: 0.3203 - val_acc: 0.9129\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9483\n",
      "Epoch 00046: val_loss improved from 0.30684 to 0.30035, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/046-0.3004.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1595 - acc: 0.9483 - val_loss: 0.3004 - val_acc: 0.9192\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9500\n",
      "Epoch 00047: val_loss did not improve from 0.30035\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1544 - acc: 0.9500 - val_loss: 0.3488 - val_acc: 0.9015\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9505\n",
      "Epoch 00048: val_loss did not improve from 0.30035\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1502 - acc: 0.9505 - val_loss: 0.3121 - val_acc: 0.9166\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9520\n",
      "Epoch 00049: val_loss did not improve from 0.30035\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1475 - acc: 0.9520 - val_loss: 0.3203 - val_acc: 0.9145\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9526\n",
      "Epoch 00050: val_loss improved from 0.30035 to 0.29317, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/050-0.2932.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1462 - acc: 0.9526 - val_loss: 0.2932 - val_acc: 0.9229\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9532\n",
      "Epoch 00051: val_loss did not improve from 0.29317\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1410 - acc: 0.9532 - val_loss: 0.3244 - val_acc: 0.9159\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9545\n",
      "Epoch 00052: val_loss did not improve from 0.29317\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1401 - acc: 0.9545 - val_loss: 0.4283 - val_acc: 0.8854\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9567\n",
      "Epoch 00053: val_loss improved from 0.29317 to 0.29110, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv_checkpoint/053-0.2911.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1352 - acc: 0.9567 - val_loss: 0.2911 - val_acc: 0.9220\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9569\n",
      "Epoch 00054: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1343 - acc: 0.9569 - val_loss: 0.3210 - val_acc: 0.9117\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9583\n",
      "Epoch 00055: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1283 - acc: 0.9583 - val_loss: 0.3368 - val_acc: 0.9119\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9589\n",
      "Epoch 00056: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1258 - acc: 0.9589 - val_loss: 0.3243 - val_acc: 0.9124\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9585\n",
      "Epoch 00057: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1273 - acc: 0.9585 - val_loss: 0.3199 - val_acc: 0.9157\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9597\n",
      "Epoch 00058: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1230 - acc: 0.9596 - val_loss: 0.3038 - val_acc: 0.9192\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9607\n",
      "Epoch 00059: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1215 - acc: 0.9607 - val_loss: 0.3158 - val_acc: 0.9131\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9636\n",
      "Epoch 00060: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1124 - acc: 0.9636 - val_loss: 0.2953 - val_acc: 0.9238\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9609\n",
      "Epoch 00061: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1171 - acc: 0.9609 - val_loss: 0.3531 - val_acc: 0.9057\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9643\n",
      "Epoch 00062: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1136 - acc: 0.9643 - val_loss: 0.3380 - val_acc: 0.9126\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9631\n",
      "Epoch 00063: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1122 - acc: 0.9631 - val_loss: 0.3116 - val_acc: 0.9194\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9652\n",
      "Epoch 00064: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1084 - acc: 0.9651 - val_loss: 0.3116 - val_acc: 0.9180\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9618\n",
      "Epoch 00065: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1148 - acc: 0.9619 - val_loss: 0.3097 - val_acc: 0.9196\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9656\n",
      "Epoch 00066: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1036 - acc: 0.9656 - val_loss: 0.3186 - val_acc: 0.9192\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9655\n",
      "Epoch 00067: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1074 - acc: 0.9655 - val_loss: 0.3162 - val_acc: 0.9213\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9673\n",
      "Epoch 00068: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1008 - acc: 0.9673 - val_loss: 0.3476 - val_acc: 0.9115\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9680\n",
      "Epoch 00069: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0990 - acc: 0.9679 - val_loss: 0.2941 - val_acc: 0.9236\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9682\n",
      "Epoch 00070: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0984 - acc: 0.9682 - val_loss: 0.3342 - val_acc: 0.9201\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9674\n",
      "Epoch 00071: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0978 - acc: 0.9674 - val_loss: 0.3138 - val_acc: 0.9203\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9688\n",
      "Epoch 00072: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0968 - acc: 0.9688 - val_loss: 0.3660 - val_acc: 0.9089\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9663\n",
      "Epoch 00073: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1031 - acc: 0.9663 - val_loss: 0.3399 - val_acc: 0.9124\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9705\n",
      "Epoch 00074: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0906 - acc: 0.9705 - val_loss: 0.3539 - val_acc: 0.9115\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9688\n",
      "Epoch 00075: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0946 - acc: 0.9688 - val_loss: 0.3375 - val_acc: 0.9185\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9701\n",
      "Epoch 00076: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0897 - acc: 0.9701 - val_loss: 0.2955 - val_acc: 0.9292\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9709\n",
      "Epoch 00077: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0892 - acc: 0.9709 - val_loss: 0.3125 - val_acc: 0.9243\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9715\n",
      "Epoch 00078: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0881 - acc: 0.9715 - val_loss: 0.3365 - val_acc: 0.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9714\n",
      "Epoch 00079: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0891 - acc: 0.9714 - val_loss: 0.3796 - val_acc: 0.9019\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9715\n",
      "Epoch 00080: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0862 - acc: 0.9715 - val_loss: 0.3338 - val_acc: 0.9117\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9729\n",
      "Epoch 00081: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0824 - acc: 0.9728 - val_loss: 0.3120 - val_acc: 0.9248\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9710\n",
      "Epoch 00082: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0870 - acc: 0.9710 - val_loss: 0.3337 - val_acc: 0.9220\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9715\n",
      "Epoch 00083: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0837 - acc: 0.9714 - val_loss: 0.3275 - val_acc: 0.9213\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9737\n",
      "Epoch 00084: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0809 - acc: 0.9737 - val_loss: 0.3313 - val_acc: 0.9196\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9742\n",
      "Epoch 00085: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0804 - acc: 0.9742 - val_loss: 0.3312 - val_acc: 0.9147\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9743\n",
      "Epoch 00086: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0801 - acc: 0.9743 - val_loss: 0.3925 - val_acc: 0.9033\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9744\n",
      "Epoch 00087: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0759 - acc: 0.9744 - val_loss: 0.3633 - val_acc: 0.9143\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9744\n",
      "Epoch 00088: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0766 - acc: 0.9744 - val_loss: 0.3160 - val_acc: 0.9257\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9743\n",
      "Epoch 00089: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0784 - acc: 0.9743 - val_loss: 0.3186 - val_acc: 0.9224\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9768\n",
      "Epoch 00090: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0701 - acc: 0.9768 - val_loss: 0.3261 - val_acc: 0.9213\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9750\n",
      "Epoch 00091: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0749 - acc: 0.9750 - val_loss: 0.3186 - val_acc: 0.9250\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9779\n",
      "Epoch 00092: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0718 - acc: 0.9779 - val_loss: 0.3187 - val_acc: 0.9262\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9769\n",
      "Epoch 00093: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0719 - acc: 0.9769 - val_loss: 0.3364 - val_acc: 0.9199\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9773\n",
      "Epoch 00094: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0694 - acc: 0.9772 - val_loss: 0.3420 - val_acc: 0.9187\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9767\n",
      "Epoch 00095: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0728 - acc: 0.9767 - val_loss: 0.3524 - val_acc: 0.9208\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9773\n",
      "Epoch 00096: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0693 - acc: 0.9773 - val_loss: 0.3150 - val_acc: 0.9257\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9786\n",
      "Epoch 00097: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0653 - acc: 0.9786 - val_loss: 0.3087 - val_acc: 0.9290\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9776\n",
      "Epoch 00098: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0686 - acc: 0.9775 - val_loss: 0.3555 - val_acc: 0.9161\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9781\n",
      "Epoch 00099: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0675 - acc: 0.9781 - val_loss: 0.3470 - val_acc: 0.9166\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9790\n",
      "Epoch 00100: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0665 - acc: 0.9790 - val_loss: 0.3802 - val_acc: 0.9115\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9783\n",
      "Epoch 00101: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0657 - acc: 0.9783 - val_loss: 0.3272 - val_acc: 0.9224\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9802\n",
      "Epoch 00102: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0628 - acc: 0.9802 - val_loss: 0.3306 - val_acc: 0.9224\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9804\n",
      "Epoch 00103: val_loss did not improve from 0.29110\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0625 - acc: 0.9804 - val_loss: 0.3313 - val_acc: 0.9234\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8V9X9+PHX+ewssghJCCPsEUaYxaKgVSyiUhSROqut+OvX1l2rtbW1VlurHe4qtu5JQas4i8pwoQRkCpRNNtn7s8/vj5NPEiAJAfIhIXk/H4/7+Hw+93Pvued+xnnfc8695yqtNUIIIQSApaMzIIQQovOQoCCEEKKBBAUhhBANJCgIIYRoIEFBCCFEAwkKQgghGkhQEEII0UCCghBCiAYSFIQQQjSwdXQGjlbPnj11enp6R2dDCCFOKmvXri3WWicdabmTLiikp6eTlZXV0dkQQoiTilJqX1uWk+YjIYQQDSQoCCGEaCBBQQghRIOTrk+hOT6fj5ycHNxud0dn5aTlcrno06cPdru9o7MihOhAXSIo5OTkEBMTQ3p6Okqpjs7OSUdrTUlJCTk5OQwYMKCjsyOE6EBdovnI7XaTmJgoAeEYKaVITEyUmpYQomsEBUACwnGSz08IAV0oKBxJIFCHx5NLMOjr6KwIIUSn1W2CQjDoxuvNR+v2Dwrl5eU88cQTx7TurFmzKC8vb/Pyd999N3/5y1+OaVtCCHEk3SYoKGV2Vetgu6fdWlDw+/2trvvee+8RFxfX7nkSQohj0W2CQuOutn9QuOOOO9i1axeZmZncdtttrFixgtNOO43Zs2czcuRIAObMmcOECRPIyMhg4cKFDeump6dTXFzM3r17GTFiBAsWLCAjI4Ozzz6burq6Vre7fv16pkyZwpgxY7jgggsoKysD4JFHHmHkyJGMGTOGH/7whwCsXLmSzMxMMjMzGTduHFVVVe3+OQghTn5d4pTUpnbsuInq6vXNvBMgEKjFYolAqaPb7ejoTIYMeajF9++//342b97M+vVmuytWrGDdunVs3ry54RTPZ555hoSEBOrq6pg0aRJz584lMTHxkLzv4NVXX+Xpp5/m4osvZsmSJVx++eUtbvfKK6/k0UcfZfr06fz2t7/l97//PQ899BD3338/e/bswel0NjRN/eUvf+Hxxx9n6tSpVFdX43K5juozEEJ0D92ophA6u0afkK1Nnjz5oHP+H3nkEcaOHcuUKVPIzs5mx44dh60zYMAAMjMzAZgwYQJ79+5tMf2KigrKy8uZPn06AD/60Y9YtWoVAGPGjOGyyy7jpZdewmYzAXDq1KnccsstPPLII5SXlzfMF0KIprpcydDSEX0w6KWmZiNOZ38cjiOOHnvcoqKiGp6vWLGCjz76iC+//JLIyEhOP/30Zq8JcDqdDc+tVusRm49a8u6777Jq1SqWLl3Kfffdx6ZNm7jjjjs499xzee+995g6dSoffvghw4cPP6b0hRBdVzeqKYSvTyEmJqbVNvqKigri4+OJjIxk27ZtrF69+ri3GRsbS3x8PJ9++ikAL774ItOnTycYDJKdnc0ZZ5zBn//8ZyoqKqiurmbXrl2MHj2a22+/nUmTJrFt27bjzoMQouvpcjWFloTz7KPExESmTp3KqFGjOOecczj33HMPen/mzJk8+eSTjBgxgmHDhjFlypR22e7zzz/PT3/6U2praxk4cCDPPvssgUCAyy+/nIqKCrTW3HDDDcTFxXHXXXexfPlyLBYLGRkZnHPOOe2SByFE16K0PjFt7O1l4sSJ+tCb7GzdupURI0a0up7WmurqtTgcqTidaeHM4kmrLZ+jEOLkpJRaq7WeeKTluk3zkRnGwYrWgY7OihBCdFrdJihAqAmp/ZuPhBCiq+hWQQEsYelTEEKIrqJbBQWlJCgIIURrulVQMLsrfQpCCNGSbhUUlLJKTUEIIVrRrYKC2d3OERSio6OPar4QQpwI3SooSJ+CEEK0rtsFhXANnf344483vA7dCKe6upozzzyT8ePHM3r0aN566602p6m15rbbbmPUqFGMHj2a119/HYD8/HymTZtGZmYmo0aN4tNPPyUQCHDVVVc1LPv3v/+93fdRCNE9dL1hLm66CdY3N3Q2OIIebNoH1qNsosnMhIdaHjp7/vz53HTTTfzsZz8DYNGiRXz44Ye4XC7efPNNevToQXFxMVOmTGH27Nltuh/yG2+8wfr169mwYQPFxcVMmjSJadOm8corr/D973+fX//61wQCAWpra1m/fj25ubls3rwZ4Kju5CaEEE11vaBwRBpN40Da7WHcuHEcOHCAvLw8ioqKiI+Pp2/fvvh8Pu68805WrVqFxWIhNzeXwsJCUlJSjpjmZ599xiWXXILVaiU5OZnp06ezZs0aJk2axI9//GN8Ph9z5swhMzOTgQMHsnv3bq6//nrOPfdczj777HbcOyFEd9L1gkIrR/Q+Tz5eby7R0eNBtW/L2bx581i8eDEFBQXMnz8fgJdffpmioiLWrl2L3W4nPT292SGzj8a0adNYtWoV7777LldddRW33HILV155JRs2bODDDz/kySefZNGiRTzzzDPtsVtCiG6mG/YphGek1Pnz5/Paa6+xePFi5s2bB5ghs3v16oXdbmf58uXs27evzemddtppvP766wQCAYqKili1ahWTJ09m3759JCcns2DBAq655hrWrVtHcXExwWCQuXPncu+997Ju3bp23z8hRPcQtpqCUqov8AKQjLnd2UKt9cOHLKOAh4FZQC1wldY6jCVa+O6pkJGRQVVVFWlpaaSmpgJw2WWXcf755zN69GgmTpx4VDe1ueCCC/jyyy8ZO3YsSikeeOABUlJSeP7553nwwQex2+1ER0fzwgsvkJuby9VXX00waPbrT3/6U7vvnxCiewjb0NlKqVQgVWu9TikVA6wF5mitv22yzCzgekxQ+A7wsNb6O62le6xDZwP4fCW43XuIjMzAao046n3q6mTobCG6rg4fOltrnR866tdaVwFbgUNvZPAD4AVtrAbi6oNJmFjrH+VaBSGEaM4J6VNQSqUD44CvDnkrDchu8jqHwwNHO+YjfH0KQgjRFYQ9KCilooElwE1a68pjTONapVSWUiqrqKjoOHITvj4FIYToCsIaFJRSdkxAeFlr/UYzi+QCfZu87lM/7yBa64Va64la64lJSUnHkZ9QTUFGShVCiOaELSjUn1n0L2Cr1vpvLSz2NnClMqYAFVrr/PDlSWoKQgjRmnBevDYVuALYpJQKjTtxJ9APQGv9JPAe5syjnZhTUq8OY34IdTRLn4IQQjQvbEFBa/0ZRxhNQpvzYX8WrjwcKlwdzeXl5bzyyitcd911R73urFmzeOWVV4iLi2vXPAkhxLHoVlc0h6ujuby8nCeeeKLZ9/x+f6vrvvfeexIQhBCdRrcKCqabQ7V7TeGOO+5g165dZGZmctttt7FixQpOO+00Zs+ezciRIwGYM2cOEyZMICMjg4ULFzasm56eTnFxMXv37mXEiBEsWLCAjIwMzj77bOrq6g7b1tKlS/nOd77DuHHjOOussygsLASgurqaq6++mtGjRzNmzBiWLFkCwAcffMD48eMZO3YsZ555ZrvutxCi6+lyA+K1MnI2AIHAMJSyYTmKcHiEkbO5//772bx5M+vrN7xixQrWrVvH5s2bGTBgAADPPPMMCQkJ1NXVMWnSJObOnUtiYuJB6ezYsYNXX32Vp59+mosvvpglS5Zw+eWXH7TMqaeeyurVq1FK8c9//pMHHniAv/71r/zhD38gNjaWTZs2AVBWVkZRURELFixg1apVDBgwgNLS0rbvtBCiW+pyQaFtwjO0R1OTJ09uCAgAjzzyCG+++SYA2dnZ7Nix47CgMGDAADIzMwGYMGECe/fuPSzdnJwc5s+fT35+Pl6vt2EbH330Ea+99lrDcvHx8SxdupRp06Y1LJOQkNCu+yiE6Hq6XFBo7YgeoKZmLxZLBBERg8Kaj6ioqIbnK1as4KOPPuLLL78kMjKS008/vdkhtJ1OZ8Nzq9XabPPR9ddfzy233MLs2bNZsWIFd999d1jyL4TonrpPn0JFBWzejPK2f59CTEwMVVVVrWy6gvj4eCIjI9m2bRurV68+5m1VVFSQlmZGAnn++ecb5s+YMeOgW4KWlZUxZcoUVq1axZ49ewCk+UgIcUTdJyhoDW43Kqho77OPEhMTmTp1KqNGjeK222477P2ZM2fi9/sZMWIEd9xxB1OmTDnmbd19993MmzePCRMm0LNnz4b5v/nNbygrK2PUqFGMHTuW5cuXk5SUxMKFC7nwwgsZO3Zsw81/hBCiJWEbOjtcjnno7MpK+N//cKdHEYjQREWNDGMuT04ydLYQXVeHD53d6dSfbhSOmoIQQnQV3ScoWM0QF0q3f5+CEEJ0Fd0nKDTUFGTsIyGEaEm3Cwqm5UiGzhZCiOZ0n6DQ0HwEoDnZOtiFEOJE6D5BQdUP2NrQciRNSEIIcajuFRSsVlR9LOjofoXo6OgO3b4QQjSn+wQFMP0KwVCzkdQUhBDiUN0rKBxUU2i/zuY77rjjoCEm7r77bv7yl79QXV3NmWeeyfjx4xk9ejRvvfXWEdNqaYjt5obAbmm4bCGEOFZdbkC8mz64ifUFLYydXVsLCgKOABZLJEpZ25RmZkomD81seaS9+fPnc9NNN/Gzn5mbyC1atIgPP/wQl8vFm2++SY8ePSguLmbKlCnMnj27/r4OzWtuiO1gMNjsENjNDZcthBDHo8sFhSMKw0lH48aN48CBA+Tl5VFUVER8fDx9+/bF5/Nx5513smrVKiwWC7m5uRQWFpKSktJiWs0NsV1UVNTsENjNDZcthBDHo8sFhdaO6NmxA+3zUN3Xjcs1GLu9/W6DOW/ePBYvXkxBQUHDwHMvv/wyRUVFrF27FrvdTnp6erNDZoe0dYhtIYQIl+7VpxDGjub58+fz2muvsXjxYubNmweYYa579eqF3W5n+fLl7Nu3r9U0Whpiu6UhsJsbLlsIIY5HNwwKJhi0Z0czQEZGBlVVVaSlpZGamgrAZZddRlZWFqNHj+aFF15g+PDhrabR0hDbLQ2B3dxw2UIIcTy6z9DZAPv3o0tKqB4cwOnsi8ORHKZcnpxk6Gwhui4ZOrs5B9UU5DoFIYQ4VPcKClYrSuv6M5AkKAghxKG6TFBoUzNYaPhsbZGawiFOtmZEIUR4dImg4HK5KCkpOXLB1nBPBQsyfHYjrTUlJSW4XK6OzooQooN1iesU+vTpQ05ODkVFRa0vWFMDxcV4saIcVdjtdScmgycBl8tFnz59OjobQogO1iWCgt1ub7jat1XvvAPnn8+3zw0iOGE0I0a8Gf7MCSHESaRLNB+1Wf1w1Ta3g0CgpoMzI4QQnU/3CgpRUQDYPBIUhBCiOd0rKDTUFGwSFIQQohndNChYCQYlKAghxKG6aVCwSE1BCCGa0b2CQn2fgsWtJCgIIUQzwhYUlFLPKKUOKKU2t/D+6UqpCqXU+vrpt+HKSwOHA+x2rHUQDNaGfXNCCHGyCed1Cs8BjwEvtLLMp1rr88KYh8NFR2Ot02jtJxj0YrE4TujmhRCiMwtbTUFrvQooDVf6xyw6GmutGfdImpCEEOJgHd2ncIpSaoNS6n2lVMYJ2WJ0NJZaPyBBQQghDtWRQWEd0F9rPRZ4FPhPSwsqpa5VSmUppbKOOL7RkURFYakL1RQqji8tIYToYjosKGitK7XW1fXP3wPsSqmeLSy7UGs9UWs9MSkp6fg2HB2NtT4oeDx5x5eWEEJ0MR0WFJRSKUopVf98cn1eSsK+4ehoLLVm2GyPJzvsmxNCiJNJ2M4+Ukq9CpwO9FRK5QC/A+wAWusngYuA/1NK+YE64If6RNzpJToaVesBlAQFIYQ4RNiCgtb6kiO8/xjmlNUTKyoKVVOLw5GM2y1BQQghmuros49OvOhoqK7G6ewrNQUhhDhE9w0Kjj4SFIQQ4hDdMygEg7hIxePJlhvWCyFEE90zKACuQC8CgWoCgcoOzpAQQnQe3S8o1I+U6vInAEhnsxBCNNH9gkJ9TcHpiwfkWgUhhGiq2wYFhzcGkKAghBBNddugYPe4AIsEBSGEaKL7BYXQ3dfq3DidvaVPQQghmuh+QaG+pmAuYJNrFYQQoqluHhT64vHkdGx+hBCiE5GgIBewCSFEg+4XFOr7FKipwensSzBYh9/f+e4aKoQQHaH7BQWrFVwuqK7G5eoLyAVsQggR0v2CAhw0UirItQpCCBEiQQEJCkIIEdKtg4LDkYxSNgkKQghRr01BQSl1o1KqhzL+pZRap5Q6O9yZC5uoKKipQSkLDkeanJYqhBD12lpT+LHWuhI4G4gHrgDuD1uuwq2+pgDgcvWVjmYhhKjX1qCg6h9nAS9qrbc0mXfyaRIU5LacQgjRqK1BYa1S6r+YoPChUioGCIYvW2F2WFDIQeuTd3eEEKK92Nq43E+ATGC31rpWKZUAXB2+bIVZdDTU1AAmKGjtxecrwuFI7uCMCSFEx2prTeEUYLvWulwpdTnwG6AifNkKs6iohppCZOQQAGprt3VkjoQQolNoa1D4B1CrlBoL3ArsAl4IW67CLVRTCAaJihoDQHX1hg7OlBBCdLy2BgW/NqPG/QB4TGv9OBATvmyFWWhQvNpaHI4U7PYkqqs3dmyehBCiE2hrUKhSSv0Kcyrqu0opC2APX7bCLBQUampQShEVNYaaGqkpCCFEW4PCfMCDuV6hAOgDPBi2XIVbKChUVNS/HEtNzWa0DnRgpoQQouO1KSjUB4KXgVil1HmAW2t98vYpDBtmHjeY2kF09BiCQTe1tTs6MFNCCNHx2jrMxcXA18A84GLgK6XUReHMWFiNGwcREfD55wBERY0FkCYkIUS319brFH4NTNJaHwBQSiUBHwGLw5WxsLLbYfLkJkFhBErZqK7eSK9e8zs4c0II0XHa2qdgCQWEeiVHsW7nNHUqfPMN1NRgsTiJjBwup6UKIbq9thbsHyilPlRKXaWUugp4F3gvfNk6AaZOhUAAvv4aME1INTVyWqoQontra0fzbcBCYEz9tFBrfXs4MxZ2p5xiHuubkKKjx+DxZOPzyf2ahRDdV1v7FNBaLwGWhDEvJ1Z8PGRkNAkKoc7mTcTFTe/InAkhRIdptaaglKpSSlU2M1UppSpPVCbDZupU+PJLGe5CCCHqtRoUtNYxWusezUwxWuseJyqTYTN1qrmAbcsWGe5CCCEI4xlESqlnlFIHlFKbW3hfKaUeUUrtVEptVEqND1deWjR1qnn8/HMZ7kIIIQjvaaXPATNbef8cYEj9dC1mJNYTa+BASE4+qF+hpmYzwaD/hGdFCCE6g7AFBa31KqC1U3l+ALygjdVAnFIqNVz5aZZSprbQEBTG1Q93seWEZkMIITqLjrwALQ1oenPknPp5J9bUqbBnDxQUEBd3OgBlZZ+c8GwIIURn0OZTUjuSUupaTBMT/fr1a9/Ex40zj5s24Zoxg4iIoZSVfUzfvje373aEEJ2azwclJaA1xMSYGzT6/VBUBAcOgNcLLpeZLBYIBs1ksYDNZiatTTp+P9TVQW2tmZRqXFepxmW83sZJKTMCj90Obrc5B6ay0ryntZkmToRTTw3v59CRQSEX6NvkdZ/6eYfRWi/EXDzHxIkTdbvmYuRI8/jttzBjBvHxZ1JY+CLBoA+L5eS9ZYToPrQ2BUpTbrcplGJjTaEVUltrCrjQOsGguTNtZaW5GaHX21hghQo9MAWVw2HWCRV2gQAkJJjJbof9+2HvXlOwRkebgtXhMMuHpqaFYGhbwaBJNzQFgyZ/fn9joao19OhhJpfLvBcImPfKysxUU2Pmh/KudePn03RqKrTN2lrzGTQVKvg7k9tv79pB4W3g50qp14DvABVa6/wTnotevSAx0QQFIC7ue+Tl/YOqqjXExn73hGdHnDxCR4VutylUamqgvNwUUBUVjYVTIGAKxJoas5zP11jwWixgtZpHj6exMHe7zetQIX7ggEk3Lg6SkkyBW1QEeXmmELbbITLSPIaOLsG8Tk42wSE/H0pPwAX7LpfJ/6EcDnA6zWMoyNjtZt+bFtoWiymobTazT5GRZv3sbLNvbnfjkbnLZYJS374mENlsjZ9nqMCHg4NO0wAa2qbLZYqBxESzflWV+Q5Dn1+vXibvHo/5fprmMxTAfD4zz25vzFtUlMm/1o3fLTTmP/SZ2OuPP0OB0uUy31mPHo3B2GIx88MtbEFBKfUqcDrQUymVA/yO+ru1aa2fxIydNAvYCdQCV4crL0fIqKktbDGdy/HxZwCKsrKPJSh0YloffLTp85kj3vJyM5WUQHGxeYyMbPxjezymYCwrMwVMdfXhk9fb+CcPFfaHHkWHjnCPVajJIRQ0tDZ//ogIU0hERJgCICLCBIHBg81F+OXlZr8qKszJc6eeCj17mvyEjsR79DAFitNpAkd+vll+2jRTeCYnNxbESpnlY2JMoRpqvmhauIYKPa/X5DdUUCtlPseSEvNev37Qv78pCAOBxs8sMrKxyUV0fmELClrrS47wvgZ+Fq7tH5WRI2HRItAauz2R6OhMyso+Jj39ro7OWZejtSlYSkvNUW5enik4QoVRZSUUFkJBgSlsQgV4ebkp2MrLG4+4A+1wo7zQ0VxMTGPB6HA0zu/d28yLjGw8orPZDn6MiGgsKOPiGqfQUbDFYpaJijKPDsfhBWRzTUAng5a6+KxWE2zEyeek6GgOu4wMU/IUFkJKCvHxZ5KT8wiBQC1Wa2RH565TcbtNQZ6TY6bsbDMVFTUepVdXH9wE4vE0Hn0f2qbbEqXMkXFCgnmMizMFUGysKbwjIxuPqkMBJTraLBcba5oBkpLM+rW15qs9cMAU9vHxZoqJaay2t1VxbTFZeVn07dGX4T2HY7VYm11uy4Et9ItLJ8oR1eb9bc3O0p2sy1/HOYPPIcYZ0+JyQR3EoiyHzfP4PViUBZvF/OUrPBWUu8vRWjMwfiCqSQZ2lOzgq9yvSIpMIq1HGj0jewKgtSbSHkmsK7ZN+wTm81q6fSlvbHuDHSU7GBg/kCEJQ8hMyWTO8DnER8S3ur4v4OP9ne+ztWgrPSN7khSVRJwrDrvFjsPqwGVzEe2IJsYZQ6wztsXvo6nsimwWf7uYguoCUqJTSI1JJSEigQhbBBH2CGq8NeRU5pBdmU1ZXRl1/jrqfHUkRiaSmZLJ2OSx9I3tS4QtAqvFSlAHqfJUUeYuY3/FfnaU7GBn6U6yK7PJr86noLqAYYnDuDrzas4Zck7Dd9CcoA6SlZdFcW1xw2fuC/rw+D14Ah4ykjKY0HtCmz//YyFBARo7m7dsgZQU4uLOJDv7L1RUfE5CwoyOzVuY1NWZZoi8PHNG7t69pkAPHdmG3i8qMkfsoam8/PC0Qu3cocK7b9/Gpg+XyxwZN20/tljMcmlpkJpqlgk1AcXEmOaNnj3NkXi5u5wvsr+goLqAOl8dbr8bjUahUEpRVlfW8Mdz2Vyk2FJIJpmoqijsNXYc2Q7SeqQxJGEIkwcNoKS2hK3FW/lg51Y2FG5gfcF6tpds56yBZ3HvGfcyImkEgWCAt7a/xb+++Rf+oJ84VxwRtgjW5a9j04FNDfsd7YhmUu9J3H/W/UxOm9ww/+HVD3PThzfhsrk4a+BZzBo8i/iIeII6SFAHCQQDBHQAj9/DgZoD5Ffnk1+dT25lLnlVeZTWlTIoYRBjksfQO7o3y3Yva9hur6he/Hbab7l2wrV4Ah7W5a8jKy+rYdpRugO7xU6UIwqH1UGtr5Yabw2alqNx3x59mTVkFoMTBrP428V8lftVi8talIUz0s9gfsZ8Tk8/neLaYnKrcil3l+OwOnBanVR5q8jKy2JN3ho2FGwgoAP0j+3P+NTx7C3fy6p9q6jx1fDTd3/K+UPP59LRl/L9Qd9vCKBaazYd2MSrm17luQ3PUVBd0KbftN1iN0EncQgTUycyL2MeI5PMfzunMoc3tr7Boi2L+DzbXJfksDrwBrytpum0OomwRxBhi6C4thhf0HfYNv1B/2Gfr91ip0+PPqTGpDI0cShfZH/Bm9veJDkqmZFJIwnoAIFggKSoJIYlDmNwwmA2FGzgjW1vkFeV12J+fvndX4Y9KCjd1kO3TmLixIk6KyurfRPNzzftBI88AtdfTyBQw2efxdGnz60MGnR/+24rzCorYds2M2VnNza9FBebJpmCAlPQ1/pqYcyLEL8HygZC6WBsuNBR+QSj8rFF1BAVZSE6StFLjWCodSY9E6z06gUpfdzscb1BQoLiysk/oFd8y7Wpb4u+5d9b/s2b294kzhXHFWOu4KKRFxHrisXj97C/Yj+fZ3/Ost3L+Hj3x2g06XHp9Ivtx+6y3XyT/02rBZpFWUiOSiYlOoU6fx2F1YWUucva9FklRCSQmZJJ/9j+LP52MTW+Gi4aeRFr89ayq2wX/WL7kRKdQoW7gipvFRlJGZyefjpT+kwhpzKHNblr+M/2/1BaV8qSi5cwc/BMXtr4Ele8eQXnDT2PgXEDeWv7W+yr2NdqPpIik0iNSaV3TG/SYtKIc8Wxo3QHGws3kl2Rzan9TuWC4RcwImkEf/z0j6zct5KEiATK3eUEtenY6NujLxN7TzQFTjBAra8Wt99NlCOKGEcMEfaIhoCk0cQ6Y4lzxVHnr+O/u/7Lst3LqPZWMzZ5LJePuZyZg2dS7i4nryqP4tpiFAqLspBdmc2iLYvYUdr6/cxjnbFM7D2R7/b9LnOGz2FcyriG2ojWmrX5a3lp40u8uvlVDtQcwGl1ctbAs0iNTuWDXR+QU5mDVVk5d+i5/GTcTzgj/QxK60opqi2iwl2BL+jDG/BS56ujxldDtbea/Kp8dpTuYHvJdrYc2IJGM7rXaKIcUazOWQ3AqF6j+GHGD7k442IGJwymzF1GflU+5e7yhhpBhD2Cvj36ktYjjWhHdMM+eQNethVvY33B+oaDlDp/HQ6rgzhXHLHOWNJ6pDE0cSj9YvsdVCMI1Xhe3PgiBdUF2Cw2LMpCXlUeu0p34Qv6cNlczBoyi7kj5jI4YXDDunaLHafNidPqJCEi4Yi1q5YopdZqrScecTkJCpg2jYQE+OEP4R9mtI0jGLrNAAAgAElEQVRvvjmNYNDNhAlr2ndbRyH0Bw79uHw+2LwZsrJg377G0+9KS2HnTtix20tBsRs8jY25ERGNzTApqZqotD0Upr7ARufj1FKMFRsBjjysR98efc3Rqd/DU2ufoqi2CIAYRwwXZ1zMvJHzmNpvKtGOaOp8dbyy6RUe/fpRNhRuQKE4td+pFFQXsKN0By6bizhX3EFHgL2ienHWwLOIskext3wv+yr2kRqdyhnpZzA9fToD4gYQYY/AZXNhURa01gR1kGhH9GFNBt6AF7ffjS/gw+13k12Zzf9K/sfust30jOzJ8J7DGdFzBL1jejcUVMW1xfzp0z/x+JrHyUzJ5Bff/QUXDL/giM0RBdUFnPPyOWw+sJkbJt/AI18/wmn9TuO9y97DZXOhtWZP+R68AS8WZUGhsFlsWC1W7BY7PSN7Yre23IZ1aHOQ1pr3d77Py5teZkjCECb1nsSktEn0iup1xO+wNR6/h8KaQvrFHvk6IK016wvW803BN6REp9A7pjcJEQl4A148fg9Om5OB8QMPa8Zqjj/o59N9n/L29rd5a/tbFNcWM2PQDM4dci6zhswiJTrlmPYnvyqfxd8uZtG3i3D73Vww/ALmjpjLsJ7Djim9cPIH/ewr30dydPJBQai9SVA4WqeeanrHVq4EYM+eu9m37x6++90DOBw92397zQjqIB/sWMai9e+woXAd2ys2oLSNsdW34//sRjaujcTjMctaLKZJxmYzHXpJ479g99gr8dvL+PWgN7lo0jT69weHM8hTWU/x2pbXWF+wnkqPORn7/KHn88upv+SUPqeQW5XLztKdeANeUqNTSYlOIcYZg9Yaf9DPx3s+5h9Z/+Cj3R+hUJw/7HxumHwDdqud59Y/x6Iti6jx1WCz2JiQOoGdpTspqSthdK/RXDvhWuaOmEtqTCpaa77O/ZqXN71Mra+WfrH96Bfbj3Ep4xidPLpNhUi4hf4P6ih6fSvcFcx5fQ4r9q5gfOp4lv9oOT2c0st6tI7lsxdtJ0HhaF17LbzxhmlbUYqqqvWsXTuOIUOeIC3t/44pydK6Uv67679sKNiAP+jHH/QzMmkkCyYsaFjG74ev11fzwCf/4KPyp6hx7gJvFOSPg/zxEL8bhr2Dw5PKCDWHYNxOCoKbwBLgnCHncN6Q81iXv44HvniAfrH9cFqd7C7bzcLzF3JG+hlc/dbVLN+7nHEp4zilzylkpmQyPX06QxOHHvX+7Cnbg9ViPexossZbwxfZX7Bi7wpW7V9FclQyP5/8c6b3n95t/uBuv5vn1z/P3JFzGzpmhehMJCgcrYcegptvNqep9OqF1po1a0Zjs/Vg/Pgvjiqpj3Z/xO9W/I7VOasJ6iA2iw27xY5SilpfLb8fvhTPxvP4/HP4eo2mbvYcGP42zsLTOMX2f1yUcSHJiU5iY01HbHHUZ/x2xZ2szV/LiJ4jGJ08uqGNsrTOXI10zbhr+Nv3/0ZAB5j373l8tPujhrMjHvr+Q/x43I+7TQEthDicBIWjtWwZnH02LF8Op58OwP79f2b37jv4znd2EhExqE3JPPb1Y9z0wU0MiB/AJaMu4eyB56BzJvPJR1Y+WOZl9bixYPFheXILE8Y66TXtTd6NuZDbx/+Z+8//Zatpa60PKtj9QT+rc1Zjs9iY0mdKw3xfwMcvl/2SnWU7efScR0mPSz/qj0MI0bVIUDhaubnQpw889hj8zFxT53bnsHp1P9LTf0d6+u8aFi2sLuSDnR/w/s732VG6gwmpE5jadypf5X7FP7L+wblDzuNnya/wzhsxLF5szo9XCiZNgkEzlvGq/Wx+d+ofufXUnzPi8REkRiaStSCr1Q5HIYQ4Hm0NCnKdQkjv3qbHtn4MJACXqw9xcWdQUPAi/fv/FqUU9626j7uW34VGkxKdwsikkSzasoin1z0NQEb5L1h90/28W2QlIgLOOw/mzoUZM8wZQDCDutfn8OBX9/K/8k3kVeWx+OLFEhCEEJ2CBIWQ0BhITYICQHLyFWzffjWVlat5bMMn/Gb5b5ifMZ/bp97O2JSx+H0W/vVMkN8/vpXC0lpyaydx7rkwezace64Z2uBQfzv7b4x4fASvbn6V6yZed1DTjxBCdCQJCk1lZMDbbx80KynpQnbsuI4/LL+Fv25YzRVjruDZHzxLMGDl2Wfg3nth714LU6Zk8OwDcNZZRx46YUD8AP545h95bv1z/PHMP4Zxh4QQ4uhIUGhq5Ej4179gzx68/dJYtmsZK/au4P1tLraUreaSUfN5+txnef45K/fea4aHmDgRnngCZs48ugHNbjnlFm6ecrOcESSE6FQkKDQ1dy6Bu37Ny/fM5e7McvaU78FhdTAxeRjXxpXx08EX8f2zraxcaYLBo4/CrFnHPrqlBAQhRGcjQaGJnTE+Zt/eg636G8a5B/DWD99ixsAZuGxOnn76Es4+63RqauC55+DKK0/OoY6FEKI1EhSa+P3K35Ntr+Xfn6Vz4Td1WH46HewRLFwIP/vZKyQn72bp0hKmTOl846cIIUR76PjBZjqJguoCXt/8OldnXs1Fdy/CUniA4B13cued8P/+H5x5ZpCnnppGfPxfOzqrQggRNhIU6j2Z9SS+oI/rJ18Pkybhue5mLn9yKn/6kwkK77xjZ9CgcyksfBmfr5mbCgghRBcgQQEzbPCTWU8ya8gshiQOQWu4puR+XuVS7nf8ln/csBWbDdLSriMYrKWg4LmOzrIQQoSFBAVg0ZZFFNYUcuN3bgTg73+Hl161cc+tFdwe9xRqzg+grIyYmPH06DGFvLwn0Po47touhBCdVLcPClprHv7qYYb3HM6MgTNYtgxuuw0uvBB+/UAsLFli7lV5ySUQDJKWdiN1dTsoLHylo7MuhBDtrtsHhRV7V7A2fy03TL6BffsU8+eba9ief97cyIZTT4UHHoAPP4RPP6VXr4uJjh7Pnj2/JhCo6+jsCyFEu+rWQSGvKo/L3riMAXEDuHLslfzqV+D1wn/+A9FN74q3YIGZ8fzzKGVh0KAH8Xj2k5v7SIflXQghwqHbBgW3380Fr19AlbeKty95m51bo3jtNbjxRhh06K0ToqLgoovg3/+G2lri479HYuJ57Nv3R7ze4g7JvxBChEO3DApaa65dei1f537Nixe8yKheo7jrLoiLg1/8ooWVfvQjqK6GN98EYODAPxMIVLNv3z0nLuNCCBFm3TIofLDzA17c+CJ3T7+bOcPnsHo1LF1qOpjj41tYado06N/fdDYAUVEjSU1dQF7eP6iq+ubEZV4IIcKoWwaFDYUbALj1u7cC8OtfQ69ecMMNraxksZgBjz76CHJyABg48I/Y7Uls23YlwaAn3NkWQoiw65ZBYVfpLnpF9SLaEc3KlfDJJyYwHNS53JwrrwSt4aWXALDbExg27J/U1Gxmz57fHWHlQ7z+urkn9El2O1QhRNfWLYPC7vLdDIo3vcnPPmvuwnnttW1YcfBgmDrVNCHVF+aJibNITb2G7OwHqaj4ou2ZWLQIli0zN2UQQohOolsGhV2luxgYPxC32/QbX3ghuFxtXPnaa2HbNnj88YZZgwb9DZerH1u3XonPV9q2dLKyzOOaNUeXeSGECKNuFxS8AS/ZldkMih/EBx9AZSX88IdHkcAVV8B558Gtt8I3poPZZothxIiX8Hiy2bx5DoGAu/U0iopg/37zXIKCEKIT6XZBYV/5PoI6yMD4gbz2GvTsCd/73lEkoJRpc0pKgvnzoaoKvF5iPy5k/BeXUlH+Kdu2XdH62Ehr15pHl0uCghCiU+l2QWF32W4AUiMGsnSpuSbNbj/KRHr2hJdfhl27TERJS4O5c4m58zlGVFxPUdFidu68Gd1SJ3IoKMyfb54HAse+Q0II0Y66XVDYVbYLgJ1rBlFbe5RNR01Nnw5/+ANs2ACnnw5vvAHR0SS/XU2fPjeTm/sIOTkPNb9uVhYMHQpnngk1NbB16zFmQggh2le3ux3n7rLduGwuPlycQu/eZry7Y3bnneaKt1BV49JL4cUXGfS3XNzu/ezadSsuVz+SkuYevN7atWbDkyaZ12vWwKhRx5ERIYRoH2GtKSilZiqltiuldiql7mjm/auUUkVKqfX10zXhzA+YoJDeYyAfvG/h4ovBaj3OBJu2PS1YAHV1qFdeZcSIF+nRYwpbt15ORcWXjcscOADZ2TBxoqkt9Ogh/QpCiE4jbEFBKWUFHgfOAUYClyilRjaz6Ota68z66Z/hyk/IrrJdRHoG4vXCxRe3c+ITJsC4cbBwIVaLi1Gj3sLhSGPTpvOprjZXUTf0J0yYYK6SnjBBgoIQotMIZ01hMrBTa71ba+0FXgN+EMbtHZHWmt1lu7FWDsJqhfHj23kDSpnrGDZsgKwsHI4kxo79EKs1gvXrz6Cqaq3pT1DKBA8wTUgbNoBHhskQQnS8cAaFNCC7yeuc+nmHmquU2qiUWqyU6hvG/FBUW0S1txpPwUAGDwanMwwbufRSiIyEhQsBiIgYRGbmKqzWHqxffya+rz5qbDYCExR8Pti4MQyZEUKIo9PRZx8tBdK11mOAZcDzzS2klLpWKZWllMoqKio65o2FTkct3TWQkc01ZLWHHj3MKU0vvQRPPQXBIBERAxg3biV2eyLBrz+lLiOhcfmmnc3tyeMx11AIIcRRCGdQyAWaHvn3qZ/XQGtdorUOtZv8E5jQXEJa64Va64la64lJSUnHnKFdpeZ01Lwtgxgx4piTObJ774VTToGf/tScdrprFy5Xf8alLsFZpMlN/ZKtW6/A76+Cfv3MhXDtHRR+/GPTPub1tm+6QoguLZxBYQ0wRCk1QCnlAH4IvN10AaVUapOXs4GwnrAfqikES9LDV1MASE2Fjz82TUjr1sGwYTBjBs4HngYgatqPKSx8hbVrx1NZtdbUFr766uhHTA0G4eGH4Z5DbvRTXg6LF8POnfDCC+20U0KI7iBsQUFr7Qd+DnyIKewXaa23KKXuUUrNrl/sBqXUFqXUBuAG4Kpw5QfMmUcJtjTwR4Q3KIDpTF6wAL79Fm6/3YyG+sQTYLOROuthMjNXEAx6+OabUyidbDcXsP3tby2n98wzZsylpUtNMCgpgfPPh5tugt/9zmwnZMkSU0NIS4P77jN9FkII0RZa65NqmjBhgj5Wpz1zmu5/92laKa1rao45mWMTDGqdlaX1ypUNs7zeUr1p01y9/GN02VnJOqiU1kuWHL7u9u1au1xa22xag9ZDhmjdp4/WDofWf/yj1k6n1v/3f43Ln366Weadd8zy//znCdjBem631j7fidueEKJNgCzdhjK2ozuaT6hdZbtQ5YNITzcnCJ1QSplrEqZNa5hlt8eTkfFvhg5/is2/LKd6pA19+aXw9deN6wWDcM015lSpXbvg1VchIQESE+GLL+BXvzJnPD3/PJSVmQvjVq6Eyy+HWbPMRXL33ntiagt1deZU24suCv+2OqPPPwe/v6NzIcRx6TZBoc5XR15VHrW5YTzz6Bgopejd+1rGTlnN9gd6447zEJwxneATj5mA8NRT8OmnpmmpXz9zZtPq1bB+vQkyYO4jWltrmphefdX0TVx6qQlEd98Ne/fCiy+Gf2f+8AfTDPbWWwcHtu7gq6/M0CXShyNOct0mKOwpN3c4C+vpqMchJiaTsTPWs/9fM6gY6Mbys+vxThyC/uUvYcYMuPrqllfOzDQ1kMceM4X/lCnmLnHQWFu49Vb4z3/CtwObNsGDD5qRXxMSTIDoTv79b/P4yScdmw8hjlO3CQqhM4/8B8J8OupxsNvjGDrzQ4LL3mX37/vD3t0EAzVUPHi1OepvzQ03mBrB5s1w2WWN85Uyt/4cPBguuMAsd6xXT5eWmjObDhUImE71uDhzR7qbb4Z33mm4CVGXp7Xp3AfTdCf33e483G7TrCnarNsEhZToFM5OvBZKhnTKmkKIUorEnrMYcNceKrNeYMMrffim7FK+/fZyPJ6Cllf8wQ9M85LVevigTgMGmPbum2+GRx81w36XlBxdxjZtguHD4TvfMcN9N/Xkk6b55KGHTF/Hz39uLuK7776j28bJ6ptvTECeMAFycsxzcWzaM6Bqbc7YGzCg8fa3nVUg0HkOJtrSG92ZpuM5++hPfzIn41RUHHMSJ5zfX6t3775Lr1jh0KtW9dDZ2Y/qYNDf/MLvvKP1Qw+1nuCSJeZspVGjtM7La5xfUtLyB7NundaJiVonJWmtlNbXXdf43hdfmPS+/31zhlXIb35jPuzNmw9Pr6pK68LC1vN5MrnzTq2tVq2XLzf7/OyzHZ2jk9MNN2g9ZozWZWXtk95bb5nvIypK68hIrZcubZ9029u335qzCX/+87BuhjaefdThhfzRTscTFK64Quu0tGNevUPV1PxPr18/Qy9fjl6zZoIuKVmm/f7qY0vs44/NH2XQIK2fekrrmTNNodajh9YLFzYW7sGgWTYuTut+/bTeuVPrW24xP5v339d61y4TKAYN0rqo6OBtFBebbUyZcvB7q1ZpHR9v0ujZU+tp07R+/vlj24/OYvhwrb/3Pa0DAa0TErS++uqOzlH4vf++1rNnm99Ae3j6afObAK3nzTv4AONY+Hzmexk2TOucHK0nTtTaYjHb6Uy2bNG6Vy+TN9B6xYqwbUqCQjMmTNB6xoxjXr3DBYNBXVDwqv788xS9fDl6+XKlV68eprdv/6murT3KP+eXX5rCHrTu31/rX/7SXN8AWp9xhta//73WQ4ea1wMGaL13r1mvrk7rjAytU1O1HjHCFPDbtze/jUWLTC1iwABTY3j9dXNtxbBhWv/1r1pfc42psYDW115rrnEIp2XLtD5woPVltm/X2uNpe5pbtpj8P/64eT1njtYDBx57HtsiL8/U1v773+MvPEOCQVNbbIuSElOQgfkNvfNOY77uuUfrq67S+quv2r7trCzzO5kxQ+v77jPpLlx49PvQ1JNPmnT+8x/zurraHPzYbFp//fXxpX2oQMCk+dvfNn4WzfF6ze/j9NPN/+vf/zYHVamppjY+YID5z9XVtW/+6klQOEQgYA5cb7zxmFbvVHy+Sl1U9Jbes+duvXHjbL1ihVMvX27VW7f+WNfU/K/tCe3aZYJDIGBeB4Pmz9ijh/lpTJ9ujqwObVb65hut7XYzHenI5quvtE5JMdV30PrUU00tIsTv1/pXvzLvTZ6s9XPPmSawe+4x22kvDzzQGAC3bm1+mWXLzDLnnGP+wG1xzz1mndxc8/rvfzevs7MPXzYY1HrHjuO/uG/+/Maj6tGjTU0r9B0eC49H64svNt9nVtaRl7/qKlO4vvmm1uPGNR5IhC6ujI42jzNmmN9Xa4qLzXfSr5+pUQYCZj2Xq/mmx7aorNQ6Odn81poGzbIyrfv2NRd2VlUdW9pam3y++675Tf3oR6b5IfR92O1af/RR8+v9+tdmmZEjTTMsaN27d+NB1Ycfmnl33XXseWuFBIVD7N1r9vbJJ49p9U7N7c7V//vfDfXBAZ2VNUnv2/egrqvbf2wJlpY2FnIteffdln/8h8rONs1EV17Z8lHQkiWNhUloiow0R8PH6/HHGwv7Xr1ME89nnx28TGmp+XMnJZllL7+8bQXt2LFaf/e7ja/XrjXrv/xy4zy/39Saxo8376WkaP2LX2i9YYN572isXGnSuOMO03cxerR5ff75WpeXH11aWptL+2fObCzMx48/OGg9/LD5XBYuNJ/Hf/9rlr3zTvN+ba3WCxaYNvGbb9b6f/8zBe4DD5jP2mYzTYbNWbfONPE4HAcfvRcUmEJ9wICDv6ecHK3nzjVB5NxzzcHEX/5ijvTmztX6rLPMNHasyePq1Ydvc8UKUyBfc83h7/n9Wm/cqPWmTc1/L6Wl5nOPiGj8jaakaH3BBVq/8II5yBo9WuuYmMMPaELb/fGPzeuSEq3fe0/r/PyDl7v8chNY7rtP61tv1fqSS0wz3TnnmH07juYvCQqHeO89s7ct/T67Arc7T+/b94Bes2ZCQ/PS+vUzdEHBK9rvr+3o7B1Zaan5Y5WUmAJg9GhTYLzxRuMydXXmaPuTT7R+8UWtP/+85SPvkhKtH3ussdD0ek36Q4aYI9Ennmj88196qSnA1qzR+t57zTo33mg6j+++W+sLLzQ/oqaef94s99e/Ns7z+7WOjTXNYVqbgj/UDDd0qCks58xpPKq2202fzMyZJnA0LYxKSkzgDTVn+f1aZ2aao93QOC3BoNaPPGLSGzLEFGgt2bXL9CFdd50p3B580BxNK2UKm9dfN3n6+9/N8m+/bd7r2dPMnzJF6/T0tjdxlJWZZXv1OrjmFAiYwtxuN0fKH398+Lpffmm2pZTpgH3iCVODjYgwAWDUqMbPMCrKBJdTTmmc7r235XzdcUdjYLvnHq2vv17rM880hXmosI+JMYXwNddo/ZOfmBpBXJzJz6WXmoKkuea2nBzz/aSkmKDk9ZrfdZ8+bauhHDjQeGDicpnAOGaM6RM55RTzORwjCQqH+OorE4Sbtlx0ZTU1O/Tu3b/VX3zRTy9fjl65Mkpv3Dhb5+Y+qd3unI7OXtuUlpqCyGIxzRShP8uhU1yc1hddZP68c+eapozevRvfP/PMgwuxoiIzD0y6v/udeX7PPeb9YNCcCRNaP1QwKmXGmgoGzRE0mA7m6kM6/M891/SbfPyxKcjS0kz7cdMC/8ABrf/1L1NAzZ9v+iHAFG4PPKD1D35gCs3QvE8+MQU6aP3aa4d/Vp9+agoiMIWlzWYCamKiKVj69j24wAsVqA6HCQah/Z41yxSy77xjag4TJpj9e+GFxn6EJuN3HdGWLSadyZPNd/Duu6aAAxMcW/tDVlWZ7yHU1HLGGeZkhxC32/xGjrZfxeMxBWzo84iNNb+D664zBxovvKD1T39qahy9e5spLc0csa9f37Z9Dp1M4XCY7yV0wNEWFRXHtl9H0NagoMyyJ4+JEyfqrM5+znEnonWQ8vLlFBW9QUnJu3g8+wBFXNx0evW6jKSki7Db4zo6my2rroYbb4T8fOjbF/r0MY99+5ohyjdvhg8+MEOV+3wQH2+mgQNh9Ggzfe974HAcnK7W5qK+W2+F3Fxz/cVnn4HNZt4PBuH11yEmxgxf4XDAT34Cr71m7lOxbh3MmWOGFXG5Dk77gQfMyLh2u7nL3vvvm/y2JhAwF8Ddd5+5C19qqhmqZMwYM1TJnj1m/KvJk80Fcs1dzJiXB//8Z+PFiYGAudFSZaV5PnWqub/HsGHm/cpKk07oLoBgrrHIyDDDpvTubYYrSau/YWJ5uRl/a0Kztz1p2ZtvwoUXQnIyFBZCejr8/vdwxRVHvigTzDUw+/bBvHltW74tAgEoKjJX3x/622gP+fmwYoUZjmbjRrP/Cxa0/3aOglJqrdZ64hEXbEvk6EzT8Zx91N0Fg0FdXb1Z79lzt169ekh9DcKlv/32Cl1WtlIH2/nI5KRQVWWamJrrGD5UMGiO4i0W05zQUrNVVpY5Spw2zRzxHY1g0LTLN61V1Naa6z7692/bkerxevRR0+/S1iPbtrjvPlN7Wrjw6M7uEu0GqSmI1mitqarKoqDgWQoLXyYQqMRmi8Pp7I/L1Y+YmIkkJ19GRMSgjs5q51NaamojrR21ZmWZWkpYbgR+AgQC5up40WW0taYgQUEQCNRSVLSYysqv8Hj243bvpaZmC6Dp0eMU4uK+h92egM0WR1TUaGJiJqLaqxovhDgh2hoUbCciM6Jzs1ojSUm5kpSUKxvmud05HDjwCoWFL7F//5+AYMN7ERFDSU6+gqSkC4mMHCEBQoguRGoK4oi0DhIIVOHzlVJe/gkFBS9SUbESAJstkdjYU4mPP4ukpAtwOtM6OLdCiOZI85EIK7d7P2VlH1FR8Rnl5atwu3cB1Dc3TcflGkRExCBcrn44HKlYrSf6VndCiKYkKIgTqqZmK8XFb1BU9AY1NRvR+uDbUlqtsdjt8VgskVitUURFjSYpaS7x8WdisZyknbFCnEQkKIgOEwz68XiyqavbhceTg9ebj9ebj99fQSBQQyBQRWXlagKBSqzWHkRGDkUpBxaLg6io0fTseSGxsadisUiXlxDtRTqaRYexWGxERAwgImJAi8sEgx7Kyj6muPg/eDy5aO0lEKglP/9pcnMfxW7vSWzsqURGjiQqKoOoqDFERg6XQCFEmMk/THQIi8VJYuIsEhNnHTTf76+mtPQDiov/Q3X1WoqLlwIBAJRyEh09mqiosfWPo3A6+2K3J2KzxaNUt7mRoBBhI81HolMLBr3U1v6PmpoNVFV9Q3X1N9TUbMLnKzpkSQsREUPo0WMS0dETcLn6YbPFY7PF4veX4/Hk4vUWEBWVQVzc97BaXc1uT4iuSpqPRJdgsTiIjh5FdPQokpMva5jv9RZSU7MZjycfv78Er/cANTWbKSv7hMLCl46QZhQJCTNwuQZitUZhtUbjdPYjMnIYkZFDsVqjwr1bQnRaEhTEScnhSMbhSG72Pa+3EK+3AJ+vDL+/HJstFqezD3Z7Tyorv6Kk5G1KS9+ntHQZwWAtcHBt2W5PJiJiAE5nfxyOFByOJOz2nlit0VgsEVitUbhc6bhcA7FY7Cdgb4U4cSQoiC6ntYCRmDiTxMSZDa+11gQCNbjde6it3U5d3Xbc7r3U1e2hqioLn+8AgUBVs2kpZcPlGojNFotSDqzWCJzO/kRGDiUiYjBWawwWiwOLJYKIiCHY7fFh2V8h2pMEBdGtKaWw2aKJjh5NdPToZpcJBj34fCUEAjUEg7X4/VW43buprd1GXd2O+vlu/P4qqquXUlBwoNl0nM6+REWNwuFIre8cjyMY9BAI1KC1B4ejd30NpB9WayxWazQ2W1znHtpcdDkSFIQ4AovFidPZ+5C5p7a4vM9Xjtu9m2CwjmDQSyBQTW3tVqqrN1Jbu4Xq6o34fMVo7alPPxKl7AQCFc2mFxk5kvj4GcTGnoLXe4C6up14vf2Vg4gAAAo5SURBVHnY7Uk4nWn1V4zH1PePxOB09sbp7INSdtzuPVRVfYPXm098/Ayiooa318ciuigJCkK0M7s9Drt9/CFzzz/oldaaYNCDxeJoOJXWNGPtw+3eTyBQRSBQjddbQHn5cvLznyI392EArNZoHI7e+HzF+P2lLebDYnERDLoPmhcVNZr4+LPw+8twu7MJBKqIihpFTMx4oqJGYbPFY7X2qO8/saOUDaXs9dPBAx9qrXG791FdvZ5AoJrExPOkVtMFyCmpQpwEAgE3tbVbcTp7Y7f3aiigA4FavN7C+iasGvz+SjyeXDyeHPz+MiIjRxATMw6bLYGSkqUUFf2bysrV2O3JuFx9sVgiqakxNZcjs9QHMWd9wKkjEKhseFcpJz17ziY+/iy83kI8nv0EAjVERAwiImIITmdflLLXBxs7FosLiyUCpSwEgx6CQQ9KWbHbk7DbE1Dq8Ps5hMorGZn36MkwF0KIZmmtDypUtdZ4PLnU1m4jEKjE768kEKhCaz9a+9DaRzDoq3/trS/A3Shlb7iYUClFYeHLFBa+gt9fApizuKzWSNzu/YQuQGw7C3Z7Ei5X34ZgUle3i7q6nQSDtfVnhaXWP/bCbu9V3+Fvq5+sgAWlLFitsbhc/XG50rHZ4ur3wUcwWEsgUIXfX4Xd3pOIiEFdOthIUBBCnHDBoBePJweHo3fDBYLBoBe3ey8eT16TQOMlGHQTDLrROoDF4kQpJxDA6y3C5zuA15uP252Nx5ON1t76kXcHY7VGN4ynZU49Lqrvo/G3nrkjcDhSiYubjs0WR13dTurqdqK1H7s9GYcjpX6kX1NeWiwR9ScLJKK1B48nD683D6Xs9f08afUBzdzJUOsAXm8eXm9B/f666idnw3Mz/pcTpRzY7aYZrz2DlFy8JoQ44SwWBxERAw+bFxk5lMjIoWHbrumjqasPOv76AKHROojfX1rfV7MXv7+yvgnMjsUSgc3WA6s1Bo9nP+XlKykvX0kwWFd/dfx3sVgc9de95BEM1gGNzXZ+fwmBQDVgaai5aO2jouKLhtrS8VDKicPRC4vFxf9v7+5i7KrKMI7/n06Z2mkNFSikX9IiE6QYKdiQKmIa6gUIsVyggoCEiHiBEYhGwWiMJF6QGKsGghBAizQVLAUbL/wqpMoFhYGiQquxQYEhpTOJpYriDO28Xqx1jofTmc7p9HzM2fv5JZM5+2P2rJV35rxnr733uypnPQsWfI4lS2466mMfjpOCmXU9SRPO2TFr1gLmzDlj0mMsXPj5I/69Y2MjQM8hhRoPHnyTkZHBPL3tS0jHVIe7ZszozWdJb1aH4tLXKBHp2sqBA/sYHR1idHQvESP5WsoYvb0nHnEbj5STgpnZFE00F0hPz2z6+vrp6+tvc4uOXkvLSkq6QNJfJO2WdPM422dJejBv3y5paSvbY2Zmh9eypKB0+f8O4EJgOXC5pOV1u30W2BcRpwLrgNta1R4zM5tcK88UzgF2R8SLETEK/BRYW7fPWmB9fr0JWKMi3xNmZjbNtTIpLAJeqVkezOvG3SfS7QL7gePrDyTpOkkDkgaGh+vr6JuZWbN0xVRVEXF3RKyMiJXz58/vdHPMzAqrlUnhVWBJzfLivG7cfSTNBI4Fjv4GXzMzm5JWJoWngX5JyyT1ApcBW+r22QJcnV9fCjwW3faItZlZgbTsOYWIOCDpC8CvgB7gvoh4QdKtwEBEbAHuBX4iaTfwD1LiMDOzDum62keShoGXpvjjJwCNlIMsCve3uMrUV3B/m+HkiJj0omzXJYWjIWmgkYJQReH+FleZ+grubzt1xd1HZmbWHk4KZmZWVbakcHenG9Bm7m9xlamv4P62TamuKZiZ2eGV7UzBzMwOozRJYbIy3t1O0hJJj0vaKekFSTfk9cdJ+o2kv+bv7+p0W5tFUo+kHZJ+kZeX5RLsu3NJ9t5Ot7FZJM2TtEnSnyXtkvTBgsf2pvx3/LykjZLeUaT4SrpP0pCk52vWjRtPJT/I/f6jpLNb2bZSJIUGy3h3uwPAlyJiObAKuD738WZga0T0A1vzclHcAOyqWb4NWJdLse8jlWYviu8Dv4yI9wJnkvpdyNhKWgR8EVgZEe8jPfx6GcWK74+BC+rWTRTPC4H+/HUdcGcrG1aKpEBjZby7WkTsiYhn8+t/kd40FvH28uTrgUs608LmkrQYuAi4Jy8LOJ9Ugh2K1ddjgY+QKgAQEaMR8ToFjW02E5ida6L1AXsoUHwj4nekKg61JornWuD+SJ4E5kla0Kq2lSUpNFLGuzDyDHZnAduBkyJiT970GnBSh5rVbN8DvgKM5eXjgddzCXYoVoyXAcPAj/Jw2T2S5lDQ2EbEq8B3gJdJyWA/8AzFjW/FRPFs6/tXWZJCaUiaCzwM3BgR/6zdlosNdv3tZpIuBoYi4plOt6VNZgJnA3dGxFnAv6kbKipKbAHyWPpaUjJcCMzh0KGWQutkPMuSFBop4931JB1DSggbImJzXr23cqqZvw91qn1NdC7wcUl/Jw0Fnk8ac5+XhxugWDEeBAYjYnte3kRKEkWMLcBHgb9FxHBEvAVsJsW8qPGtmCiebX3/KktSaKSMd1fLY+r3Arsi4rs1m2rLk18N/LzdbWu2iLglIhZHxFJSLB+LiCuAx0kl2KEgfQWIiNeAVySdlletAXZSwNhmLwOrJPXlv+tKfwsZ3xoTxXML8Jl8F9IqYH/NMFPTlebhNUkfI41DV8p4f7vDTWoqSR8Gfg/8if+Ps3+NdF3hIeDdpOqyn4yI+gtcXUvSauDLEXGxpFNIZw7HATuAKyNipJPtaxZJK0gX1XuBF4FrSB/qChlbSd8CPkW6q24HcC1pHL0Q8ZW0EVhNqoa6F/gm8CjjxDMnxttJQ2j/Aa6JiIGWta0sScHMzCZXluEjMzNrgJOCmZlVOSmYmVmVk4KZmVU5KZiZWZWTglkbSVpdqepqNh05KZiZWZWTgtk4JF0p6SlJz0m6K8/d8IakdbnO/1ZJ8/O+KyQ9mWvdP1JTB/9USb+V9AdJz0p6Tz783Jq5ETbkh5PMpgUnBbM6kk4nPU17bkSsAA4CV5AKsw1ExBnANtJTqAD3A1+NiPeTniivrN8A3BERZwIfIlX8hFTB9kbS3B6nkOr6mE0LMyffxax01gAfAJ7OH+Jnk4qTjQEP5n0eADbnuQ7mRcS2vH498DNJ7wQWRcQjABHxX4B8vKciYjAvPwcsBZ5ofbfMJuekYHYoAesj4pa3rZS+UbffVGvE1NbrOYj/D20a8fCR2aG2ApdKOhGqc+eeTPp/qVTp/DTwRETsB/ZJOi+vvwrYlme/G5R0ST7GLEl9be2F2RT4E4pZnYjYKenrwK8lzQDeAq4nTW5zTt42RLruAKnM8Q/zm36lgimkBHGXpFvzMT7Rxm6YTYmrpJo1SNIbETG30+0wayUPH5mZWZXPFMzMrMpnCmZmVuWkYGZmVU4KZmZW5aRgZmZVTgpmZlblpGBmZlX/A6OuYDUNcawkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 442us/sample - loss: 0.3543 - acc: 0.9018\n",
      "Loss: 0.3543392738511134 Accuracy: 0.9017653\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4558 - acc: 0.2752\n",
      "Epoch 00001: val_loss improved from inf to 1.69170, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/001-1.6917.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 2.4559 - acc: 0.2753 - val_loss: 1.6917 - val_acc: 0.4200\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4958 - acc: 0.5218\n",
      "Epoch 00002: val_loss improved from 1.69170 to 1.04447, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/002-1.0445.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 1.4958 - acc: 0.5218 - val_loss: 1.0445 - val_acc: 0.6792\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1253 - acc: 0.6470\n",
      "Epoch 00003: val_loss improved from 1.04447 to 0.79514, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/003-0.7951.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 1.1252 - acc: 0.6470 - val_loss: 0.7951 - val_acc: 0.7696\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9021 - acc: 0.7213\n",
      "Epoch 00004: val_loss improved from 0.79514 to 0.67014, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/004-0.6701.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.9022 - acc: 0.7213 - val_loss: 0.6701 - val_acc: 0.8027\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7441 - acc: 0.7748\n",
      "Epoch 00005: val_loss improved from 0.67014 to 0.57857, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/005-0.5786.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.7440 - acc: 0.7748 - val_loss: 0.5786 - val_acc: 0.8402\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6266 - acc: 0.8085\n",
      "Epoch 00006: val_loss improved from 0.57857 to 0.48429, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/006-0.4843.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.6265 - acc: 0.8086 - val_loss: 0.4843 - val_acc: 0.8628\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8351\n",
      "Epoch 00007: val_loss improved from 0.48429 to 0.46769, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/007-0.4677.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.5481 - acc: 0.8351 - val_loss: 0.4677 - val_acc: 0.8668\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.8522\n",
      "Epoch 00008: val_loss improved from 0.46769 to 0.39871, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/008-0.3987.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.4867 - acc: 0.8521 - val_loss: 0.3987 - val_acc: 0.8887\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8653\n",
      "Epoch 00009: val_loss improved from 0.39871 to 0.36305, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/009-0.3630.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.4418 - acc: 0.8653 - val_loss: 0.3630 - val_acc: 0.8975\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8762\n",
      "Epoch 00010: val_loss did not improve from 0.36305\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.4079 - acc: 0.8762 - val_loss: 0.4283 - val_acc: 0.8735\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8854\n",
      "Epoch 00011: val_loss improved from 0.36305 to 0.31860, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/011-0.3186.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3829 - acc: 0.8854 - val_loss: 0.3186 - val_acc: 0.9147\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8931\n",
      "Epoch 00012: val_loss improved from 0.31860 to 0.30604, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/012-0.3060.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.3478 - acc: 0.8931 - val_loss: 0.3060 - val_acc: 0.9133\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9013\n",
      "Epoch 00013: val_loss improved from 0.30604 to 0.30006, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/013-0.3001.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.3245 - acc: 0.9013 - val_loss: 0.3001 - val_acc: 0.9152\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9040\n",
      "Epoch 00014: val_loss did not improve from 0.30006\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.3101 - acc: 0.9040 - val_loss: 0.3230 - val_acc: 0.9117\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9104\n",
      "Epoch 00015: val_loss improved from 0.30006 to 0.28384, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/015-0.2838.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2926 - acc: 0.9104 - val_loss: 0.2838 - val_acc: 0.9238\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9140\n",
      "Epoch 00016: val_loss did not improve from 0.28384\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.2788 - acc: 0.9140 - val_loss: 0.3347 - val_acc: 0.9036\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9191\n",
      "Epoch 00017: val_loss improved from 0.28384 to 0.28141, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/017-0.2814.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.2646 - acc: 0.9191 - val_loss: 0.2814 - val_acc: 0.9213\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9238\n",
      "Epoch 00018: val_loss improved from 0.28141 to 0.25700, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/018-0.2570.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.2440 - acc: 0.9238 - val_loss: 0.2570 - val_acc: 0.9297\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9243\n",
      "Epoch 00019: val_loss did not improve from 0.25700\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2415 - acc: 0.9243 - val_loss: 0.3033 - val_acc: 0.9131\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9281\n",
      "Epoch 00020: val_loss improved from 0.25700 to 0.24989, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/020-0.2499.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2318 - acc: 0.9281 - val_loss: 0.2499 - val_acc: 0.9331\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9323\n",
      "Epoch 00021: val_loss did not improve from 0.24989\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2210 - acc: 0.9323 - val_loss: 0.2702 - val_acc: 0.9278\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9323\n",
      "Epoch 00022: val_loss did not improve from 0.24989\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2159 - acc: 0.9323 - val_loss: 0.2617 - val_acc: 0.9292\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9363\n",
      "Epoch 00023: val_loss improved from 0.24989 to 0.23723, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/023-0.2372.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.2034 - acc: 0.9363 - val_loss: 0.2372 - val_acc: 0.9311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9368\n",
      "Epoch 00024: val_loss did not improve from 0.23723\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1991 - acc: 0.9369 - val_loss: 0.2638 - val_acc: 0.9266\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9415\n",
      "Epoch 00025: val_loss did not improve from 0.23723\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1897 - acc: 0.9415 - val_loss: 0.2397 - val_acc: 0.9371\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9425\n",
      "Epoch 00026: val_loss did not improve from 0.23723\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1826 - acc: 0.9425 - val_loss: 0.2608 - val_acc: 0.9287\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9451\n",
      "Epoch 00027: val_loss improved from 0.23723 to 0.21369, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/027-0.2137.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1766 - acc: 0.9451 - val_loss: 0.2137 - val_acc: 0.9408\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9455\n",
      "Epoch 00028: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1722 - acc: 0.9455 - val_loss: 0.2234 - val_acc: 0.9380\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9468\n",
      "Epoch 00029: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1697 - acc: 0.9467 - val_loss: 0.2437 - val_acc: 0.9364\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9504\n",
      "Epoch 00030: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1606 - acc: 0.9504 - val_loss: 0.2157 - val_acc: 0.9418\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9514\n",
      "Epoch 00031: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1563 - acc: 0.9514 - val_loss: 0.2284 - val_acc: 0.9362\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9534\n",
      "Epoch 00032: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1503 - acc: 0.9534 - val_loss: 0.2194 - val_acc: 0.9418\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9540\n",
      "Epoch 00033: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1483 - acc: 0.9540 - val_loss: 0.2327 - val_acc: 0.9390\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9546\n",
      "Epoch 00034: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1439 - acc: 0.9546 - val_loss: 0.2475 - val_acc: 0.9331\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9539\n",
      "Epoch 00035: val_loss did not improve from 0.21369\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1437 - acc: 0.9538 - val_loss: 0.2483 - val_acc: 0.9369\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9558\n",
      "Epoch 00036: val_loss improved from 0.21369 to 0.21125, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/036-0.2113.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1385 - acc: 0.9558 - val_loss: 0.2113 - val_acc: 0.9441\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9584\n",
      "Epoch 00037: val_loss did not improve from 0.21125\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1292 - acc: 0.9583 - val_loss: 0.2152 - val_acc: 0.9415\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9604\n",
      "Epoch 00038: val_loss did not improve from 0.21125\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1243 - acc: 0.9604 - val_loss: 0.2354 - val_acc: 0.9378\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9613\n",
      "Epoch 00039: val_loss did not improve from 0.21125\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1211 - acc: 0.9613 - val_loss: 0.2186 - val_acc: 0.9441\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9618\n",
      "Epoch 00040: val_loss did not improve from 0.21125\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1212 - acc: 0.9617 - val_loss: 0.2457 - val_acc: 0.9422\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9606\n",
      "Epoch 00041: val_loss improved from 0.21125 to 0.20781, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/041-0.2078.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1209 - acc: 0.9605 - val_loss: 0.2078 - val_acc: 0.9436\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9634\n",
      "Epoch 00042: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1146 - acc: 0.9634 - val_loss: 0.2162 - val_acc: 0.9469\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9647\n",
      "Epoch 00043: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1114 - acc: 0.9647 - val_loss: 0.2339 - val_acc: 0.9364\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9638\n",
      "Epoch 00044: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1129 - acc: 0.9638 - val_loss: 0.2144 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9654\n",
      "Epoch 00045: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1069 - acc: 0.9653 - val_loss: 0.2156 - val_acc: 0.9464\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9667\n",
      "Epoch 00046: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1040 - acc: 0.9666 - val_loss: 0.2283 - val_acc: 0.9432\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9659\n",
      "Epoch 00047: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1054 - acc: 0.9659 - val_loss: 0.2304 - val_acc: 0.9373\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9685\n",
      "Epoch 00048: val_loss did not improve from 0.20781\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0980 - acc: 0.9685 - val_loss: 0.2429 - val_acc: 0.9413\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9699\n",
      "Epoch 00049: val_loss improved from 0.20781 to 0.20567, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/049-0.2057.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0957 - acc: 0.9699 - val_loss: 0.2057 - val_acc: 0.9502\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9679\n",
      "Epoch 00050: val_loss did not improve from 0.20567\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1027 - acc: 0.9679 - val_loss: 0.2178 - val_acc: 0.9441\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9697\n",
      "Epoch 00051: val_loss did not improve from 0.20567\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0926 - acc: 0.9697 - val_loss: 0.2149 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9709\n",
      "Epoch 00052: val_loss improved from 0.20567 to 0.19189, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv_checkpoint/052-0.1919.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0909 - acc: 0.9709 - val_loss: 0.1919 - val_acc: 0.9476\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9722\n",
      "Epoch 00053: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0871 - acc: 0.9722 - val_loss: 0.2529 - val_acc: 0.9390\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9733\n",
      "Epoch 00054: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0836 - acc: 0.9733 - val_loss: 0.2113 - val_acc: 0.9467\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9749\n",
      "Epoch 00055: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0790 - acc: 0.9748 - val_loss: 0.2069 - val_acc: 0.9469\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9738\n",
      "Epoch 00056: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0839 - acc: 0.9738 - val_loss: 0.2183 - val_acc: 0.9474\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9738\n",
      "Epoch 00057: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0803 - acc: 0.9738 - val_loss: 0.2265 - val_acc: 0.9476\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9733\n",
      "Epoch 00058: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0827 - acc: 0.9733 - val_loss: 0.2075 - val_acc: 0.9518\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9754\n",
      "Epoch 00059: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0759 - acc: 0.9754 - val_loss: 0.2257 - val_acc: 0.9453\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9745\n",
      "Epoch 00060: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0762 - acc: 0.9744 - val_loss: 0.2286 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9740\n",
      "Epoch 00061: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0818 - acc: 0.9739 - val_loss: 0.2301 - val_acc: 0.9429\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9758\n",
      "Epoch 00062: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0759 - acc: 0.9758 - val_loss: 0.2026 - val_acc: 0.9455\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9785\n",
      "Epoch 00063: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0681 - acc: 0.9785 - val_loss: 0.2224 - val_acc: 0.9460\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9784\n",
      "Epoch 00064: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0681 - acc: 0.9784 - val_loss: 0.2248 - val_acc: 0.9455\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9784\n",
      "Epoch 00065: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0670 - acc: 0.9783 - val_loss: 0.2319 - val_acc: 0.9406\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9750\n",
      "Epoch 00066: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0763 - acc: 0.9750 - val_loss: 0.2272 - val_acc: 0.9467\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9786\n",
      "Epoch 00067: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0650 - acc: 0.9786 - val_loss: 0.2233 - val_acc: 0.9478\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9780\n",
      "Epoch 00068: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0675 - acc: 0.9780 - val_loss: 0.2182 - val_acc: 0.9462\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9793\n",
      "Epoch 00069: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0642 - acc: 0.9793 - val_loss: 0.2263 - val_acc: 0.9488\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9798\n",
      "Epoch 00070: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0618 - acc: 0.9798 - val_loss: 0.2345 - val_acc: 0.9460\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9801\n",
      "Epoch 00071: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0608 - acc: 0.9801 - val_loss: 0.2270 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9801\n",
      "Epoch 00072: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0636 - acc: 0.9801 - val_loss: 0.2438 - val_acc: 0.9411\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9806\n",
      "Epoch 00073: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0613 - acc: 0.9806 - val_loss: 0.2307 - val_acc: 0.9408\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9817\n",
      "Epoch 00074: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0583 - acc: 0.9817 - val_loss: 0.2074 - val_acc: 0.9490\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9822\n",
      "Epoch 00075: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0578 - acc: 0.9822 - val_loss: 0.2167 - val_acc: 0.9443\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9818\n",
      "Epoch 00076: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0569 - acc: 0.9818 - val_loss: 0.2238 - val_acc: 0.9460\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9823\n",
      "Epoch 00077: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0552 - acc: 0.9823 - val_loss: 0.2180 - val_acc: 0.9476\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9824\n",
      "Epoch 00078: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0530 - acc: 0.9824 - val_loss: 0.2188 - val_acc: 0.9453\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9833\n",
      "Epoch 00079: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0519 - acc: 0.9833 - val_loss: 0.2463 - val_acc: 0.9432\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9846\n",
      "Epoch 00080: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0511 - acc: 0.9846 - val_loss: 0.2178 - val_acc: 0.9467\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9831\n",
      "Epoch 00081: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0522 - acc: 0.9831 - val_loss: 0.2389 - val_acc: 0.9432\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9837\n",
      "Epoch 00082: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0505 - acc: 0.9837 - val_loss: 0.2521 - val_acc: 0.9448\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9835\n",
      "Epoch 00083: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0523 - acc: 0.9835 - val_loss: 0.2605 - val_acc: 0.9441\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9813\n",
      "Epoch 00084: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0573 - acc: 0.9813 - val_loss: 0.2554 - val_acc: 0.9439\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9835\n",
      "Epoch 00085: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0510 - acc: 0.9835 - val_loss: 0.2130 - val_acc: 0.9502\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9846\n",
      "Epoch 00086: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0483 - acc: 0.9846 - val_loss: 0.2356 - val_acc: 0.9471\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9842\n",
      "Epoch 00087: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0482 - acc: 0.9841 - val_loss: 0.2149 - val_acc: 0.9492\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9845\n",
      "Epoch 00088: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0463 - acc: 0.9845 - val_loss: 0.2338 - val_acc: 0.9469\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9864\n",
      "Epoch 00089: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0436 - acc: 0.9864 - val_loss: 0.2271 - val_acc: 0.9492\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9858\n",
      "Epoch 00090: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0454 - acc: 0.9858 - val_loss: 0.2290 - val_acc: 0.9488\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9865\n",
      "Epoch 00091: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0430 - acc: 0.9865 - val_loss: 0.2438 - val_acc: 0.9462\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9845\n",
      "Epoch 00092: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0478 - acc: 0.9845 - val_loss: 0.2936 - val_acc: 0.9399\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9839\n",
      "Epoch 00093: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0493 - acc: 0.9839 - val_loss: 0.2204 - val_acc: 0.9511\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9872\n",
      "Epoch 00094: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0404 - acc: 0.9872 - val_loss: 0.2293 - val_acc: 0.9495\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9867\n",
      "Epoch 00095: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0428 - acc: 0.9867 - val_loss: 0.2378 - val_acc: 0.9471\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9819\n",
      "Epoch 00096: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0549 - acc: 0.9819 - val_loss: 0.2496 - val_acc: 0.9441\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9864\n",
      "Epoch 00097: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0416 - acc: 0.9864 - val_loss: 0.2433 - val_acc: 0.9481\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 00098: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0391 - acc: 0.9870 - val_loss: 0.2258 - val_acc: 0.9541\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9865\n",
      "Epoch 00099: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0417 - acc: 0.9865 - val_loss: 0.2395 - val_acc: 0.9478\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9873\n",
      "Epoch 00100: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0398 - acc: 0.9873 - val_loss: 0.2504 - val_acc: 0.9467\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9879\n",
      "Epoch 00101: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0381 - acc: 0.9879 - val_loss: 0.2315 - val_acc: 0.9518\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9880\n",
      "Epoch 00102: val_loss did not improve from 0.19189\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0372 - acc: 0.9880 - val_loss: 0.2283 - val_acc: 0.9532\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 500us/sample - loss: 0.2457 - acc: 0.9325\n",
      "Loss: 0.24573215460356398 Accuracy: 0.93250257\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6129 - acc: 0.2370\n",
      "Epoch 00001: val_loss improved from inf to 1.76782, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/001-1.7678.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.6128 - acc: 0.2370 - val_loss: 1.7678 - val_acc: 0.4379\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5610 - acc: 0.4987\n",
      "Epoch 00002: val_loss improved from 1.76782 to 0.99925, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/002-0.9993.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 1.5610 - acc: 0.4987 - val_loss: 0.9993 - val_acc: 0.6897\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1183 - acc: 0.6463\n",
      "Epoch 00003: val_loss improved from 0.99925 to 0.74120, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/003-0.7412.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 1.1183 - acc: 0.6463 - val_loss: 0.7412 - val_acc: 0.7824\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8429 - acc: 0.7382\n",
      "Epoch 00004: val_loss improved from 0.74120 to 0.57350, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/004-0.5735.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.8430 - acc: 0.7382 - val_loss: 0.5735 - val_acc: 0.8314\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6811 - acc: 0.7885\n",
      "Epoch 00005: val_loss improved from 0.57350 to 0.42914, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/005-0.4291.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.6812 - acc: 0.7885 - val_loss: 0.4291 - val_acc: 0.8789\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.8258\n",
      "Epoch 00006: val_loss improved from 0.42914 to 0.38850, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/006-0.3885.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.5721 - acc: 0.8258 - val_loss: 0.3885 - val_acc: 0.8880\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8456\n",
      "Epoch 00007: val_loss improved from 0.38850 to 0.35602, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/007-0.3560.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.5014 - acc: 0.8456 - val_loss: 0.3560 - val_acc: 0.8961\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4486 - acc: 0.8641\n",
      "Epoch 00008: val_loss improved from 0.35602 to 0.34988, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/008-0.3499.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.4486 - acc: 0.8641 - val_loss: 0.3499 - val_acc: 0.9005\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8756\n",
      "Epoch 00009: val_loss improved from 0.34988 to 0.30598, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/009-0.3060.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4038 - acc: 0.8756 - val_loss: 0.3060 - val_acc: 0.9129\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8882\n",
      "Epoch 00010: val_loss improved from 0.30598 to 0.25379, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/010-0.2538.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3669 - acc: 0.8881 - val_loss: 0.2538 - val_acc: 0.9273\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8951\n",
      "Epoch 00011: val_loss did not improve from 0.25379\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.3418 - acc: 0.8950 - val_loss: 0.2641 - val_acc: 0.9273\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9015\n",
      "Epoch 00012: val_loss improved from 0.25379 to 0.22569, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/012-0.2257.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3153 - acc: 0.9015 - val_loss: 0.2257 - val_acc: 0.9359\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9080\n",
      "Epoch 00013: val_loss did not improve from 0.22569\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.2997 - acc: 0.9080 - val_loss: 0.2343 - val_acc: 0.9357\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9141\n",
      "Epoch 00014: val_loss did not improve from 0.22569\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.2764 - acc: 0.9141 - val_loss: 0.2266 - val_acc: 0.9313\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9176\n",
      "Epoch 00015: val_loss did not improve from 0.22569\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2632 - acc: 0.9175 - val_loss: 0.2281 - val_acc: 0.9320\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9230\n",
      "Epoch 00016: val_loss improved from 0.22569 to 0.19606, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/016-0.1961.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2493 - acc: 0.9230 - val_loss: 0.1961 - val_acc: 0.9436\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9242\n",
      "Epoch 00017: val_loss improved from 0.19606 to 0.19267, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/017-0.1927.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.2396 - acc: 0.9242 - val_loss: 0.1927 - val_acc: 0.9464\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9312\n",
      "Epoch 00018: val_loss did not improve from 0.19267\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2201 - acc: 0.9312 - val_loss: 0.1933 - val_acc: 0.9418\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9332\n",
      "Epoch 00019: val_loss improved from 0.19267 to 0.18551, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/019-0.1855.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.2149 - acc: 0.9332 - val_loss: 0.1855 - val_acc: 0.9467\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9372\n",
      "Epoch 00020: val_loss improved from 0.18551 to 0.17298, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/020-0.1730.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.2027 - acc: 0.9372 - val_loss: 0.1730 - val_acc: 0.9485\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9386\n",
      "Epoch 00021: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1956 - acc: 0.9386 - val_loss: 0.1826 - val_acc: 0.9450\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9418\n",
      "Epoch 00022: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1869 - acc: 0.9419 - val_loss: 0.1928 - val_acc: 0.9427\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9438\n",
      "Epoch 00023: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1772 - acc: 0.9438 - val_loss: 0.1839 - val_acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9452\n",
      "Epoch 00024: val_loss did not improve from 0.17298\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1716 - acc: 0.9452 - val_loss: 0.1801 - val_acc: 0.9471\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9478\n",
      "Epoch 00025: val_loss improved from 0.17298 to 0.15921, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/025-0.1592.hdf5\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.1657 - acc: 0.9478 - val_loss: 0.1592 - val_acc: 0.9546\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9502\n",
      "Epoch 00026: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.1612 - acc: 0.9502 - val_loss: 0.1622 - val_acc: 0.9529\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9521\n",
      "Epoch 00027: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.1522 - acc: 0.9520 - val_loss: 0.1668 - val_acc: 0.9499\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9529\n",
      "Epoch 00028: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1468 - acc: 0.9529 - val_loss: 0.1873 - val_acc: 0.9441\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9548\n",
      "Epoch 00029: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1447 - acc: 0.9548 - val_loss: 0.1658 - val_acc: 0.9520\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9555\n",
      "Epoch 00030: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1372 - acc: 0.9555 - val_loss: 0.1654 - val_acc: 0.9478\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9574\n",
      "Epoch 00031: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.1339 - acc: 0.9574 - val_loss: 0.1604 - val_acc: 0.9495\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9590\n",
      "Epoch 00032: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1311 - acc: 0.9591 - val_loss: 0.1912 - val_acc: 0.9460\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9598\n",
      "Epoch 00033: val_loss did not improve from 0.15921\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.1263 - acc: 0.9598 - val_loss: 0.1594 - val_acc: 0.9541\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9619\n",
      "Epoch 00034: val_loss improved from 0.15921 to 0.15740, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/034-0.1574.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1215 - acc: 0.9619 - val_loss: 0.1574 - val_acc: 0.9541\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9634\n",
      "Epoch 00035: val_loss did not improve from 0.15740\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.1129 - acc: 0.9634 - val_loss: 0.1584 - val_acc: 0.9541\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9645\n",
      "Epoch 00036: val_loss did not improve from 0.15740\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.1104 - acc: 0.9645 - val_loss: 0.1616 - val_acc: 0.9527\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9642\n",
      "Epoch 00037: val_loss improved from 0.15740 to 0.14308, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv_checkpoint/037-0.1431.hdf5\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.1096 - acc: 0.9642 - val_loss: 0.1431 - val_acc: 0.9578\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9674\n",
      "Epoch 00038: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.1008 - acc: 0.9674 - val_loss: 0.1814 - val_acc: 0.9446\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9667\n",
      "Epoch 00039: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1033 - acc: 0.9667 - val_loss: 0.1572 - val_acc: 0.9564\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9681\n",
      "Epoch 00040: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.1000 - acc: 0.9680 - val_loss: 0.1563 - val_acc: 0.9513\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9688\n",
      "Epoch 00041: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0961 - acc: 0.9688 - val_loss: 0.1717 - val_acc: 0.9455\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9705\n",
      "Epoch 00042: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0940 - acc: 0.9705 - val_loss: 0.1593 - val_acc: 0.9527\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9719\n",
      "Epoch 00043: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0884 - acc: 0.9718 - val_loss: 0.1857 - val_acc: 0.9464\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9695\n",
      "Epoch 00044: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0982 - acc: 0.9695 - val_loss: 0.1586 - val_acc: 0.9560\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9718\n",
      "Epoch 00045: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0881 - acc: 0.9718 - val_loss: 0.2005 - val_acc: 0.9455\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9726\n",
      "Epoch 00046: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0862 - acc: 0.9726 - val_loss: 0.1558 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9757\n",
      "Epoch 00047: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0791 - acc: 0.9757 - val_loss: 0.1676 - val_acc: 0.9509\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9744\n",
      "Epoch 00048: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0794 - acc: 0.9744 - val_loss: 0.1642 - val_acc: 0.9539\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9766\n",
      "Epoch 00049: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0750 - acc: 0.9766 - val_loss: 0.1531 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9751\n",
      "Epoch 00050: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0757 - acc: 0.9751 - val_loss: 0.1598 - val_acc: 0.9525\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9766\n",
      "Epoch 00051: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0722 - acc: 0.9766 - val_loss: 0.1453 - val_acc: 0.9588\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9774\n",
      "Epoch 00052: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0696 - acc: 0.9774 - val_loss: 0.1909 - val_acc: 0.9469\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9769\n",
      "Epoch 00053: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0719 - acc: 0.9769 - val_loss: 0.1466 - val_acc: 0.9588\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9781\n",
      "Epoch 00054: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0655 - acc: 0.9781 - val_loss: 0.1502 - val_acc: 0.9583\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9776\n",
      "Epoch 00055: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0676 - acc: 0.9776 - val_loss: 0.1617 - val_acc: 0.9550\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9786\n",
      "Epoch 00056: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0655 - acc: 0.9786 - val_loss: 0.1502 - val_acc: 0.9576\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9801\n",
      "Epoch 00057: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0608 - acc: 0.9801 - val_loss: 0.1725 - val_acc: 0.9529\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9808\n",
      "Epoch 00058: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0605 - acc: 0.9807 - val_loss: 0.1676 - val_acc: 0.9543\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9790\n",
      "Epoch 00059: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0637 - acc: 0.9790 - val_loss: 0.1674 - val_acc: 0.9543\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9811\n",
      "Epoch 00060: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0578 - acc: 0.9811 - val_loss: 0.1681 - val_acc: 0.9543\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9808\n",
      "Epoch 00061: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0598 - acc: 0.9808 - val_loss: 0.1654 - val_acc: 0.9553\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9807\n",
      "Epoch 00062: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0616 - acc: 0.9807 - val_loss: 0.1521 - val_acc: 0.9590\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9828\n",
      "Epoch 00063: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0536 - acc: 0.9828 - val_loss: 0.1727 - val_acc: 0.9504\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9834\n",
      "Epoch 00064: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0534 - acc: 0.9834 - val_loss: 0.1816 - val_acc: 0.9504\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9831\n",
      "Epoch 00065: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0526 - acc: 0.9830 - val_loss: 0.1613 - val_acc: 0.9581\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9834\n",
      "Epoch 00066: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0517 - acc: 0.9833 - val_loss: 0.1936 - val_acc: 0.9488\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9805\n",
      "Epoch 00067: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0610 - acc: 0.9805 - val_loss: 0.1639 - val_acc: 0.9578\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9858\n",
      "Epoch 00068: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0442 - acc: 0.9858 - val_loss: 0.1991 - val_acc: 0.9448\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9838\n",
      "Epoch 00069: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0500 - acc: 0.9838 - val_loss: 0.1726 - val_acc: 0.9511\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9852\n",
      "Epoch 00070: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0454 - acc: 0.9852 - val_loss: 0.1783 - val_acc: 0.9536\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9863\n",
      "Epoch 00071: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0434 - acc: 0.9863 - val_loss: 0.1605 - val_acc: 0.9595\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9836\n",
      "Epoch 00072: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0500 - acc: 0.9836 - val_loss: 0.1478 - val_acc: 0.9611\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9836\n",
      "Epoch 00073: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0522 - acc: 0.9836 - val_loss: 0.1611 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9852\n",
      "Epoch 00074: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0451 - acc: 0.9852 - val_loss: 0.1600 - val_acc: 0.9583\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9868\n",
      "Epoch 00075: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0412 - acc: 0.9867 - val_loss: 0.1677 - val_acc: 0.9557\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9830\n",
      "Epoch 00076: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0533 - acc: 0.9830 - val_loss: 0.1636 - val_acc: 0.9581\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9883\n",
      "Epoch 00077: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0378 - acc: 0.9883 - val_loss: 0.2002 - val_acc: 0.9492\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9865\n",
      "Epoch 00078: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0443 - acc: 0.9865 - val_loss: 0.1869 - val_acc: 0.9534\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9877\n",
      "Epoch 00079: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0399 - acc: 0.9877 - val_loss: 0.1626 - val_acc: 0.9548\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9874\n",
      "Epoch 00080: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0389 - acc: 0.9874 - val_loss: 0.1606 - val_acc: 0.9599\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9864\n",
      "Epoch 00081: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0451 - acc: 0.9864 - val_loss: 0.1954 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9879\n",
      "Epoch 00082: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0380 - acc: 0.9879 - val_loss: 0.1558 - val_acc: 0.9595\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9884\n",
      "Epoch 00083: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0365 - acc: 0.9883 - val_loss: 0.1603 - val_acc: 0.9557\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9864\n",
      "Epoch 00084: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0403 - acc: 0.9864 - val_loss: 0.1727 - val_acc: 0.9534\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9896\n",
      "Epoch 00085: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0338 - acc: 0.9896 - val_loss: 0.1865 - val_acc: 0.9520\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9876\n",
      "Epoch 00086: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0406 - acc: 0.9876 - val_loss: 0.1532 - val_acc: 0.9599\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9886\n",
      "Epoch 00087: val_loss did not improve from 0.14308\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0352 - acc: 0.9886 - val_loss: 0.1525 - val_acc: 0.9611\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 542us/sample - loss: 0.2036 - acc: 0.9377\n",
      "Loss: 0.20359950304681268 Accuracy: 0.9376947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 246464)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,954,320\n",
      "Trainable params: 3,954,128\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 563us/sample - loss: 1.9066 - acc: 0.5223\n",
      "Loss: 1.9066379495870287 Accuracy: 0.52232605\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 82112)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,968\n",
      "Trainable params: 1,329,712\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 518us/sample - loss: 1.2882 - acc: 0.6754\n",
      "Loss: 1.288201423696516 Accuracy: 0.6753894\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 29408)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 497,264\n",
      "Trainable params: 496,880\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 568us/sample - loss: 0.7644 - acc: 0.8154\n",
      "Loss: 0.7643917271281329 Accuracy: 0.81536865\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 236,464\n",
      "Trainable params: 235,952\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 629us/sample - loss: 0.3543 - acc: 0.9018\n",
      "Loss: 0.3543392738511134 Accuracy: 0.9017653\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 5952)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 163,568\n",
      "Trainable params: 162,928\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 601us/sample - loss: 0.2457 - acc: 0.9325\n",
      "Loss: 0.24573215460356398 Accuracy: 0.93250257\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1920)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 119,856\n",
      "Trainable params: 119,088\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 635us/sample - loss: 0.2036 - acc: 0.9377\n",
      "Loss: 0.20359950304681268 Accuracy: 0.9376947\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 246464)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,954,320\n",
      "Trainable params: 3,954,128\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 622us/sample - loss: 3.0336 - acc: 0.5909\n",
      "Loss: 3.0336455184227455 Accuracy: 0.5908619\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 82112)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,968\n",
      "Trainable params: 1,329,712\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 677us/sample - loss: 2.0450 - acc: 0.6646\n",
      "Loss: 2.0450161249590564 Accuracy: 0.6645898\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 29408)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 497,264\n",
      "Trainable params: 496,880\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 659us/sample - loss: 0.8197 - acc: 0.8293\n",
      "Loss: 0.8197246539134722 Accuracy: 0.8292835\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 236,464\n",
      "Trainable params: 235,952\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 671us/sample - loss: 0.4024 - acc: 0.9024\n",
      "Loss: 0.4023934615518693 Accuracy: 0.9023884\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 5952)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 163,568\n",
      "Trainable params: 162,928\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 775us/sample - loss: 0.2778 - acc: 0.9331\n",
      "Loss: 0.2777513097379809 Accuracy: 0.9331257\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1920)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 119,856\n",
      "Trainable params: 119,088\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.2230 - acc: 0.9445\n",
      "Loss: 0.22295345576627032 Accuracy: 0.9445483\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
