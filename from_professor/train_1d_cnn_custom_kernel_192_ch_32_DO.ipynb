{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    kernel_size = 64\n",
    "    filter_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3*kernel_size, filters=filter_size, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        target_kernel_size = 3 * (kernel_size//(2**(i+1)))\n",
    "        model.add(Conv1D (kernel_size=target_kernel_size if target_kernel_size != 0 else 3, \n",
    "                          filters=filter_size*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#     model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9906 - acc: 0.3459\n",
      "Epoch 00001: val_loss improved from inf to 1.54743, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/001-1.5474.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.9905 - acc: 0.3459 - val_loss: 1.5474 - val_acc: 0.5055\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4220 - acc: 0.5515\n",
      "Epoch 00002: val_loss improved from 1.54743 to 1.21399, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/002-1.2140.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.4220 - acc: 0.5515 - val_loss: 1.2140 - val_acc: 0.6252\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.6465\n",
      "Epoch 00003: val_loss improved from 1.21399 to 1.06481, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/003-1.0648.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.1613 - acc: 0.6465 - val_loss: 1.0648 - val_acc: 0.6834\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9976 - acc: 0.6961\n",
      "Epoch 00004: val_loss improved from 1.06481 to 0.94503, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/004-0.9450.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.9975 - acc: 0.6961 - val_loss: 0.9450 - val_acc: 0.7065\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8776 - acc: 0.7352\n",
      "Epoch 00005: val_loss improved from 0.94503 to 0.88133, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/005-0.8813.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8776 - acc: 0.7352 - val_loss: 0.8813 - val_acc: 0.7403\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7609\n",
      "Epoch 00006: val_loss improved from 0.88133 to 0.80696, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/006-0.8070.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7908 - acc: 0.7609 - val_loss: 0.8070 - val_acc: 0.7696\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7879\n",
      "Epoch 00007: val_loss did not improve from 0.80696\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7161 - acc: 0.7879 - val_loss: 0.8748 - val_acc: 0.7480\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.8084\n",
      "Epoch 00008: val_loss improved from 0.80696 to 0.75157, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/008-0.7516.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.6471 - acc: 0.8084 - val_loss: 0.7516 - val_acc: 0.7862\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.8195\n",
      "Epoch 00009: val_loss improved from 0.75157 to 0.74213, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/009-0.7421.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.6013 - acc: 0.8195 - val_loss: 0.7421 - val_acc: 0.7899\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8416\n",
      "Epoch 00010: val_loss did not improve from 0.74213\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.5347 - acc: 0.8416 - val_loss: 0.7520 - val_acc: 0.7876\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8532\n",
      "Epoch 00011: val_loss did not improve from 0.74213\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.4930 - acc: 0.8532 - val_loss: 0.7474 - val_acc: 0.7976\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.8652\n",
      "Epoch 00012: val_loss improved from 0.74213 to 0.70319, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_3_conv_checkpoint/012-0.7032.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.4447 - acc: 0.8652 - val_loss: 0.7032 - val_acc: 0.8036\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8817\n",
      "Epoch 00013: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3960 - acc: 0.8817 - val_loss: 0.7164 - val_acc: 0.7985\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8925\n",
      "Epoch 00014: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3565 - acc: 0.8925 - val_loss: 0.7575 - val_acc: 0.7899\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.9039\n",
      "Epoch 00015: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3201 - acc: 0.9039 - val_loss: 0.7643 - val_acc: 0.7952\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9127\n",
      "Epoch 00016: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2876 - acc: 0.9127 - val_loss: 0.7717 - val_acc: 0.7983\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9224\n",
      "Epoch 00017: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2519 - acc: 0.9225 - val_loss: 0.7686 - val_acc: 0.8062\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9299\n",
      "Epoch 00018: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2307 - acc: 0.9300 - val_loss: 0.7785 - val_acc: 0.8041\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9382\n",
      "Epoch 00019: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2063 - acc: 0.9382 - val_loss: 0.8195 - val_acc: 0.8055\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9448\n",
      "Epoch 00020: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1842 - acc: 0.9448 - val_loss: 0.8207 - val_acc: 0.8053\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9494\n",
      "Epoch 00021: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1708 - acc: 0.9494 - val_loss: 0.8208 - val_acc: 0.8057\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9535\n",
      "Epoch 00022: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1559 - acc: 0.9535 - val_loss: 0.8795 - val_acc: 0.8008\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9584\n",
      "Epoch 00023: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1415 - acc: 0.9584 - val_loss: 0.8766 - val_acc: 0.8029\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9600\n",
      "Epoch 00024: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1354 - acc: 0.9600 - val_loss: 0.8770 - val_acc: 0.8050\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9632\n",
      "Epoch 00025: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1238 - acc: 0.9632 - val_loss: 0.9051 - val_acc: 0.8123\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9676\n",
      "Epoch 00026: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1157 - acc: 0.9676 - val_loss: 0.9130 - val_acc: 0.8001\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9686\n",
      "Epoch 00027: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1097 - acc: 0.9686 - val_loss: 1.0154 - val_acc: 0.7957\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9697\n",
      "Epoch 00028: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1040 - acc: 0.9697 - val_loss: 0.9338 - val_acc: 0.8111\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9739\n",
      "Epoch 00029: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0923 - acc: 0.9739 - val_loss: 0.9746 - val_acc: 0.8102\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9742\n",
      "Epoch 00030: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0932 - acc: 0.9742 - val_loss: 0.9618 - val_acc: 0.8104\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9766\n",
      "Epoch 00031: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0826 - acc: 0.9766 - val_loss: 0.9810 - val_acc: 0.8143\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9775\n",
      "Epoch 00032: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0811 - acc: 0.9775 - val_loss: 1.0000 - val_acc: 0.8088\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9805\n",
      "Epoch 00033: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0723 - acc: 0.9805 - val_loss: 1.0297 - val_acc: 0.8092\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9751\n",
      "Epoch 00034: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0848 - acc: 0.9751 - val_loss: 1.0237 - val_acc: 0.8099\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9795\n",
      "Epoch 00035: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0751 - acc: 0.9795 - val_loss: 1.0807 - val_acc: 0.7959\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9833\n",
      "Epoch 00036: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0632 - acc: 0.9833 - val_loss: 1.0033 - val_acc: 0.8148\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9799\n",
      "Epoch 00037: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0717 - acc: 0.9799 - val_loss: 1.0098 - val_acc: 0.8109\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9823\n",
      "Epoch 00038: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0654 - acc: 0.9823 - val_loss: 1.0480 - val_acc: 0.8150\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9831\n",
      "Epoch 00039: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0632 - acc: 0.9831 - val_loss: 1.0428 - val_acc: 0.8160\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9816\n",
      "Epoch 00040: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0671 - acc: 0.9816 - val_loss: 1.2749 - val_acc: 0.7768\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9838\n",
      "Epoch 00041: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0626 - acc: 0.9838 - val_loss: 1.0767 - val_acc: 0.8083\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9825\n",
      "Epoch 00042: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0648 - acc: 0.9825 - val_loss: 1.0223 - val_acc: 0.8237\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9846\n",
      "Epoch 00043: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0571 - acc: 0.9846 - val_loss: 1.0829 - val_acc: 0.8171\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9857\n",
      "Epoch 00044: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0525 - acc: 0.9857 - val_loss: 1.1217 - val_acc: 0.8062\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9839\n",
      "Epoch 00045: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0599 - acc: 0.9839 - val_loss: 1.0980 - val_acc: 0.8157\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9858\n",
      "Epoch 00046: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0534 - acc: 0.9858 - val_loss: 1.0481 - val_acc: 0.8148\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9860\n",
      "Epoch 00047: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0527 - acc: 0.9860 - val_loss: 1.0982 - val_acc: 0.8092\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9863\n",
      "Epoch 00048: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0520 - acc: 0.9863 - val_loss: 1.1388 - val_acc: 0.8148\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9867\n",
      "Epoch 00049: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0507 - acc: 0.9867 - val_loss: 1.1694 - val_acc: 0.8008\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9871\n",
      "Epoch 00050: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0496 - acc: 0.9871 - val_loss: 1.1274 - val_acc: 0.8150\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9867\n",
      "Epoch 00051: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0489 - acc: 0.9867 - val_loss: 1.1656 - val_acc: 0.8134\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9872\n",
      "Epoch 00052: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0499 - acc: 0.9872 - val_loss: 1.1462 - val_acc: 0.8169\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9892\n",
      "Epoch 00053: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0429 - acc: 0.9892 - val_loss: 1.1091 - val_acc: 0.8178\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9883\n",
      "Epoch 00054: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0450 - acc: 0.9883 - val_loss: 1.0949 - val_acc: 0.8272\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9881\n",
      "Epoch 00055: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0462 - acc: 0.9881 - val_loss: 1.1569 - val_acc: 0.8199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9863\n",
      "Epoch 00056: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0505 - acc: 0.9863 - val_loss: 1.1578 - val_acc: 0.8130\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9880\n",
      "Epoch 00057: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0458 - acc: 0.9880 - val_loss: 1.1081 - val_acc: 0.8171\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9901\n",
      "Epoch 00058: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0387 - acc: 0.9901 - val_loss: 1.1377 - val_acc: 0.8195\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9885\n",
      "Epoch 00059: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0447 - acc: 0.9885 - val_loss: 1.1044 - val_acc: 0.8183\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9888\n",
      "Epoch 00060: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0415 - acc: 0.9888 - val_loss: 1.2249 - val_acc: 0.8164\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9896\n",
      "Epoch 00061: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0396 - acc: 0.9896 - val_loss: 1.1622 - val_acc: 0.8227\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9886\n",
      "Epoch 00062: val_loss did not improve from 0.70319\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0420 - acc: 0.9886 - val_loss: 1.1890 - val_acc: 0.8160\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 627us/sample - loss: 0.7908 - acc: 0.7765\n",
      "Loss: 0.7907614937328723 Accuracy: 0.7765317\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2384 - acc: 0.2531\n",
      "Epoch 00001: val_loss improved from inf to 1.71516, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/001-1.7152.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 2.2383 - acc: 0.2531 - val_loss: 1.7152 - val_acc: 0.4573\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6124 - acc: 0.4731\n",
      "Epoch 00002: val_loss improved from 1.71516 to 1.39420, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/002-1.3942.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.6129 - acc: 0.4731 - val_loss: 1.3942 - val_acc: 0.5574\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2985 - acc: 0.5933\n",
      "Epoch 00003: val_loss improved from 1.39420 to 1.20614, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/003-1.2061.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.2985 - acc: 0.5933 - val_loss: 1.2061 - val_acc: 0.6289\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1031 - acc: 0.6589\n",
      "Epoch 00004: val_loss improved from 1.20614 to 0.99855, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/004-0.9985.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.1030 - acc: 0.6589 - val_loss: 0.9985 - val_acc: 0.7000\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.7025\n",
      "Epoch 00005: val_loss improved from 0.99855 to 0.90252, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/005-0.9025.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.9719 - acc: 0.7025 - val_loss: 0.9025 - val_acc: 0.7405\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.7344- ETA: 0s - loss: 0.8793 - acc: 0.7\n",
      "Epoch 00006: val_loss improved from 0.90252 to 0.83217, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/006-0.8322.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.8788 - acc: 0.7344 - val_loss: 0.8322 - val_acc: 0.7489\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8033 - acc: 0.7601\n",
      "Epoch 00007: val_loss improved from 0.83217 to 0.80243, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/007-0.8024.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.8033 - acc: 0.7601 - val_loss: 0.8024 - val_acc: 0.7736\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7833\n",
      "Epoch 00008: val_loss improved from 0.80243 to 0.77382, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/008-0.7738.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.7304 - acc: 0.7833 - val_loss: 0.7738 - val_acc: 0.7806\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6817 - acc: 0.7979\n",
      "Epoch 00009: val_loss improved from 0.77382 to 0.72717, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/009-0.7272.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.6817 - acc: 0.7979 - val_loss: 0.7272 - val_acc: 0.7841\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.8102\n",
      "Epoch 00010: val_loss improved from 0.72717 to 0.66102, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/010-0.6610.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.6383 - acc: 0.8102 - val_loss: 0.6610 - val_acc: 0.8141\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8263\n",
      "Epoch 00011: val_loss improved from 0.66102 to 0.62238, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/011-0.6224.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.5961 - acc: 0.8263 - val_loss: 0.6224 - val_acc: 0.8276\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.8334\n",
      "Epoch 00012: val_loss improved from 0.62238 to 0.61374, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/012-0.6137.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.5612 - acc: 0.8333 - val_loss: 0.6137 - val_acc: 0.8300\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8430\n",
      "Epoch 00013: val_loss improved from 0.61374 to 0.59128, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/013-0.5913.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.5332 - acc: 0.8430 - val_loss: 0.5913 - val_acc: 0.8332\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.8488\n",
      "Epoch 00014: val_loss improved from 0.59128 to 0.58456, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/014-0.5846.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.5102 - acc: 0.8488 - val_loss: 0.5846 - val_acc: 0.8367\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8567\n",
      "Epoch 00015: val_loss improved from 0.58456 to 0.57702, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/015-0.5770.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4813 - acc: 0.8567 - val_loss: 0.5770 - val_acc: 0.8369\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8650\n",
      "Epoch 00016: val_loss improved from 0.57702 to 0.53008, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_4_conv_checkpoint/016-0.5301.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4497 - acc: 0.8650 - val_loss: 0.5301 - val_acc: 0.8521\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.8720\n",
      "Epoch 00017: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4304 - acc: 0.8720 - val_loss: 0.5516 - val_acc: 0.8400\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8773\n",
      "Epoch 00018: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4092 - acc: 0.8773 - val_loss: 0.5427 - val_acc: 0.8493\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8833\n",
      "Epoch 00019: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3897 - acc: 0.8833 - val_loss: 0.5371 - val_acc: 0.8523\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8887\n",
      "Epoch 00020: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3716 - acc: 0.8887 - val_loss: 0.5322 - val_acc: 0.8563\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8909\n",
      "Epoch 00021: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3592 - acc: 0.8909 - val_loss: 0.5335 - val_acc: 0.8523\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8974\n",
      "Epoch 00022: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3356 - acc: 0.8974 - val_loss: 0.5399 - val_acc: 0.8507\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8991\n",
      "Epoch 00023: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3279 - acc: 0.8991 - val_loss: 0.5384 - val_acc: 0.8491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9074\n",
      "Epoch 00024: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3038 - acc: 0.9075 - val_loss: 0.5746 - val_acc: 0.8526\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9107\n",
      "Epoch 00025: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2954 - acc: 0.9106 - val_loss: 0.5572 - val_acc: 0.8642\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9126\n",
      "Epoch 00026: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2835 - acc: 0.9126 - val_loss: 0.5447 - val_acc: 0.8633\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9162\n",
      "Epoch 00027: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2692 - acc: 0.9162 - val_loss: 0.5929 - val_acc: 0.8565\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9220\n",
      "Epoch 00028: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2532 - acc: 0.9220 - val_loss: 0.5700 - val_acc: 0.8535\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9219\n",
      "Epoch 00029: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2502 - acc: 0.9219 - val_loss: 0.5840 - val_acc: 0.8532\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9240\n",
      "Epoch 00030: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2460 - acc: 0.9240 - val_loss: 0.5661 - val_acc: 0.8621\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9304\n",
      "Epoch 00031: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2231 - acc: 0.9304 - val_loss: 0.5513 - val_acc: 0.8649\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9311\n",
      "Epoch 00032: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2217 - acc: 0.9311 - val_loss: 0.5836 - val_acc: 0.8600\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9323\n",
      "Epoch 00033: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2112 - acc: 0.9323 - val_loss: 0.5723 - val_acc: 0.8654\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9383\n",
      "Epoch 00034: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1946 - acc: 0.9383 - val_loss: 0.5804 - val_acc: 0.8626\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9399\n",
      "Epoch 00035: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1914 - acc: 0.9398 - val_loss: 0.5718 - val_acc: 0.8542\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9429\n",
      "Epoch 00036: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1800 - acc: 0.9429 - val_loss: 0.5679 - val_acc: 0.8682\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9439\n",
      "Epoch 00037: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1759 - acc: 0.9439 - val_loss: 0.6171 - val_acc: 0.8528\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9459\n",
      "Epoch 00038: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1712 - acc: 0.9459 - val_loss: 0.6060 - val_acc: 0.8698\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1659 - acc: 0.9469 - val_loss: 0.6063 - val_acc: 0.8619\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9489\n",
      "Epoch 00040: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1612 - acc: 0.9489 - val_loss: 0.6138 - val_acc: 0.8523\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9479\n",
      "Epoch 00041: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1664 - acc: 0.9479 - val_loss: 0.6137 - val_acc: 0.8549\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9506\n",
      "Epoch 00042: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1551 - acc: 0.9506 - val_loss: 0.6161 - val_acc: 0.8649\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9542\n",
      "Epoch 00043: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1440 - acc: 0.9542 - val_loss: 0.6490 - val_acc: 0.8640\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9560\n",
      "Epoch 00044: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1376 - acc: 0.9560 - val_loss: 0.6310 - val_acc: 0.8689\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9582\n",
      "Epoch 00045: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1300 - acc: 0.9582 - val_loss: 0.6187 - val_acc: 0.8703\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9598\n",
      "Epoch 00046: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1265 - acc: 0.9598 - val_loss: 0.6590 - val_acc: 0.8675\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9584\n",
      "Epoch 00047: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1278 - acc: 0.9584 - val_loss: 0.6646 - val_acc: 0.8595\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9574\n",
      "Epoch 00048: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1305 - acc: 0.9574 - val_loss: 0.6459 - val_acc: 0.8649\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9603\n",
      "Epoch 00049: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1218 - acc: 0.9603 - val_loss: 0.6489 - val_acc: 0.8656\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9618\n",
      "Epoch 00050: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1171 - acc: 0.9618 - val_loss: 0.6268 - val_acc: 0.8675\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9628\n",
      "Epoch 00051: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1180 - acc: 0.9628 - val_loss: 0.6671 - val_acc: 0.8644\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9635\n",
      "Epoch 00052: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1136 - acc: 0.9635 - val_loss: 0.6567 - val_acc: 0.8691\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9661\n",
      "Epoch 00053: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1043 - acc: 0.9661 - val_loss: 0.6484 - val_acc: 0.8642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9651\n",
      "Epoch 00054: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1096 - acc: 0.9651 - val_loss: 0.6667 - val_acc: 0.8663\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9658\n",
      "Epoch 00055: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1045 - acc: 0.9658 - val_loss: 0.6682 - val_acc: 0.8682\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9678\n",
      "Epoch 00056: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1009 - acc: 0.9678 - val_loss: 0.7018 - val_acc: 0.8656\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9689\n",
      "Epoch 00057: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0990 - acc: 0.9689 - val_loss: 0.6782 - val_acc: 0.8647\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9689\n",
      "Epoch 00058: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0999 - acc: 0.9689 - val_loss: 0.6604 - val_acc: 0.8644\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9693\n",
      "Epoch 00059: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0986 - acc: 0.9693 - val_loss: 0.7203 - val_acc: 0.8691\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9696\n",
      "Epoch 00060: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0949 - acc: 0.9695 - val_loss: 0.6743 - val_acc: 0.8707\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9720\n",
      "Epoch 00061: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0895 - acc: 0.9719 - val_loss: 0.6704 - val_acc: 0.8672\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9690\n",
      "Epoch 00062: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0969 - acc: 0.9690 - val_loss: 0.7204 - val_acc: 0.8689\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9711\n",
      "Epoch 00063: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0914 - acc: 0.9711 - val_loss: 0.7150 - val_acc: 0.8644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9720\n",
      "Epoch 00064: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0887 - acc: 0.9720 - val_loss: 0.7215 - val_acc: 0.8693\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9721\n",
      "Epoch 00065: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0852 - acc: 0.9722 - val_loss: 0.7391 - val_acc: 0.8726\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9720\n",
      "Epoch 00066: val_loss did not improve from 0.53008\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0894 - acc: 0.9720 - val_loss: 0.7044 - val_acc: 0.8665\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmT2Z7AmEQAIBZQcJm6Io4IZLFbUW0WrV2uqvrUutrRVta7HVqlVr3Wqr1latdde6UVEri6KogGxC2MKWkITs6ySZ5fz+OJNJgoEEyGSSzPt5nvNMMnPn3neGcN57z3aV1hohhBACwBLpAIQQQvQckhSEEEKESFIQQggRIklBCCFEiCQFIYQQIZIUhBBChEhSEEIIESJJQQghRIgkBSGEECG2SAdwqNLS0nR2dnakwxBCiF5l1apVpVrrfh1t1+uSQnZ2NitXrox0GEII0asopXZ1ZjtpPhJCCBEiSUEIIUSIJAUhhBAhva5PoT1er5f8/HwaGhoiHUqv5XK5yMzMxG63RzoUIUQE9YmkkJ+fT3x8PNnZ2SilIh1Or6O1pqysjPz8fIYOHRrpcIQQEdQnmo8aGhpITU2VhHCYlFKkpqbKlZYQom8kBUASwhGS708IAX0oKXTE7/fQ2FhAIOCLdChCCNFjRU1SCAQaaGoqROumLt93ZWUlf/nLXw7rvWeffTaVlZWd3n7BggXcf//9h3UsIYToSNQkBaVMn7rW3i7f98GSgs938CuThQsXkpSU1OUxCSHE4YjCpND1zUfz589n+/bt5OTkcPPNN7NkyRJOOukk5syZw5gxYwA4//zzmTx5MmPHjuWJJ54IvTc7O5vS0lJ27tzJ6NGjufrqqxk7diyzZ8/G4/Ec9Lhr1qxh2rRpHHPMMVxwwQVUVFQA8PDDDzNmzBiOOeYYLr74YgCWLl1KTk4OOTk5TJw4kZqami7/HoQQvV+fGJLa2tatN1Jbu6adVzR+fy0WixOlHIe0z7i4HIYP//MBX7/nnnvYsGEDa9aY4y5ZsoTVq1ezYcOG0BDPp59+mpSUFDweD1OnTuXCCy8kNTV1v9i38sILL/Dkk09y0UUX8dprr3HZZZcd8LiXX345jzzyCDNnzuT222/njjvu4M9//jP33HMPO3bswOl0hpqm7r//fh577DGmT59ObW0tLpfrkL4DIUR0iJorBWgeXaO75WjHHntsmzH/Dz/8MBMmTGDatGns2bOHrVu3fuM9Q4cOJScnB4DJkyezc+fOA+6/qqqKyspKZs6cCcAVV1zBsmXLADjmmGO49NJL+de//oXNZvL+9OnTuemmm3j44YeprKwMPS+EEK31uZrhYGf0tbVrsNmScLmywx6H2+0O/bxkyRI+/PBDPvvsM2JjY5k1a1a7cwKcTmfoZ6vV2mHz0YG8++67LFu2jLfffpu77rqL9evXM3/+fL71rW+xcOFCpk+fzqJFixg1atRh7V8I0XdF0ZUCKGUPS59CfHz8Qdvoq6qqSE5OJjY2ltzcXFasWHHEx0xMTCQ5OZmPP/4YgOeee46ZM2cSCATYs2cPJ598Mvfeey9VVVXU1tayfft2xo8fzy233MLUqVPJzc094hiEEH1Pn7tSOBilbGFJCqmpqUyfPp1x48Zx1lln8a1vfavN62eeeSZ//etfGT16NCNHjmTatGldctxnnnmGH/3oR9TX1zNs2DD+8Y9/4Pf7ueyyy6iqqkJrzQ033EBSUhK/+c1vWLx4MRaLhbFjx3LWWWd1SQxCiL5Fad09bexdZcqUKXr/m+xs2rSJ0aNHd/hej2c7fr+HuLhx4QqvV+vs9yiE6H2UUqu01lM62i7Kmo/Cc6UghBB9RZQlBTvgo7ddHQkhRHeJsqQQvglsQgjRF0hSEEIIERKlSaHr1z8SQoi+IEqTglwpCCFEe6IsKZj7D/eEpBAXF3dIzwshRHeIsqRgBXpGUhBCiJ4oypKCBbB2eVKYP38+jz32WOj35hvh1NbWcuqppzJp0iTGjx/Pm2++2el9aq25+eabGTduHOPHj+ell14CoLCwkBkzZpCTk8O4ceP4+OOP8fv9XHnllaFtH3zwwS79fEKI6BG2ZS6UUlnAs0A6ZmnSJ7TWD+23jQIeAs4G6oErtdarj+jAN94Ia9pbOtuI9deBsoAlpvP7zMmBPx94ob158+Zx4403cu211wLw8ssvs2jRIlwuF2+88QYJCQmUlpYybdo05syZ06n7Ib/++uusWbOGtWvXUlpaytSpU5kxYwb//ve/OeOMM/jVr36F3++nvr6eNWvWUFBQwIYNGwAO6U5uQgjRWjjXPvIBP9dar1ZKxQOrlFIfaK03ttrmLGB4sBwHPB58DCNFVy+fPXHiRPbt28fevXspKSkhOTmZrKwsvF4vt912G8uWLcNisVBQUEBxcTEDBgzocJ+ffPIJl1xyCVarlfT0dGbOnMmXX37J1KlTueqqq/B6vZx//vnk5OQwbNgw8vLyuP766/nWt77F7Nmzu/TzCSGiR9iSgta6ECgM/lyjlNoEDAJaJ4XzgGe1mWK8QimVpJTKCL738BzkjB6gsX4bWjfido897EO0Z+7cubz66qsUFRUxb948AJ5//nlKSkpYtWoVdrud7OzsdpfMPhQzZsxg2bJlvPvuu1x55ZXcdNNNXH755axdu5ZFixbx17/+lZdffpmnn366Kz6WECLKdEufglIqG5gIfL7fS4OAPa1+zw8+FzYWS3jWP5o3bx4vvvgir776KnPnzgXMktn9+/fHbrezePFidu3a1en9nXTSSbz00kv4/X5KSkpYtmwZxx57LLt27SI9PZ2rr76aH/7wh6xevZrS0lICgQAXXnghd955J6tXH1kLnBAieoV96WylVBzwGnCj1rr6MPdxDXANwODBg48wHhtae9Fad6ptv7PGjh1LTU0NgwYNIiMjA4BLL72Uc889l/HjxzNlypRDuqnNBRdcwGeffcaECRNQSvHHP/6RAQMG8Mwzz3Dfffdht9uJi4vj2WefpaCggO9///sEAgEA7r777i77XEKI6BLWpbOVmRjwDrBIa/2ndl7/G7BEa/1C8PfNwKyDNR8dydLZAE1NxTQ27sHtzsFiiarbSXRIls4Wou+K+NLZwZFFfwc2tZcQgt4CLlfGNKDqiPoTOhWXzGoWQogDCeep8nTge8B6pVTzGNHbgMEAWuu/Agsxw1G3YYakfj+M8QCSFIQQ4mDCOfroE8z4z4Nto4FrwxVDe2RRPCGEOLComtEMcqUghBAHE4VJoecsiieEED1NFCYFC2CRpCCEEO2IuqQALXMVukplZSV/+ctfDuu9Z599tqxVJIToMaI0Kdi79ErhYEnB5zv4cRYuXEhSUlKXxSKEEEciSpNC1y51MX/+fLZv305OTg4333wzS5Ys4aSTTmLOnDmMGTMGgPPPP5/JkyczduxYnnjiidB7s7OzKS0tZefOnYwePZqrr76asWPHMnv2bDwezzeO9fbbb3PccccxceJETjvtNIqLiwGora3l+9//PuPHj+eYY47htddeA+C9995j0qRJTJgwgVNPPbXLPrMQom/qc1N6O1g5G4BAIBOt/VitndtnBytnc88997BhwwbWBA+8ZMkSVq9ezYYNGxg6dCgATz/9NCkpKXg8HqZOncqFF15Iampqm/1s3bqVF154gSeffJKLLrqI1157jcsuu6zNNieeeCIrVqxAKcVTTz3FH//4Rx544AF+//vfk5iYyPr16wGoqKigpKSEq6++mmXLljF06FDKy8s794GFEFGrzyWFzmlePlvTwVSKw3bssceGEgLAww8/zBtvvAHAnj172Lp16zeSwtChQ8nJyQFg8uTJ7Ny58xv7zc/PZ968eRQWFtLU1BQ6xocffsiLL74Y2i45OZm3336bGTNmhLZJSUnp0s8ohOh7+lxS6GDlbAAaGytoaiogLm5i6BadXc3tdod+XrJkCR9++CGfffYZsbGxzJo1q90ltJ1OZ+hnq9XabvPR9ddfz0033cScOXNYsmQJCxYsCEv8QojoFKV9Cl07VyE+Pp6ampoDvl5VVUVycjKxsbHk5uayYsWKwz5WVVUVgwaZ1cWfeeaZ0POnn356m1uCVlRUMG3aNJYtW8aOHTsApPlICNGhKE0KXTurOTU1lenTpzNu3Dhuvvnmb7x+5pln4vP5GD16NPPnz2fatGmHfawFCxYwd+5cJk+eTFpaWuj5X//611RUVDBu3DgmTJjA4sWL6devH0888QTf/va3mTBhQujmP0IIcSBhXTo7HI506WwAn68WjyeXmJijsdlkOGgzWTpbiL4r4ktn92TN91EIBGRWsxBCtBaVSUHWPxJCiPZFZVIwH1tJUhBCiP1EZVJQSnX5rGYhhOgLojIpQPP6R3KjHSGEaC2Kk4JcKQghxP4kKURIXFxcxI4thBAHEuVJQZqPhBCitehJCh4PFBSA3w80z2oOoHXgiHc9f/78NktMLFiwgPvvv5/a2lpOPfVUJk2axPjx43nzzTc73NeBlthubwnsAy2XLYQQh6vPLYh343s3sqaonbWzfT6TGGJjwWpFay+BQANWaxwdrZSaMyCHP5954JX25s2bx4033si1114LwMsvv8yiRYtwuVy88cYbJCQkUFpayrRp05gzZw5KHfh47S2xHQgE2l0Cu73lsoUQ4kj0uaRwQJbgRVEggLmRgqmYtdYHraQ7Y+LEiezbt4+9e/dSUlJCcnIyWVlZeL1ebrvtNpYtW4bFYqGgoIDi4mIGDBhwwH21t8R2SUlJu0tgt7dcthBCHIk+lxQOeEavNaxeDenpkJmJz1eDx7OZmJgR2GwJR3zcuXPn8uqrr1JUVBRaeO7555+npKSEVatWYbfbyc7ObnfJ7GadXWJbCCHCJXr6FJQCpxOClWzLSqld09k8b948XnzxRV599VXmzp0LmGWu+/fvj91uZ/Hixezateug+zjQEtsHWgK7veWyhRDiSERPUgCIiTH9CnT98tljx46lpqaGQYMGkZGRAcCll17KypUrGT9+PM8++yyjRo066D4OtMT2gZbAbm+5bCGEOBLRtXR2QQEUFsKkSWilqK1dhcORgdM5KEzR9i6ydLYQfZcsnd0el8s8NjbK+kdCCNGO6EwKrZqQZAKbEEK06DNJoVPNYM1JIdTZbJcrhaDe1owohAiPPpEUXC4XZWVlHVdsVis4HG1GIAUCcqWgtaasrAxXc9IUQkStPjFPITMzk/z8fEpKSjreuLwcSkuhsRGfrwKfrxqn03rEE9h6O5fLRWZmZqTDEEJEWJ9ICna7PTTbt0N/+xs8+STU1FC073lycy9n6tSNuN0y6kYIIfpE89EhGT0a6uthzx5iY00iqK/fFOGghBCiZ4jOpACwaROxsWYyWV3dxggGJIQQPUf0JYXmWcW5udhscTidQ+RKQQghgqIvKfTrBykpsMkkArd7NPX1cqUghBAQxqSglHpaKbVPKbXhAK/PUkpVKaXWBMvt4YplvwObJqRgUoiNHUN9fS5a+7vl8EII0ZOF80rhn8CZHWzzsdY6J1h+F8ZY2ho1CnJzAYiNHU0g0EBDw8FXMBVCiGgQtqSgtV4GlIdr/0dk9GgoKYGyMtzuMYB0NgshBES+T+F4pdRapdR/lVJju+2obUYgybBUIYRoFsmksBoYorWeADwC/OdAGyqlrlFKrVRKrezUrOWONCeF3Fzs9mQcjgHS2SyEEEQwKWitq7XWtcGfFwJ2pVTaAbZ9Qms9RWs9pV+/fkd+8MGDzeJ4oc7m0dTVyZWCEEJELCkopQao4IJDSqljg7GUdcvBrVYYOXK/EUgbZaVQIUTUC9vaR0qpF4BZQJpSKh/4LWAH0Fr/FfgO8GOllA/wABfr7qyVR4+Gzz8HzFwFv7+Gpqa9chc2IURUC1tS0Fpf0sHrjwKPhuv4HRo1Cl56CTweYmNbRiBJUhBCRLNIjz6KnNGjQWvYvFlGIAkhRFB0JwWA3FwcjnRstmSZqyCEiHrRmxSGDweLBTZtQilFbOxouVIQQkS96E0KLhcMHdpqYbwxMldBCBH1ojcpgGlC2mDW64uNHY3XW0pTUxdMjhNCiF4qupPCCSeYK4Xi4tAIJGlCEkJEs+hOCqefbh4//DB0j2ZJCkKIaBbdSWHiREhNhQ8+wOnMwmJxywgkIURUi+6kYLXCqafC+++jUMTGjpIrBSFEVIvupACmCamwEDZuxO0eI1cKQoioJkmhuV/hgw+IjR1DU1MBPl9VZGMSQogIkaQwZAiMGAHvv9+qszk3wkEJIURkSFIAmD0bli4l1no0ILfmFEJEL0kKYJqQ6utxrS7EYnFRV7cu0hEJIURESFIAmDULbDYs//uIhITjqahYHOmIhBAiIiQpACQkwLRp8MEHJCefSl3dWlnuQggRlSQpNJs9G1atIjkwBYDKSrlaEEJEH0kKzU4/HbQm7vNyrNZ4Kio+inREQgjR7SQpNJsyBZKSsHz4P5KSZlJZ+b9IRySEEN2uU0lBKfVTpVSCMv6ulFqtlJod7uC6lc0Gp5wCH3xAUuIpeDzbaGjYHemohBCiW3X2SuEqrXU1MBtIBr4H3BO2qCJl9mzYvZuUMjNfoaJCrhaEENGls0lBBR/PBp7TWn/d6rm+I7jkRewnO7Hb+1FZKf0KQojo0tmksEop9T4mKSxSSsUDgfCFFSHDhkF2NmrJEpKSTqGi4n9orSMdlRBCdJvOJoUfAPOBqVrresAOfD9sUUXSzJmwbBnJSafQ1FQo6yAJIaJKZ5PC8cBmrXWlUuoy4NdA31xKdOZMKC0lpWgwIP0KQojo0tmk8DhQr5SaAPwc2A48G7aoImnmTABcn+fhcmVLv4IQIqp0Nin4tGlcPw94VGv9GBAfvrAiaOhQyMyEpUtJSjqFysrFaO2PdFRCCNEtOpsUapRSt2KGor6rlLJg+hX6HqXM1cLSpSQnnYLPV0lNzVeRjkoIIbpFZ5PCPKARM1+hCMgE7gtbVJE2cyYUF5Ncmg0gs5uFEFGjU0khmAieBxKVUucADVrrvtmnAKF+BcdnG4mNHSvrIAkhokZnl7m4CPgCmAtcBHyulPpOOAOLqOHDYcAA04SUfApVVR8TCDRGOiohhAi7zjYf/QozR+EKrfXlwLHAb8IXVoQpBTNmBPsVTiMQ8MjVghAiKnQ2KVi01vta/V52CO/tnWbOhPx8UqqGY7OlUlT0dKQjEkKIsOtsxf6eUmqRUupKpdSVwLvAwvCF1QME+xUsH69gwIDLKS19U+7GJoTo8zrb0Xwz8ARwTLA8obW+JZyBRdyYMZCWBkuXkpHxA7T2Ulz8XKSjEkKIsLJ1dkOt9WvAa2GMpWdp1a/gdv+ThIRpFBb+nczMn6FU31sgVgghoIMrBaVUjVKqup1So5Sq7q4gI2bmTNi5E3bvJiPjh9TXb6S6ekWkoxJCiLA5aFLQWsdrrRPaKfFa64SDvVcp9bRSap9SasMBXldKqYeVUtuUUuuUUpOO5IOERbBfgaVL6dfvIiwWN4WFT0U2JiGECKNwjiD6J3DmQV4/CxgeLNdgFt3rWcaPh+RkWLoUmy2e/v0vZt++l/D5aiIdmRBChEXYkoLWehlQfpBNzgOe1cYKIEkplRGueA6LxQInnQRLlwKQkfFDAoE69u17KcKBCSFEeERyrsEgYE+r3/ODz/UsM2bAtm2Qn09CwnHExo6RJiQhRJ/VKyagKaWuUUqtVEqtLCnp5rkC555rrhj+/GeUUmRk/JCams+prW23q0QIIXq1Tg9JDYMCIKvV75nB575Ba/0EZp4EU6ZM6d6bJo8YAZdfDo8+Cj/9Kenp3yMv7xaKiv7O0Uc/2K2hCCEiQ2tTAoGWn7UGjwdqa1tKYyPExUFiIiQkmGK1mu3q61tKUxN4veDztTy23m9rSrWUwYPNreTDKZJJ4S3gOqXUi8BxQJXWujCC8RzYggXw73/DHXfgeOop0tIuoKjonwwZ8hvs9pRIRydEWGltKq2GhpYSCIDDAU5nS7FaTcW1v8ZGqKqC6mpTcTY0mOeaS3MF2bqibC5+v3lsbDTvralpKYEA2O0txWYzz7V+n9fbtjKurzfH93rbHrO9zxwImP00J4LDpdSRvb+1W26Be+7pmn0dSNiSglLqBWAWkKaUygd+S/DGPFrrv2KWyTgb2AbUA98PVyxHbMgQ+MlP4OGH4Re/YMiQX1NS8gq7d9/NUUf13dtKiJ5La1NRNld0Pl/bCtJuN2en1dUtlWhVFZSXt5SyMvO6x2MqSo+n5Yy2thbq6lpKINBxTEq1Pb7Vao7b1NQ1n9nphPh4U+LizP6bK/fmZGK1muRgs5mf7XaIjTUlLQ1iYsDlMs87HC2xtpfMrFbTctz8qFTLY3OJjTWxNBeHw3x3VVUtidDnA7e7JY7YWPNZbLaWZGaztd1vczytrx60NlcK4aZ0V6WwbjJlyhS9cuXK7j9wSYm5bjvzTHjlFXJzr6K4+HmOPXYzMTHZ3R+P6JH8figshMrKtmfWzRV0c2VRVWUqj6amlrNVr9dsu/+Z7f5NDU1NZn+dqagPxGIxo60TEkxF2VxcLlOB7V+aX3M6zaPFYuJo74y/ufj9pgJPSGhpTomLa9lPc2munJsfmyvL1hV8c+UtDp9SapXWekpH20Wy+ah36dcPfvEL05T05Zdkj/8d+/a9wI4dv2bMmH9FOjrRRZqaYPdu2LHDTGavqTGVW3NpXTE3l+pq857du6GgwGzTEYfDVJgOR9szVqfTVMJxcdC/v6mM968sHY62Z51ud/tnzS6XqYibK+b4eEhNhZQUU0lbesUwE9Hd5ErhUNTUmKuFnBz44APy8m5j9+67mTx5FfHxPW9CdrTxeqGiAoqLYc8eU/LzTfF42m7b3ElYX2+aR+rrTZNKQUHH7b9Wa0tl7nCYSjkry7QyDh5sSmqqqZRbn10nJEBSkqmQXa7wfQ9CtEeuFMIhPh5+9Sv42c/gww8ZPOsWCgufZPv2m5kw4UNZKK+LlJfDxo2mUq+sbFtqa1uaVZpHflRUmPdUt7Mal8UCGRnmzHt/MTGmQo+Ph/R0mDgRsrNNGTrUVPLJySYJNDdlNLcvC9FXyZXCoWpogJEjzang55+TX/xXtm27gfHjF5Kaelbk4urh/H5zxr5zJ+zaZdrUW3dwVlbC5s0mGRQXf/P9Lpc5w46PN5V5bGzLY0qKKc1NI/36mTP3rCyTEGxy6iOEXCmEjcsFf/oTfOc78NvfMvDOBeTnP0Re3i9JSZmNUtZIR9gtAgHYt880t7QuxcXfHLmyb5856z9QW7vDYc7kR4yAb33L3MpizBhztp6cLM0tQnQnuVI4XFdfDX//O3z4IfvGlbFx40WMGPEkAwf+MNKRHbHmCn/vXlPR791ryp495ix/927zc2Nj2/dZLKZzND6+7ciV1FRTwe/fLNM8osUaHXlUiE5p8jdR4anAoiykxaZ1WbN0Z68UJCkcrro6mDIFqqrQa9eyJv9C6urWM3XqBpzOnreE0/60NqNsN26ETZtg61bYvt2UvLxvdswqZZpiWnemDh4MmZkwaJAp6enhq+BL6kr4ouALappqsFvs2K12bBYbLpuLwYmDGZw4GIfVcdj7r26sZmflTqobq9sUl81FRlwGA+IGkBGfQUpMChWeCvbW7KWgpoCC6gKqGquId8ST4Ewg0ZVIgjOBQfGDyErMwqLadkA0+Zv4dM+nvL/9fXZU7uDUoadyzohzGBA3ILSN1pq1xWt55etX+O+2/5LoSmRk6khGpY1iZOpIRvcbzZDEIZ2uLCobKllduJqtZVtRSmG3mO/ObrXj8Xooqi2iqLaIwtpCimqLqG2qpdHfSKOvkUZ/I76AjzhHnPl8TvP5+rv7h2IalTaK7KRsNJqi2iLz3VQXUFRbRJO/CV/Ah1/78QV8eP1ePD4P9d56PF4PHp8Hh9VBojORRFciic5E4hxxlHvKKaotoriumOK6YjxeD+lx6Qxwm3+HjLgMbBYb9d566rx11DXVUe+tN/v1eULH8Af8xDniiHPEEe+IJ94ZT4wtJvT5m78Li7KglEKhUErh9XvZV7eP4rriUBxVDVU0+BpCpdHfiNvuJjkmmZSYFFJiUohzxNHkb2qznc1io7+7P+nudPq7+5MWm0ZtU23oOy+sKaS4rpjKhkoqPBV4fC3/+WJsMaG/78GJgzlv5HmcO/Lcw/obl6TQHdauhWOPhTPOoP6FB1i5agJJSbMYP/7dHtPp7PebdvzNmyE315RNm0wpKwOUH/p/jaspk6MGpnDUUXDUUWaQVWYmDBzYUuFj8VHVUEVVYxVVDVVUNlRS7imntL40VGqaakh2JZMWm0ZabBqpsakkOhOJscfgsrmIsZlHl80Ves5uMQPQPT4PFZ4Kyj3llHvK2Vq+leV7lvPpnk/ZUrbloJ/ToixkJWQxLHkYmQmZJLmS2lQ0Tpsz9B9eofAGvOSW5rJh3wbW71vP7qrdXf7du+1uRvcbzZh+Y8hOzGZV4SqW7FxCnbcOm8VGWmwaRbVFAEwdOJVzR5xLg6+BVza+wtbyrViVlRMHn4g34GVTySYqGipC+050JjJhwARy0nPIGZBDelw6NY011DTVUNtUS2VDJV+XfM3qwtXkVeR1GGuCMyGU/OKd8TitTpw2J06rE6uyUueta5Ms99bspaS+ZR0yu8WOX/sJ6I4nTzitTmLsMcTYYoixx9Dkb6KqoYqaprZL0rvtbgbEDSA9Lp0YWwzFdcUU1hRS5ilrd78xthhi7bHE2mND+7darNQ21bb5bg5FsiuZ9Lh00t3pJLmSQvt12Vw4rA7qmuoobygP/d3WNNWEPl/z33mjr5GS+hL21e2jsqGyTbwZ8eY7T3enkxKTQpIriWRXMkmuJHwBH7urdrO7erd5rNrNj6f8mNtn3n5In6GZJIXu8vDD8NOfwiOPkH++Ztu2Gxg58ikyMn7QbSF4PLB+PaxbZ5p39uxpaeLZv5knLQ2OHluFe8IiqtLfYYteSLXP/Ccb138cJ2adyElDTmJk6khyS3NZW7yWtcVrWVe8LlSBHUi8Ix63w01lQyUNvoZOx69Q2Cw2vIFvrjeQFpvGCVknMD1RVT2bAAAgAElEQVRrOsdnHk9abBregBev34sv4KPeW8+uql3kVeSRV5HHjsodobP3qoYqNAf++7Zb7IxKG8X49PGM6zeOo1OOJsmVRIIzgQRnAvHOeDxeT+gMurCmkNL6UlJjUxkUP4iB8QMZlDCIJFcStU21bSrNnZU72ViyMVQKagoYnjKc2UfNZvZRs5mVPYt4Rzzr963n7c1v8/aWt/mi4AssysLJQ09m7pi5XDDqAvq5+wHm6qG0vpTc0ly+LvmatUVrWVO8hnXF66j31rf7+YYlD2NyxmQmZUxiUsYkxvQbg0VZ8Pq9eAPm+7Nb7GTEZxBrj+30v1ezck85m0s3k1uay+ayzTisDgbFD2JQwiAGxQ9iQNwAXDYXVosVq7Jis9iwWWxYLe1fTvoDfqobq6ltqiUlJgW3w93udk3+Jopri/FrP267G7fDjcvm+sZVWXsCOhC6emn+Hrx+LxqN1jr0aLVY6RfbD6fNecjfy8E0+hoprS8NXXkd6smj1vqwTzglKXQXrc1Kqh9+iF62lLWO+dTUrGLq1A24XF03J90f8LO3tI6N22rZnFfHtt21bNpRRe7OSgrKKtDOSoipQMVUEJNSjiOxAktcORZnLXZHAIdTY7drlCXAjsod+AI+UmNSOXv42Zw27DT2VO3h490f8+meT9ucsTmsDsb2G8uEARMYmjT0G2fgqbGp5oogJjX0H0hrTb23njJPGaX1pVQ1VNHob8Tj9dDga8Dj89Doawz93OBrwOv3kuRKIiUmJXQ5npWQxdEpRx/2f4KADlDbVEtVQxVN/qY2//GtysrgxMHYrd0zTbbB14DLdvDe8pK6EizKQmpsaqf36w/42Va+jYqGilDzSLwjnjhHXLd9NtE7SFLoTiUlZpB7cTG+H32PL856Gfeg4znmmPcPu0KraazhnQ1L+eeyD1lR/D+qYzpeqluhSHQlmorVZSrWeGc8VmVt016anZjNOSPOYVrmtG+ctfkCPtYVryOvIi/Uhi2VixC9nySF7lZSAr/+NTz5JP6UOLZdWUPcDY8yaPC1nd6F1vDPpYv5w/IFbPcuRys/eF04i09idPw0BqUmkdnfzeCMOIYOcjMgKZEkV5Jph4xJJt4Rf8BLcyFEdJOkEClffYW+4QbUJ59QM8KC49F/4Tz9knY31dr0AyxeDO99tpvF9l/QNPwVqBxC6t7vMvvo0/jxOScw/TiXzKIVQhwRmbwWKRMnopYto+m5v+D4xfU4Z38Xff7LqPvug6OPprISPvgA/vtfeO89KCxpgBPuR834AxaL5vzEO7jz4psZOzIm0p9ECBGFJCmEg1I4Lr+WvGlWch/8MZ5P3+GjK9excPBkdioPuMqxxFXg+EE5Vls5frxcOOY73H/6/QxJGhLp6IUQUUySQhjkluZy9yd38/y65/EPAL7tA/JQ9RX0r3GTNTiL7KFjSYlNJtmVzJlHn8nJQ0+OdNhCCCFJoSutK17HXR/fxStfv4JduYjb9BOq1k8nzRrLxbOXc9ec80m49CKo+hreuhtmzox0yEII0YYkhcNU4aloMznpq6KvWLprKS4VT+L6+VS+dyNjR/Tn1t/CySd/xIYN91KUXk3C8uVwxhmmvPginH9+pD+KEEKESFI4RFUNVVz11lW8vun10HOx9ljSLaNJWXsH5f+9nlGjklnwPMyZ03yv1VPIzLyJ/Pw/kTr+HFI//tgsB3rhhfDEE/CD7pv9LITowWpq4K234KWXzG32HnjArCTZjSQpHIJNJZu44KUL2Fa+jVtPvJUTB5+Iq2YMd948mMUfWZgwAf7+Apx33jdvBD506F1UVHxAbu4VTJjwIXH/+59JCj/8obljzPXXR+ZDCSE61tho/lM7DnHRRa/XrCc/YED7N/Zovv/rV1/Byy/DO++Ym4xkZZmbjIwfDw8+aE4cu2s9Na11ryqTJ0/WkfD6xtd13B/idP/7+uslO5bohgatf/c7rZ1OrRMTtX78ca39/oPvo65ui/7000z98cfJuqrqc60bG7U+7zytrVatlyzpng8ihDg0FRVajxun9bBhWm/Z0vn3FRZqPXas1qC1xaJ1RobWU6dqPWeO1iedpHVmptZKmddB6/R0ra+/Xuvly01lsmOH1iefbF476yytCwqO6GMAK3Un6liZvNaBgA7w28W/5c6P7+TYQcfy2kWvUbApkyuvNCuOzptnEnlGRuf25/HsZO3aU/F6Sxg//h2SLDkwdaq5FdlXX3V+R0KIQ/fJJ+bsfdaszp15e72mqXfxYnOTbasVFi40y+YfzN69cMop5naDt99u7jzVfCeqvXvNzbqHDm0pw4ebFZf3v5oIBOCxx+CWW8zNvv/2N7joosP66J2dvBbxM/9DLd19pXDXsrs0C9BX/ecq7fF69AsvmKuDIUO0Xrjw8PbZ0JCvP/98lF66NEaXlS3Sev16rWNjtZ4xQ2uvt0vjF0JorcvKtL7sspaz8okTtX7hhYP/fwsEtP6//zPbP/201ps3a52drbXbrfWiRQd+3549Wg8frnVcnNYff9w18W/erPW0aVo/8cRh74JOXilEvJI/1NKdSWHFnhXaeodVX/zqxdrvD+jf/958YyedpHVJyZHtu7GxWH/xxQS9ZIlD79v3utb/+pfZ+c03d03wQgjj9ddN04zNpvXtt2v91FNajxxp/r8NHar1o49qXVX1zfc98IDZ5tZbW57bu1frY44x+/rXv775np07TTNTQoLWn37atZ/D5zOJ6jBJUjhCVQ1VethDw/SQB4fo4qoKfcUV5tu67DKtGxq65hhNTeV61appevFipXfvvl8Hfvxjc5DXX++aAwjRXY6gsvrGftavP/SzLr9f6zVrtF68WOv33tP6zTe1fuklrefNa7ky+Oqrttu/8YY5+watXS6tL7xQ61df1drj0fo//zHt/d/5zjc7CysrtZ41y7zvqKO0Pu44rc8+W+vLL9d68GCtk5K0/vzzI/4qupokhSP0vde/py13WPSijZ/omTPNN3XHHV33t9/M56vXGzbM1YsXozevu0oHjp1qzjK++KJrDyREV/nsM61/9jNTYU6bZjpMrVatx4/X+o9/PPwO0bo6rS+9VIeaeLKzzTHuucc01+Tnf/M/4MaNWv/qV2bb5ve1Lna71nfeqXVTU/vHDAS0XrHCdPCmp5v3xMdrHRNjOoXr6tp/n8ej9YIFWl9yidazZ2s9aZJpUx4xQuuVKw/v84dZZ5OCdDS349/r/82lr1/K7TN+y/pHFvDWW/Dss/Dd74bneFoH2LHjdnbvvot+numM+fFuVGEx/OlP8JOfdN9QNBE9tDZDIPPy4DvfMR2dnXnPI4/Az39uxtA336Q7M9Pcr3XZMlixAiwWOO00mDvXbFdV1VKSkuCqq8x9XlvLy4Nvf9ssGzx/PqSkwMqVpmzf3rJdQgKMHg2jRsGGDbBqlTne6afDxRebm4g7neBymcf0dHO7wc7w+UyH8gsvmHhefNEMJe0jpKP5MOWV5+mEuxP0CX8/Qf/hHq8Grf/0p7AeMqSw8J96yRK7Xvn+cO07KzgU7aKL2m/vFOJw7dih9RlntD2jnjJF6wcfNMMo21NXp/V3v2u2nTPHNKG0Z8sWrX/zm/bP3GNjTZOMzab1xRebKw6ttf7vf7VOTjbNLu2N3igr0/qjj7R+7DGtr71W61NOMcM7J08+eMyiDaT56NB5/V59/FPH64S7E/Tz7+7QFoupk7u6yehgKiqW6I8/TtEfL03WdQuuMZflw4ebNsruDET0PT6f1g89ZEbPxMVp/cgjWu/apfX995s29+bx9FOnav3zn5t29dJSrbdvN52rSmn9+993PCFHa7PNpk3mvaWlLc0327ebpqeEBHO8cePMfo85Rutt28L7+aNcZ5OCNB+1smDJAu5YegePzPo3v5t7CWlp8MUXEBcXlsMdkMeznfXr5+DxbGFM+U/pd90LZmxzXByMHdtSZs+GceO6NzgRHlrDU0+Z5pPjjjPl6KMP3nTo9cKOHbBlC1RUtDzf/B6Px4yPby5Ll8Lnn8NZZ8Ff/2qaf1rbtMksr/DRR2a7pibzvNMJsbHw/PPmvV2hpsa0yf7jH5CTAw89BG531+xbtEvuvHaIlu9ezox/zuCSsZey/f5n2bABvvzSNF1Ggs9XzcaNl1BevpBM11UM+2oylo2b4OuvTVtqSYnZ8NRT4Wc/M/9Z5fZsvZPWcNttcM89ZhmF5so4NdVMbExIMNsEAqbU18O2bbBzJ/j9nTuG02nax++8Ey69tON+qoYG8x9g2TLYtcu08w8bdkQfU0SWJIVDUNVQRc7fclAoZuet4W8PJ/DKK6b/LZK09rN9+y3k5z9AUtLJjB79HE7nIPNiYaE503rkETNLcsQIuOEGmD7d/OdNSIhs8JGyZQusWWMWoHI6Ix1Nx7Q2Sf2hh+BHP4KHHzZT5VesMGfrK1eaCtpiMRW5xWI+11FHmc7hESPMY79+5vXm/89am7N7t9sUuz2yn1NEnCSFQ3DZ65fx4oYXeeH0j5k3/Xiuu8783+wpCgv/ydat12KxOBg+/FH69/8uqvlMz+uFV181a218+WXLm1JTzfT5YcNMxdFchg83Izv6ouamkYoKs1zI9dfD//1f28+7d6+5H+qqVXDSSXDOORDTyVuf5uebZpdBg+D4403TXetlCbQ2V3C5ueaMf8yYgyfnQMCMLvvb3+DGG81oMxlpJsJEkkInPb/ueS574zLumHUHFf+5ncceM4sW9rSRaPX1W8nNvYLq6s9IS7uQESMex+Ho17KB1qZpKTfXDKfLyzPtzc3NDIFAy7azZ8Mrr/TcqwmfD+6/H9avN2f855xjznoPZskSOPdc6N8f7roLnn7aVP6xsWYIpMMB779vmt7AnDl7vRAfDxdcAJdcYoZRtreSpd9vksGtt5q28GZut1mvZsgQ2LrVtMmXl7d9b1aWSR5jxphhmGlpJmGnpcHjj8Mzz5j93nWXJAQRVjIktROah59O//t0XVHl1YmJZi5KTxUI+PSuXffoJUsc+pNP+uvi4pd0oDMjkhobzUiQN9800/xtNjPZprg4/EEfqm3bWmaZJiWZR7fbDGH8z3/an0z0zjtmRuqYMW0nTq1dq/UVV5gJTE6n1qefrvV995nnvV6tP/xQ6x/8wCxzC1qnpJhhl//6lxkxo7XW69a1xHP66Wb0zPbtWj//vNbXXWeGRaanm7VPrrnGDJH873/Nd/2HP5j9TZhgjt/e5Krf/U5GlYlugQxJ7djZz5+tE+5O0DsqdujHHzffRlcvVxIONTXr9Jdf5ujFi9GrVk3TlZWfHNoO3n3XzNgcMcKs1dJaQ4PWzz6r9dVXa33vvWbb3bvDX3EFAlr/4x9mqGRiolmszOcz49OvuUbr1FQdmqF6/PFaz59vxrQ/84xJcpMnH3hphIqKA89M1dp85jfeMMsU9OunQ0MzJ082+05L0/q5547sOwgEzNj+bdvMDNp33tF62bLD358Qh6izSSFqm48+2/MZJzx9Aveedi83n/BLxo83/XcrV/aOq3it/RQV/ZMdO35DU1MhaWkXMGzYPcTGjujcDpYvN80ybrdpVomPN00kTz5p2sUTEqC6umX7hATTJ5GZaZpEsrLMzwMHmra2AQPMNu19edXVpgmruezaZZpkWs88XbkS3ngDZsyA55775nBJr9c0Ef3vf2ZEzJdfmmYmMH0D77zTNc1hgYCJZeFC0/w0erQZFdTZWbFC9FA9ok9BKXUm8BBgBZ7SWt+z3+tXAvcBBcGnHtVaP3WwfXZVUjj9udNZW7SWHT/dwRfL3ZxyihkyfeWVR7zrbuX317Fnz4Ps2XMvgUAD/ftfTGbmz4mPz+n4zevWwZlnmkrb4zHPnXsuXHedGepaWWna4DdsMO37eXmwZ4/pcG2dMJq5XKa93Oczwyqbi9fbdruYGNOm39hoCpg2/zvugJtvNmvWd6SuzozQ2bXLLG/QUZ+DEFEu4klBKWUFtgCnA/nAl8AlWuuNrba5Epiitb6us/vtiqSwbNcyZv5zJg/MfoCbjr+Jb3/bnHzu2dP5gSg9TVNTMbt338PevU8SCNSRlHQqWVm/ICXljJaRSu3JyzNDISdNMo+dvR9sdbX5woqK2payMlPhOxzmCsDhMOvdZGeb0VDZ2easuzkmrdtOkhJChEVPSArHAwu01mcEf78VQGt9d6ttriQCSeHkZ04mtzSXvBvyKCmMYehQ+OUv4e67O35vT+f1VlBY+AT5+Q/T1LQXt3s8w4b9kdTUMyMdmhAigjqbFMI5BXYQsKfV7/nB5/Z3oVJqnVLqVaVUVhjjAeCjHR+xZOcSbjvxNmLsMTz+uHn+Rz8K95G7h92ezODBtzBt2g5GjXqWQMDD+vVnsW7dWdTVbex4B0KIqBbpdRHeBrK11scAHwDPtLeRUuoapdRKpdTKkublHQ6D1prbF99OZkImV0++Go/H9Kued54Zat6XWCwOBgz4HlOnfs1RR/2J6uoVfPnlMWzZci1NTYf/HQoh+rZwJoUCoPWZfyYtHcoAaK3LtNbBnkaeAia3tyOt9RNa6yla6yn9+vVrb5NOeX/7+yzfs5xfnfQrXDYXL71kmsCv63TjVe9jsTjIyvoZxx67lUGDfszevX/jiy9GsnfvU2gd6HgHQoioEs6k8CUwXCk1VCnlAC4G3mq9gVIqo9Wvc4BN4QpGa83tS25ncOJgrpp4FWAWhBw+HE4+OVxH7TkcjjSGD3+EqVPX4XaPZ8uWq1mzZiZ1dV9HOjQhRA8StqSgtfYB1wGLMJX9y1rrr5VSv1NKzQludoNS6mul1FrgBuDKcMWzcOtCvij4gt/M+A0OqwO/Hz79FE45pXfMS+gqbvcYcnKWMHLk09TVbWLlyhzy8m7F663o+M1CiD4vaiavbS/fziNfPMJ9p9+H3Wpn3TqYMMHMk7rssjAE2gs0NZWSl/dLior+gVJO+vW7gAEDvk9y8qmYEcVCiL4i4kNSw6WrJq89/rhZoDIvzwyfj2Y1NV9RVPQ0xcXP4/NV4HRmkp5+BQMHXo3L1cd64IWIUj1hSGqP9sknZnXlzs7V6svi4ycyfPgjnHBCIWPGvIzbfQy7d9/NihXDWL/+PMrLF0mntBBRop11gqPD8uVw4onR1Z/QEYvFSf/+c+nffy4NDbvZu/cJCgufpKzsLVyuoxg48BrS0y/H6exh64oLIbpMVF4p5OebJXOmT490JD2XyzWYYcPu5PjjdzN69L9xOjPIy7uFzz7LZP368yktfYtAwNvxjoQQvUpUXiksX24eJSl0zGJxkp5+Cenpl1BXl0tR0T8oKnqGsrI3sdvTSU//LunplxIXN+ngaywJIXqFqLxSWL7crBid04mFREULt3sURx11L8cfv4dx494kIWEaBQWPsmrVFL74YjQ7d/6e+vptkQ5TCHEEonL00aRJkJxsluYXR8brLaek5FWKi/9NVdVSAJzOwSQlzSQxcQZJSTOIiRkuVxFCRFhnRx9FXfNRTQ2sXQu/+lWkI+kb7PYUBg68hoEDr6GhYQ+lpf+hqmoZ5eWLKC5+DoCYmOFkZf2C9PTLsVpdEY5YCHEwUZcUPv/c3FzrxBMjHUnf43JlkZl5PZmZ16O1pr5+M1VVSyksfIotW/6PHTtuJzPzpwwc+CPs9uRIhyuEaEfUJYVPPgGLBaZNi3QkfZtSCrd7FG73KDIyrqGycgm7d9/Ljh23sXv3H0hMPJGYmBHExo4kJmYEbvcYnM6BkQ5biKgXdUlh+XIYP75rbucrOkcpRXLyySQnn0xt7VoKCh6lpmY1VVWf4PfXhrZLTT2XwYNvJTHx+AhGK0R0i6qk4POZ2/pecUWkI4lecXETGDnyScCsXNvUVEh9/RYqKxdTUPAoX311AomJMxky5FaSk2dLB7UQ3SyqksK6dVBbK/MTegqlFE7nQJzOgSQnzyIr62YKC59kz54HWLfuTFyuobjdx+B2j8XtHkNs7Bjc7rFYLI5Ihy5EnxVVSUEmrfVsNlscWVk/Y9Cgn1Bc/Dzl5f+lru5rysvfxazEDhaLi/j4qSQmnkhi4nQSEk6QTmshulBUzVOYNw8++wx27+7ioERYBQJNeDxbqavbQHX151RVLae2dnUwUSji46eSknIGKSlnEh9/LBZLVJ3rCNEpsnT2frSGrCw46SR44YUwBCa6ld9fT3X1F6E5EdXVK4AANlsSycmzSUs7n9TUs7HZEiMdqhA9gkxe28/u3VBQIPMT+gqrNZbk5FkkJ88iO/t2vN4KKio+pLz8v5SVvUtJycsoZScp6RTS0s4nLm4CNltisCRhscRKJ7YQ7YiapPDJJ+ZR+hP6Jrs9ObTst9Z+qqtXUFr6H0pK3mDr1h9/Y3ulbNhsqTgc/bDbTYmNHUlGxtW4XFkR+ARC9AxR03xUXW06mmfPBqvcaTJqmJnVm2ho2IXPV4XfX4XPV4XPV4nXW4rXW0JTUwlebwkez3aUUvTrN4+srJ8THz8x0uEL0WWk+Wg/CQlw1lmRjkJ0NzOzegxu95gOt21o2EV+/kMUFj7Jvn3Pk5R0MsnJp2GxOFHKicXixGKJCe5vnAyNFX1S1FwpCNFZXm8lhYVPUlDwMI2N+e1uo5STuLgJxMdPITHxBFJSzpahsaJHk9FHQhwhrTVaewkEGgkEGtG6EZ+vhrq6tdTUrKS6+ktqa1fh99eilI3k5NNIS7uQtLTzcTjSIh2+EG1IUhCiG2gdoKbmS0pKXqOk5DUaGvIAKzExR2GzJQVLMnZ7Mm73MSQmnoDbPQ6lpGNLdC9JCkJ0M601tbVrKC19nfr6rfh8laHi9e7D56sAwGqNIz7+OOLjJ+NwDMBuT8Nu7xccCdUfh6M/Foszwp9G9DXS0SxEN1NKER8/sd1RS1prGhp2Ul39KVVVn1Fd/Sn5+X8KLd+xP5stCbs9HYdjAE5nBg7HoOA6UYNwOAbicAzA4UjHao0PzbfQWuPzVdDYuBevtxiXKxuXa5jMxxCHRJKCEN1AKUVMzFBiYoaSnn4p0FyJV+H1loRKU1MJTU1FeL3FNDUV0dRURHX1lzQ1/YdAoOEb+7VYYnA4BgDQ2LgXrRvbvG639ychYRoJCceTmHg88fFTsFrd4f/AoteSpCBEhCilsNuTsNuTgOEH3dYkkEqamvbS2Lg3mDBaEgdonM6BOBxm1Vm7vR/19Vuorv6M6urPKCt7K7gnC273eBISjiMhYRpxcTm4XEODMQghfQpCRIWmplKqq1dQU/M51dWm+P3Vodet1kRcruzg1czRwbvijSAmZgQOxwBpguoDpE9BCBHicKSRlnYOaWnnAGbUVH19bnC2985Qqa/fSnn5e22aqiyW2GBneAo2WzI2WwpWawyBQFNwqK559Pvr8Pur8fmq8furCQQaiIk5Grd7XKjExIzA6czAZkuRRNNDSVIQIgopZTngTG+tAzQ27qG+fgsezxY8nu14vWX4fBX4fBXU128iEPAEZ3o7Qo82WxIuVxZWawI2WwJK2amv30JNzWpKSl4FdKvj24Id6elYLA609hEIeIMd7wFsthQcjv6h0VhO5yDc7vG43eOx2eLbiVnj99dgtcahlCWM31zfJ0lBCNGGUhZcriG4XEOA07tkn35/HXV1m/B4tgU70YtD/SJae1HKhlJ2lLIBCp+vnPr6LXi9n+D1ltI6obhcw4iLm4BS9lZ9LHsJBBpCyaa5f8UM+U3BZksJPdps8VgssVitsVgsMVgsMUAglJS09gKBNgnPYnFitcZhtcZ0yffRk0lSEEKEndXqJiFhCgkJHTZpf4PWfhoa9lBXt47a2nXU1a2ltnYdoHE4BpKQMC3Yud6/TWd8Q0Me1dWf4vWWA/4u+RwWS0wwwaRit6cFhwwPCg4VHoTNlkBDww7q67fi8ZgSCDTgdGYGt8vE4RiE3Z6C1RqPzZaA1RqP1ZoQap6zWOxtjun314dGptntqcTEDO2Sz3IgkhSEED2aUlZiYrKJickmLW3OIb/fNC3V4vOV4/WW4ffX4vfXEwh4CATq8fs9KGUJXamYRxW8cmgM9Z34/TXBZrQyvN4yvN5Sqqs/DQ4FbmpzTIvFRUzM0cTGjsJiiaGxseCgQ4tbs1rjsNlSAIXXW0IgUB96LSvrFo466p5D/g4OhSQFIUSfppTCZovHZosPNol1La01Xm8pjY0F+P3VuFzZOJ2Z7fZtmKHF5cGZ7tX4/TXBxyq83opgv005Xm8F4A/e66N/6L4fsbEdr/Z7pCQpCCHEEVBK4XCYZUo6s61pekrthsgOj3TTCyGECJGkIIQQIiSsSUEpdaZSarNSaptSan47rzuVUi8FX/9cKZUdzniEEEIcXNiSgjILxj8GnAWMAS5RSu3fS/IDoEJrfTTwIHBvuOIRQgjRsXBeKRwLbNNa52kzXutF4Lz9tjkPeCb486vAqUrmvgshRMSEMykMAva0+j0/+Fy722gzv70K6Lnd8kII0cf1io5mpdQ1SqmVSqmVJSUlkQ5HCCH6rHAmhQIgq9XvmcHn2t1GmUVPEoGy/XektX5Caz1Faz2lX7+OxwILIYQ4POGcvPYlMFwpNRRT+V8MfHe/bd4CrgA+A74DfKQ7uMHDqlWrSpVSuw4zpjSg9DDfG2kSe2RI7JHRW2PvyXF3ajp32JKC1tqnlLoOWARYgae11l8rpX4HrNRavwX8HXhOKbUNKMckjo72e9iXCkqplZ25yURPJLFHhsQeGb019t4ad2thXeZCa70QWLjfc7e3+rkBmBvOGIQQQnRer+hoFkII0T2iLSk8EekAjoDEHhkSe2T01th7a9whqoN+XSGEEFEk2q4UhBBCHETUJIWOFufrSZRSTyul9imlNrR6LkUp9YFSamvwMTmSMR6IUipLKbVYKbVRKfW1Uuqnwed7dPxKKZdS6gul1Npg3HcEnx8aXKxxW3DxRkekYz0QpRSwEAEAAAUDSURBVJRVKfWVUuqd4O+9Inal1E6l1Hql1Bql1Mrgcz3676WZUipJKfWqUipXKbVJKXV8b4n9QKIiKXRycb6e5J/Amfs9Nx/4n9Z6OPC/4O89kQ/4udZ6DDANuDb4Xff0+BuBU7TWE4Ac4Eyl1DTMIo0PBhdtrMAs4thT/RTY1Or33hT7yVrrnFbDOXv630uzh4D3tNajgAmY77+3xN4+rXWfL8DxwKJWv98K3BrpuDqIORvY0Or3zUBG8OcMYHOkY+zk53gTOL03xQ/EAquB4zATkWzt/R31pIJZMeB/wCnAO4DqRbHvBNL2e67H/71gVmDYQbBvtjfFfrASFVcKdG5xvp4uXWtdGPy5CEiPZDCdEbw/xkTgc3pB/MHmlzXAPuADYDtQqc1ijdCz/27+DPwSCAR/T6X3xK6B95VSq5RS1wSf6/F/L8BQoAT4R7DZ7imllJveEfsBRUtS6FO0OQXp0cPGlFJxwGvAjVrr6tav9dT4tdZ+rXUO5qz7WGBUhEPqFKXUOcA+rfWqSMdymE7UWk/CNO9eq5Sa0frFnvr3gpn8Owl4XGs9Eahjv6aiHhz7AUVLUujM4nw9XbFSKgMg+LgvwvEckFLKjkkIz2utXw8+3Wvi11pXAosxTS5JwcUaoef+3UwH5iildmLuW3IKpq27N8SO1rog+LgPeAOTkHvD30s+kK+1/jz4+6uYJNEbYj+gaEkKocX5giMwLsYsxtebNC8eSPDxzQjGckDBmyT9Hdiktf5Tq5d6dPxKqX5KqaTgzzGYfpBNmOTwneBmPS5uAK31rVrrTK11NuZv+yOt9aX0gtiVUm6lVHzzz8BsYAM9/O8FQGtdBOxRSo0MPnUqsJFeEPtBRbpTo7sKcDaw5f/bu3fXKKIoAOPfESGoAR+gjYWgNiKEgGDhAwJ2VhaKoKYQSxs7EV/gP2AlmDJiEAkYC8ukCKQQDRrjo1GsAoKNiCkUicfi3gwxMSQEkmzI94OB3buzwxmY2TNzlzmHMk98fbXjWSDWR8AX4DflauQSZY54CPgIDAI7VjvOeWI/RrldHgfG6nKy1eMHOoDXNe53wK06vhd4AXwC+oG21Y51gf3oAp6tldhrjG/q8n763Gz142VG/J3AaD1ungLb10rs8y0+0SxJaqyX6SNJ0iKYFCRJDZOCJKlhUpAkNUwKkqSGSUFaQRHRNV3FVGpFJgVJUsOkIP1HRFyo/RXGIqKnFsubjIi7td/CUETsrOt2RsTziBiPiIHp+vkRsT8iBmuPhlcRsa9uvn1GDf6++hS41BJMCtIsEXEAOAsczVIgbwo4D2wBRjPzIDAM3K5feQBczcwO4O2M8T7gXpYeDUcoT6lDqRx7hdLbYy+ldpHUEjYuvIq07pwADgEv60X8JkpRsz/A47rOQ+BJRGwFtmXmcB3vBfprPZ/dmTkAkJk/Aer2XmTmRH0/RumdMbL8uyUtzKQgzRVAb2Ze+2cw4uas9ZZaI+bXjNdTeB6qhTh9JM01BJyOiF3Q9AveQzlfpquOngNGMvM78C0ijtfxbmA4M38AExFxqm6jLSI2r+heSEvgFYo0S2Z+iIgblG5gGyjVai9Tmqgcrp99pfzvAKU88v36o/8ZuFjHu4GeiLhTt3FmBXdDWhKrpEqLFBGTmdm+2nFIy8npI0lSwzsFSVLDOwVJUsOkIElqmBQkSQ2TgiSpYVKQJDVMCpKkxl8tRMzCwCYfpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 645us/sample - loss: 0.6292 - acc: 0.8249\n",
      "Loss: 0.6291707925088433 Accuracy: 0.82492214\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2589 - acc: 0.2479\n",
      "Epoch 00001: val_loss improved from inf to 1.80858, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/001-1.8086.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 2.2588 - acc: 0.2479 - val_loss: 1.8086 - val_acc: 0.4260\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6624 - acc: 0.4575\n",
      "Epoch 00002: val_loss improved from 1.80858 to 1.43201, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/002-1.4320.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.6625 - acc: 0.4575 - val_loss: 1.4320 - val_acc: 0.5516\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3179 - acc: 0.5796\n",
      "Epoch 00003: val_loss improved from 1.43201 to 1.22547, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/003-1.2255.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.3180 - acc: 0.5796 - val_loss: 1.2255 - val_acc: 0.6266\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1204 - acc: 0.6472\n",
      "Epoch 00004: val_loss improved from 1.22547 to 0.98201, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/004-0.9820.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1203 - acc: 0.6472 - val_loss: 0.9820 - val_acc: 0.6995\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9756 - acc: 0.6975\n",
      "Epoch 00005: val_loss improved from 0.98201 to 0.86981, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/005-0.8698.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9756 - acc: 0.6975 - val_loss: 0.8698 - val_acc: 0.7473\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.7368\n",
      "Epoch 00006: val_loss improved from 0.86981 to 0.78760, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/006-0.7876.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8663 - acc: 0.7369 - val_loss: 0.7876 - val_acc: 0.7682\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.7655\n",
      "Epoch 00007: val_loss improved from 0.78760 to 0.73961, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/007-0.7396.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7759 - acc: 0.7655 - val_loss: 0.7396 - val_acc: 0.7820\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.7889\n",
      "Epoch 00008: val_loss improved from 0.73961 to 0.62730, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/008-0.6273.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6952 - acc: 0.7889 - val_loss: 0.6273 - val_acc: 0.8097\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.8092\n",
      "Epoch 00009: val_loss improved from 0.62730 to 0.59382, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/009-0.5938.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6349 - acc: 0.8092 - val_loss: 0.5938 - val_acc: 0.8276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8267\n",
      "Epoch 00010: val_loss improved from 0.59382 to 0.53083, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/010-0.5308.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5797 - acc: 0.8267 - val_loss: 0.5308 - val_acc: 0.8521\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8372\n",
      "Epoch 00011: val_loss improved from 0.53083 to 0.51178, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/011-0.5118.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5428 - acc: 0.8372 - val_loss: 0.5118 - val_acc: 0.8567\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4960 - acc: 0.8498\n",
      "Epoch 00012: val_loss improved from 0.51178 to 0.49385, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/012-0.4938.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4959 - acc: 0.8498 - val_loss: 0.4938 - val_acc: 0.8637\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8597\n",
      "Epoch 00013: val_loss improved from 0.49385 to 0.47895, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/013-0.4790.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4633 - acc: 0.8597 - val_loss: 0.4790 - val_acc: 0.8705\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8673\n",
      "Epoch 00014: val_loss improved from 0.47895 to 0.47618, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/014-0.4762.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4366 - acc: 0.8673 - val_loss: 0.4762 - val_acc: 0.8635\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8743\n",
      "Epoch 00015: val_loss improved from 0.47618 to 0.46428, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/015-0.4643.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4118 - acc: 0.8744 - val_loss: 0.4643 - val_acc: 0.8682\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8814\n",
      "Epoch 00016: val_loss improved from 0.46428 to 0.44956, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/016-0.4496.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3852 - acc: 0.8814 - val_loss: 0.4496 - val_acc: 0.8796\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8906\n",
      "Epoch 00017: val_loss improved from 0.44956 to 0.43942, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/017-0.4394.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3575 - acc: 0.8906 - val_loss: 0.4394 - val_acc: 0.8826\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8942\n",
      "Epoch 00018: val_loss improved from 0.43942 to 0.42352, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/018-0.4235.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3473 - acc: 0.8942 - val_loss: 0.4235 - val_acc: 0.8784\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8971\n",
      "Epoch 00019: val_loss did not improve from 0.42352\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3306 - acc: 0.8970 - val_loss: 0.4680 - val_acc: 0.8705\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8979\n",
      "Epoch 00020: val_loss improved from 0.42352 to 0.41900, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/020-0.4190.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3354 - acc: 0.8979 - val_loss: 0.4190 - val_acc: 0.8884\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9049\n",
      "Epoch 00021: val_loss improved from 0.41900 to 0.40569, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/021-0.4057.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3034 - acc: 0.9049 - val_loss: 0.4057 - val_acc: 0.8926\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9087\n",
      "Epoch 00022: val_loss did not improve from 0.40569\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2880 - acc: 0.9087 - val_loss: 0.4081 - val_acc: 0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9147\n",
      "Epoch 00023: val_loss did not improve from 0.40569\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2754 - acc: 0.9147 - val_loss: 0.4152 - val_acc: 0.8908\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9163\n",
      "Epoch 00024: val_loss did not improve from 0.40569\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2667 - acc: 0.9163 - val_loss: 0.4090 - val_acc: 0.9017\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9196\n",
      "Epoch 00025: val_loss did not improve from 0.40569\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2527 - acc: 0.9196 - val_loss: 0.4286 - val_acc: 0.8945\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9232\n",
      "Epoch 00026: val_loss did not improve from 0.40569\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2428 - acc: 0.9232 - val_loss: 0.4299 - val_acc: 0.8938\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9279\n",
      "Epoch 00027: val_loss improved from 0.40569 to 0.39489, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/027-0.3949.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2326 - acc: 0.9279 - val_loss: 0.3949 - val_acc: 0.9015\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9280\n",
      "Epoch 00028: val_loss did not improve from 0.39489\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2270 - acc: 0.9279 - val_loss: 0.3968 - val_acc: 0.9019\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9306\n",
      "Epoch 00029: val_loss did not improve from 0.39489\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2165 - acc: 0.9306 - val_loss: 0.4096 - val_acc: 0.8987\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9350\n",
      "Epoch 00030: val_loss improved from 0.39489 to 0.38658, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_5_conv_checkpoint/030-0.3866.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2046 - acc: 0.9350 - val_loss: 0.3866 - val_acc: 0.8949\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9343\n",
      "Epoch 00031: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2092 - acc: 0.9343 - val_loss: 0.3910 - val_acc: 0.9059\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9381\n",
      "Epoch 00032: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1907 - acc: 0.9381 - val_loss: 0.4099 - val_acc: 0.9064\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9395\n",
      "Epoch 00033: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1900 - acc: 0.9395 - val_loss: 0.3972 - val_acc: 0.9071\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9403\n",
      "Epoch 00034: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1819 - acc: 0.9403 - val_loss: 0.3972 - val_acc: 0.9029\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9416\n",
      "Epoch 00035: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1784 - acc: 0.9416 - val_loss: 0.3965 - val_acc: 0.8989\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9419\n",
      "Epoch 00036: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1746 - acc: 0.9419 - val_loss: 0.4096 - val_acc: 0.9003\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9448\n",
      "Epoch 00037: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1664 - acc: 0.9448 - val_loss: 0.4503 - val_acc: 0.8921\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9477\n",
      "Epoch 00038: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1582 - acc: 0.9477 - val_loss: 0.4273 - val_acc: 0.9054\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9466\n",
      "Epoch 00039: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1612 - acc: 0.9466 - val_loss: 0.4541 - val_acc: 0.8977\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9494\n",
      "Epoch 00040: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1549 - acc: 0.9494 - val_loss: 0.4257 - val_acc: 0.9073\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9514\n",
      "Epoch 00041: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1464 - acc: 0.9514 - val_loss: 0.3987 - val_acc: 0.9101\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9536\n",
      "Epoch 00042: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1388 - acc: 0.9536 - val_loss: 0.4348 - val_acc: 0.9059\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9552\n",
      "Epoch 00043: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1379 - acc: 0.9553 - val_loss: 0.4226 - val_acc: 0.9031\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9543\n",
      "Epoch 00044: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1382 - acc: 0.9543 - val_loss: 0.4326 - val_acc: 0.9047\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9564\n",
      "Epoch 00045: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1312 - acc: 0.9564 - val_loss: 0.4180 - val_acc: 0.9054\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9558\n",
      "Epoch 00046: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1329 - acc: 0.9557 - val_loss: 0.4403 - val_acc: 0.9094\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9573\n",
      "Epoch 00047: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1300 - acc: 0.9573 - val_loss: 0.4189 - val_acc: 0.9131\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9580\n",
      "Epoch 00048: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1250 - acc: 0.9580 - val_loss: 0.4213 - val_acc: 0.9103\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9586\n",
      "Epoch 00049: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1225 - acc: 0.9586 - val_loss: 0.4496 - val_acc: 0.9047\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9623\n",
      "Epoch 00050: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1145 - acc: 0.9623 - val_loss: 0.4231 - val_acc: 0.9036\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9647\n",
      "Epoch 00051: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1082 - acc: 0.9647 - val_loss: 0.4677 - val_acc: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9627\n",
      "Epoch 00052: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1151 - acc: 0.9627 - val_loss: 0.4194 - val_acc: 0.9126\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9601\n",
      "Epoch 00053: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1255 - acc: 0.9601 - val_loss: 0.4227 - val_acc: 0.9140\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9645\n",
      "Epoch 00054: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1050 - acc: 0.9645 - val_loss: 0.4437 - val_acc: 0.9092\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9654\n",
      "Epoch 00055: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1074 - acc: 0.9653 - val_loss: 0.4757 - val_acc: 0.9050\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9641\n",
      "Epoch 00056: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1078 - acc: 0.9641 - val_loss: 0.4416 - val_acc: 0.9131\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9674\n",
      "Epoch 00057: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0985 - acc: 0.9674 - val_loss: 0.4556 - val_acc: 0.9108\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9683\n",
      "Epoch 00058: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0949 - acc: 0.9683 - val_loss: 0.4720 - val_acc: 0.9082\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9667\n",
      "Epoch 00059: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0982 - acc: 0.9667 - val_loss: 0.4516 - val_acc: 0.9145\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9662\n",
      "Epoch 00060: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1037 - acc: 0.9661 - val_loss: 0.4841 - val_acc: 0.9052\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9674\n",
      "Epoch 00061: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.1001 - acc: 0.9675 - val_loss: 0.4598 - val_acc: 0.9087\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9709\n",
      "Epoch 00062: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0857 - acc: 0.9709 - val_loss: 0.4782 - val_acc: 0.9064\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9676\n",
      "Epoch 00063: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0978 - acc: 0.9676 - val_loss: 0.4856 - val_acc: 0.9136\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9714\n",
      "Epoch 00064: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0850 - acc: 0.9714 - val_loss: 0.4574 - val_acc: 0.9057\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9697\n",
      "Epoch 00065: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0926 - acc: 0.9697 - val_loss: 0.4658 - val_acc: 0.9117\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9710\n",
      "Epoch 00066: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0906 - acc: 0.9710 - val_loss: 0.4636 - val_acc: 0.9087\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9715\n",
      "Epoch 00067: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0861 - acc: 0.9715 - val_loss: 0.4648 - val_acc: 0.9131\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9705\n",
      "Epoch 00068: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0881 - acc: 0.9705 - val_loss: 0.4527 - val_acc: 0.9124\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9731\n",
      "Epoch 00069: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0833 - acc: 0.9731 - val_loss: 0.4628 - val_acc: 0.9124\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9723\n",
      "Epoch 00070: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0832 - acc: 0.9723 - val_loss: 0.4796 - val_acc: 0.9129\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9721\n",
      "Epoch 00071: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0832 - acc: 0.9721 - val_loss: 0.4636 - val_acc: 0.9133\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9735\n",
      "Epoch 00072: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0805 - acc: 0.9735 - val_loss: 0.4894 - val_acc: 0.9071\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9742\n",
      "Epoch 00073: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0795 - acc: 0.9742 - val_loss: 0.4576 - val_acc: 0.9115\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9726\n",
      "Epoch 00074: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0843 - acc: 0.9726 - val_loss: 0.5390 - val_acc: 0.8994\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9710\n",
      "Epoch 00075: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0879 - acc: 0.9710 - val_loss: 0.4638 - val_acc: 0.9131\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9773\n",
      "Epoch 00076: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0698 - acc: 0.9773 - val_loss: 0.5074 - val_acc: 0.9103\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9738\n",
      "Epoch 00077: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0802 - acc: 0.9738 - val_loss: 0.4714 - val_acc: 0.9113\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9751\n",
      "Epoch 00078: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0767 - acc: 0.9751 - val_loss: 0.4479 - val_acc: 0.9147\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9747\n",
      "Epoch 00079: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0763 - acc: 0.9747 - val_loss: 0.4843 - val_acc: 0.9064\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9767\n",
      "Epoch 00080: val_loss did not improve from 0.38658\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0731 - acc: 0.9767 - val_loss: 0.5238 - val_acc: 0.9119\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 668us/sample - loss: 0.4770 - acc: 0.8758\n",
      "Loss: 0.4769584953970627 Accuracy: 0.8758048\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3697 - acc: 0.2144\n",
      "Epoch 00001: val_loss improved from inf to 1.77170, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/001-1.7717.hdf5\n",
      "36805/36805 [==============================] - 69s 2ms/sample - loss: 2.3696 - acc: 0.2145 - val_loss: 1.7717 - val_acc: 0.4330\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6570 - acc: 0.4637\n",
      "Epoch 00002: val_loss improved from 1.77170 to 1.34155, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/002-1.3415.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.6569 - acc: 0.4638 - val_loss: 1.3415 - val_acc: 0.5751\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3953 - acc: 0.5486\n",
      "Epoch 00003: val_loss improved from 1.34155 to 1.20963, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/003-1.2096.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.3952 - acc: 0.5487 - val_loss: 1.2096 - val_acc: 0.6117\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2493 - acc: 0.5963\n",
      "Epoch 00004: val_loss improved from 1.20963 to 1.10043, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/004-1.1004.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.2494 - acc: 0.5963 - val_loss: 1.1004 - val_acc: 0.6508\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1204 - acc: 0.6463\n",
      "Epoch 00005: val_loss improved from 1.10043 to 0.94449, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/005-0.9445.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.1203 - acc: 0.6463 - val_loss: 0.9445 - val_acc: 0.7237\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9832 - acc: 0.6914\n",
      "Epoch 00006: val_loss improved from 0.94449 to 0.81182, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/006-0.8118.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.9832 - acc: 0.6914 - val_loss: 0.8118 - val_acc: 0.7577\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8822 - acc: 0.7242\n",
      "Epoch 00007: val_loss improved from 0.81182 to 0.70045, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/007-0.7004.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.8822 - acc: 0.7242 - val_loss: 0.7004 - val_acc: 0.7934\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7983 - acc: 0.7528\n",
      "Epoch 00008: val_loss improved from 0.70045 to 0.66404, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/008-0.6640.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7983 - acc: 0.7528 - val_loss: 0.6640 - val_acc: 0.8048\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.7768\n",
      "Epoch 00009: val_loss improved from 0.66404 to 0.63113, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/009-0.6311.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7251 - acc: 0.7768 - val_loss: 0.6311 - val_acc: 0.8143\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.7957\n",
      "Epoch 00010: val_loss improved from 0.63113 to 0.58525, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/010-0.5852.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6744 - acc: 0.7957 - val_loss: 0.5852 - val_acc: 0.8293\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.8133\n",
      "Epoch 00011: val_loss improved from 0.58525 to 0.54411, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/011-0.5441.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6205 - acc: 0.8133 - val_loss: 0.5441 - val_acc: 0.8481\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8249\n",
      "Epoch 00012: val_loss improved from 0.54411 to 0.49597, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/012-0.4960.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5735 - acc: 0.8249 - val_loss: 0.4960 - val_acc: 0.8628\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8390\n",
      "Epoch 00013: val_loss improved from 0.49597 to 0.46888, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/013-0.4689.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5395 - acc: 0.8390 - val_loss: 0.4689 - val_acc: 0.8696\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8461\n",
      "Epoch 00014: val_loss improved from 0.46888 to 0.46237, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/014-0.4624.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5038 - acc: 0.8461 - val_loss: 0.4624 - val_acc: 0.8703\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8545\n",
      "Epoch 00015: val_loss improved from 0.46237 to 0.44124, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/015-0.4412.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4818 - acc: 0.8545 - val_loss: 0.4412 - val_acc: 0.8784\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8646\n",
      "Epoch 00016: val_loss improved from 0.44124 to 0.41758, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/016-0.4176.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4498 - acc: 0.8646 - val_loss: 0.4176 - val_acc: 0.8833\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8706\n",
      "Epoch 00017: val_loss improved from 0.41758 to 0.39443, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/017-0.3944.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4286 - acc: 0.8706 - val_loss: 0.3944 - val_acc: 0.8873\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8756\n",
      "Epoch 00018: val_loss improved from 0.39443 to 0.37802, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/018-0.3780.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4057 - acc: 0.8756 - val_loss: 0.3780 - val_acc: 0.8949\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8819\n",
      "Epoch 00019: val_loss improved from 0.37802 to 0.35222, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/019-0.3522.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3877 - acc: 0.8819 - val_loss: 0.3522 - val_acc: 0.9019\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8855\n",
      "Epoch 00020: val_loss did not improve from 0.35222\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3695 - acc: 0.8855 - val_loss: 0.3807 - val_acc: 0.8954\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8889\n",
      "Epoch 00021: val_loss improved from 0.35222 to 0.34036, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/021-0.3404.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3578 - acc: 0.8890 - val_loss: 0.3404 - val_acc: 0.9061\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8934\n",
      "Epoch 00022: val_loss improved from 0.34036 to 0.32194, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/022-0.3219.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3470 - acc: 0.8934 - val_loss: 0.3219 - val_acc: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8973\n",
      "Epoch 00023: val_loss did not improve from 0.32194\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3335 - acc: 0.8973 - val_loss: 0.3535 - val_acc: 0.9019\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8971\n",
      "Epoch 00024: val_loss did not improve from 0.32194\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3287 - acc: 0.8971 - val_loss: 0.3411 - val_acc: 0.9071\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9031\n",
      "Epoch 00025: val_loss improved from 0.32194 to 0.31294, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/025-0.3129.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3094 - acc: 0.9031 - val_loss: 0.3129 - val_acc: 0.9138\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9076\n",
      "Epoch 00026: val_loss did not improve from 0.31294\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2932 - acc: 0.9075 - val_loss: 0.3136 - val_acc: 0.9087\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9074\n",
      "Epoch 00027: val_loss did not improve from 0.31294\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2954 - acc: 0.9075 - val_loss: 0.3235 - val_acc: 0.9119\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9093\n",
      "Epoch 00028: val_loss improved from 0.31294 to 0.30248, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/028-0.3025.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2859 - acc: 0.9093 - val_loss: 0.3025 - val_acc: 0.9136\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9136\n",
      "Epoch 00029: val_loss improved from 0.30248 to 0.29205, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/029-0.2920.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2716 - acc: 0.9136 - val_loss: 0.2920 - val_acc: 0.9271\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9163\n",
      "Epoch 00030: val_loss did not improve from 0.29205\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2657 - acc: 0.9163 - val_loss: 0.3061 - val_acc: 0.9199\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9174\n",
      "Epoch 00031: val_loss did not improve from 0.29205\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2604 - acc: 0.9174 - val_loss: 0.3160 - val_acc: 0.9147\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9212\n",
      "Epoch 00032: val_loss improved from 0.29205 to 0.27957, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/032-0.2796.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2502 - acc: 0.9213 - val_loss: 0.2796 - val_acc: 0.9220\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9213\n",
      "Epoch 00033: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2461 - acc: 0.9213 - val_loss: 0.2882 - val_acc: 0.9276\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9223\n",
      "Epoch 00034: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2411 - acc: 0.9223 - val_loss: 0.3070 - val_acc: 0.9208\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9248\n",
      "Epoch 00035: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2305 - acc: 0.9247 - val_loss: 0.2939 - val_acc: 0.9231\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9267\n",
      "Epoch 00036: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2294 - acc: 0.9267 - val_loss: 0.2920 - val_acc: 0.9194\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9285\n",
      "Epoch 00037: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2202 - acc: 0.9285 - val_loss: 0.2893 - val_acc: 0.9273\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9288\n",
      "Epoch 00038: val_loss improved from 0.27957 to 0.27454, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/038-0.2745.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2185 - acc: 0.9288 - val_loss: 0.2745 - val_acc: 0.9220\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9314\n",
      "Epoch 00039: val_loss improved from 0.27454 to 0.26234, saving model to model/checkpoint/1D_CNN_custom_kernel_192_ch_32_DO_6_conv_checkpoint/039-0.2623.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2107 - acc: 0.9314 - val_loss: 0.2623 - val_acc: 0.9290\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9321\n",
      "Epoch 00040: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2096 - acc: 0.9321 - val_loss: 0.2790 - val_acc: 0.9297\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9338\n",
      "Epoch 00041: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2023 - acc: 0.9338 - val_loss: 0.2848 - val_acc: 0.9243\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9353\n",
      "Epoch 00042: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1968 - acc: 0.9353 - val_loss: 0.2669 - val_acc: 0.9250\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9331\n",
      "Epoch 00043: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1977 - acc: 0.9331 - val_loss: 0.2876 - val_acc: 0.9269\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9390\n",
      "Epoch 00044: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1870 - acc: 0.9390 - val_loss: 0.2933 - val_acc: 0.9243\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9368\n",
      "Epoch 00045: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1892 - acc: 0.9368 - val_loss: 0.2928 - val_acc: 0.9257\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9403\n",
      "Epoch 00046: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1834 - acc: 0.9403 - val_loss: 0.2701 - val_acc: 0.9292\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9395\n",
      "Epoch 00047: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1830 - acc: 0.9395 - val_loss: 0.2661 - val_acc: 0.9341\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9432\n",
      "Epoch 00048: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1753 - acc: 0.9432 - val_loss: 0.3009 - val_acc: 0.9192\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9437\n",
      "Epoch 00049: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1705 - acc: 0.9437 - val_loss: 0.2831 - val_acc: 0.9262\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9443\n",
      "Epoch 00050: val_loss did not improve from 0.26234\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1701 - acc: 0.9443 - val_loss: 0.2700 - val_acc: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      " 4992/36805 [===>..........................] - ETA: 53s - loss: 0.1608 - acc: 0.9465"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_kernel_192_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_465 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_465 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_466 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_466 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_387 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_467 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_467 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_388 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_78 (Flatten)         (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 1,063,536\n",
      "Trainable params: 1,063,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 714us/sample - loss: 0.7908 - acc: 0.7765\n",
      "Loss: 0.7907614937328723 Accuracy: 0.7765317\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_468 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_468 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_469 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_469 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_389 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_470 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_470 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_390 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_471 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_471 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_391 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_79 (Flatten)         (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 481,424\n",
      "Trainable params: 481,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 739us/sample - loss: 0.6292 - acc: 0.8249\n",
      "Loss: 0.6291707925088433 Accuracy: 0.82492214\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_472 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_472 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_473 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_473 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_392 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_474 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_474 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_393 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_475 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_475 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_394 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_476 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_476 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_395 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_80 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 404,688\n",
      "Trainable params: 404,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 778us/sample - loss: 0.4770 - acc: 0.8758\n",
      "Loss: 0.4769584953970627 Accuracy: 0.8758048\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_477 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_477 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_478 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_478 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_396 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_479 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_479 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_397 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_480 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_480 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_398 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_481 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_481 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_399 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_482 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_482 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_400 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_81 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 294,160\n",
      "Trainable params: 294,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 783us/sample - loss: 0.3332 - acc: 0.9213\n",
      "Loss: 0.33323031511633566 Accuracy: 0.92128766\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_483 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_483 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_484 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_484 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_401 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_485 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_485 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_402 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_486 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_486 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_403 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_487 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_487 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_404 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_488 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_488 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_405 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_489 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_489 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_406 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_82 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 261,456\n",
      "Trainable params: 261,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 808us/sample - loss: 0.2239 - acc: 0.9340\n",
      "Loss: 0.22393909785296934 Accuracy: 0.9339564\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_490 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_490 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_491 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_491 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_407 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_492 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_492 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_408 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_493 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_493 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_409 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_494 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_494 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_410 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_495 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_495 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_411 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_496 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_496 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_412 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_497 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_497 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_413 (MaxPoolin (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_83 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 259,472\n",
      "Trainable params: 259,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 811us/sample - loss: 0.1864 - acc: 0.9483\n",
      "Loss: 0.18638711526974464 Accuracy: 0.9482866\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_498 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_498 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_499 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_499 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_414 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_500 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_500 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_415 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_501 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_501 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_416 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_502 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_502 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_417 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_503 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_503 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_418 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_504 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_504 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_419 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_505 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_505 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_420 (MaxPoolin (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_506 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "activation_506 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_421 (MaxPoolin (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_84 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 281,104\n",
      "Trainable params: 281,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 834us/sample - loss: 0.2022 - acc: 0.9445\n",
      "Loss: 0.20224866479526923 Accuracy: 0.9445483\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_kernel_192_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_465 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_465 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_466 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_466 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_387 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_467 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_467 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_388 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_78 (Flatten)         (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 1,063,536\n",
      "Trainable params: 1,063,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 779us/sample - loss: 1.3741 - acc: 0.7900\n",
      "Loss: 1.3741299269355347 Accuracy: 0.79003114\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_468 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_468 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_469 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_469 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_389 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_470 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_470 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_390 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_471 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_471 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_391 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_79 (Flatten)         (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 481,424\n",
      "Trainable params: 481,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 817us/sample - loss: 0.8512 - acc: 0.8428\n",
      "Loss: 0.851219586534416 Accuracy: 0.842783\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_472 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_472 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_473 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_473 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_392 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_474 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_474 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_393 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_475 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_475 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_394 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_476 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_476 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_395 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_80 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 404,688\n",
      "Trainable params: 404,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 837us/sample - loss: 0.6361 - acc: 0.8831\n",
      "Loss: 0.6360933659016034 Accuracy: 0.88307375\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_477 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_477 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_478 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_478 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_396 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_479 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_479 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_397 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_480 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_480 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_398 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_481 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_481 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_399 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_482 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_482 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_400 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_81 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 294,160\n",
      "Trainable params: 294,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 866us/sample - loss: 0.3469 - acc: 0.9269\n",
      "Loss: 0.3468662891927662 Accuracy: 0.92689514\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_483 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_483 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_484 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_484 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_401 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_485 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_485 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_402 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_486 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_486 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_403 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_487 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_487 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_404 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_488 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_488 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_405 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_489 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_489 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_406 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_82 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 261,456\n",
      "Trainable params: 261,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 897us/sample - loss: 0.2345 - acc: 0.9410\n",
      "Loss: 0.23451450358329656 Accuracy: 0.9410176\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_490 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_490 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_491 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_491 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_407 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_492 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_492 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_408 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_493 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_493 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_409 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_494 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_494 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_410 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_495 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_495 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_411 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_496 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_496 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_412 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_497 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_497 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_413 (MaxPoolin (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_83 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 259,472\n",
      "Trainable params: 259,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 894us/sample - loss: 0.2100 - acc: 0.9470\n",
      "Loss: 0.21002699334951946 Accuracy: 0.9470405\n",
      "\n",
      "1D_CNN_custom_kernel_192_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_498 (Conv1D)          (None, 16000, 32)         6176      \n",
      "_________________________________________________________________\n",
      "activation_498 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_499 (Conv1D)          (None, 16000, 32)         98336     \n",
      "_________________________________________________________________\n",
      "activation_499 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_414 (MaxPoolin (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_500 (Conv1D)          (None, 5333, 32)          49184     \n",
      "_________________________________________________________________\n",
      "activation_500 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_415 (MaxPoolin (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_501 (Conv1D)          (None, 1777, 32)          24608     \n",
      "_________________________________________________________________\n",
      "activation_501 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_416 (MaxPoolin (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_502 (Conv1D)          (None, 592, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_502 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_417 (MaxPoolin (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_503 (Conv1D)          (None, 197, 64)           24640     \n",
      "_________________________________________________________________\n",
      "activation_503 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_418 (MaxPoolin (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_504 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_504 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_419 (MaxPoolin (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_505 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_505 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_420 (MaxPoolin (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_506 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "activation_506 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_421 (MaxPoolin (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_84 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 281,104\n",
      "Trainable params: 281,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 931us/sample - loss: 0.2235 - acc: 0.9466\n",
      "Loss: 0.2234834822519515 Accuracy: 0.9466251\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
