{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid', \n",
    "                          activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,456\n",
      "Trainable params: 511,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,648\n",
      "Trainable params: 257,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,688\n",
      "Trainable params: 140,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 123,856\n",
      "Trainable params: 123,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 288,848\n",
      "Trainable params: 288,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4697 - acc: 0.2267\n",
      "Epoch 00001: val_loss improved from inf to 2.17973, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/001-2.1797.hdf5\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 2.4688 - acc: 0.2270 - val_loss: 2.1797 - val_acc: 0.3531\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9721 - acc: 0.4087\n",
      "Epoch 00002: val_loss improved from 2.17973 to 1.84732, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/002-1.8473.hdf5\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.9714 - acc: 0.4089 - val_loss: 1.8473 - val_acc: 0.4496\n",
      "Epoch 3/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.6849 - acc: 0.4990\n",
      "Epoch 00003: val_loss improved from 1.84732 to 1.67990, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/003-1.6799.hdf5\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 1.6849 - acc: 0.4990 - val_loss: 1.6799 - val_acc: 0.4959\n",
      "Epoch 4/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.5142 - acc: 0.5490\n",
      "Epoch 00004: val_loss improved from 1.67990 to 1.61226, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/004-1.6123.hdf5\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.5141 - acc: 0.5489 - val_loss: 1.6123 - val_acc: 0.5041\n",
      "Epoch 5/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.3972 - acc: 0.5822\n",
      "Epoch 00005: val_loss improved from 1.61226 to 1.55286, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/005-1.5529.hdf5\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 1.3977 - acc: 0.5823 - val_loss: 1.5529 - val_acc: 0.5129\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3077 - acc: 0.6097\n",
      "Epoch 00006: val_loss improved from 1.55286 to 1.54007, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/006-1.5401.hdf5\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.3076 - acc: 0.6098 - val_loss: 1.5401 - val_acc: 0.5129\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2375 - acc: 0.6310\n",
      "Epoch 00007: val_loss improved from 1.54007 to 1.52601, saving model to model/checkpoint/1D_CNN_1_only_conv_checkpoint/007-1.5260.hdf5\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.2372 - acc: 0.6310 - val_loss: 1.5260 - val_acc: 0.5183\n",
      "Epoch 8/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1758 - acc: 0.6506\n",
      "Epoch 00008: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 154us/sample - loss: 1.1757 - acc: 0.6506 - val_loss: 1.5269 - val_acc: 0.5136\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1217 - acc: 0.6657\n",
      "Epoch 00009: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.1221 - acc: 0.6657 - val_loss: 1.5290 - val_acc: 0.5178\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0746 - acc: 0.6815\n",
      "Epoch 00010: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 1.0746 - acc: 0.6815 - val_loss: 1.5436 - val_acc: 0.5073\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0292 - acc: 0.6974\n",
      "Epoch 00011: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.0303 - acc: 0.6971 - val_loss: 1.5468 - val_acc: 0.5143\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9893 - acc: 0.7101\n",
      "Epoch 00012: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.9893 - acc: 0.7101 - val_loss: 1.5673 - val_acc: 0.5136\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.7229\n",
      "Epoch 00013: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 155us/sample - loss: 0.9527 - acc: 0.7229 - val_loss: 1.5754 - val_acc: 0.5080\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9165 - acc: 0.7352\n",
      "Epoch 00014: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9164 - acc: 0.7352 - val_loss: 1.6031 - val_acc: 0.5045\n",
      "Epoch 15/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.7448\n",
      "Epoch 00015: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.8832 - acc: 0.7447 - val_loss: 1.6091 - val_acc: 0.4990\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8522 - acc: 0.7565\n",
      "Epoch 00016: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.8523 - acc: 0.7563 - val_loss: 1.6186 - val_acc: 0.5041\n",
      "Epoch 17/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8202 - acc: 0.7664\n",
      "Epoch 00017: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.8209 - acc: 0.7661 - val_loss: 1.6371 - val_acc: 0.4985\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7916 - acc: 0.7769\n",
      "Epoch 00018: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.7918 - acc: 0.7769 - val_loss: 1.6624 - val_acc: 0.4966\n",
      "Epoch 19/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7875\n",
      "Epoch 00019: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.7632 - acc: 0.7870 - val_loss: 1.6780 - val_acc: 0.4945\n",
      "Epoch 20/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7387 - acc: 0.7949\n",
      "Epoch 00020: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.7382 - acc: 0.7950 - val_loss: 1.6914 - val_acc: 0.4950\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7123 - acc: 0.8039\n",
      "Epoch 00021: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 0.7124 - acc: 0.8039 - val_loss: 1.7148 - val_acc: 0.4901\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.8143\n",
      "Epoch 00022: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 0.6860 - acc: 0.8143 - val_loss: 1.7334 - val_acc: 0.4892\n",
      "Epoch 23/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6629 - acc: 0.8245\n",
      "Epoch 00023: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.6626 - acc: 0.8245 - val_loss: 1.7708 - val_acc: 0.4868\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6379 - acc: 0.8295\n",
      "Epoch 00024: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.6381 - acc: 0.8294 - val_loss: 1.7751 - val_acc: 0.4859\n",
      "Epoch 25/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8372\n",
      "Epoch 00025: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 155us/sample - loss: 0.6166 - acc: 0.8370 - val_loss: 1.7989 - val_acc: 0.4873\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8474\n",
      "Epoch 00026: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.5937 - acc: 0.8474 - val_loss: 1.8243 - val_acc: 0.4826\n",
      "Epoch 27/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8544\n",
      "Epoch 00027: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.5723 - acc: 0.8543 - val_loss: 1.8390 - val_acc: 0.4792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8608\n",
      "Epoch 00028: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.5516 - acc: 0.8606 - val_loss: 1.8625 - val_acc: 0.4808\n",
      "Epoch 29/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8672\n",
      "Epoch 00029: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.5321 - acc: 0.8673 - val_loss: 1.8888 - val_acc: 0.4796\n",
      "Epoch 30/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8748\n",
      "Epoch 00030: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.5143 - acc: 0.8750 - val_loss: 1.9096 - val_acc: 0.4782\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.8817\n",
      "Epoch 00031: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.4946 - acc: 0.8817 - val_loss: 1.9377 - val_acc: 0.4771\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8852\n",
      "Epoch 00032: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.4803 - acc: 0.8850 - val_loss: 1.9598 - val_acc: 0.4794\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8950\n",
      "Epoch 00033: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.4595 - acc: 0.8950 - val_loss: 1.9749 - val_acc: 0.4726\n",
      "Epoch 34/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.9014\n",
      "Epoch 00034: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 155us/sample - loss: 0.4431 - acc: 0.9011 - val_loss: 2.0089 - val_acc: 0.4701\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.9052\n",
      "Epoch 00035: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.4296 - acc: 0.9052 - val_loss: 2.0358 - val_acc: 0.4740\n",
      "Epoch 36/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.9103\n",
      "Epoch 00036: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.4149 - acc: 0.9102 - val_loss: 2.0543 - val_acc: 0.4705\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.9146\n",
      "Epoch 00037: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.3988 - acc: 0.9147 - val_loss: 2.0801 - val_acc: 0.4715\n",
      "Epoch 38/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.9203\n",
      "Epoch 00038: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.3839 - acc: 0.9203 - val_loss: 2.1071 - val_acc: 0.4717\n",
      "Epoch 39/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.9243\n",
      "Epoch 00039: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.3701 - acc: 0.9243 - val_loss: 2.1372 - val_acc: 0.4724\n",
      "Epoch 40/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.9291\n",
      "Epoch 00040: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.3559 - acc: 0.9289 - val_loss: 2.1767 - val_acc: 0.4666\n",
      "Epoch 41/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.9322\n",
      "Epoch 00041: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 154us/sample - loss: 0.3450 - acc: 0.9322 - val_loss: 2.1968 - val_acc: 0.4684\n",
      "Epoch 42/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.9386\n",
      "Epoch 00042: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.3288 - acc: 0.9385 - val_loss: 2.2275 - val_acc: 0.4666\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9420\n",
      "Epoch 00043: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.3176 - acc: 0.9420 - val_loss: 2.2524 - val_acc: 0.4705\n",
      "Epoch 44/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9455\n",
      "Epoch 00044: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.3038 - acc: 0.9453 - val_loss: 2.2925 - val_acc: 0.4598\n",
      "Epoch 45/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9493\n",
      "Epoch 00045: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 154us/sample - loss: 0.2916 - acc: 0.9490 - val_loss: 2.3041 - val_acc: 0.4698\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9515\n",
      "Epoch 00046: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.2821 - acc: 0.9515 - val_loss: 2.3291 - val_acc: 0.4673\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9552\n",
      "Epoch 00047: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.2704 - acc: 0.9549 - val_loss: 2.3684 - val_acc: 0.4684\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9575\n",
      "Epoch 00048: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.2610 - acc: 0.9576 - val_loss: 2.3923 - val_acc: 0.4645\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9607\n",
      "Epoch 00049: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.2486 - acc: 0.9607 - val_loss: 2.4197 - val_acc: 0.4649\n",
      "Epoch 50/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9642\n",
      "Epoch 00050: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.2398 - acc: 0.9641 - val_loss: 2.4326 - val_acc: 0.4642\n",
      "Epoch 51/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9668\n",
      "Epoch 00051: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.2294 - acc: 0.9667 - val_loss: 2.4703 - val_acc: 0.4649\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9674\n",
      "Epoch 00052: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.2225 - acc: 0.9674 - val_loss: 2.5044 - val_acc: 0.4624\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9706\n",
      "Epoch 00053: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.2124 - acc: 0.9706 - val_loss: 2.5256 - val_acc: 0.4610\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9718\n",
      "Epoch 00054: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.2076 - acc: 0.9718 - val_loss: 2.5691 - val_acc: 0.4601\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9760\n",
      "Epoch 00055: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.1971 - acc: 0.9760 - val_loss: 2.5992 - val_acc: 0.4580\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9757\n",
      "Epoch 00056: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.1911 - acc: 0.9757 - val_loss: 2.6151 - val_acc: 0.4628\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9781\n",
      "Epoch 00057: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.1850 - acc: 0.9781 - val_loss: 2.6322 - val_acc: 0.4663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9790\n",
      "Epoch 00058: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.1767 - acc: 0.9790 - val_loss: 2.6632 - val_acc: 0.4621\n",
      "Epoch 59/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9818\n",
      "Epoch 00059: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 0.1684 - acc: 0.9816 - val_loss: 2.7004 - val_acc: 0.4594\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9829\n",
      "Epoch 00060: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.1627 - acc: 0.9829 - val_loss: 2.7175 - val_acc: 0.4663\n",
      "Epoch 61/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9838\n",
      "Epoch 00061: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.1576 - acc: 0.9837 - val_loss: 2.7539 - val_acc: 0.4638\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9846\n",
      "Epoch 00062: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.1546 - acc: 0.9846 - val_loss: 2.7648 - val_acc: 0.4582\n",
      "Epoch 63/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9867\n",
      "Epoch 00063: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.1450 - acc: 0.9866 - val_loss: 2.8136 - val_acc: 0.4573\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9873\n",
      "Epoch 00064: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.1424 - acc: 0.9872 - val_loss: 2.8200 - val_acc: 0.4566\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9886\n",
      "Epoch 00065: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.1353 - acc: 0.9886 - val_loss: 2.8574 - val_acc: 0.4559\n",
      "Epoch 66/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9899\n",
      "Epoch 00066: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.1300 - acc: 0.9899 - val_loss: 2.8838 - val_acc: 0.4549\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9911\n",
      "Epoch 00067: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.1239 - acc: 0.9911 - val_loss: 2.9216 - val_acc: 0.4531\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.1272 - acc: 0.9895 - val_loss: 2.9341 - val_acc: 0.4573\n",
      "Epoch 69/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9922\n",
      "Epoch 00069: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.1155 - acc: 0.9922 - val_loss: 2.9661 - val_acc: 0.4552\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9921\n",
      "Epoch 00070: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.1124 - acc: 0.9921 - val_loss: 2.9686 - val_acc: 0.4545\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9927\n",
      "Epoch 00071: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.1099 - acc: 0.9927 - val_loss: 3.0082 - val_acc: 0.4577\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9940\n",
      "Epoch 00072: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.1021 - acc: 0.9940 - val_loss: 3.0524 - val_acc: 0.4528\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9928\n",
      "Epoch 00073: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.1055 - acc: 0.9928 - val_loss: 3.0742 - val_acc: 0.4556\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.0988 - acc: 0.9945 - val_loss: 3.0899 - val_acc: 0.4491\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9948\n",
      "Epoch 00075: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0967 - acc: 0.9948 - val_loss: 3.0955 - val_acc: 0.4566\n",
      "Epoch 76/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9944\n",
      "Epoch 00076: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 155us/sample - loss: 0.0924 - acc: 0.9945 - val_loss: 3.1328 - val_acc: 0.4545\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9955\n",
      "Epoch 00077: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0904 - acc: 0.9955 - val_loss: 3.1429 - val_acc: 0.4556\n",
      "Epoch 78/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9961\n",
      "Epoch 00078: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.0838 - acc: 0.9961 - val_loss: 3.1721 - val_acc: 0.4547\n",
      "Epoch 79/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9964\n",
      "Epoch 00079: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.0820 - acc: 0.9964 - val_loss: 3.2092 - val_acc: 0.4517\n",
      "Epoch 80/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9964\n",
      "Epoch 00080: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0819 - acc: 0.9963 - val_loss: 3.2277 - val_acc: 0.4517\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9963\n",
      "Epoch 00081: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.0803 - acc: 0.9963 - val_loss: 3.2877 - val_acc: 0.4512\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.0777 - acc: 0.9966 - val_loss: 3.2725 - val_acc: 0.4526\n",
      "Epoch 83/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9971\n",
      "Epoch 00083: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.0729 - acc: 0.9971 - val_loss: 3.2841 - val_acc: 0.4580\n",
      "Epoch 84/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9973\n",
      "Epoch 00084: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.0691 - acc: 0.9973 - val_loss: 3.2987 - val_acc: 0.4538\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9969\n",
      "Epoch 00085: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.0706 - acc: 0.9969 - val_loss: 3.3371 - val_acc: 0.4528\n",
      "Epoch 86/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9968\n",
      "Epoch 00086: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.0698 - acc: 0.9968 - val_loss: 3.3566 - val_acc: 0.4554\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9978\n",
      "Epoch 00087: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.0629 - acc: 0.9978 - val_loss: 3.3766 - val_acc: 0.4580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9971\n",
      "Epoch 00088: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.0652 - acc: 0.9971 - val_loss: 3.3913 - val_acc: 0.4510\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9981\n",
      "Epoch 00089: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.0594 - acc: 0.9982 - val_loss: 3.4131 - val_acc: 0.4524\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9977\n",
      "Epoch 00090: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.0596 - acc: 0.9977 - val_loss: 3.4242 - val_acc: 0.4498\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9980\n",
      "Epoch 00091: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.0571 - acc: 0.9980 - val_loss: 3.4458 - val_acc: 0.4486\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9982\n",
      "Epoch 00092: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.0556 - acc: 0.9982 - val_loss: 3.4697 - val_acc: 0.4491\n",
      "Epoch 93/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9980\n",
      "Epoch 00093: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0550 - acc: 0.9980 - val_loss: 3.5178 - val_acc: 0.4524\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9977\n",
      "Epoch 00094: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.0558 - acc: 0.9977 - val_loss: 3.5160 - val_acc: 0.4505\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9983\n",
      "Epoch 00095: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.0494 - acc: 0.9983 - val_loss: 3.5432 - val_acc: 0.4477\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.0514 - acc: 0.9981 - val_loss: 3.5857 - val_acc: 0.4435\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9985\n",
      "Epoch 00097: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0480 - acc: 0.9985 - val_loss: 3.5724 - val_acc: 0.4482\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9981\n",
      "Epoch 00098: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.0479 - acc: 0.9981 - val_loss: 3.5875 - val_acc: 0.4519\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9984\n",
      "Epoch 00099: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 156us/sample - loss: 0.0456 - acc: 0.9984 - val_loss: 3.6453 - val_acc: 0.4482\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9983\n",
      "Epoch 00100: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0462 - acc: 0.9983 - val_loss: 3.6528 - val_acc: 0.4507\n",
      "Epoch 101/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9983\n",
      "Epoch 00101: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 0.0441 - acc: 0.9983 - val_loss: 3.6547 - val_acc: 0.4493\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9978\n",
      "Epoch 00102: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.0480 - acc: 0.9978 - val_loss: 3.7039 - val_acc: 0.4472\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9978\n",
      "Epoch 00103: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.0472 - acc: 0.9978 - val_loss: 3.6867 - val_acc: 0.4477\n",
      "Epoch 104/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9991\n",
      "Epoch 00104: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.0372 - acc: 0.9991 - val_loss: 3.6799 - val_acc: 0.4484\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9981\n",
      "Epoch 00105: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 157us/sample - loss: 0.0426 - acc: 0.9981 - val_loss: 3.7400 - val_acc: 0.4433\n",
      "Epoch 106/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9982\n",
      "Epoch 00106: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.0418 - acc: 0.9982 - val_loss: 3.7261 - val_acc: 0.4489\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9987\n",
      "Epoch 00107: val_loss did not improve from 1.52601\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.0386 - acc: 0.9988 - val_loss: 3.7449 - val_acc: 0.4477\n",
      "\n",
      "1D_CNN_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 180us/sample - loss: 1.6917 - acc: 0.4621\n",
      "Loss: 1.6916703453687865 Accuracy: 0.46209762\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.2949 - acc: 0.2574\n",
      "Epoch 00001: val_loss improved from inf to 1.89528, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/001-1.8953.hdf5\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 2.2939 - acc: 0.2580 - val_loss: 1.8953 - val_acc: 0.3841\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8091 - acc: 0.4097\n",
      "Epoch 00002: val_loss improved from 1.89528 to 1.71637, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/002-1.7164.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.8091 - acc: 0.4097 - val_loss: 1.7164 - val_acc: 0.4475\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6556 - acc: 0.4740\n",
      "Epoch 00003: val_loss improved from 1.71637 to 1.61623, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/003-1.6162.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.6560 - acc: 0.4739 - val_loss: 1.6162 - val_acc: 0.4808\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5494 - acc: 0.5145\n",
      "Epoch 00004: val_loss improved from 1.61623 to 1.55532, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/004-1.5553.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.5494 - acc: 0.5144 - val_loss: 1.5553 - val_acc: 0.5052\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4661 - acc: 0.5466\n",
      "Epoch 00005: val_loss improved from 1.55532 to 1.51046, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/005-1.5105.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.4661 - acc: 0.5467 - val_loss: 1.5105 - val_acc: 0.5211\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4012 - acc: 0.5668\n",
      "Epoch 00006: val_loss improved from 1.51046 to 1.48236, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/006-1.4824.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.4009 - acc: 0.5669 - val_loss: 1.4824 - val_acc: 0.5334\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3435 - acc: 0.5865\n",
      "Epoch 00007: val_loss improved from 1.48236 to 1.46026, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/007-1.4603.hdf5\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.3432 - acc: 0.5866 - val_loss: 1.4603 - val_acc: 0.5418\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2948 - acc: 0.6027\n",
      "Epoch 00008: val_loss improved from 1.46026 to 1.45789, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/008-1.4579.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.2945 - acc: 0.6027 - val_loss: 1.4579 - val_acc: 0.5455\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2500 - acc: 0.6191\n",
      "Epoch 00009: val_loss improved from 1.45789 to 1.42995, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/009-1.4300.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.2499 - acc: 0.6191 - val_loss: 1.4300 - val_acc: 0.5472\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2074 - acc: 0.6344\n",
      "Epoch 00010: val_loss did not improve from 1.42995\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.2072 - acc: 0.6342 - val_loss: 1.4326 - val_acc: 0.5432\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1666 - acc: 0.6473\n",
      "Epoch 00011: val_loss did not improve from 1.42995\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.1661 - acc: 0.6475 - val_loss: 1.4304 - val_acc: 0.5539\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1258 - acc: 0.6591\n",
      "Epoch 00012: val_loss improved from 1.42995 to 1.41405, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/012-1.4140.hdf5\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.1259 - acc: 0.6591 - val_loss: 1.4140 - val_acc: 0.5584\n",
      "Epoch 13/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0895 - acc: 0.6697\n",
      "Epoch 00013: val_loss improved from 1.41405 to 1.40458, saving model to model/checkpoint/1D_CNN_2_only_conv_checkpoint/013-1.4046.hdf5\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.0896 - acc: 0.6694 - val_loss: 1.4046 - val_acc: 0.5530\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.6812\n",
      "Epoch 00014: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.0520 - acc: 0.6812 - val_loss: 1.4183 - val_acc: 0.5551\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0172 - acc: 0.6947\n",
      "Epoch 00015: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.0171 - acc: 0.6948 - val_loss: 1.4126 - val_acc: 0.5670\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9845 - acc: 0.7057\n",
      "Epoch 00016: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9843 - acc: 0.7057 - val_loss: 1.4098 - val_acc: 0.5604\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9550 - acc: 0.7118\n",
      "Epoch 00017: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.9551 - acc: 0.7118 - val_loss: 1.4412 - val_acc: 0.5565\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9218 - acc: 0.7231\n",
      "Epoch 00018: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9219 - acc: 0.7232 - val_loss: 1.4339 - val_acc: 0.5616\n",
      "Epoch 19/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8931 - acc: 0.7335\n",
      "Epoch 00019: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8933 - acc: 0.7334 - val_loss: 1.4165 - val_acc: 0.5614\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7418\n",
      "Epoch 00020: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8678 - acc: 0.7418 - val_loss: 1.4350 - val_acc: 0.5590\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8396 - acc: 0.7493\n",
      "Epoch 00021: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8396 - acc: 0.7493 - val_loss: 1.4228 - val_acc: 0.5597\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8114 - acc: 0.7590\n",
      "Epoch 00022: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.8114 - acc: 0.7591 - val_loss: 1.4493 - val_acc: 0.5588\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7657\n",
      "Epoch 00023: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.7893 - acc: 0.7657 - val_loss: 1.4606 - val_acc: 0.5586\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7756\n",
      "Epoch 00024: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.7624 - acc: 0.7756 - val_loss: 1.4669 - val_acc: 0.5563\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7389 - acc: 0.7821\n",
      "Epoch 00025: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7389 - acc: 0.7820 - val_loss: 1.4941 - val_acc: 0.5549\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.7903\n",
      "Epoch 00026: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7167 - acc: 0.7904 - val_loss: 1.5032 - val_acc: 0.5600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7944\n",
      "Epoch 00027: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.6949 - acc: 0.7944 - val_loss: 1.5014 - val_acc: 0.5570\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6737 - acc: 0.8039\n",
      "Epoch 00028: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.6737 - acc: 0.8039 - val_loss: 1.5480 - val_acc: 0.5521\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6512 - acc: 0.8112\n",
      "Epoch 00029: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.6511 - acc: 0.8112 - val_loss: 1.5546 - val_acc: 0.5558\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.8185\n",
      "Epoch 00030: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.6281 - acc: 0.8184 - val_loss: 1.5409 - val_acc: 0.5574\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.8250\n",
      "Epoch 00031: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.6097 - acc: 0.8251 - val_loss: 1.5550 - val_acc: 0.5567\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5900 - acc: 0.8307\n",
      "Epoch 00032: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5899 - acc: 0.8308 - val_loss: 1.5779 - val_acc: 0.5530\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8357\n",
      "Epoch 00033: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.5717 - acc: 0.8358 - val_loss: 1.5932 - val_acc: 0.5525\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8412\n",
      "Epoch 00034: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5526 - acc: 0.8412 - val_loss: 1.6287 - val_acc: 0.5563\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8508\n",
      "Epoch 00035: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5339 - acc: 0.8507 - val_loss: 1.6592 - val_acc: 0.5490\n",
      "Epoch 36/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.8550\n",
      "Epoch 00036: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5173 - acc: 0.8552 - val_loss: 1.6428 - val_acc: 0.5549\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.8604\n",
      "Epoch 00037: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.4995 - acc: 0.8604 - val_loss: 1.6848 - val_acc: 0.5525\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8666\n",
      "Epoch 00038: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.4828 - acc: 0.8665 - val_loss: 1.6965 - val_acc: 0.5502\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8714\n",
      "Epoch 00039: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.4656 - acc: 0.8713 - val_loss: 1.7121 - val_acc: 0.5539\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8761\n",
      "Epoch 00040: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.4503 - acc: 0.8761 - val_loss: 1.7299 - val_acc: 0.5458\n",
      "Epoch 41/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8833\n",
      "Epoch 00041: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.4339 - acc: 0.8832 - val_loss: 1.7835 - val_acc: 0.5430\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8882\n",
      "Epoch 00042: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.4179 - acc: 0.8882 - val_loss: 1.7778 - val_acc: 0.5474\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8939\n",
      "Epoch 00043: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.4038 - acc: 0.8938 - val_loss: 1.8132 - val_acc: 0.5462\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8961\n",
      "Epoch 00044: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3895 - acc: 0.8960 - val_loss: 1.8340 - val_acc: 0.5460\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.9038\n",
      "Epoch 00045: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.3736 - acc: 0.9036 - val_loss: 1.8486 - val_acc: 0.5453\n",
      "Epoch 46/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.9054\n",
      "Epoch 00046: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.3619 - acc: 0.9053 - val_loss: 1.9213 - val_acc: 0.5372\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.9122\n",
      "Epoch 00047: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.3456 - acc: 0.9122 - val_loss: 1.9047 - val_acc: 0.5409\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.9150\n",
      "Epoch 00048: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.3354 - acc: 0.9150 - val_loss: 1.9560 - val_acc: 0.5390\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.9212\n",
      "Epoch 00049: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.3212 - acc: 0.9211 - val_loss: 1.9641 - val_acc: 0.5409\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.9255\n",
      "Epoch 00050: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.3080 - acc: 0.9256 - val_loss: 1.9796 - val_acc: 0.5425\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9280\n",
      "Epoch 00051: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.2976 - acc: 0.9279 - val_loss: 2.0056 - val_acc: 0.5402\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9296\n",
      "Epoch 00052: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.2880 - acc: 0.9296 - val_loss: 2.0297 - val_acc: 0.5402\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9353\n",
      "Epoch 00053: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.2743 - acc: 0.9353 - val_loss: 2.0581 - val_acc: 0.5420\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9376\n",
      "Epoch 00054: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.2647 - acc: 0.9376 - val_loss: 2.1230 - val_acc: 0.5320\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9411\n",
      "Epoch 00055: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.2544 - acc: 0.9411 - val_loss: 2.1330 - val_acc: 0.5332\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9463\n",
      "Epoch 00056: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.2434 - acc: 0.9463 - val_loss: 2.1425 - val_acc: 0.5365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9496\n",
      "Epoch 00057: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.2338 - acc: 0.9495 - val_loss: 2.1822 - val_acc: 0.5339\n",
      "Epoch 58/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9524\n",
      "Epoch 00058: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.2240 - acc: 0.9523 - val_loss: 2.2169 - val_acc: 0.5330\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9554\n",
      "Epoch 00059: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.2143 - acc: 0.9554 - val_loss: 2.2583 - val_acc: 0.5297\n",
      "Epoch 60/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9574\n",
      "Epoch 00060: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.2055 - acc: 0.9575 - val_loss: 2.2534 - val_acc: 0.5346\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9605\n",
      "Epoch 00061: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.1965 - acc: 0.9605 - val_loss: 2.3022 - val_acc: 0.5337\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9630\n",
      "Epoch 00062: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.1898 - acc: 0.9630 - val_loss: 2.3935 - val_acc: 0.5269\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9655\n",
      "Epoch 00063: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.1810 - acc: 0.9655 - val_loss: 2.3490 - val_acc: 0.5362\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9674\n",
      "Epoch 00064: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.1736 - acc: 0.9673 - val_loss: 2.3765 - val_acc: 0.5311\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9689\n",
      "Epoch 00065: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.1673 - acc: 0.9689 - val_loss: 2.3981 - val_acc: 0.5313\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9707\n",
      "Epoch 00066: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1615 - acc: 0.9707 - val_loss: 2.4752 - val_acc: 0.5229\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9741\n",
      "Epoch 00067: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.1511 - acc: 0.9741 - val_loss: 2.4567 - val_acc: 0.5318\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9743\n",
      "Epoch 00068: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.1477 - acc: 0.9743 - val_loss: 2.5168 - val_acc: 0.5246\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9768\n",
      "Epoch 00069: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.1403 - acc: 0.9768 - val_loss: 2.5049 - val_acc: 0.5327\n",
      "Epoch 70/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9786\n",
      "Epoch 00070: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1334 - acc: 0.9787 - val_loss: 2.5982 - val_acc: 0.5274\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9796\n",
      "Epoch 00071: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.1288 - acc: 0.9795 - val_loss: 2.5784 - val_acc: 0.5253\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9814\n",
      "Epoch 00072: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.1235 - acc: 0.9814 - val_loss: 2.6521 - val_acc: 0.5295\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9842\n",
      "Epoch 00073: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1165 - acc: 0.9842 - val_loss: 2.6446 - val_acc: 0.5325\n",
      "Epoch 74/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9830\n",
      "Epoch 00074: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1139 - acc: 0.9830 - val_loss: 2.6627 - val_acc: 0.5248\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9858\n",
      "Epoch 00075: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1073 - acc: 0.9857 - val_loss: 2.7534 - val_acc: 0.5255\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9848\n",
      "Epoch 00076: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.1062 - acc: 0.9848 - val_loss: 2.7290 - val_acc: 0.5334\n",
      "Epoch 77/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9877\n",
      "Epoch 00077: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0975 - acc: 0.9878 - val_loss: 2.7875 - val_acc: 0.5288\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9881\n",
      "Epoch 00078: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.0953 - acc: 0.9881 - val_loss: 2.7497 - val_acc: 0.5290\n",
      "Epoch 79/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9893\n",
      "Epoch 00079: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0916 - acc: 0.9893 - val_loss: 2.8138 - val_acc: 0.5290\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9896\n",
      "Epoch 00080: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0887 - acc: 0.9896 - val_loss: 2.8707 - val_acc: 0.5206\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9900\n",
      "Epoch 00081: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0864 - acc: 0.9899 - val_loss: 2.8866 - val_acc: 0.5246\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9906\n",
      "Epoch 00082: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0819 - acc: 0.9906 - val_loss: 2.9415 - val_acc: 0.5190\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9926\n",
      "Epoch 00083: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.0768 - acc: 0.9925 - val_loss: 2.8974 - val_acc: 0.5236\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9925\n",
      "Epoch 00084: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0751 - acc: 0.9925 - val_loss: 2.9424 - val_acc: 0.5276\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9933\n",
      "Epoch 00085: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.0695 - acc: 0.9933 - val_loss: 3.0224 - val_acc: 0.5236\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9923\n",
      "Epoch 00086: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0722 - acc: 0.9923 - val_loss: 3.0119 - val_acc: 0.5229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9929\n",
      "Epoch 00087: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.0693 - acc: 0.9928 - val_loss: 3.0555 - val_acc: 0.5264\n",
      "Epoch 88/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9905\n",
      "Epoch 00088: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0743 - acc: 0.9905 - val_loss: 3.0253 - val_acc: 0.5239\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9944\n",
      "Epoch 00089: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0614 - acc: 0.9944 - val_loss: 3.0561 - val_acc: 0.5232\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9945\n",
      "Epoch 00090: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0595 - acc: 0.9945 - val_loss: 3.0793 - val_acc: 0.5255\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9943\n",
      "Epoch 00091: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0601 - acc: 0.9943 - val_loss: 3.1573 - val_acc: 0.5195\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9944\n",
      "Epoch 00092: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.0583 - acc: 0.9944 - val_loss: 3.1666 - val_acc: 0.5206\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9961\n",
      "Epoch 00093: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0528 - acc: 0.9961 - val_loss: 3.1624 - val_acc: 0.5239\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9952\n",
      "Epoch 00094: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0541 - acc: 0.9952 - val_loss: 3.1714 - val_acc: 0.5276\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9967\n",
      "Epoch 00095: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0481 - acc: 0.9967 - val_loss: 3.1768 - val_acc: 0.5239\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9940\n",
      "Epoch 00096: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.0544 - acc: 0.9939 - val_loss: 3.3134 - val_acc: 0.5225\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9929\n",
      "Epoch 00097: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0592 - acc: 0.9929 - val_loss: 3.2066 - val_acc: 0.5183\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9968\n",
      "Epoch 00098: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0446 - acc: 0.9968 - val_loss: 3.2373 - val_acc: 0.5183\n",
      "Epoch 99/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9976\n",
      "Epoch 00099: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0423 - acc: 0.9976 - val_loss: 3.2578 - val_acc: 0.5167\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9973\n",
      "Epoch 00100: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0413 - acc: 0.9973 - val_loss: 3.3429 - val_acc: 0.5250\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9932\n",
      "Epoch 00101: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0549 - acc: 0.9933 - val_loss: 3.2868 - val_acc: 0.5185\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9972\n",
      "Epoch 00102: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0393 - acc: 0.9972 - val_loss: 3.3376 - val_acc: 0.5201\n",
      "Epoch 103/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9954\n",
      "Epoch 00103: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0455 - acc: 0.9954 - val_loss: 3.4019 - val_acc: 0.5153\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9964\n",
      "Epoch 00104: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0433 - acc: 0.9964 - val_loss: 3.3796 - val_acc: 0.5178\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9966\n",
      "Epoch 00105: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.0400 - acc: 0.9966 - val_loss: 3.4124 - val_acc: 0.5211\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9967\n",
      "Epoch 00106: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0400 - acc: 0.9967 - val_loss: 3.3967 - val_acc: 0.5185\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9981\n",
      "Epoch 00107: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0328 - acc: 0.9981 - val_loss: 3.4057 - val_acc: 0.5241\n",
      "Epoch 108/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9980\n",
      "Epoch 00108: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.0322 - acc: 0.9979 - val_loss: 3.4530 - val_acc: 0.5236\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9949\n",
      "Epoch 00109: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0446 - acc: 0.9949 - val_loss: 3.5247 - val_acc: 0.5225\n",
      "Epoch 110/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9961\n",
      "Epoch 00110: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.0392 - acc: 0.9961 - val_loss: 3.5103 - val_acc: 0.5185\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9976\n",
      "Epoch 00111: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.0337 - acc: 0.9976 - val_loss: 3.4497 - val_acc: 0.5199\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9967\n",
      "Epoch 00112: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.0355 - acc: 0.9967 - val_loss: 3.6005 - val_acc: 0.5195\n",
      "Epoch 113/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9950\n",
      "Epoch 00113: val_loss did not improve from 1.40458\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.0418 - acc: 0.9951 - val_loss: 3.4845 - val_acc: 0.5155\n",
      "\n",
      "1D_CNN_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 218us/sample - loss: 1.4719 - acc: 0.5321\n",
      "Loss: 1.4719076889202478 Accuracy: 0.5320872\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.2514 - acc: 0.2573\n",
      "Epoch 00001: val_loss improved from inf to 1.77898, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/001-1.7790.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 2.2495 - acc: 0.2580 - val_loss: 1.7790 - val_acc: 0.4216\n",
      "Epoch 2/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7404 - acc: 0.4398\n",
      "Epoch 00002: val_loss improved from 1.77898 to 1.60094, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/002-1.6009.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 1.7400 - acc: 0.4402 - val_loss: 1.6009 - val_acc: 0.4896\n",
      "Epoch 3/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.6063 - acc: 0.4938\n",
      "Epoch 00003: val_loss improved from 1.60094 to 1.48382, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/003-1.4838.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 1.6056 - acc: 0.4939 - val_loss: 1.4838 - val_acc: 0.5397\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4796 - acc: 0.5452\n",
      "Epoch 00004: val_loss improved from 1.48382 to 1.36529, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/004-1.3653.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 1.4797 - acc: 0.5451 - val_loss: 1.3653 - val_acc: 0.5889\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3669 - acc: 0.5826\n",
      "Epoch 00005: val_loss improved from 1.36529 to 1.30386, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/005-1.3039.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 1.3667 - acc: 0.5826 - val_loss: 1.3039 - val_acc: 0.6215\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2748 - acc: 0.6163\n",
      "Epoch 00006: val_loss improved from 1.30386 to 1.20766, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/006-1.2077.hdf5\n",
      "36805/36805 [==============================] - 10s 258us/sample - loss: 1.2744 - acc: 0.6163 - val_loss: 1.2077 - val_acc: 0.6364\n",
      "Epoch 7/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1982 - acc: 0.6440\n",
      "Epoch 00007: val_loss improved from 1.20766 to 1.13648, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/007-1.1365.hdf5\n",
      "36805/36805 [==============================] - 10s 258us/sample - loss: 1.1980 - acc: 0.6440 - val_loss: 1.1365 - val_acc: 0.6685\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1320 - acc: 0.6658\n",
      "Epoch 00008: val_loss improved from 1.13648 to 1.09643, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/008-1.0964.hdf5\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 1.1314 - acc: 0.6659 - val_loss: 1.0964 - val_acc: 0.6811\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0758 - acc: 0.6806\n",
      "Epoch 00009: val_loss improved from 1.09643 to 1.06074, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/009-1.0607.hdf5\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 1.0760 - acc: 0.6806 - val_loss: 1.0607 - val_acc: 0.6825\n",
      "Epoch 10/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0270 - acc: 0.6967\n",
      "Epoch 00010: val_loss improved from 1.06074 to 1.01333, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/010-1.0133.hdf5\n",
      "36805/36805 [==============================] - 10s 258us/sample - loss: 1.0271 - acc: 0.6968 - val_loss: 1.0133 - val_acc: 0.7091\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9769 - acc: 0.7136\n",
      "Epoch 00011: val_loss improved from 1.01333 to 0.97693, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/011-0.9769.hdf5\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.9769 - acc: 0.7137 - val_loss: 0.9769 - val_acc: 0.7063\n",
      "Epoch 12/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.7267\n",
      "Epoch 00012: val_loss improved from 0.97693 to 0.94188, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/012-0.9419.hdf5\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.9353 - acc: 0.7266 - val_loss: 0.9419 - val_acc: 0.7268\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8968 - acc: 0.7377\n",
      "Epoch 00013: val_loss improved from 0.94188 to 0.93872, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/013-0.9387.hdf5\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.8968 - acc: 0.7378 - val_loss: 0.9387 - val_acc: 0.7296\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8597 - acc: 0.7497\n",
      "Epoch 00014: val_loss improved from 0.93872 to 0.89652, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/014-0.8965.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.8598 - acc: 0.7497 - val_loss: 0.8965 - val_acc: 0.7459\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8250 - acc: 0.7608\n",
      "Epoch 00015: val_loss improved from 0.89652 to 0.87413, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/015-0.8741.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.8250 - acc: 0.7608 - val_loss: 0.8741 - val_acc: 0.7496\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.7708\n",
      "Epoch 00016: val_loss improved from 0.87413 to 0.86186, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/016-0.8619.hdf5\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.7909 - acc: 0.7708 - val_loss: 0.8619 - val_acc: 0.7531\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7803\n",
      "Epoch 00017: val_loss did not improve from 0.86186\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.7605 - acc: 0.7804 - val_loss: 0.8661 - val_acc: 0.7449\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.7885\n",
      "Epoch 00018: val_loss improved from 0.86186 to 0.82032, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/018-0.8203.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.7318 - acc: 0.7885 - val_loss: 0.8203 - val_acc: 0.7603\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7037 - acc: 0.7972\n",
      "Epoch 00019: val_loss improved from 0.82032 to 0.81247, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/019-0.8125.hdf5\n",
      "36805/36805 [==============================] - 9s 258us/sample - loss: 0.7038 - acc: 0.7971 - val_loss: 0.8125 - val_acc: 0.7619\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.8076\n",
      "Epoch 00020: val_loss improved from 0.81247 to 0.79586, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/020-0.7959.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.6769 - acc: 0.8075 - val_loss: 0.7959 - val_acc: 0.7699\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.8139\n",
      "Epoch 00021: val_loss improved from 0.79586 to 0.79490, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/021-0.7949.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.6534 - acc: 0.8139 - val_loss: 0.7949 - val_acc: 0.7754\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.8188\n",
      "Epoch 00022: val_loss improved from 0.79490 to 0.77027, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/022-0.7703.hdf5\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.6296 - acc: 0.8188 - val_loss: 0.7703 - val_acc: 0.7813\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.8268\n",
      "Epoch 00023: val_loss improved from 0.77027 to 0.76728, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/023-0.7673.hdf5\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.6054 - acc: 0.8267 - val_loss: 0.7673 - val_acc: 0.7834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.8300\n",
      "Epoch 00024: val_loss improved from 0.76728 to 0.74663, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/024-0.7466.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5875 - acc: 0.8302 - val_loss: 0.7466 - val_acc: 0.7885\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8387\n",
      "Epoch 00025: val_loss improved from 0.74663 to 0.73769, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/025-0.7377.hdf5\n",
      "36805/36805 [==============================] - 10s 258us/sample - loss: 0.5648 - acc: 0.8388 - val_loss: 0.7377 - val_acc: 0.7899\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8418\n",
      "Epoch 00026: val_loss did not improve from 0.73769\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.5490 - acc: 0.8418 - val_loss: 0.7508 - val_acc: 0.7936\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8475\n",
      "Epoch 00027: val_loss improved from 0.73769 to 0.73573, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/027-0.7357.hdf5\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.5295 - acc: 0.8475 - val_loss: 0.7357 - val_acc: 0.7864\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8526\n",
      "Epoch 00028: val_loss did not improve from 0.73573\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.5116 - acc: 0.8526 - val_loss: 0.7373 - val_acc: 0.7929\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8581\n",
      "Epoch 00029: val_loss improved from 0.73573 to 0.72911, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/029-0.7291.hdf5\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.4944 - acc: 0.8581 - val_loss: 0.7291 - val_acc: 0.7927\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.8625\n",
      "Epoch 00030: val_loss did not improve from 0.72911\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.4772 - acc: 0.8623 - val_loss: 0.7304 - val_acc: 0.7964\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8655\n",
      "Epoch 00031: val_loss improved from 0.72911 to 0.70991, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/031-0.7099.hdf5\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.4669 - acc: 0.8655 - val_loss: 0.7099 - val_acc: 0.8102\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8710\n",
      "Epoch 00032: val_loss did not improve from 0.70991\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.4490 - acc: 0.8709 - val_loss: 0.7406 - val_acc: 0.7941\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8738\n",
      "Epoch 00033: val_loss did not improve from 0.70991\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.4369 - acc: 0.8737 - val_loss: 0.7168 - val_acc: 0.8036\n",
      "Epoch 34/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8788\n",
      "Epoch 00034: val_loss improved from 0.70991 to 0.70691, saving model to model/checkpoint/1D_CNN_3_only_conv_checkpoint/034-0.7069.hdf5\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.4206 - acc: 0.8787 - val_loss: 0.7069 - val_acc: 0.8008\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8839\n",
      "Epoch 00035: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.4064 - acc: 0.8839 - val_loss: 0.7340 - val_acc: 0.7971\n",
      "Epoch 36/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8873\n",
      "Epoch 00036: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.3931 - acc: 0.8874 - val_loss: 0.7142 - val_acc: 0.8046\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8912\n",
      "Epoch 00037: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.3798 - acc: 0.8912 - val_loss: 0.7320 - val_acc: 0.7987\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8949\n",
      "Epoch 00038: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.3681 - acc: 0.8949 - val_loss: 0.7331 - val_acc: 0.8043\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8979\n",
      "Epoch 00039: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.3584 - acc: 0.8980 - val_loss: 0.7315 - val_acc: 0.8057\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.9033\n",
      "Epoch 00040: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3422 - acc: 0.9034 - val_loss: 0.7273 - val_acc: 0.8053\n",
      "Epoch 41/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.9082\n",
      "Epoch 00041: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3314 - acc: 0.9081 - val_loss: 0.7379 - val_acc: 0.8036\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.9098\n",
      "Epoch 00042: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3200 - acc: 0.9099 - val_loss: 0.7343 - val_acc: 0.8078\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9145\n",
      "Epoch 00043: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3063 - acc: 0.9146 - val_loss: 0.7478 - val_acc: 0.8020\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.9187\n",
      "Epoch 00044: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.2987 - acc: 0.9186 - val_loss: 0.7445 - val_acc: 0.8057\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9210\n",
      "Epoch 00045: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.2862 - acc: 0.9212 - val_loss: 0.7688 - val_acc: 0.7987\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9249\n",
      "Epoch 00046: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2751 - acc: 0.9250 - val_loss: 0.7545 - val_acc: 0.8046\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9278\n",
      "Epoch 00047: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.2646 - acc: 0.9278 - val_loss: 0.7956 - val_acc: 0.7939\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9313\n",
      "Epoch 00048: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.2550 - acc: 0.9314 - val_loss: 0.7876 - val_acc: 0.7966\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9347\n",
      "Epoch 00049: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.2430 - acc: 0.9347 - val_loss: 0.7877 - val_acc: 0.8041\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9373\n",
      "Epoch 00050: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.2357 - acc: 0.9372 - val_loss: 0.8139 - val_acc: 0.7971\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9403\n",
      "Epoch 00051: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.2253 - acc: 0.9403 - val_loss: 0.7936 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9431\n",
      "Epoch 00052: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.2156 - acc: 0.9431 - val_loss: 0.8366 - val_acc: 0.7955\n",
      "Epoch 53/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9462\n",
      "Epoch 00053: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.2074 - acc: 0.9461 - val_loss: 0.8218 - val_acc: 0.7973\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9518\n",
      "Epoch 00054: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.1945 - acc: 0.9517 - val_loss: 0.8422 - val_acc: 0.7973\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9518\n",
      "Epoch 00055: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.1881 - acc: 0.9518 - val_loss: 0.8560 - val_acc: 0.7980\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9547\n",
      "Epoch 00056: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.1812 - acc: 0.9547 - val_loss: 0.8757 - val_acc: 0.7983\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9580\n",
      "Epoch 00057: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.1726 - acc: 0.9579 - val_loss: 0.8544 - val_acc: 0.8001\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9614\n",
      "Epoch 00058: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.1631 - acc: 0.9613 - val_loss: 0.8779 - val_acc: 0.7973\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9627\n",
      "Epoch 00059: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.1558 - acc: 0.9627 - val_loss: 0.9139 - val_acc: 0.7936\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9664\n",
      "Epoch 00060: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.1481 - acc: 0.9663 - val_loss: 0.9096 - val_acc: 0.7941\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9667\n",
      "Epoch 00061: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.1419 - acc: 0.9667 - val_loss: 0.9220 - val_acc: 0.7983\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9712\n",
      "Epoch 00062: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.1318 - acc: 0.9712 - val_loss: 0.9724 - val_acc: 0.7883\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9726\n",
      "Epoch 00063: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.1260 - acc: 0.9725 - val_loss: 0.9990 - val_acc: 0.7785\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9726\n",
      "Epoch 00064: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.1256 - acc: 0.9727 - val_loss: 0.9817 - val_acc: 0.7915\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9758\n",
      "Epoch 00065: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.1135 - acc: 0.9758 - val_loss: 0.9930 - val_acc: 0.7901\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9783\n",
      "Epoch 00066: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.1083 - acc: 0.9783 - val_loss: 1.0000 - val_acc: 0.7934\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9786\n",
      "Epoch 00067: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.1045 - acc: 0.9786 - val_loss: 1.0154 - val_acc: 0.7913\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9815\n",
      "Epoch 00068: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.0972 - acc: 0.9815 - val_loss: 1.0067 - val_acc: 0.7943\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9835\n",
      "Epoch 00069: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0918 - acc: 0.9835 - val_loss: 1.0513 - val_acc: 0.7880\n",
      "Epoch 70/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9799\n",
      "Epoch 00070: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.0957 - acc: 0.9800 - val_loss: 1.0616 - val_acc: 0.7852\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9855\n",
      "Epoch 00071: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0821 - acc: 0.9855 - val_loss: 1.0586 - val_acc: 0.7915\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9852\n",
      "Epoch 00072: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0806 - acc: 0.9852 - val_loss: 1.0743 - val_acc: 0.7911\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9879\n",
      "Epoch 00073: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.0730 - acc: 0.9880 - val_loss: 1.1140 - val_acc: 0.7892\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9885\n",
      "Epoch 00074: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0711 - acc: 0.9885 - val_loss: 1.1402 - val_acc: 0.7873\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9890\n",
      "Epoch 00075: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.0685 - acc: 0.9890 - val_loss: 1.1513 - val_acc: 0.7859\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9905\n",
      "Epoch 00076: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0641 - acc: 0.9905 - val_loss: 1.1557 - val_acc: 0.7824\n",
      "Epoch 77/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9901\n",
      "Epoch 00077: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0624 - acc: 0.9902 - val_loss: 1.1859 - val_acc: 0.7857\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9907\n",
      "Epoch 00078: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0600 - acc: 0.9907 - val_loss: 1.1799 - val_acc: 0.7857\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9918\n",
      "Epoch 00079: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0545 - acc: 0.9918 - val_loss: 1.2251 - val_acc: 0.7829\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9930\n",
      "Epoch 00080: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.0533 - acc: 0.9930 - val_loss: 1.2481 - val_acc: 0.7794\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9925\n",
      "Epoch 00081: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.0515 - acc: 0.9924 - val_loss: 1.3272 - val_acc: 0.7657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9938\n",
      "Epoch 00082: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.0501 - acc: 0.9938 - val_loss: 1.2272 - val_acc: 0.7873\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9959\n",
      "Epoch 00083: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.0414 - acc: 0.9959 - val_loss: 1.2595 - val_acc: 0.7820\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9945\n",
      "Epoch 00084: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0438 - acc: 0.9945 - val_loss: 1.2874 - val_acc: 0.7834\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9928\n",
      "Epoch 00085: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0497 - acc: 0.9928 - val_loss: 1.2859 - val_acc: 0.7824\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9971\n",
      "Epoch 00086: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.0361 - acc: 0.9971 - val_loss: 1.3000 - val_acc: 0.7852\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9972\n",
      "Epoch 00087: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.0347 - acc: 0.9971 - val_loss: 1.3234 - val_acc: 0.7838\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9947\n",
      "Epoch 00088: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0412 - acc: 0.9946 - val_loss: 1.3995 - val_acc: 0.7708\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9917\n",
      "Epoch 00089: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.0494 - acc: 0.9917 - val_loss: 1.3220 - val_acc: 0.7813\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9976\n",
      "Epoch 00090: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.0304 - acc: 0.9976 - val_loss: 1.3301 - val_acc: 0.7866\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9947\n",
      "Epoch 00091: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.0391 - acc: 0.9948 - val_loss: 1.3298 - val_acc: 0.7836\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9946\n",
      "Epoch 00092: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 258us/sample - loss: 0.0389 - acc: 0.9945 - val_loss: 1.4732 - val_acc: 0.7673\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9955\n",
      "Epoch 00093: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.0355 - acc: 0.9955 - val_loss: 1.3772 - val_acc: 0.7822\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9986\n",
      "Epoch 00094: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.0265 - acc: 0.9986 - val_loss: 1.3762 - val_acc: 0.7801\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9974\n",
      "Epoch 00095: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0284 - acc: 0.9973 - val_loss: 1.4731 - val_acc: 0.7741\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9941\n",
      "Epoch 00096: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0401 - acc: 0.9942 - val_loss: 1.4027 - val_acc: 0.7855\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9970\n",
      "Epoch 00097: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.0299 - acc: 0.9970 - val_loss: 1.4021 - val_acc: 0.7843\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9980\n",
      "Epoch 00098: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.0264 - acc: 0.9980 - val_loss: 1.4442 - val_acc: 0.7841\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9978\n",
      "Epoch 00099: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.0261 - acc: 0.9978 - val_loss: 1.5572 - val_acc: 0.7692\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9947\n",
      "Epoch 00100: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0356 - acc: 0.9947 - val_loss: 1.4534 - val_acc: 0.7789\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9988\n",
      "Epoch 00101: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0226 - acc: 0.9988 - val_loss: 1.4520 - val_acc: 0.7794\n",
      "Epoch 102/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9940\n",
      "Epoch 00102: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0359 - acc: 0.9940 - val_loss: 1.5128 - val_acc: 0.7754\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9968\n",
      "Epoch 00103: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.0289 - acc: 0.9968 - val_loss: 1.4610 - val_acc: 0.7815\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9989\n",
      "Epoch 00104: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.0209 - acc: 0.9989 - val_loss: 1.4728 - val_acc: 0.7820\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9934\n",
      "Epoch 00105: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.0373 - acc: 0.9934 - val_loss: 1.5954 - val_acc: 0.7713\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9971\n",
      "Epoch 00106: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.0270 - acc: 0.9971 - val_loss: 1.4887 - val_acc: 0.7801\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9987\n",
      "Epoch 00107: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.0208 - acc: 0.9987 - val_loss: 1.4970 - val_acc: 0.7810\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9989\n",
      "Epoch 00108: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0199 - acc: 0.9989 - val_loss: 1.5097 - val_acc: 0.7810\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9965\n",
      "Epoch 00109: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.0268 - acc: 0.9964 - val_loss: 1.6994 - val_acc: 0.7659\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9940\n",
      "Epoch 00110: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.0355 - acc: 0.9940 - val_loss: 1.4930 - val_acc: 0.7838\n",
      "Epoch 111/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9989\n",
      "Epoch 00111: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0193 - acc: 0.9989 - val_loss: 1.5226 - val_acc: 0.7789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9990\n",
      "Epoch 00112: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.0186 - acc: 0.9990 - val_loss: 1.5417 - val_acc: 0.7820\n",
      "Epoch 113/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9981\n",
      "Epoch 00113: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0219 - acc: 0.9979 - val_loss: 1.7016 - val_acc: 0.7680\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9928\n",
      "Epoch 00114: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0383 - acc: 0.9928 - val_loss: 1.5472 - val_acc: 0.7834\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9986\n",
      "Epoch 00115: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.0199 - acc: 0.9986 - val_loss: 1.5478 - val_acc: 0.7794\n",
      "Epoch 116/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9989\n",
      "Epoch 00116: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.0183 - acc: 0.9989 - val_loss: 1.5530 - val_acc: 0.7789\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9919\n",
      "Epoch 00117: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.0391 - acc: 0.9919 - val_loss: 1.6047 - val_acc: 0.7764\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9989\n",
      "Epoch 00118: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0189 - acc: 0.9989 - val_loss: 1.5602 - val_acc: 0.7838\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9989\n",
      "Epoch 00119: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.0182 - acc: 0.9989 - val_loss: 1.5592 - val_acc: 0.7810\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9959\n",
      "Epoch 00120: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.0266 - acc: 0.9959 - val_loss: 1.7504 - val_acc: 0.7587\n",
      "Epoch 121/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9920\n",
      "Epoch 00121: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.0410 - acc: 0.9921 - val_loss: 1.5774 - val_acc: 0.7796\n",
      "Epoch 122/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9990\n",
      "Epoch 00122: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.0181 - acc: 0.9990 - val_loss: 1.5760 - val_acc: 0.7782\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9990\n",
      "Epoch 00123: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0173 - acc: 0.9990 - val_loss: 1.5877 - val_acc: 0.7806\n",
      "Epoch 124/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9944\n",
      "Epoch 00124: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.0315 - acc: 0.9945 - val_loss: 1.5817 - val_acc: 0.7766\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9990\n",
      "Epoch 00125: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 257us/sample - loss: 0.0180 - acc: 0.9990 - val_loss: 1.5935 - val_acc: 0.7782\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9958\n",
      "Epoch 00126: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.0262 - acc: 0.9958 - val_loss: 1.7833 - val_acc: 0.7568\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9973\n",
      "Epoch 00127: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.0240 - acc: 0.9973 - val_loss: 1.6020 - val_acc: 0.7794\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9990\n",
      "Epoch 00128: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.0170 - acc: 0.9990 - val_loss: 1.5951 - val_acc: 0.7796\n",
      "Epoch 129/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9990\n",
      "Epoch 00129: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.0164 - acc: 0.9990 - val_loss: 1.6137 - val_acc: 0.7810\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9990\n",
      "Epoch 00130: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.0163 - acc: 0.9990 - val_loss: 1.6657 - val_acc: 0.7801\n",
      "Epoch 131/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9903\n",
      "Epoch 00131: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.0473 - acc: 0.9904 - val_loss: 1.5919 - val_acc: 0.7806\n",
      "Epoch 132/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9990\n",
      "Epoch 00132: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.0171 - acc: 0.9990 - val_loss: 1.6099 - val_acc: 0.7801\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9990\n",
      "Epoch 00133: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.0162 - acc: 0.9990 - val_loss: 1.6139 - val_acc: 0.7827\n",
      "Epoch 134/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9990\n",
      "Epoch 00134: val_loss did not improve from 0.70691\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0162 - acc: 0.9990 - val_loss: 1.6197 - val_acc: 0.7808\n",
      "\n",
      "1D_CNN_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 228us/sample - loss: 0.7853 - acc: 0.7747\n",
      "Loss: 0.7853394545622456 Accuracy: 0.7746625\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2511 - acc: 0.2601\n",
      "Epoch 00001: val_loss improved from inf to 1.82843, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/001-1.8284.hdf5\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 2.2511 - acc: 0.2600 - val_loss: 1.8284 - val_acc: 0.4316\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5902 - acc: 0.5104\n",
      "Epoch 00002: val_loss improved from 1.82843 to 1.36279, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/002-1.3628.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 1.5892 - acc: 0.5108 - val_loss: 1.3628 - val_acc: 0.5870\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2272 - acc: 0.6312\n",
      "Epoch 00003: val_loss improved from 1.36279 to 1.10465, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/003-1.1046.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 1.2270 - acc: 0.6312 - val_loss: 1.1046 - val_acc: 0.6760\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0413 - acc: 0.6899\n",
      "Epoch 00004: val_loss improved from 1.10465 to 0.98400, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/004-0.9840.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 1.0413 - acc: 0.6899 - val_loss: 0.9840 - val_acc: 0.7070\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9318 - acc: 0.7218\n",
      "Epoch 00005: val_loss improved from 0.98400 to 0.88428, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/005-0.8843.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.9316 - acc: 0.7218 - val_loss: 0.8843 - val_acc: 0.7410\n",
      "Epoch 6/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8412 - acc: 0.7483\n",
      "Epoch 00006: val_loss improved from 0.88428 to 0.80639, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/006-0.8064.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.8402 - acc: 0.7486 - val_loss: 0.8064 - val_acc: 0.7647\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7732\n",
      "Epoch 00007: val_loss improved from 0.80639 to 0.74375, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/007-0.7438.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.7597 - acc: 0.7731 - val_loss: 0.7438 - val_acc: 0.7855\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.7947\n",
      "Epoch 00008: val_loss improved from 0.74375 to 0.69335, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/008-0.6934.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.6930 - acc: 0.7947 - val_loss: 0.6934 - val_acc: 0.8004\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6419 - acc: 0.8074\n",
      "Epoch 00009: val_loss improved from 0.69335 to 0.65950, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/009-0.6595.hdf5\n",
      "36805/36805 [==============================] - 10s 275us/sample - loss: 0.6418 - acc: 0.8075 - val_loss: 0.6595 - val_acc: 0.8095\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8231\n",
      "Epoch 00010: val_loss improved from 0.65950 to 0.62321, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/010-0.6232.hdf5\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.5926 - acc: 0.8234 - val_loss: 0.6232 - val_acc: 0.8288\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.8364\n",
      "Epoch 00011: val_loss improved from 0.62321 to 0.59804, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/011-0.5980.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.5525 - acc: 0.8363 - val_loss: 0.5980 - val_acc: 0.8337\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8426\n",
      "Epoch 00012: val_loss improved from 0.59804 to 0.55059, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/012-0.5506.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.5235 - acc: 0.8427 - val_loss: 0.5506 - val_acc: 0.8458\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8529\n",
      "Epoch 00013: val_loss improved from 0.55059 to 0.52954, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/013-0.5295.hdf5\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.4910 - acc: 0.8529 - val_loss: 0.5295 - val_acc: 0.8542\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.8612\n",
      "Epoch 00014: val_loss improved from 0.52954 to 0.49916, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/014-0.4992.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.4621 - acc: 0.8612 - val_loss: 0.4992 - val_acc: 0.8644\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8683\n",
      "Epoch 00015: val_loss improved from 0.49916 to 0.49286, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/015-0.4929.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.4395 - acc: 0.8684 - val_loss: 0.4929 - val_acc: 0.8635\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8736\n",
      "Epoch 00016: val_loss improved from 0.49286 to 0.47648, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/016-0.4765.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.4164 - acc: 0.8736 - val_loss: 0.4765 - val_acc: 0.8744\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8812\n",
      "Epoch 00017: val_loss improved from 0.47648 to 0.46469, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/017-0.4647.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.3938 - acc: 0.8812 - val_loss: 0.4647 - val_acc: 0.8705\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8851\n",
      "Epoch 00018: val_loss improved from 0.46469 to 0.45135, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/018-0.4513.hdf5\n",
      "36805/36805 [==============================] - 10s 275us/sample - loss: 0.3768 - acc: 0.8850 - val_loss: 0.4513 - val_acc: 0.8772\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8894\n",
      "Epoch 00019: val_loss improved from 0.45135 to 0.43548, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/019-0.4355.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.3633 - acc: 0.8894 - val_loss: 0.4355 - val_acc: 0.8812\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8958\n",
      "Epoch 00020: val_loss improved from 0.43548 to 0.41578, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/020-0.4158.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.3426 - acc: 0.8957 - val_loss: 0.4158 - val_acc: 0.8854\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.8991\n",
      "Epoch 00021: val_loss did not improve from 0.41578\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.3304 - acc: 0.8991 - val_loss: 0.4333 - val_acc: 0.8838\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9017\n",
      "Epoch 00022: val_loss improved from 0.41578 to 0.41147, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/022-0.4115.hdf5\n",
      "36805/36805 [==============================] - 10s 275us/sample - loss: 0.3197 - acc: 0.9018 - val_loss: 0.4115 - val_acc: 0.8852\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9079\n",
      "Epoch 00023: val_loss improved from 0.41147 to 0.39624, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/023-0.3962.hdf5\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.3015 - acc: 0.9080 - val_loss: 0.3962 - val_acc: 0.8926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9098\n",
      "Epoch 00024: val_loss did not improve from 0.39624\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.2926 - acc: 0.9098 - val_loss: 0.3969 - val_acc: 0.8938\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9120\n",
      "Epoch 00025: val_loss improved from 0.39624 to 0.38949, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/025-0.3895.hdf5\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.2833 - acc: 0.9120 - val_loss: 0.3895 - val_acc: 0.8945\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9167\n",
      "Epoch 00026: val_loss did not improve from 0.38949\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.2709 - acc: 0.9167 - val_loss: 0.4039 - val_acc: 0.8926\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9194\n",
      "Epoch 00027: val_loss did not improve from 0.38949\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.2609 - acc: 0.9194 - val_loss: 0.3954 - val_acc: 0.8921\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9223\n",
      "Epoch 00028: val_loss improved from 0.38949 to 0.37401, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/028-0.3740.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.2521 - acc: 0.9223 - val_loss: 0.3740 - val_acc: 0.8996\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9243\n",
      "Epoch 00029: val_loss did not improve from 0.37401\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.2446 - acc: 0.9243 - val_loss: 0.3792 - val_acc: 0.8952\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9284\n",
      "Epoch 00030: val_loss did not improve from 0.37401\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.2327 - acc: 0.9284 - val_loss: 0.3810 - val_acc: 0.9001\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9310\n",
      "Epoch 00031: val_loss improved from 0.37401 to 0.37137, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/031-0.3714.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.2261 - acc: 0.9307 - val_loss: 0.3714 - val_acc: 0.9036\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9339\n",
      "Epoch 00032: val_loss did not improve from 0.37137\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.2162 - acc: 0.9338 - val_loss: 0.3832 - val_acc: 0.8989\n",
      "Epoch 33/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9332\n",
      "Epoch 00033: val_loss did not improve from 0.37137\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.2108 - acc: 0.9332 - val_loss: 0.3867 - val_acc: 0.8980\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9381\n",
      "Epoch 00034: val_loss did not improve from 0.37137\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.2038 - acc: 0.9381 - val_loss: 0.3764 - val_acc: 0.9005\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9381\n",
      "Epoch 00035: val_loss did not improve from 0.37137\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.1959 - acc: 0.9381 - val_loss: 0.3724 - val_acc: 0.9029\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9419\n",
      "Epoch 00036: val_loss did not improve from 0.37137\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.1889 - acc: 0.9419 - val_loss: 0.3770 - val_acc: 0.8996\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9419\n",
      "Epoch 00037: val_loss improved from 0.37137 to 0.37036, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/037-0.3704.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1843 - acc: 0.9419 - val_loss: 0.3704 - val_acc: 0.9064\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9450\n",
      "Epoch 00038: val_loss did not improve from 0.37036\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1773 - acc: 0.9450 - val_loss: 0.3820 - val_acc: 0.9059\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.37036\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.1700 - acc: 0.9467 - val_loss: 0.3968 - val_acc: 0.8982\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9468\n",
      "Epoch 00040: val_loss improved from 0.37036 to 0.36979, saving model to model/checkpoint/1D_CNN_4_only_conv_checkpoint/040-0.3698.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.1698 - acc: 0.9469 - val_loss: 0.3698 - val_acc: 0.9064\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9511\n",
      "Epoch 00041: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1579 - acc: 0.9511 - val_loss: 0.3796 - val_acc: 0.9052\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9515\n",
      "Epoch 00042: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1563 - acc: 0.9514 - val_loss: 0.3779 - val_acc: 0.9057\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9531\n",
      "Epoch 00043: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1495 - acc: 0.9532 - val_loss: 0.4055 - val_acc: 0.9045\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9543\n",
      "Epoch 00044: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1458 - acc: 0.9543 - val_loss: 0.3962 - val_acc: 0.9024\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9564\n",
      "Epoch 00045: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.1395 - acc: 0.9564 - val_loss: 0.3851 - val_acc: 0.9087\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9574\n",
      "Epoch 00046: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.1345 - acc: 0.9573 - val_loss: 0.4084 - val_acc: 0.9001\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9575\n",
      "Epoch 00047: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.1341 - acc: 0.9576 - val_loss: 0.3852 - val_acc: 0.9064\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9624\n",
      "Epoch 00048: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1247 - acc: 0.9623 - val_loss: 0.3827 - val_acc: 0.9101\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9632\n",
      "Epoch 00049: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1204 - acc: 0.9632 - val_loss: 0.3941 - val_acc: 0.9047\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9637\n",
      "Epoch 00050: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1172 - acc: 0.9637 - val_loss: 0.4339 - val_acc: 0.8998\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9641\n",
      "Epoch 00051: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1160 - acc: 0.9641 - val_loss: 0.3980 - val_acc: 0.9073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9669\n",
      "Epoch 00052: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.1079 - acc: 0.9669 - val_loss: 0.4039 - val_acc: 0.9080\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9666\n",
      "Epoch 00053: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.1074 - acc: 0.9666 - val_loss: 0.4096 - val_acc: 0.9101\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9673\n",
      "Epoch 00054: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1035 - acc: 0.9673 - val_loss: 0.4116 - val_acc: 0.9080\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9694\n",
      "Epoch 00055: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0995 - acc: 0.9695 - val_loss: 0.4130 - val_acc: 0.9085\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9689\n",
      "Epoch 00056: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0986 - acc: 0.9689 - val_loss: 0.4365 - val_acc: 0.9059\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9720\n",
      "Epoch 00057: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.0902 - acc: 0.9719 - val_loss: 0.4351 - val_acc: 0.9031\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9723\n",
      "Epoch 00058: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0895 - acc: 0.9723 - val_loss: 0.4401 - val_acc: 0.9038\n",
      "Epoch 59/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9733\n",
      "Epoch 00059: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.0872 - acc: 0.9733 - val_loss: 0.4533 - val_acc: 0.9017\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9745\n",
      "Epoch 00060: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0839 - acc: 0.9745 - val_loss: 0.4547 - val_acc: 0.9003\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9736\n",
      "Epoch 00061: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0849 - acc: 0.9736 - val_loss: 0.4643 - val_acc: 0.8994\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9756\n",
      "Epoch 00062: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.0787 - acc: 0.9756 - val_loss: 0.4686 - val_acc: 0.9024\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9773\n",
      "Epoch 00063: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0749 - acc: 0.9773 - val_loss: 0.4645 - val_acc: 0.9047\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9785\n",
      "Epoch 00064: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0715 - acc: 0.9786 - val_loss: 0.4559 - val_acc: 0.9054\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9783\n",
      "Epoch 00065: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0714 - acc: 0.9783 - val_loss: 0.4741 - val_acc: 0.9029\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9792\n",
      "Epoch 00066: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0687 - acc: 0.9793 - val_loss: 0.4869 - val_acc: 0.8998\n",
      "Epoch 67/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9805\n",
      "Epoch 00067: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0654 - acc: 0.9805 - val_loss: 0.4843 - val_acc: 0.9047\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9805\n",
      "Epoch 00068: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0634 - acc: 0.9805 - val_loss: 0.4919 - val_acc: 0.9029\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9819\n",
      "Epoch 00069: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0596 - acc: 0.9819 - val_loss: 0.4908 - val_acc: 0.9036\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9818\n",
      "Epoch 00070: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0601 - acc: 0.9819 - val_loss: 0.5110 - val_acc: 0.9089\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9837\n",
      "Epoch 00071: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0562 - acc: 0.9836 - val_loss: 0.5002 - val_acc: 0.9082\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9831\n",
      "Epoch 00072: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.0570 - acc: 0.9832 - val_loss: 0.5528 - val_acc: 0.8987\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9835\n",
      "Epoch 00073: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.0555 - acc: 0.9835 - val_loss: 0.5380 - val_acc: 0.9040\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9846\n",
      "Epoch 00074: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0519 - acc: 0.9845 - val_loss: 0.5219 - val_acc: 0.9054\n",
      "Epoch 75/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9847\n",
      "Epoch 00075: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0530 - acc: 0.9847 - val_loss: 0.5256 - val_acc: 0.9033\n",
      "Epoch 76/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9832\n",
      "Epoch 00076: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0556 - acc: 0.9832 - val_loss: 0.5505 - val_acc: 0.9029\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9880\n",
      "Epoch 00077: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0434 - acc: 0.9880 - val_loss: 0.5635 - val_acc: 0.9040\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9869\n",
      "Epoch 00078: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0461 - acc: 0.9869 - val_loss: 0.5542 - val_acc: 0.8998\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9879\n",
      "Epoch 00079: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0432 - acc: 0.9879 - val_loss: 0.5475 - val_acc: 0.9043\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9871\n",
      "Epoch 00080: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0450 - acc: 0.9871 - val_loss: 0.6564 - val_acc: 0.8903\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9852\n",
      "Epoch 00081: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.0495 - acc: 0.9852 - val_loss: 0.5845 - val_acc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9888\n",
      "Epoch 00082: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0402 - acc: 0.9889 - val_loss: 0.5974 - val_acc: 0.9019\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9883\n",
      "Epoch 00083: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0414 - acc: 0.9882 - val_loss: 0.6274 - val_acc: 0.8987\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9864\n",
      "Epoch 00084: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0457 - acc: 0.9864 - val_loss: 0.5840 - val_acc: 0.9026\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9899\n",
      "Epoch 00085: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0360 - acc: 0.9899 - val_loss: 0.5867 - val_acc: 0.9015\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9893\n",
      "Epoch 00086: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0384 - acc: 0.9893 - val_loss: 0.5836 - val_acc: 0.9057\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9921\n",
      "Epoch 00087: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0310 - acc: 0.9921 - val_loss: 0.6092 - val_acc: 0.8996\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9866\n",
      "Epoch 00088: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0466 - acc: 0.9866 - val_loss: 0.6191 - val_acc: 0.8991\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9918\n",
      "Epoch 00089: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0317 - acc: 0.9917 - val_loss: 0.6095 - val_acc: 0.9003\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9909\n",
      "Epoch 00090: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0353 - acc: 0.9909 - val_loss: 0.6515 - val_acc: 0.8989\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9903\n",
      "Epoch 00091: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0361 - acc: 0.9902 - val_loss: 0.6303 - val_acc: 0.8975\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9918\n",
      "Epoch 00092: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0305 - acc: 0.9918 - val_loss: 0.6247 - val_acc: 0.9001\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9940\n",
      "Epoch 00093: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0261 - acc: 0.9940 - val_loss: 0.6253 - val_acc: 0.8984\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9873\n",
      "Epoch 00094: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0419 - acc: 0.9873 - val_loss: 0.6322 - val_acc: 0.8977\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9919\n",
      "Epoch 00095: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0312 - acc: 0.9919 - val_loss: 0.6275 - val_acc: 0.8994\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9934\n",
      "Epoch 00096: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0274 - acc: 0.9934 - val_loss: 0.6379 - val_acc: 0.9031\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9950\n",
      "Epoch 00097: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0235 - acc: 0.9950 - val_loss: 0.6805 - val_acc: 0.8966\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9881\n",
      "Epoch 00098: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0389 - acc: 0.9881 - val_loss: 0.6660 - val_acc: 0.8991\n",
      "Epoch 99/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9922\n",
      "Epoch 00099: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0295 - acc: 0.9923 - val_loss: 0.6779 - val_acc: 0.8956\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9948\n",
      "Epoch 00100: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0231 - acc: 0.9948 - val_loss: 0.6617 - val_acc: 0.9012\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9893\n",
      "Epoch 00101: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0379 - acc: 0.9893 - val_loss: 0.6620 - val_acc: 0.8996\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9933\n",
      "Epoch 00102: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0258 - acc: 0.9933 - val_loss: 0.7123 - val_acc: 0.8933\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9937\n",
      "Epoch 00103: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.0253 - acc: 0.9937 - val_loss: 0.6586 - val_acc: 0.9008\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9938\n",
      "Epoch 00104: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0241 - acc: 0.9938 - val_loss: 0.6849 - val_acc: 0.8982\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9900\n",
      "Epoch 00105: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0351 - acc: 0.9900 - val_loss: 0.6528 - val_acc: 0.9010\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9962\n",
      "Epoch 00106: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0185 - acc: 0.9963 - val_loss: 0.6616 - val_acc: 0.9017\n",
      "Epoch 107/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9892\n",
      "Epoch 00107: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0355 - acc: 0.9893 - val_loss: 0.6963 - val_acc: 0.8928\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9926\n",
      "Epoch 00108: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0299 - acc: 0.9926 - val_loss: 0.6698 - val_acc: 0.9026\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9962\n",
      "Epoch 00109: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0178 - acc: 0.9962 - val_loss: 0.6628 - val_acc: 0.8987\n",
      "Epoch 110/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9934\n",
      "Epoch 00110: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0257 - acc: 0.9933 - val_loss: 0.6965 - val_acc: 0.9003\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9904\n",
      "Epoch 00111: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0333 - acc: 0.9904 - val_loss: 0.6900 - val_acc: 0.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9966\n",
      "Epoch 00112: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0166 - acc: 0.9965 - val_loss: 0.7214 - val_acc: 0.8975\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9949\n",
      "Epoch 00113: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0218 - acc: 0.9949 - val_loss: 0.7182 - val_acc: 0.8989\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9960\n",
      "Epoch 00114: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0180 - acc: 0.9960 - val_loss: 0.7155 - val_acc: 0.9001\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9914\n",
      "Epoch 00115: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0321 - acc: 0.9914 - val_loss: 0.8306 - val_acc: 0.8896\n",
      "Epoch 116/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9938\n",
      "Epoch 00116: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.0237 - acc: 0.9938 - val_loss: 0.6998 - val_acc: 0.9008\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9982\n",
      "Epoch 00117: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0119 - acc: 0.9982 - val_loss: 0.6918 - val_acc: 0.9033\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9913\n",
      "Epoch 00118: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.0341 - acc: 0.9913 - val_loss: 0.6984 - val_acc: 0.9050\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9946\n",
      "Epoch 00119: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0220 - acc: 0.9946 - val_loss: 0.7084 - val_acc: 0.8994\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9943\n",
      "Epoch 00120: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0217 - acc: 0.9943 - val_loss: 0.7066 - val_acc: 0.8996\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9972\n",
      "Epoch 00121: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0157 - acc: 0.9972 - val_loss: 0.7338 - val_acc: 0.8973\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9927\n",
      "Epoch 00122: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0286 - acc: 0.9927 - val_loss: 0.7346 - val_acc: 0.9003\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9952\n",
      "Epoch 00123: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0191 - acc: 0.9952 - val_loss: 0.7672 - val_acc: 0.9001\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9910\n",
      "Epoch 00124: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0324 - acc: 0.9910 - val_loss: 0.7218 - val_acc: 0.9024\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9967\n",
      "Epoch 00125: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0155 - acc: 0.9967 - val_loss: 0.6911 - val_acc: 0.9043\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9985\n",
      "Epoch 00126: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0104 - acc: 0.9985 - val_loss: 0.7369 - val_acc: 0.9008\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9929\n",
      "Epoch 00127: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0256 - acc: 0.9929 - val_loss: 0.7853 - val_acc: 0.8917\n",
      "Epoch 128/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9963\n",
      "Epoch 00128: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0175 - acc: 0.9963 - val_loss: 0.7206 - val_acc: 0.9022\n",
      "Epoch 129/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9970\n",
      "Epoch 00129: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0145 - acc: 0.9970 - val_loss: 0.7753 - val_acc: 0.8980\n",
      "Epoch 130/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9925\n",
      "Epoch 00130: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0289 - acc: 0.9925 - val_loss: 0.7685 - val_acc: 0.8952\n",
      "Epoch 131/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9949\n",
      "Epoch 00131: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0215 - acc: 0.9949 - val_loss: 0.7215 - val_acc: 0.8998\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9986\n",
      "Epoch 00132: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.0105 - acc: 0.9986 - val_loss: 0.7463 - val_acc: 0.8996\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9934\n",
      "Epoch 00133: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.0238 - acc: 0.9934 - val_loss: 0.7703 - val_acc: 0.8959\n",
      "Epoch 134/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9941\n",
      "Epoch 00134: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.0239 - acc: 0.9941 - val_loss: 0.7538 - val_acc: 0.8984\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9976\n",
      "Epoch 00135: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0131 - acc: 0.9976 - val_loss: 0.7089 - val_acc: 0.9047\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9991\n",
      "Epoch 00136: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0088 - acc: 0.9991 - val_loss: 0.7183 - val_acc: 0.9040\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9953\n",
      "Epoch 00137: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0204 - acc: 0.9953 - val_loss: 0.8201 - val_acc: 0.8949\n",
      "Epoch 138/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9875\n",
      "Epoch 00138: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.0423 - acc: 0.9876 - val_loss: 0.7507 - val_acc: 0.8996\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9973\n",
      "Epoch 00139: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.0130 - acc: 0.9973 - val_loss: 0.7319 - val_acc: 0.9043\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9992\n",
      "Epoch 00140: val_loss did not improve from 0.36979\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.0086 - acc: 0.9992 - val_loss: 0.7177 - val_acc: 0.9050\n",
      "\n",
      "1D_CNN_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 298us/sample - loss: 0.4364 - acc: 0.8866\n",
      "Loss: 0.43644130462674213 Accuracy: 0.88660437\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0045 - acc: 0.3501\n",
      "Epoch 00001: val_loss improved from inf to 1.40957, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/001-1.4096.hdf5\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 2.0033 - acc: 0.3504 - val_loss: 1.4096 - val_acc: 0.5679\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2565 - acc: 0.6091\n",
      "Epoch 00002: val_loss improved from 1.40957 to 1.00324, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/002-1.0032.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 1.2562 - acc: 0.6093 - val_loss: 1.0032 - val_acc: 0.6976\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9800 - acc: 0.6953\n",
      "Epoch 00003: val_loss improved from 1.00324 to 0.84230, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/003-0.8423.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.9799 - acc: 0.6954 - val_loss: 0.8423 - val_acc: 0.7389\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8371 - acc: 0.7406\n",
      "Epoch 00004: val_loss improved from 0.84230 to 0.72691, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/004-0.7269.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.8367 - acc: 0.7406 - val_loss: 0.7269 - val_acc: 0.7768\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7409 - acc: 0.7670\n",
      "Epoch 00005: val_loss improved from 0.72691 to 0.66853, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/005-0.6685.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.7404 - acc: 0.7672 - val_loss: 0.6685 - val_acc: 0.7901\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6601 - acc: 0.7914\n",
      "Epoch 00006: val_loss improved from 0.66853 to 0.58706, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/006-0.5871.hdf5\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.6601 - acc: 0.7913 - val_loss: 0.5871 - val_acc: 0.8225\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.8086\n",
      "Epoch 00007: val_loss did not improve from 0.58706\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.6036 - acc: 0.8086 - val_loss: 0.6001 - val_acc: 0.8150\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.8271\n",
      "Epoch 00008: val_loss improved from 0.58706 to 0.49599, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/008-0.4960.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.5528 - acc: 0.8271 - val_loss: 0.4960 - val_acc: 0.8498\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.8434\n",
      "Epoch 00009: val_loss improved from 0.49599 to 0.46434, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/009-0.4643.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.5030 - acc: 0.8437 - val_loss: 0.4643 - val_acc: 0.8598\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8553\n",
      "Epoch 00010: val_loss improved from 0.46434 to 0.44721, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/010-0.4472.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4649 - acc: 0.8553 - val_loss: 0.4472 - val_acc: 0.8684\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8664\n",
      "Epoch 00011: val_loss improved from 0.44721 to 0.41602, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/011-0.4160.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.4248 - acc: 0.8663 - val_loss: 0.4160 - val_acc: 0.8726\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8763\n",
      "Epoch 00012: val_loss improved from 0.41602 to 0.38485, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/012-0.3848.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3947 - acc: 0.8762 - val_loss: 0.3848 - val_acc: 0.8901\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8856\n",
      "Epoch 00013: val_loss did not improve from 0.38485\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3644 - acc: 0.8856 - val_loss: 0.3889 - val_acc: 0.8852\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8934\n",
      "Epoch 00014: val_loss improved from 0.38485 to 0.34579, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/014-0.3458.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3408 - acc: 0.8934 - val_loss: 0.3458 - val_acc: 0.8975\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9010\n",
      "Epoch 00015: val_loss improved from 0.34579 to 0.32681, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/015-0.3268.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3187 - acc: 0.9010 - val_loss: 0.3268 - val_acc: 0.9057\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9060\n",
      "Epoch 00016: val_loss did not improve from 0.32681\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.2968 - acc: 0.9059 - val_loss: 0.3319 - val_acc: 0.9022\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9122\n",
      "Epoch 00017: val_loss improved from 0.32681 to 0.31752, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/017-0.3175.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2760 - acc: 0.9121 - val_loss: 0.3175 - val_acc: 0.9064\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9165\n",
      "Epoch 00018: val_loss improved from 0.31752 to 0.30685, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/018-0.3069.hdf5\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2640 - acc: 0.9165 - val_loss: 0.3069 - val_acc: 0.9089\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9225\n",
      "Epoch 00019: val_loss did not improve from 0.30685\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.2450 - acc: 0.9225 - val_loss: 0.3210 - val_acc: 0.9099\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9270\n",
      "Epoch 00020: val_loss improved from 0.30685 to 0.29416, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/020-0.2942.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.2311 - acc: 0.9270 - val_loss: 0.2942 - val_acc: 0.9143\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9301\n",
      "Epoch 00021: val_loss improved from 0.29416 to 0.29367, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/021-0.2937.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2193 - acc: 0.9300 - val_loss: 0.2937 - val_acc: 0.9161\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9327\n",
      "Epoch 00022: val_loss improved from 0.29367 to 0.28586, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/022-0.2859.hdf5\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.2091 - acc: 0.9327 - val_loss: 0.2859 - val_acc: 0.9206\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9376\n",
      "Epoch 00023: val_loss did not improve from 0.28586\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1939 - acc: 0.9376 - val_loss: 0.3018 - val_acc: 0.9131\n",
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9406\n",
      "Epoch 00024: val_loss did not improve from 0.28586\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1863 - acc: 0.9407 - val_loss: 0.2886 - val_acc: 0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9445\n",
      "Epoch 00025: val_loss improved from 0.28586 to 0.28203, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/025-0.2820.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1730 - acc: 0.9445 - val_loss: 0.2820 - val_acc: 0.9194\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9468\n",
      "Epoch 00026: val_loss did not improve from 0.28203\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1667 - acc: 0.9468 - val_loss: 0.2868 - val_acc: 0.9203\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9514\n",
      "Epoch 00027: val_loss improved from 0.28203 to 0.27192, saving model to model/checkpoint/1D_CNN_5_only_conv_checkpoint/027-0.2719.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1528 - acc: 0.9514 - val_loss: 0.2719 - val_acc: 0.9255\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9540\n",
      "Epoch 00028: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1458 - acc: 0.9540 - val_loss: 0.3012 - val_acc: 0.9259\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9557\n",
      "Epoch 00029: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1388 - acc: 0.9557 - val_loss: 0.2852 - val_acc: 0.9276\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9569\n",
      "Epoch 00030: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1318 - acc: 0.9568 - val_loss: 0.2812 - val_acc: 0.9243\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9587\n",
      "Epoch 00031: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1261 - acc: 0.9587 - val_loss: 0.3072 - val_acc: 0.9192\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9613\n",
      "Epoch 00032: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1186 - acc: 0.9612 - val_loss: 0.2902 - val_acc: 0.9269\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9641\n",
      "Epoch 00033: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1094 - acc: 0.9641 - val_loss: 0.3188 - val_acc: 0.9189\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9673\n",
      "Epoch 00034: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1032 - acc: 0.9672 - val_loss: 0.2974 - val_acc: 0.9290\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9688\n",
      "Epoch 00035: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.0963 - acc: 0.9688 - val_loss: 0.3117 - val_acc: 0.9238\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9686\n",
      "Epoch 00036: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0949 - acc: 0.9686 - val_loss: 0.3279 - val_acc: 0.9215\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9688\n",
      "Epoch 00037: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0943 - acc: 0.9688 - val_loss: 0.3042 - val_acc: 0.9252\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9738\n",
      "Epoch 00038: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0816 - acc: 0.9738 - val_loss: 0.3269 - val_acc: 0.9245\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9738\n",
      "Epoch 00039: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0816 - acc: 0.9738 - val_loss: 0.3604 - val_acc: 0.9238\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9746\n",
      "Epoch 00040: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0777 - acc: 0.9746 - val_loss: 0.3424 - val_acc: 0.9273\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9791\n",
      "Epoch 00041: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.0672 - acc: 0.9791 - val_loss: 0.3413 - val_acc: 0.9259\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9753\n",
      "Epoch 00042: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0767 - acc: 0.9752 - val_loss: 0.3306 - val_acc: 0.9294\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9791\n",
      "Epoch 00043: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0651 - acc: 0.9792 - val_loss: 0.3594 - val_acc: 0.9273\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9792\n",
      "Epoch 00044: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0644 - acc: 0.9792 - val_loss: 0.3334 - val_acc: 0.9327\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9818\n",
      "Epoch 00045: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0583 - acc: 0.9818 - val_loss: 0.3804 - val_acc: 0.9264\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9804\n",
      "Epoch 00046: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0613 - acc: 0.9804 - val_loss: 0.3667 - val_acc: 0.9301\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9822\n",
      "Epoch 00047: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0564 - acc: 0.9823 - val_loss: 0.3949 - val_acc: 0.9248\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9835\n",
      "Epoch 00048: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0546 - acc: 0.9835 - val_loss: 0.3574 - val_acc: 0.9311\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9842\n",
      "Epoch 00049: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0494 - acc: 0.9842 - val_loss: 0.3718 - val_acc: 0.9301\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9848\n",
      "Epoch 00050: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0505 - acc: 0.9847 - val_loss: 0.3898 - val_acc: 0.9227\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9841\n",
      "Epoch 00051: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0521 - acc: 0.9840 - val_loss: 0.3805 - val_acc: 0.9255\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9877\n",
      "Epoch 00052: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0405 - acc: 0.9877 - val_loss: 0.4063 - val_acc: 0.9283\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9871\n",
      "Epoch 00053: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0438 - acc: 0.9871 - val_loss: 0.4066 - val_acc: 0.9278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9856\n",
      "Epoch 00054: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.0459 - acc: 0.9856 - val_loss: 0.3977 - val_acc: 0.9276\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9864\n",
      "Epoch 00055: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.0441 - acc: 0.9864 - val_loss: 0.4759 - val_acc: 0.9143\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9868\n",
      "Epoch 00056: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.0436 - acc: 0.9867 - val_loss: 0.4612 - val_acc: 0.9199\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9851\n",
      "Epoch 00057: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0499 - acc: 0.9852 - val_loss: 0.3948 - val_acc: 0.9287\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9917\n",
      "Epoch 00058: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.0308 - acc: 0.9917 - val_loss: 0.4071 - val_acc: 0.9278\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9892\n",
      "Epoch 00059: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.0368 - acc: 0.9891 - val_loss: 0.4940 - val_acc: 0.9178\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9867\n",
      "Epoch 00060: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.0435 - acc: 0.9867 - val_loss: 0.4749 - val_acc: 0.9210\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9905\n",
      "Epoch 00061: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0348 - acc: 0.9905 - val_loss: 0.4386 - val_acc: 0.9278\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9910\n",
      "Epoch 00062: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0340 - acc: 0.9910 - val_loss: 0.4417 - val_acc: 0.9269\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9878\n",
      "Epoch 00063: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0408 - acc: 0.9878 - val_loss: 0.4569 - val_acc: 0.9201\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9905\n",
      "Epoch 00064: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0324 - acc: 0.9904 - val_loss: 0.4406 - val_acc: 0.9301\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9896\n",
      "Epoch 00065: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0369 - acc: 0.9896 - val_loss: 0.4941 - val_acc: 0.9175\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9899\n",
      "Epoch 00066: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.0356 - acc: 0.9899 - val_loss: 0.4221 - val_acc: 0.9301\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9917\n",
      "Epoch 00067: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0299 - acc: 0.9918 - val_loss: 0.4924 - val_acc: 0.9182\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9952\n",
      "Epoch 00068: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0205 - acc: 0.9952 - val_loss: 0.4470 - val_acc: 0.9301\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9885\n",
      "Epoch 00069: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.0411 - acc: 0.9885 - val_loss: 0.4727 - val_acc: 0.9238\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9919\n",
      "Epoch 00070: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0297 - acc: 0.9919 - val_loss: 0.4718 - val_acc: 0.9250\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9899\n",
      "Epoch 00071: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0348 - acc: 0.9899 - val_loss: 0.4352 - val_acc: 0.9311\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0255 - acc: 0.9929 - val_loss: 0.5199 - val_acc: 0.9248\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9943\n",
      "Epoch 00073: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0220 - acc: 0.9943 - val_loss: 0.4944 - val_acc: 0.9255\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9899\n",
      "Epoch 00074: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0372 - acc: 0.9899 - val_loss: 0.4668 - val_acc: 0.9306\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 00075: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0144 - acc: 0.9968 - val_loss: 0.4524 - val_acc: 0.9317\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9891\n",
      "Epoch 00076: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0381 - acc: 0.9890 - val_loss: 0.4428 - val_acc: 0.9283\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9934\n",
      "Epoch 00077: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0253 - acc: 0.9934 - val_loss: 0.4402 - val_acc: 0.9308\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 00078: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0191 - acc: 0.9950 - val_loss: 0.4588 - val_acc: 0.9264\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9906\n",
      "Epoch 00079: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0326 - acc: 0.9906 - val_loss: 0.4783 - val_acc: 0.9257\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9912\n",
      "Epoch 00080: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0306 - acc: 0.9911 - val_loss: 0.4862 - val_acc: 0.9269\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9948\n",
      "Epoch 00081: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0204 - acc: 0.9948 - val_loss: 0.4680 - val_acc: 0.9287\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9915\n",
      "Epoch 00082: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0286 - acc: 0.9915 - val_loss: 0.4624 - val_acc: 0.9292\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9942\n",
      "Epoch 00083: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0227 - acc: 0.9942 - val_loss: 0.4784 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9946\n",
      "Epoch 00084: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0222 - acc: 0.9946 - val_loss: 0.5599 - val_acc: 0.9185\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9896\n",
      "Epoch 00085: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0364 - acc: 0.9896 - val_loss: 0.4805 - val_acc: 0.9336\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9956\n",
      "Epoch 00086: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0164 - acc: 0.9956 - val_loss: 0.4822 - val_acc: 0.9278\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9944\n",
      "Epoch 00087: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0217 - acc: 0.9945 - val_loss: 0.5622 - val_acc: 0.9252\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9912\n",
      "Epoch 00088: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0325 - acc: 0.9912 - val_loss: 0.4914 - val_acc: 0.9283\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9926\n",
      "Epoch 00089: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0281 - acc: 0.9926 - val_loss: 0.4935 - val_acc: 0.9299\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9964\n",
      "Epoch 00090: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0164 - acc: 0.9964 - val_loss: 0.4495 - val_acc: 0.9343\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9976\n",
      "Epoch 00091: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0123 - acc: 0.9976 - val_loss: 0.5388 - val_acc: 0.9248\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9918\n",
      "Epoch 00092: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0299 - acc: 0.9918 - val_loss: 0.4933 - val_acc: 0.9329\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9930\n",
      "Epoch 00093: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0263 - acc: 0.9930 - val_loss: 0.4853 - val_acc: 0.9329\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9941\n",
      "Epoch 00094: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0224 - acc: 0.9941 - val_loss: 0.4959 - val_acc: 0.9338\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9959\n",
      "Epoch 00095: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.0167 - acc: 0.9959 - val_loss: 0.4880 - val_acc: 0.9315\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9960\n",
      "Epoch 00096: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0166 - acc: 0.9960 - val_loss: 0.5579 - val_acc: 0.9290\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9927\n",
      "Epoch 00097: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0252 - acc: 0.9927 - val_loss: 0.4937 - val_acc: 0.9315\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9941\n",
      "Epoch 00098: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0244 - acc: 0.9941 - val_loss: 0.5073 - val_acc: 0.9299\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9968\n",
      "Epoch 00099: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.0134 - acc: 0.9968 - val_loss: 0.4695 - val_acc: 0.9362\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9957\n",
      "Epoch 00100: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0175 - acc: 0.9957 - val_loss: 0.5432 - val_acc: 0.9252\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9933\n",
      "Epoch 00101: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0260 - acc: 0.9932 - val_loss: 0.5324 - val_acc: 0.9278\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9945\n",
      "Epoch 00102: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.0218 - acc: 0.9945 - val_loss: 0.5314 - val_acc: 0.9269\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9963\n",
      "Epoch 00103: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.0156 - acc: 0.9963 - val_loss: 0.4935 - val_acc: 0.9315\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 00104: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0192 - acc: 0.9950 - val_loss: 0.5034 - val_acc: 0.9324\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9954\n",
      "Epoch 00105: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0182 - acc: 0.9954 - val_loss: 0.5263 - val_acc: 0.9304\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9955\n",
      "Epoch 00106: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0165 - acc: 0.9955 - val_loss: 0.5212 - val_acc: 0.9285\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9944\n",
      "Epoch 00107: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0212 - acc: 0.9944 - val_loss: 0.4962 - val_acc: 0.9338\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0124 - acc: 0.9973 - val_loss: 0.5730 - val_acc: 0.9229\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9917\n",
      "Epoch 00109: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0297 - acc: 0.9917 - val_loss: 0.5133 - val_acc: 0.9315\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9970\n",
      "Epoch 00110: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0134 - acc: 0.9970 - val_loss: 0.5072 - val_acc: 0.9301\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9961\n",
      "Epoch 00111: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0172 - acc: 0.9961 - val_loss: 0.5332 - val_acc: 0.9283\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9945\n",
      "Epoch 00112: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.0212 - acc: 0.9945 - val_loss: 0.5491 - val_acc: 0.9271\n",
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9956\n",
      "Epoch 00113: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0177 - acc: 0.9956 - val_loss: 0.5626 - val_acc: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9970\n",
      "Epoch 00114: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.0136 - acc: 0.9970 - val_loss: 0.4879 - val_acc: 0.9364\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9990\n",
      "Epoch 00115: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0068 - acc: 0.9990 - val_loss: 0.4983 - val_acc: 0.9345\n",
      "Epoch 116/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9989\n",
      "Epoch 00116: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0064 - acc: 0.9989 - val_loss: 0.5217 - val_acc: 0.9327\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9883\n",
      "Epoch 00117: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0455 - acc: 0.9883 - val_loss: 0.5010 - val_acc: 0.9343\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9962\n",
      "Epoch 00118: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0160 - acc: 0.9962 - val_loss: 0.5036 - val_acc: 0.9315\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9964\n",
      "Epoch 00119: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 0.4853 - val_acc: 0.9359\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9962\n",
      "Epoch 00120: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0173 - acc: 0.9963 - val_loss: 0.5593 - val_acc: 0.9266\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9955\n",
      "Epoch 00121: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0201 - acc: 0.9955 - val_loss: 0.5171 - val_acc: 0.9357\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9972\n",
      "Epoch 00122: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.0123 - acc: 0.9972 - val_loss: 0.5200 - val_acc: 0.9317\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9976\n",
      "Epoch 00123: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.0110 - acc: 0.9976 - val_loss: 0.5348 - val_acc: 0.9345\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9913\n",
      "Epoch 00124: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.0311 - acc: 0.9913 - val_loss: 0.4972 - val_acc: 0.9324\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9972\n",
      "Epoch 00125: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0117 - acc: 0.9972 - val_loss: 0.4949 - val_acc: 0.9364\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9993\n",
      "Epoch 00126: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0059 - acc: 0.9993 - val_loss: 0.4946 - val_acc: 0.9369\n",
      "Epoch 127/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9990\n",
      "Epoch 00127: val_loss did not improve from 0.27192\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0062 - acc: 0.9989 - val_loss: 0.5188 - val_acc: 0.9371\n",
      "\n",
      "1D_CNN_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 266us/sample - loss: 0.4004 - acc: 0.8870\n",
      "Loss: 0.40042313160307064 Accuracy: 0.88701975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,456\n",
      "Trainable params: 511,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 220us/sample - loss: 1.6917 - acc: 0.4621\n",
      "Loss: 1.6916703453687865 Accuracy: 0.46209762\n",
      "\n",
      "1D_CNN_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,648\n",
      "Trainable params: 257,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 252us/sample - loss: 1.4719 - acc: 0.5321\n",
      "Loss: 1.4719076889202478 Accuracy: 0.5320872\n",
      "\n",
      "1D_CNN_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,688\n",
      "Trainable params: 140,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 262us/sample - loss: 0.7853 - acc: 0.7747\n",
      "Loss: 0.7853394545622456 Accuracy: 0.7746625\n",
      "\n",
      "1D_CNN_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 123,856\n",
      "Trainable params: 123,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 279us/sample - loss: 0.4364 - acc: 0.8866\n",
      "Loss: 0.43644130462674213 Accuracy: 0.88660437\n",
      "\n",
      "1D_CNN_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 288,848\n",
      "Trainable params: 288,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 321us/sample - loss: 0.4004 - acc: 0.8870\n",
      "Loss: 0.40042313160307064 Accuracy: 0.88701975\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_DO_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid', \n",
    "                          activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,456\n",
      "Trainable params: 511,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,648\n",
      "Trainable params: 257,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,688\n",
      "Trainable params: 140,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 123,856\n",
      "Trainable params: 123,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_40 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 288,848\n",
      "Trainable params: 288,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_DO_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4480 - acc: 0.2370\n",
      "Epoch 00001: val_loss improved from inf to 2.14335, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/001-2.1434.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 2.4480 - acc: 0.2370 - val_loss: 2.1434 - val_acc: 0.3648\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9919 - acc: 0.3920\n",
      "Epoch 00002: val_loss improved from 2.14335 to 1.89048, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/002-1.8905.hdf5\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 1.9919 - acc: 0.3920 - val_loss: 1.8905 - val_acc: 0.4267\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7738 - acc: 0.4583\n",
      "Epoch 00003: val_loss improved from 1.89048 to 1.76412, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/003-1.7641.hdf5\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 1.7738 - acc: 0.4584 - val_loss: 1.7641 - val_acc: 0.4621\n",
      "Epoch 4/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.6287 - acc: 0.5045\n",
      "Epoch 00004: val_loss improved from 1.76412 to 1.69021, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/004-1.6902.hdf5\n",
      "36805/36805 [==============================] - 7s 185us/sample - loss: 1.6288 - acc: 0.5047 - val_loss: 1.6902 - val_acc: 0.4789\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5223 - acc: 0.5369\n",
      "Epoch 00005: val_loss improved from 1.69021 to 1.64099, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/005-1.6410.hdf5\n",
      "36805/36805 [==============================] - 7s 185us/sample - loss: 1.5226 - acc: 0.5367 - val_loss: 1.6410 - val_acc: 0.4955\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4383 - acc: 0.5604\n",
      "Epoch 00006: val_loss improved from 1.64099 to 1.60794, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/006-1.6079.hdf5\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 1.4378 - acc: 0.5605 - val_loss: 1.6079 - val_acc: 0.5045\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.5801\n",
      "Epoch 00007: val_loss improved from 1.60794 to 1.58627, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/007-1.5863.hdf5\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 1.3771 - acc: 0.5801 - val_loss: 1.5863 - val_acc: 0.5069\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3234 - acc: 0.5941\n",
      "Epoch 00008: val_loss improved from 1.58627 to 1.56764, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/008-1.5676.hdf5\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 1.3228 - acc: 0.5941 - val_loss: 1.5676 - val_acc: 0.5176\n",
      "Epoch 9/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2748 - acc: 0.6078\n",
      "Epoch 00009: val_loss improved from 1.56764 to 1.55631, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/009-1.5563.hdf5\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 1.2748 - acc: 0.6077 - val_loss: 1.5563 - val_acc: 0.5143\n",
      "Epoch 10/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2376 - acc: 0.6188\n",
      "Epoch 00010: val_loss improved from 1.55631 to 1.54726, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/010-1.5473.hdf5\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 1.2382 - acc: 0.6187 - val_loss: 1.5473 - val_acc: 0.5178\n",
      "Epoch 11/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1986 - acc: 0.6296\n",
      "Epoch 00011: val_loss improved from 1.54726 to 1.54405, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/011-1.5440.hdf5\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 1.1990 - acc: 0.6299 - val_loss: 1.5440 - val_acc: 0.5185\n",
      "Epoch 12/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1674 - acc: 0.6399\n",
      "Epoch 00012: val_loss improved from 1.54405 to 1.53948, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/012-1.5395.hdf5\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 1.1674 - acc: 0.6400 - val_loss: 1.5395 - val_acc: 0.5185\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1385 - acc: 0.6463\n",
      "Epoch 00013: val_loss improved from 1.53948 to 1.53752, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/013-1.5375.hdf5\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 1.1386 - acc: 0.6462 - val_loss: 1.5375 - val_acc: 0.5167\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1077 - acc: 0.6570\n",
      "Epoch 00014: val_loss did not improve from 1.53752\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 1.1086 - acc: 0.6567 - val_loss: 1.5381 - val_acc: 0.5188\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0889 - acc: 0.6608\n",
      "Epoch 00015: val_loss did not improve from 1.53752\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 1.0886 - acc: 0.6609 - val_loss: 1.5413 - val_acc: 0.5162\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0648 - acc: 0.6672\n",
      "Epoch 00016: val_loss improved from 1.53752 to 1.53402, saving model to model/checkpoint/1D_CNN_DO_1_only_conv_checkpoint/016-1.5340.hdf5\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 1.0647 - acc: 0.6672 - val_loss: 1.5340 - val_acc: 0.5208\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0427 - acc: 0.6769\n",
      "Epoch 00017: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 1.0426 - acc: 0.6770 - val_loss: 1.5399 - val_acc: 0.5174\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0215 - acc: 0.6827\n",
      "Epoch 00018: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 1.0221 - acc: 0.6826 - val_loss: 1.5451 - val_acc: 0.5250\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0095 - acc: 0.6830\n",
      "Epoch 00019: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 1.0095 - acc: 0.6830 - val_loss: 1.5439 - val_acc: 0.5225\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9909 - acc: 0.6897\n",
      "Epoch 00020: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 0.9910 - acc: 0.6898 - val_loss: 1.5521 - val_acc: 0.5215\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6946\n",
      "Epoch 00021: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.9704 - acc: 0.6946 - val_loss: 1.5611 - val_acc: 0.5122\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9611 - acc: 0.7001\n",
      "Epoch 00022: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 0.9609 - acc: 0.7003 - val_loss: 1.5551 - val_acc: 0.5178\n",
      "Epoch 23/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9459 - acc: 0.7023\n",
      "Epoch 00023: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.9457 - acc: 0.7023 - val_loss: 1.5613 - val_acc: 0.5153\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.7064\n",
      "Epoch 00024: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.9294 - acc: 0.7065 - val_loss: 1.5669 - val_acc: 0.5222\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9132 - acc: 0.7116\n",
      "Epoch 00025: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 0.9136 - acc: 0.7114 - val_loss: 1.5761 - val_acc: 0.5162\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9031 - acc: 0.7143\n",
      "Epoch 00026: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 0.9026 - acc: 0.7145 - val_loss: 1.5790 - val_acc: 0.5153\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8912 - acc: 0.7181\n",
      "Epoch 00027: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 0.8906 - acc: 0.7184 - val_loss: 1.5886 - val_acc: 0.5183\n",
      "Epoch 28/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8831 - acc: 0.7190\n",
      "Epoch 00028: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.8828 - acc: 0.7192 - val_loss: 1.5779 - val_acc: 0.5278\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8753 - acc: 0.7210\n",
      "Epoch 00029: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.8751 - acc: 0.7211 - val_loss: 1.5911 - val_acc: 0.5199\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8611 - acc: 0.7252\n",
      "Epoch 00030: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 188us/sample - loss: 0.8615 - acc: 0.7252 - val_loss: 1.6004 - val_acc: 0.5183\n",
      "Epoch 31/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8552 - acc: 0.7268\n",
      "Epoch 00031: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.8550 - acc: 0.7269 - val_loss: 1.5985 - val_acc: 0.5253\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7304\n",
      "Epoch 00032: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 0.8430 - acc: 0.7304 - val_loss: 1.6071 - val_acc: 0.5248\n",
      "Epoch 33/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7336\n",
      "Epoch 00033: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.8336 - acc: 0.7335 - val_loss: 1.6198 - val_acc: 0.5234\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7367\n",
      "Epoch 00034: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.8230 - acc: 0.7366 - val_loss: 1.6232 - val_acc: 0.5178\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7382\n",
      "Epoch 00035: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.8176 - acc: 0.7382 - val_loss: 1.6296 - val_acc: 0.5215\n",
      "Epoch 36/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.7415\n",
      "Epoch 00036: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.8089 - acc: 0.7412 - val_loss: 1.6291 - val_acc: 0.5281\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7428\n",
      "Epoch 00037: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 185us/sample - loss: 0.8008 - acc: 0.7428 - val_loss: 1.6305 - val_acc: 0.5250\n",
      "Epoch 38/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7461\n",
      "Epoch 00038: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.7935 - acc: 0.7455 - val_loss: 1.6306 - val_acc: 0.5227\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7885 - acc: 0.7467\n",
      "Epoch 00039: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7897 - acc: 0.7465 - val_loss: 1.6423 - val_acc: 0.5215\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.7505\n",
      "Epoch 00040: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.7774 - acc: 0.7506 - val_loss: 1.6546 - val_acc: 0.5208\n",
      "Epoch 41/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.7776 - acc: 0.7534\n",
      "Epoch 00041: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.7776 - acc: 0.7534 - val_loss: 1.6511 - val_acc: 0.5225\n",
      "Epoch 42/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7659 - acc: 0.7530\n",
      "Epoch 00042: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.7656 - acc: 0.7530 - val_loss: 1.6667 - val_acc: 0.5248\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7525\n",
      "Epoch 00043: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.7647 - acc: 0.7527 - val_loss: 1.6609 - val_acc: 0.5241\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7529\n",
      "Epoch 00044: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.7586 - acc: 0.7530 - val_loss: 1.6688 - val_acc: 0.5167\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7575\n",
      "Epoch 00045: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 185us/sample - loss: 0.7530 - acc: 0.7575 - val_loss: 1.6727 - val_acc: 0.5204\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7424 - acc: 0.7583\n",
      "Epoch 00046: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 0.7424 - acc: 0.7583 - val_loss: 1.6749 - val_acc: 0.5269\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.7574\n",
      "Epoch 00047: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 183us/sample - loss: 0.7419 - acc: 0.7575 - val_loss: 1.6851 - val_acc: 0.5253\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7313 - acc: 0.7618\n",
      "Epoch 00048: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7312 - acc: 0.7618 - val_loss: 1.6908 - val_acc: 0.5236\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7626\n",
      "Epoch 00049: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.7280 - acc: 0.7626 - val_loss: 1.6954 - val_acc: 0.5248\n",
      "Epoch 50/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.7288 - acc: 0.7611\n",
      "Epoch 00050: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7292 - acc: 0.7610 - val_loss: 1.6972 - val_acc: 0.5211\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7633\n",
      "Epoch 00051: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.7230 - acc: 0.7636 - val_loss: 1.6977 - val_acc: 0.5264\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7143 - acc: 0.7662\n",
      "Epoch 00052: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7144 - acc: 0.7661 - val_loss: 1.7001 - val_acc: 0.5213\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7101 - acc: 0.7689\n",
      "Epoch 00053: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7097 - acc: 0.7689 - val_loss: 1.7054 - val_acc: 0.5278\n",
      "Epoch 54/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.7043 - acc: 0.7723\n",
      "Epoch 00054: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.7040 - acc: 0.7724 - val_loss: 1.7155 - val_acc: 0.5229\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7070 - acc: 0.7668\n",
      "Epoch 00055: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.7067 - acc: 0.7669 - val_loss: 1.7118 - val_acc: 0.5262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7717\n",
      "Epoch 00056: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.6962 - acc: 0.7717 - val_loss: 1.7197 - val_acc: 0.5215\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6938 - acc: 0.7688\n",
      "Epoch 00057: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.6935 - acc: 0.7688 - val_loss: 1.7340 - val_acc: 0.5260\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.7738\n",
      "Epoch 00058: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.6885 - acc: 0.7738 - val_loss: 1.7363 - val_acc: 0.5276\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6870 - acc: 0.7739\n",
      "Epoch 00059: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.6873 - acc: 0.7737 - val_loss: 1.7300 - val_acc: 0.5276\n",
      "Epoch 60/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6868 - acc: 0.7764\n",
      "Epoch 00060: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.6866 - acc: 0.7764 - val_loss: 1.7356 - val_acc: 0.5255\n",
      "Epoch 61/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.7770\n",
      "Epoch 00061: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 0.6791 - acc: 0.7770 - val_loss: 1.7460 - val_acc: 0.5253\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.7769\n",
      "Epoch 00062: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 181us/sample - loss: 0.6766 - acc: 0.7770 - val_loss: 1.7421 - val_acc: 0.5206\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6693 - acc: 0.7811\n",
      "Epoch 00063: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 0.6696 - acc: 0.7811 - val_loss: 1.7380 - val_acc: 0.5297\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7773\n",
      "Epoch 00064: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 0.6719 - acc: 0.7773 - val_loss: 1.7482 - val_acc: 0.5234\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6634 - acc: 0.7815\n",
      "Epoch 00065: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 194us/sample - loss: 0.6634 - acc: 0.7815 - val_loss: 1.7602 - val_acc: 0.5248\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6606 - acc: 0.7818\n",
      "Epoch 00066: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6605 - acc: 0.7818 - val_loss: 1.7718 - val_acc: 0.5225\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6624 - acc: 0.7820\n",
      "Epoch 00067: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 196us/sample - loss: 0.6623 - acc: 0.7820 - val_loss: 1.7700 - val_acc: 0.5248\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6509 - acc: 0.7851\n",
      "Epoch 00068: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 187us/sample - loss: 0.6510 - acc: 0.7853 - val_loss: 1.7707 - val_acc: 0.5204\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6570 - acc: 0.7816\n",
      "Epoch 00069: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.6571 - acc: 0.7816 - val_loss: 1.7719 - val_acc: 0.5278\n",
      "Epoch 70/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.7871\n",
      "Epoch 00070: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 182us/sample - loss: 0.6489 - acc: 0.7871 - val_loss: 1.7794 - val_acc: 0.5215\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.7855\n",
      "Epoch 00071: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 187us/sample - loss: 0.6458 - acc: 0.7855 - val_loss: 1.7828 - val_acc: 0.5253\n",
      "Epoch 72/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.7836\n",
      "Epoch 00072: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.6492 - acc: 0.7834 - val_loss: 1.7878 - val_acc: 0.5276\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.7870\n",
      "Epoch 00073: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6427 - acc: 0.7870 - val_loss: 1.7820 - val_acc: 0.5243\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6379 - acc: 0.7878\n",
      "Epoch 00074: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 195us/sample - loss: 0.6378 - acc: 0.7878 - val_loss: 1.7859 - val_acc: 0.5274\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6400 - acc: 0.7861\n",
      "Epoch 00075: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.6398 - acc: 0.7862 - val_loss: 1.7929 - val_acc: 0.5248\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6322 - acc: 0.7907\n",
      "Epoch 00076: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.6322 - acc: 0.7907 - val_loss: 1.7940 - val_acc: 0.5262\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.7908\n",
      "Epoch 00077: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.6263 - acc: 0.7908 - val_loss: 1.8037 - val_acc: 0.5299\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.7894\n",
      "Epoch 00078: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6320 - acc: 0.7894 - val_loss: 1.8058 - val_acc: 0.5269\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6183 - acc: 0.7968\n",
      "Epoch 00079: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.6185 - acc: 0.7968 - val_loss: 1.8066 - val_acc: 0.5323\n",
      "Epoch 80/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6237 - acc: 0.7917\n",
      "Epoch 00080: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6244 - acc: 0.7916 - val_loss: 1.8107 - val_acc: 0.5285\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.7930\n",
      "Epoch 00081: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6271 - acc: 0.7930 - val_loss: 1.8185 - val_acc: 0.5276\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.7923\n",
      "Epoch 00082: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.6197 - acc: 0.7923 - val_loss: 1.8199 - val_acc: 0.5234\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.7983\n",
      "Epoch 00083: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6108 - acc: 0.7982 - val_loss: 1.8246 - val_acc: 0.5225\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6123 - acc: 0.7974\n",
      "Epoch 00084: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.6123 - acc: 0.7974 - val_loss: 1.8257 - val_acc: 0.5316\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6157 - acc: 0.7947\n",
      "Epoch 00085: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6156 - acc: 0.7946 - val_loss: 1.8336 - val_acc: 0.5271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.7974\n",
      "Epoch 00086: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6070 - acc: 0.7973 - val_loss: 1.8291 - val_acc: 0.5313\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.7969\n",
      "Epoch 00087: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 188us/sample - loss: 0.6090 - acc: 0.7970 - val_loss: 1.8363 - val_acc: 0.5309\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6134 - acc: 0.7955\n",
      "Epoch 00088: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.6134 - acc: 0.7955 - val_loss: 1.8393 - val_acc: 0.5269\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.7981\n",
      "Epoch 00089: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6064 - acc: 0.7982 - val_loss: 1.8313 - val_acc: 0.5302\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.7974\n",
      "Epoch 00090: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6057 - acc: 0.7973 - val_loss: 1.8472 - val_acc: 0.5274\n",
      "Epoch 91/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.7997\n",
      "Epoch 00091: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.6008 - acc: 0.7997 - val_loss: 1.8356 - val_acc: 0.5330\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.7982\n",
      "Epoch 00092: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 188us/sample - loss: 0.6038 - acc: 0.7982 - val_loss: 1.8463 - val_acc: 0.5304\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.7988\n",
      "Epoch 00093: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 194us/sample - loss: 0.5948 - acc: 0.7985 - val_loss: 1.8530 - val_acc: 0.5278\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6001 - acc: 0.7996\n",
      "Epoch 00094: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 189us/sample - loss: 0.6003 - acc: 0.7996 - val_loss: 1.8470 - val_acc: 0.5290\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8012\n",
      "Epoch 00095: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.5938 - acc: 0.8012 - val_loss: 1.8613 - val_acc: 0.5229\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.8030\n",
      "Epoch 00096: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.5929 - acc: 0.8030 - val_loss: 1.8539 - val_acc: 0.5248\n",
      "Epoch 97/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.8019\n",
      "Epoch 00097: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.5922 - acc: 0.8018 - val_loss: 1.8599 - val_acc: 0.5274\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.8024\n",
      "Epoch 00098: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.5871 - acc: 0.8025 - val_loss: 1.8591 - val_acc: 0.5281\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5897 - acc: 0.8019\n",
      "Epoch 00099: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.5897 - acc: 0.8018 - val_loss: 1.8605 - val_acc: 0.5269\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.8014\n",
      "Epoch 00100: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 186us/sample - loss: 0.5876 - acc: 0.8014 - val_loss: 1.8606 - val_acc: 0.5288\n",
      "Epoch 101/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.5869 - acc: 0.8055\n",
      "Epoch 00101: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 194us/sample - loss: 0.5872 - acc: 0.8054 - val_loss: 1.8679 - val_acc: 0.5239\n",
      "Epoch 102/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8037\n",
      "Epoch 00102: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 190us/sample - loss: 0.5833 - acc: 0.8039 - val_loss: 1.8731 - val_acc: 0.5304\n",
      "Epoch 103/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5838 - acc: 0.8055\n",
      "Epoch 00103: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 196us/sample - loss: 0.5840 - acc: 0.8056 - val_loss: 1.8807 - val_acc: 0.5271\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5849 - acc: 0.8040\n",
      "Epoch 00104: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.5848 - acc: 0.8040 - val_loss: 1.8580 - val_acc: 0.5274\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5684 - acc: 0.8107\n",
      "Epoch 00105: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 195us/sample - loss: 0.5685 - acc: 0.8106 - val_loss: 1.8885 - val_acc: 0.5215\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8043\n",
      "Epoch 00106: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 194us/sample - loss: 0.5793 - acc: 0.8043 - val_loss: 1.8773 - val_acc: 0.5276\n",
      "Epoch 107/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8068\n",
      "Epoch 00107: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 199us/sample - loss: 0.5755 - acc: 0.8067 - val_loss: 1.8903 - val_acc: 0.5304\n",
      "Epoch 108/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.8075\n",
      "Epoch 00108: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.5755 - acc: 0.8074 - val_loss: 1.8875 - val_acc: 0.5278\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8080\n",
      "Epoch 00109: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 187us/sample - loss: 0.5743 - acc: 0.8082 - val_loss: 1.8904 - val_acc: 0.5288\n",
      "Epoch 110/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.8089\n",
      "Epoch 00110: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.5709 - acc: 0.8089 - val_loss: 1.8737 - val_acc: 0.5283\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8088\n",
      "Epoch 00111: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.5677 - acc: 0.8088 - val_loss: 1.8860 - val_acc: 0.5323\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8083\n",
      "Epoch 00112: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 192us/sample - loss: 0.5715 - acc: 0.8082 - val_loss: 1.8987 - val_acc: 0.5297\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.8085\n",
      "Epoch 00113: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.5649 - acc: 0.8085 - val_loss: 1.8889 - val_acc: 0.5299\n",
      "Epoch 114/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.8090\n",
      "Epoch 00114: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 193us/sample - loss: 0.5664 - acc: 0.8088 - val_loss: 1.8982 - val_acc: 0.5330\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8107\n",
      "Epoch 00115: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 191us/sample - loss: 0.5618 - acc: 0.8107 - val_loss: 1.8868 - val_acc: 0.5327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8113\n",
      "Epoch 00116: val_loss did not improve from 1.53402\n",
      "36805/36805 [==============================] - 7s 195us/sample - loss: 0.5621 - acc: 0.8115 - val_loss: 1.9025 - val_acc: 0.5332\n",
      "\n",
      "1D_CNN_DO_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 246us/sample - loss: 1.6133 - acc: 0.4889\n",
      "Loss: 1.6133275957741346 Accuracy: 0.4888889\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 2.3750 - acc: 0.2350\n",
      "Epoch 00001: val_loss improved from inf to 1.89725, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/001-1.8972.hdf5\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 2.3722 - acc: 0.2359 - val_loss: 1.8972 - val_acc: 0.4221\n",
      "Epoch 2/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8142 - acc: 0.4144\n",
      "Epoch 00002: val_loss improved from 1.89725 to 1.66641, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/002-1.6664.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 1.8130 - acc: 0.4149 - val_loss: 1.6664 - val_acc: 0.4810\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6545 - acc: 0.4697\n",
      "Epoch 00003: val_loss improved from 1.66641 to 1.55924, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/003-1.5592.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 1.6541 - acc: 0.4700 - val_loss: 1.5592 - val_acc: 0.5118\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5471 - acc: 0.5129\n",
      "Epoch 00004: val_loss improved from 1.55924 to 1.48965, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/004-1.4896.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.5470 - acc: 0.5130 - val_loss: 1.4896 - val_acc: 0.5316\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4689 - acc: 0.5405\n",
      "Epoch 00005: val_loss improved from 1.48965 to 1.43344, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/005-1.4334.hdf5\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 1.4685 - acc: 0.5405 - val_loss: 1.4334 - val_acc: 0.5672\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4113 - acc: 0.5623\n",
      "Epoch 00006: val_loss improved from 1.43344 to 1.39461, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/006-1.3946.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.4105 - acc: 0.5625 - val_loss: 1.3946 - val_acc: 0.5830\n",
      "Epoch 7/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3566 - acc: 0.5773\n",
      "Epoch 00007: val_loss improved from 1.39461 to 1.36228, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/007-1.3623.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.3562 - acc: 0.5775 - val_loss: 1.3623 - val_acc: 0.5893\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3145 - acc: 0.5930\n",
      "Epoch 00008: val_loss improved from 1.36228 to 1.33806, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/008-1.3381.hdf5\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 1.3143 - acc: 0.5930 - val_loss: 1.3381 - val_acc: 0.6010\n",
      "Epoch 9/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2743 - acc: 0.6082\n",
      "Epoch 00009: val_loss improved from 1.33806 to 1.32410, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/009-1.3241.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.2744 - acc: 0.6079 - val_loss: 1.3241 - val_acc: 0.6049\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2402 - acc: 0.6207\n",
      "Epoch 00010: val_loss improved from 1.32410 to 1.28764, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/010-1.2876.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.2404 - acc: 0.6205 - val_loss: 1.2876 - val_acc: 0.6168\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2064 - acc: 0.6292\n",
      "Epoch 00011: val_loss did not improve from 1.28764\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 1.2067 - acc: 0.6293 - val_loss: 1.2913 - val_acc: 0.6122\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.6399\n",
      "Epoch 00012: val_loss improved from 1.28764 to 1.26793, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/012-1.2679.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.1789 - acc: 0.6402 - val_loss: 1.2679 - val_acc: 0.6168\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1489 - acc: 0.6472\n",
      "Epoch 00013: val_loss improved from 1.26793 to 1.25676, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/013-1.2568.hdf5\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 1.1492 - acc: 0.6472 - val_loss: 1.2568 - val_acc: 0.6145\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1268 - acc: 0.6555\n",
      "Epoch 00014: val_loss improved from 1.25676 to 1.25165, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/014-1.2516.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.1275 - acc: 0.6553 - val_loss: 1.2516 - val_acc: 0.6182\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6637\n",
      "Epoch 00015: val_loss improved from 1.25165 to 1.22631, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/015-1.2263.hdf5\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 1.1024 - acc: 0.6637 - val_loss: 1.2263 - val_acc: 0.6317\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0798 - acc: 0.6715\n",
      "Epoch 00016: val_loss improved from 1.22631 to 1.22014, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/016-1.2201.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.0796 - acc: 0.6715 - val_loss: 1.2201 - val_acc: 0.6264\n",
      "Epoch 17/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0580 - acc: 0.6764\n",
      "Epoch 00017: val_loss improved from 1.22014 to 1.20688, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/017-1.2069.hdf5\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 1.0578 - acc: 0.6763 - val_loss: 1.2069 - val_acc: 0.6280\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0367 - acc: 0.6824\n",
      "Epoch 00018: val_loss did not improve from 1.20688\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.0364 - acc: 0.6825 - val_loss: 1.2108 - val_acc: 0.6294\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0185 - acc: 0.6897\n",
      "Epoch 00019: val_loss improved from 1.20688 to 1.20339, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/019-1.2034.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.0188 - acc: 0.6895 - val_loss: 1.2034 - val_acc: 0.6338\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9988 - acc: 0.6959\n",
      "Epoch 00020: val_loss improved from 1.20339 to 1.18834, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/020-1.1883.hdf5\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.9985 - acc: 0.6959 - val_loss: 1.1883 - val_acc: 0.6403\n",
      "Epoch 21/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9869 - acc: 0.6986\n",
      "Epoch 00021: val_loss improved from 1.18834 to 1.18367, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/021-1.1837.hdf5\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.9863 - acc: 0.6985 - val_loss: 1.1837 - val_acc: 0.6443\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9681 - acc: 0.7048\n",
      "Epoch 00022: val_loss improved from 1.18367 to 1.17905, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/022-1.1790.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.9680 - acc: 0.7048 - val_loss: 1.1790 - val_acc: 0.6403\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9494 - acc: 0.7086\n",
      "Epoch 00023: val_loss improved from 1.17905 to 1.17262, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/023-1.1726.hdf5\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.9491 - acc: 0.7086 - val_loss: 1.1726 - val_acc: 0.6443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9342 - acc: 0.7148\n",
      "Epoch 00024: val_loss did not improve from 1.17262\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.9346 - acc: 0.7147 - val_loss: 1.1757 - val_acc: 0.6448\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9175 - acc: 0.7180\n",
      "Epoch 00025: val_loss improved from 1.17262 to 1.16023, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/025-1.1602.hdf5\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.9176 - acc: 0.7180 - val_loss: 1.1602 - val_acc: 0.6532\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9048 - acc: 0.7232\n",
      "Epoch 00026: val_loss did not improve from 1.16023\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.9048 - acc: 0.7235 - val_loss: 1.1622 - val_acc: 0.6513\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7282\n",
      "Epoch 00027: val_loss improved from 1.16023 to 1.15638, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/027-1.1564.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.8923 - acc: 0.7282 - val_loss: 1.1564 - val_acc: 0.6511\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7336\n",
      "Epoch 00028: val_loss improved from 1.15638 to 1.15132, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/028-1.1513.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8772 - acc: 0.7337 - val_loss: 1.1513 - val_acc: 0.6569\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8648 - acc: 0.7329\n",
      "Epoch 00029: val_loss did not improve from 1.15132\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.8648 - acc: 0.7329 - val_loss: 1.1531 - val_acc: 0.6560\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8495 - acc: 0.7377\n",
      "Epoch 00030: val_loss improved from 1.15132 to 1.15098, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/030-1.1510.hdf5\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.8496 - acc: 0.7374 - val_loss: 1.1510 - val_acc: 0.6539\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8391 - acc: 0.7425\n",
      "Epoch 00031: val_loss improved from 1.15098 to 1.14777, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/031-1.1478.hdf5\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.8396 - acc: 0.7423 - val_loss: 1.1478 - val_acc: 0.6569\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8296 - acc: 0.7457\n",
      "Epoch 00032: val_loss did not improve from 1.14777\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.8299 - acc: 0.7456 - val_loss: 1.1508 - val_acc: 0.6569\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8216 - acc: 0.7448\n",
      "Epoch 00033: val_loss did not improve from 1.14777\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.8216 - acc: 0.7448 - val_loss: 1.1484 - val_acc: 0.6555\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8054 - acc: 0.7491\n",
      "Epoch 00034: val_loss improved from 1.14777 to 1.14188, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/034-1.1419.hdf5\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.8054 - acc: 0.7491 - val_loss: 1.1419 - val_acc: 0.6564\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7983 - acc: 0.7536\n",
      "Epoch 00035: val_loss improved from 1.14188 to 1.14032, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/035-1.1403.hdf5\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7985 - acc: 0.7537 - val_loss: 1.1403 - val_acc: 0.6615\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7583\n",
      "Epoch 00036: val_loss improved from 1.14032 to 1.13684, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/036-1.1368.hdf5\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.7846 - acc: 0.7584 - val_loss: 1.1368 - val_acc: 0.6650\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7735 - acc: 0.7597\n",
      "Epoch 00037: val_loss did not improve from 1.13684\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.7735 - acc: 0.7598 - val_loss: 1.1393 - val_acc: 0.6587\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7667 - acc: 0.7584\n",
      "Epoch 00038: val_loss did not improve from 1.13684\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.7665 - acc: 0.7583 - val_loss: 1.1521 - val_acc: 0.6550\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7649\n",
      "Epoch 00039: val_loss improved from 1.13684 to 1.12910, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/039-1.1291.hdf5\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7590 - acc: 0.7651 - val_loss: 1.1291 - val_acc: 0.6632\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7684\n",
      "Epoch 00040: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.7466 - acc: 0.7686 - val_loss: 1.1448 - val_acc: 0.6636\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7381 - acc: 0.7698\n",
      "Epoch 00041: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.7382 - acc: 0.7698 - val_loss: 1.1415 - val_acc: 0.6657\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7291 - acc: 0.7712\n",
      "Epoch 00042: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.7293 - acc: 0.7709 - val_loss: 1.1332 - val_acc: 0.6667\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7236 - acc: 0.7764\n",
      "Epoch 00043: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.7231 - acc: 0.7765 - val_loss: 1.1398 - val_acc: 0.6688\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7107 - acc: 0.7765\n",
      "Epoch 00044: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.7110 - acc: 0.7763 - val_loss: 1.1391 - val_acc: 0.6711\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7062 - acc: 0.7782\n",
      "Epoch 00045: val_loss did not improve from 1.12910\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.7067 - acc: 0.7783 - val_loss: 1.1327 - val_acc: 0.6697\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6938 - acc: 0.7832\n",
      "Epoch 00046: val_loss improved from 1.12910 to 1.12870, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/046-1.1287.hdf5\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6938 - acc: 0.7832 - val_loss: 1.1287 - val_acc: 0.6720\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.7851\n",
      "Epoch 00047: val_loss improved from 1.12870 to 1.12160, saving model to model/checkpoint/1D_CNN_DO_2_only_conv_checkpoint/047-1.1216.hdf5\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.6888 - acc: 0.7852 - val_loss: 1.1216 - val_acc: 0.6727\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6816 - acc: 0.7856\n",
      "Epoch 00048: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.6813 - acc: 0.7857 - val_loss: 1.1444 - val_acc: 0.6734\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7892\n",
      "Epoch 00049: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6733 - acc: 0.7892 - val_loss: 1.1330 - val_acc: 0.6744\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.7919\n",
      "Epoch 00050: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.6659 - acc: 0.7918 - val_loss: 1.1319 - val_acc: 0.6790\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7924\n",
      "Epoch 00051: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6611 - acc: 0.7924 - val_loss: 1.1395 - val_acc: 0.6744\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.7938\n",
      "Epoch 00052: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6519 - acc: 0.7938 - val_loss: 1.1370 - val_acc: 0.6762\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6418 - acc: 0.7982\n",
      "Epoch 00053: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6421 - acc: 0.7981 - val_loss: 1.1527 - val_acc: 0.6657\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.7980\n",
      "Epoch 00054: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.6394 - acc: 0.7981 - val_loss: 1.1458 - val_acc: 0.6741\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.8007\n",
      "Epoch 00055: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.6325 - acc: 0.8009 - val_loss: 1.1434 - val_acc: 0.6769\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.8041\n",
      "Epoch 00056: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6219 - acc: 0.8041 - val_loss: 1.1278 - val_acc: 0.6834\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6178 - acc: 0.8063\n",
      "Epoch 00057: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.6184 - acc: 0.8060 - val_loss: 1.1385 - val_acc: 0.6799\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6167 - acc: 0.8038\n",
      "Epoch 00058: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6165 - acc: 0.8039 - val_loss: 1.1297 - val_acc: 0.6813\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.8074\n",
      "Epoch 00059: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.6072 - acc: 0.8073 - val_loss: 1.1326 - val_acc: 0.6781\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.8109\n",
      "Epoch 00060: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.6018 - acc: 0.8108 - val_loss: 1.1282 - val_acc: 0.6809\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8121\n",
      "Epoch 00061: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.5962 - acc: 0.8121 - val_loss: 1.1391 - val_acc: 0.6783\n",
      "Epoch 62/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.8110\n",
      "Epoch 00062: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.5916 - acc: 0.8109 - val_loss: 1.1320 - val_acc: 0.6837\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8154\n",
      "Epoch 00063: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5855 - acc: 0.8155 - val_loss: 1.1316 - val_acc: 0.6830\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5806 - acc: 0.8145\n",
      "Epoch 00064: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.5808 - acc: 0.8144 - val_loss: 1.1381 - val_acc: 0.6848\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8167\n",
      "Epoch 00065: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.5733 - acc: 0.8167 - val_loss: 1.1674 - val_acc: 0.6699\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8171\n",
      "Epoch 00066: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.5702 - acc: 0.8170 - val_loss: 1.1278 - val_acc: 0.6876\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.8209\n",
      "Epoch 00067: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.5627 - acc: 0.8210 - val_loss: 1.1315 - val_acc: 0.6846\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8219\n",
      "Epoch 00068: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.5581 - acc: 0.8219 - val_loss: 1.1332 - val_acc: 0.6909\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8251\n",
      "Epoch 00069: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5525 - acc: 0.8251 - val_loss: 1.1386 - val_acc: 0.6860\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8269\n",
      "Epoch 00070: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5476 - acc: 0.8270 - val_loss: 1.1347 - val_acc: 0.6923\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.8254\n",
      "Epoch 00071: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5422 - acc: 0.8255 - val_loss: 1.1553 - val_acc: 0.6858\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8274\n",
      "Epoch 00072: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.5384 - acc: 0.8276 - val_loss: 1.1296 - val_acc: 0.6925\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8284\n",
      "Epoch 00073: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.5383 - acc: 0.8284 - val_loss: 1.1412 - val_acc: 0.6911\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.8307\n",
      "Epoch 00074: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.5296 - acc: 0.8307 - val_loss: 1.1584 - val_acc: 0.6888\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8309\n",
      "Epoch 00075: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5289 - acc: 0.8309 - val_loss: 1.1464 - val_acc: 0.6904\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8299\n",
      "Epoch 00076: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.5280 - acc: 0.8298 - val_loss: 1.1485 - val_acc: 0.6890\n",
      "Epoch 77/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8343\n",
      "Epoch 00077: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5190 - acc: 0.8343 - val_loss: 1.1502 - val_acc: 0.6879\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8345\n",
      "Epoch 00078: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5156 - acc: 0.8344 - val_loss: 1.1466 - val_acc: 0.6893\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8367\n",
      "Epoch 00079: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.5074 - acc: 0.8365 - val_loss: 1.1485 - val_acc: 0.6928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8383\n",
      "Epoch 00080: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.5074 - acc: 0.8384 - val_loss: 1.1440 - val_acc: 0.6944\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8386\n",
      "Epoch 00081: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.5002 - acc: 0.8385 - val_loss: 1.1385 - val_acc: 0.6960\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8374\n",
      "Epoch 00082: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.5016 - acc: 0.8373 - val_loss: 1.1605 - val_acc: 0.6867\n",
      "Epoch 83/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8412\n",
      "Epoch 00083: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.4949 - acc: 0.8408 - val_loss: 1.1569 - val_acc: 0.6907\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8439\n",
      "Epoch 00084: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.4896 - acc: 0.8438 - val_loss: 1.1436 - val_acc: 0.6939\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8448\n",
      "Epoch 00085: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.4849 - acc: 0.8448 - val_loss: 1.1493 - val_acc: 0.6907\n",
      "Epoch 86/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8439\n",
      "Epoch 00086: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.4842 - acc: 0.8436 - val_loss: 1.1572 - val_acc: 0.6893\n",
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8456\n",
      "Epoch 00087: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.4809 - acc: 0.8451 - val_loss: 1.1457 - val_acc: 0.6916\n",
      "Epoch 88/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8456\n",
      "Epoch 00088: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.4782 - acc: 0.8458 - val_loss: 1.1517 - val_acc: 0.6939\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8455\n",
      "Epoch 00089: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4745 - acc: 0.8455 - val_loss: 1.1684 - val_acc: 0.6895\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8492\n",
      "Epoch 00090: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4714 - acc: 0.8492 - val_loss: 1.1511 - val_acc: 0.6932\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8491\n",
      "Epoch 00091: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4641 - acc: 0.8491 - val_loss: 1.1534 - val_acc: 0.6890\n",
      "Epoch 92/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8527\n",
      "Epoch 00092: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4588 - acc: 0.8525 - val_loss: 1.1642 - val_acc: 0.6928\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.8504\n",
      "Epoch 00093: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4553 - acc: 0.8504 - val_loss: 1.1606 - val_acc: 0.6944\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8521\n",
      "Epoch 00094: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.4566 - acc: 0.8520 - val_loss: 1.1581 - val_acc: 0.6976\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8531\n",
      "Epoch 00095: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.4567 - acc: 0.8529 - val_loss: 1.1659 - val_acc: 0.6928\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8508\n",
      "Epoch 00096: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4566 - acc: 0.8509 - val_loss: 1.1561 - val_acc: 0.6995\n",
      "Epoch 97/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8549\n",
      "Epoch 00097: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4434 - acc: 0.8547 - val_loss: 1.1492 - val_acc: 0.7002\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8535\n",
      "Epoch 00098: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.4492 - acc: 0.8535 - val_loss: 1.1572 - val_acc: 0.6974\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.8581\n",
      "Epoch 00099: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4404 - acc: 0.8581 - val_loss: 1.1720 - val_acc: 0.6923\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8559\n",
      "Epoch 00100: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.4420 - acc: 0.8560 - val_loss: 1.1616 - val_acc: 0.6979\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8569\n",
      "Epoch 00101: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4382 - acc: 0.8569 - val_loss: 1.1640 - val_acc: 0.6939\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8610\n",
      "Epoch 00102: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4296 - acc: 0.8610 - val_loss: 1.1592 - val_acc: 0.7011\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8563\n",
      "Epoch 00103: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4323 - acc: 0.8562 - val_loss: 1.1609 - val_acc: 0.7035\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8626\n",
      "Epoch 00104: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.4263 - acc: 0.8625 - val_loss: 1.1702 - val_acc: 0.6944\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8618\n",
      "Epoch 00105: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.4214 - acc: 0.8617 - val_loss: 1.1568 - val_acc: 0.7030\n",
      "Epoch 106/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8628\n",
      "Epoch 00106: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.4197 - acc: 0.8628 - val_loss: 1.1574 - val_acc: 0.7002\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8631\n",
      "Epoch 00107: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4209 - acc: 0.8631 - val_loss: 1.1729 - val_acc: 0.7028\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8629\n",
      "Epoch 00108: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4149 - acc: 0.8630 - val_loss: 1.1804 - val_acc: 0.6962\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8649\n",
      "Epoch 00109: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4133 - acc: 0.8650 - val_loss: 1.1797 - val_acc: 0.6958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8675\n",
      "Epoch 00110: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4081 - acc: 0.8675 - val_loss: 1.1752 - val_acc: 0.6995\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8645\n",
      "Epoch 00111: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4148 - acc: 0.8645 - val_loss: 1.1741 - val_acc: 0.6981\n",
      "Epoch 112/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8664\n",
      "Epoch 00112: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.4087 - acc: 0.8663 - val_loss: 1.1660 - val_acc: 0.7016\n",
      "Epoch 113/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8675\n",
      "Epoch 00113: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.4051 - acc: 0.8675 - val_loss: 1.1713 - val_acc: 0.7002\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8684\n",
      "Epoch 00114: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4009 - acc: 0.8684 - val_loss: 1.1790 - val_acc: 0.6988\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8664\n",
      "Epoch 00115: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4048 - acc: 0.8664 - val_loss: 1.1822 - val_acc: 0.6921\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8710\n",
      "Epoch 00116: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3987 - acc: 0.8710 - val_loss: 1.1831 - val_acc: 0.7037\n",
      "Epoch 117/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8691\n",
      "Epoch 00117: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.4014 - acc: 0.8690 - val_loss: 1.1792 - val_acc: 0.7046\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8739\n",
      "Epoch 00118: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3884 - acc: 0.8739 - val_loss: 1.1794 - val_acc: 0.7014\n",
      "Epoch 119/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8688\n",
      "Epoch 00119: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3942 - acc: 0.8689 - val_loss: 1.1730 - val_acc: 0.6988\n",
      "Epoch 120/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8717\n",
      "Epoch 00120: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3893 - acc: 0.8716 - val_loss: 1.1813 - val_acc: 0.7023\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8728\n",
      "Epoch 00121: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3880 - acc: 0.8728 - val_loss: 1.1646 - val_acc: 0.7028\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8768\n",
      "Epoch 00122: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3802 - acc: 0.8768 - val_loss: 1.1893 - val_acc: 0.7000\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8742\n",
      "Epoch 00123: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3842 - acc: 0.8743 - val_loss: 1.1799 - val_acc: 0.7004\n",
      "Epoch 124/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8751\n",
      "Epoch 00124: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3815 - acc: 0.8750 - val_loss: 1.2001 - val_acc: 0.6972\n",
      "Epoch 125/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8764\n",
      "Epoch 00125: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3786 - acc: 0.8763 - val_loss: 1.1869 - val_acc: 0.7025\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8760\n",
      "Epoch 00126: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3744 - acc: 0.8760 - val_loss: 1.1760 - val_acc: 0.7000\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8779\n",
      "Epoch 00127: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3780 - acc: 0.8778 - val_loss: 1.1813 - val_acc: 0.7065\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8770\n",
      "Epoch 00128: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3778 - acc: 0.8771 - val_loss: 1.1793 - val_acc: 0.7018\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8779\n",
      "Epoch 00129: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3721 - acc: 0.8780 - val_loss: 1.1763 - val_acc: 0.7025\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8806\n",
      "Epoch 00130: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3645 - acc: 0.8806 - val_loss: 1.1953 - val_acc: 0.7028\n",
      "Epoch 131/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8797\n",
      "Epoch 00131: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3676 - acc: 0.8797 - val_loss: 1.1865 - val_acc: 0.7009\n",
      "Epoch 132/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8813\n",
      "Epoch 00132: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3622 - acc: 0.8815 - val_loss: 1.1876 - val_acc: 0.7037\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8806\n",
      "Epoch 00133: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3611 - acc: 0.8806 - val_loss: 1.1959 - val_acc: 0.7007\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8830\n",
      "Epoch 00134: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3604 - acc: 0.8829 - val_loss: 1.1946 - val_acc: 0.7044\n",
      "Epoch 135/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8799\n",
      "Epoch 00135: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3632 - acc: 0.8800 - val_loss: 1.1948 - val_acc: 0.6993\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8840\n",
      "Epoch 00136: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3558 - acc: 0.8841 - val_loss: 1.1824 - val_acc: 0.7032\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8823\n",
      "Epoch 00137: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3565 - acc: 0.8823 - val_loss: 1.1843 - val_acc: 0.7056\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.8842\n",
      "Epoch 00138: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3513 - acc: 0.8843 - val_loss: 1.1977 - val_acc: 0.7102\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8836\n",
      "Epoch 00139: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.3515 - acc: 0.8835 - val_loss: 1.1943 - val_acc: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8855\n",
      "Epoch 00140: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.3480 - acc: 0.8855 - val_loss: 1.1855 - val_acc: 0.7065\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8828\n",
      "Epoch 00141: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.3525 - acc: 0.8827 - val_loss: 1.1993 - val_acc: 0.7063\n",
      "Epoch 142/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8858\n",
      "Epoch 00142: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3501 - acc: 0.8855 - val_loss: 1.1938 - val_acc: 0.7032\n",
      "Epoch 143/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8851\n",
      "Epoch 00143: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3480 - acc: 0.8852 - val_loss: 1.1928 - val_acc: 0.7107\n",
      "Epoch 144/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8871\n",
      "Epoch 00144: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3424 - acc: 0.8871 - val_loss: 1.1976 - val_acc: 0.7058\n",
      "Epoch 145/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8872\n",
      "Epoch 00145: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3407 - acc: 0.8872 - val_loss: 1.1987 - val_acc: 0.7021\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8870\n",
      "Epoch 00146: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3429 - acc: 0.8871 - val_loss: 1.1957 - val_acc: 0.7079\n",
      "Epoch 147/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8856\n",
      "Epoch 00147: val_loss did not improve from 1.12160\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.3432 - acc: 0.8854 - val_loss: 1.2005 - val_acc: 0.7070\n",
      "\n",
      "1D_CNN_DO_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 261us/sample - loss: 1.1853 - acc: 0.6449\n",
      "Loss: 1.1853097205840415 Accuracy: 0.6448598\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.3765 - acc: 0.2160\n",
      "Epoch 00001: val_loss improved from inf to 1.94982, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/001-1.9498.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 2.3756 - acc: 0.2163 - val_loss: 1.9498 - val_acc: 0.3769\n",
      "Epoch 2/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8629 - acc: 0.3863\n",
      "Epoch 00002: val_loss improved from 1.94982 to 1.63934, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/002-1.6393.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.8619 - acc: 0.3868 - val_loss: 1.6393 - val_acc: 0.4838\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6639 - acc: 0.4657\n",
      "Epoch 00003: val_loss improved from 1.63934 to 1.51268, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/003-1.5127.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 1.6633 - acc: 0.4658 - val_loss: 1.5127 - val_acc: 0.5334\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5340 - acc: 0.5187\n",
      "Epoch 00004: val_loss improved from 1.51268 to 1.39550, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/004-1.3955.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 1.5340 - acc: 0.5189 - val_loss: 1.3955 - val_acc: 0.5805\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4375 - acc: 0.5531\n",
      "Epoch 00005: val_loss improved from 1.39550 to 1.33194, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/005-1.3319.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.4374 - acc: 0.5531 - val_loss: 1.3319 - val_acc: 0.5884\n",
      "Epoch 6/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3623 - acc: 0.5793\n",
      "Epoch 00006: val_loss improved from 1.33194 to 1.24894, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/006-1.2489.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 1.3630 - acc: 0.5794 - val_loss: 1.2489 - val_acc: 0.6268\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2972 - acc: 0.6037\n",
      "Epoch 00007: val_loss improved from 1.24894 to 1.19299, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/007-1.1930.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 1.2973 - acc: 0.6036 - val_loss: 1.1930 - val_acc: 0.6429\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2431 - acc: 0.6207\n",
      "Epoch 00008: val_loss improved from 1.19299 to 1.15500, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/008-1.1550.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 1.2432 - acc: 0.6206 - val_loss: 1.1550 - val_acc: 0.6546\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1953 - acc: 0.6368\n",
      "Epoch 00009: val_loss improved from 1.15500 to 1.11469, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/009-1.1147.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 1.1956 - acc: 0.6368 - val_loss: 1.1147 - val_acc: 0.6713\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1555 - acc: 0.6511\n",
      "Epoch 00010: val_loss improved from 1.11469 to 1.10845, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/010-1.1084.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 1.1557 - acc: 0.6511 - val_loss: 1.1084 - val_acc: 0.6683\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1148 - acc: 0.6632\n",
      "Epoch 00011: val_loss improved from 1.10845 to 1.06092, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/011-1.0609.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 1.1141 - acc: 0.6634 - val_loss: 1.0609 - val_acc: 0.6902\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0766 - acc: 0.6748\n",
      "Epoch 00012: val_loss improved from 1.06092 to 1.01340, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/012-1.0134.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 1.0766 - acc: 0.6747 - val_loss: 1.0134 - val_acc: 0.7046\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0453 - acc: 0.6871\n",
      "Epoch 00013: val_loss improved from 1.01340 to 0.98985, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/013-0.9899.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 1.0450 - acc: 0.6872 - val_loss: 0.9899 - val_acc: 0.7130\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0178 - acc: 0.6967\n",
      "Epoch 00014: val_loss did not improve from 0.98985\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.0180 - acc: 0.6967 - val_loss: 0.9910 - val_acc: 0.7095\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9887 - acc: 0.7054\n",
      "Epoch 00015: val_loss improved from 0.98985 to 0.95715, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/015-0.9571.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.9885 - acc: 0.7052 - val_loss: 0.9571 - val_acc: 0.7275\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9625 - acc: 0.7107\n",
      "Epoch 00016: val_loss improved from 0.95715 to 0.94408, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/016-0.9441.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.9624 - acc: 0.7107 - val_loss: 0.9441 - val_acc: 0.7181\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9331 - acc: 0.7231\n",
      "Epoch 00017: val_loss improved from 0.94408 to 0.91984, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/017-0.9198.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.9331 - acc: 0.7230 - val_loss: 0.9198 - val_acc: 0.7333\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9082 - acc: 0.7319\n",
      "Epoch 00018: val_loss improved from 0.91984 to 0.88507, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/018-0.8851.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.9073 - acc: 0.7321 - val_loss: 0.8851 - val_acc: 0.7428\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7379\n",
      "Epoch 00019: val_loss improved from 0.88507 to 0.87920, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/019-0.8792.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.8849 - acc: 0.7378 - val_loss: 0.8792 - val_acc: 0.7473\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8636 - acc: 0.7434\n",
      "Epoch 00020: val_loss improved from 0.87920 to 0.85945, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/020-0.8595.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.8637 - acc: 0.7434 - val_loss: 0.8595 - val_acc: 0.7435\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8373 - acc: 0.7546\n",
      "Epoch 00021: val_loss improved from 0.85945 to 0.84295, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/021-0.8429.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.8373 - acc: 0.7546 - val_loss: 0.8429 - val_acc: 0.7517\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8125 - acc: 0.7605\n",
      "Epoch 00022: val_loss improved from 0.84295 to 0.81759, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/022-0.8176.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.8126 - acc: 0.7605 - val_loss: 0.8176 - val_acc: 0.7636\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7641\n",
      "Epoch 00023: val_loss improved from 0.81759 to 0.80881, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/023-0.8088.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.7935 - acc: 0.7640 - val_loss: 0.8088 - val_acc: 0.7619\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.7734\n",
      "Epoch 00024: val_loss improved from 0.80881 to 0.80689, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/024-0.8069.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.7766 - acc: 0.7734 - val_loss: 0.8069 - val_acc: 0.7717\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7536 - acc: 0.7755\n",
      "Epoch 00025: val_loss improved from 0.80689 to 0.76695, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/025-0.7670.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.7536 - acc: 0.7754 - val_loss: 0.7670 - val_acc: 0.7778\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7348 - acc: 0.7825\n",
      "Epoch 00026: val_loss improved from 0.76695 to 0.75476, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/026-0.7548.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.7346 - acc: 0.7827 - val_loss: 0.7548 - val_acc: 0.7824\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7136 - acc: 0.7884\n",
      "Epoch 00027: val_loss improved from 0.75476 to 0.74108, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/027-0.7411.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.7136 - acc: 0.7883 - val_loss: 0.7411 - val_acc: 0.7899\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.7925\n",
      "Epoch 00028: val_loss improved from 0.74108 to 0.73191, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/028-0.7319.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.6997 - acc: 0.7924 - val_loss: 0.7319 - val_acc: 0.7918\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.7980\n",
      "Epoch 00029: val_loss improved from 0.73191 to 0.71019, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/029-0.7102.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6800 - acc: 0.7980 - val_loss: 0.7102 - val_acc: 0.7934\n",
      "Epoch 30/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.8024\n",
      "Epoch 00030: val_loss improved from 0.71019 to 0.70211, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/030-0.7021.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.6672 - acc: 0.8027 - val_loss: 0.7021 - val_acc: 0.7987\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.8065\n",
      "Epoch 00031: val_loss improved from 0.70211 to 0.69013, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/031-0.6901.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.6493 - acc: 0.8065 - val_loss: 0.6901 - val_acc: 0.8022\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6369 - acc: 0.8122\n",
      "Epoch 00032: val_loss did not improve from 0.69013\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.6370 - acc: 0.8121 - val_loss: 0.7092 - val_acc: 0.7943\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8143\n",
      "Epoch 00033: val_loss improved from 0.69013 to 0.67730, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/033-0.6773.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.6252 - acc: 0.8142 - val_loss: 0.6773 - val_acc: 0.8048\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6101 - acc: 0.8172\n",
      "Epoch 00034: val_loss did not improve from 0.67730\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.6101 - acc: 0.8172 - val_loss: 0.6793 - val_acc: 0.8085\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8217\n",
      "Epoch 00035: val_loss improved from 0.67730 to 0.65667, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/035-0.6567.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.5997 - acc: 0.8214 - val_loss: 0.6567 - val_acc: 0.8132\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5845 - acc: 0.8273\n",
      "Epoch 00036: val_loss improved from 0.65667 to 0.65335, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/036-0.6533.hdf5\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 0.5844 - acc: 0.8273 - val_loss: 0.6533 - val_acc: 0.8148\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8300\n",
      "Epoch 00037: val_loss improved from 0.65335 to 0.63337, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/037-0.6334.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.5732 - acc: 0.8300 - val_loss: 0.6334 - val_acc: 0.8220\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8332\n",
      "Epoch 00038: val_loss did not improve from 0.63337\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.5655 - acc: 0.8328 - val_loss: 0.6351 - val_acc: 0.8148\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.8340\n",
      "Epoch 00039: val_loss improved from 0.63337 to 0.62064, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/039-0.6206.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.5533 - acc: 0.8342 - val_loss: 0.6206 - val_acc: 0.8265\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8396\n",
      "Epoch 00040: val_loss improved from 0.62064 to 0.61975, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/040-0.6198.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.5435 - acc: 0.8395 - val_loss: 0.6198 - val_acc: 0.8260\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8427\n",
      "Epoch 00041: val_loss improved from 0.61975 to 0.61715, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/041-0.6171.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5307 - acc: 0.8427 - val_loss: 0.6171 - val_acc: 0.8262\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8452\n",
      "Epoch 00042: val_loss did not improve from 0.61715\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.5216 - acc: 0.8453 - val_loss: 0.6182 - val_acc: 0.8244\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8485\n",
      "Epoch 00043: val_loss improved from 0.61715 to 0.59426, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/043-0.5943.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.5076 - acc: 0.8485 - val_loss: 0.5943 - val_acc: 0.8351\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8510\n",
      "Epoch 00044: val_loss did not improve from 0.59426\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5026 - acc: 0.8511 - val_loss: 0.5958 - val_acc: 0.8339\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8529\n",
      "Epoch 00045: val_loss improved from 0.59426 to 0.59204, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/045-0.5920.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.4905 - acc: 0.8528 - val_loss: 0.5920 - val_acc: 0.8362\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8560\n",
      "Epoch 00046: val_loss improved from 0.59204 to 0.58867, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/046-0.5887.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.4850 - acc: 0.8559 - val_loss: 0.5887 - val_acc: 0.8367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.8569\n",
      "Epoch 00047: val_loss improved from 0.58867 to 0.57924, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/047-0.5792.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.4765 - acc: 0.8568 - val_loss: 0.5792 - val_acc: 0.8388\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8609\n",
      "Epoch 00048: val_loss did not improve from 0.57924\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4629 - acc: 0.8608 - val_loss: 0.5881 - val_acc: 0.8348\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8616\n",
      "Epoch 00049: val_loss improved from 0.57924 to 0.56216, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/049-0.5622.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4596 - acc: 0.8615 - val_loss: 0.5622 - val_acc: 0.8458\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4501 - acc: 0.8658\n",
      "Epoch 00050: val_loss did not improve from 0.56216\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.4501 - acc: 0.8659 - val_loss: 0.5696 - val_acc: 0.8395\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8659\n",
      "Epoch 00051: val_loss improved from 0.56216 to 0.54909, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/051-0.5491.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4464 - acc: 0.8659 - val_loss: 0.5491 - val_acc: 0.8484\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8684\n",
      "Epoch 00052: val_loss did not improve from 0.54909\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.4334 - acc: 0.8684 - val_loss: 0.5528 - val_acc: 0.8495\n",
      "Epoch 53/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8726\n",
      "Epoch 00053: val_loss improved from 0.54909 to 0.54389, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/053-0.5439.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.4280 - acc: 0.8728 - val_loss: 0.5439 - val_acc: 0.8530\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8705\n",
      "Epoch 00054: val_loss did not improve from 0.54389\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4262 - acc: 0.8704 - val_loss: 0.5450 - val_acc: 0.8505\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8759\n",
      "Epoch 00055: val_loss improved from 0.54389 to 0.53658, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/055-0.5366.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4123 - acc: 0.8759 - val_loss: 0.5366 - val_acc: 0.8512\n",
      "Epoch 56/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8749\n",
      "Epoch 00056: val_loss did not improve from 0.53658\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4117 - acc: 0.8748 - val_loss: 0.5492 - val_acc: 0.8558\n",
      "Epoch 57/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8783\n",
      "Epoch 00057: val_loss did not improve from 0.53658\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4052 - acc: 0.8785 - val_loss: 0.5391 - val_acc: 0.8537\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8800\n",
      "Epoch 00058: val_loss did not improve from 0.53658\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3972 - acc: 0.8797 - val_loss: 0.5585 - val_acc: 0.8472\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8802\n",
      "Epoch 00059: val_loss improved from 0.53658 to 0.53503, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/059-0.5350.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3955 - acc: 0.8802 - val_loss: 0.5350 - val_acc: 0.8544\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8820\n",
      "Epoch 00060: val_loss improved from 0.53503 to 0.52486, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/060-0.5249.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3904 - acc: 0.8820 - val_loss: 0.5249 - val_acc: 0.8584\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8817\n",
      "Epoch 00061: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3864 - acc: 0.8818 - val_loss: 0.5269 - val_acc: 0.8579\n",
      "Epoch 62/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8849\n",
      "Epoch 00062: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3782 - acc: 0.8849 - val_loss: 0.5341 - val_acc: 0.8600\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8834\n",
      "Epoch 00063: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.3775 - acc: 0.8834 - val_loss: 0.5289 - val_acc: 0.8598\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8865\n",
      "Epoch 00064: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3713 - acc: 0.8863 - val_loss: 0.5334 - val_acc: 0.8644\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8905\n",
      "Epoch 00065: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3646 - acc: 0.8905 - val_loss: 0.5306 - val_acc: 0.8593\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8901\n",
      "Epoch 00066: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3607 - acc: 0.8901 - val_loss: 0.5269 - val_acc: 0.8635\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8880\n",
      "Epoch 00067: val_loss did not improve from 0.52486\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.3605 - acc: 0.8879 - val_loss: 0.5305 - val_acc: 0.8567\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8896\n",
      "Epoch 00068: val_loss improved from 0.52486 to 0.51918, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/068-0.5192.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3585 - acc: 0.8897 - val_loss: 0.5192 - val_acc: 0.8609\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8927\n",
      "Epoch 00069: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3482 - acc: 0.8927 - val_loss: 0.5309 - val_acc: 0.8605\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8923\n",
      "Epoch 00070: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3469 - acc: 0.8923 - val_loss: 0.5199 - val_acc: 0.8672\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8950\n",
      "Epoch 00071: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3413 - acc: 0.8951 - val_loss: 0.5289 - val_acc: 0.8642\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8945\n",
      "Epoch 00072: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3406 - acc: 0.8946 - val_loss: 0.5212 - val_acc: 0.8651\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8985\n",
      "Epoch 00073: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3345 - acc: 0.8985 - val_loss: 0.5354 - val_acc: 0.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8978\n",
      "Epoch 00074: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3311 - acc: 0.8977 - val_loss: 0.5276 - val_acc: 0.8668\n",
      "Epoch 75/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8983\n",
      "Epoch 00075: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3310 - acc: 0.8982 - val_loss: 0.5289 - val_acc: 0.8658\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8989\n",
      "Epoch 00076: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3257 - acc: 0.8988 - val_loss: 0.5252 - val_acc: 0.8642\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9001\n",
      "Epoch 00077: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3221 - acc: 0.9001 - val_loss: 0.5199 - val_acc: 0.8707\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9009\n",
      "Epoch 00078: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3203 - acc: 0.9010 - val_loss: 0.5581 - val_acc: 0.8539\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9017\n",
      "Epoch 00079: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3139 - acc: 0.9017 - val_loss: 0.5226 - val_acc: 0.8670\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9034\n",
      "Epoch 00080: val_loss improved from 0.51918 to 0.51751, saving model to model/checkpoint/1D_CNN_DO_3_only_conv_checkpoint/080-0.5175.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3132 - acc: 0.9034 - val_loss: 0.5175 - val_acc: 0.8684\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9068\n",
      "Epoch 00081: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3046 - acc: 0.9068 - val_loss: 0.5261 - val_acc: 0.8640\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9044\n",
      "Epoch 00082: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3069 - acc: 0.9045 - val_loss: 0.5317 - val_acc: 0.8605\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9045\n",
      "Epoch 00083: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3064 - acc: 0.9045 - val_loss: 0.5448 - val_acc: 0.8621\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9044\n",
      "Epoch 00084: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3068 - acc: 0.9044 - val_loss: 0.5238 - val_acc: 0.8661\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9055\n",
      "Epoch 00085: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2994 - acc: 0.9055 - val_loss: 0.5198 - val_acc: 0.8696\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9067\n",
      "Epoch 00086: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2969 - acc: 0.9067 - val_loss: 0.5258 - val_acc: 0.8675\n",
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9061\n",
      "Epoch 00087: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2989 - acc: 0.9060 - val_loss: 0.5214 - val_acc: 0.8689\n",
      "Epoch 88/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9070\n",
      "Epoch 00088: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2936 - acc: 0.9068 - val_loss: 0.5264 - val_acc: 0.8656\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9086\n",
      "Epoch 00089: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2891 - acc: 0.9085 - val_loss: 0.5212 - val_acc: 0.8740\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9099\n",
      "Epoch 00090: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2847 - acc: 0.9100 - val_loss: 0.5236 - val_acc: 0.8696\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9110\n",
      "Epoch 00091: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2872 - acc: 0.9109 - val_loss: 0.5234 - val_acc: 0.8675\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9117\n",
      "Epoch 00092: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2794 - acc: 0.9118 - val_loss: 0.5205 - val_acc: 0.8698\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9121\n",
      "Epoch 00093: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2781 - acc: 0.9120 - val_loss: 0.5269 - val_acc: 0.8656\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9148\n",
      "Epoch 00094: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2771 - acc: 0.9148 - val_loss: 0.5336 - val_acc: 0.8656\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9142\n",
      "Epoch 00095: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2744 - acc: 0.9142 - val_loss: 0.5274 - val_acc: 0.8675\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9129\n",
      "Epoch 00096: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2731 - acc: 0.9128 - val_loss: 0.5294 - val_acc: 0.8719\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9117\n",
      "Epoch 00097: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.2757 - acc: 0.9117 - val_loss: 0.5307 - val_acc: 0.8705\n",
      "Epoch 98/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9152\n",
      "Epoch 00098: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.2681 - acc: 0.9153 - val_loss: 0.5287 - val_acc: 0.8682\n",
      "Epoch 99/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9173\n",
      "Epoch 00099: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2612 - acc: 0.9173 - val_loss: 0.5357 - val_acc: 0.8693\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9157\n",
      "Epoch 00100: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2668 - acc: 0.9157 - val_loss: 0.5309 - val_acc: 0.8710\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9172\n",
      "Epoch 00101: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2616 - acc: 0.9172 - val_loss: 0.5307 - val_acc: 0.8742\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9162\n",
      "Epoch 00102: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2612 - acc: 0.9162 - val_loss: 0.5534 - val_acc: 0.8644\n",
      "Epoch 103/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9179\n",
      "Epoch 00103: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2579 - acc: 0.9180 - val_loss: 0.5303 - val_acc: 0.8740\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9182\n",
      "Epoch 00104: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2586 - acc: 0.9182 - val_loss: 0.5375 - val_acc: 0.8665\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9186\n",
      "Epoch 00105: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2542 - acc: 0.9186 - val_loss: 0.5479 - val_acc: 0.8642\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9207\n",
      "Epoch 00106: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2507 - acc: 0.9207 - val_loss: 0.5234 - val_acc: 0.8698\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9207\n",
      "Epoch 00107: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2498 - acc: 0.9207 - val_loss: 0.5262 - val_acc: 0.8735\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9192\n",
      "Epoch 00108: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2545 - acc: 0.9192 - val_loss: 0.5399 - val_acc: 0.8710\n",
      "Epoch 109/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9204\n",
      "Epoch 00109: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2478 - acc: 0.9203 - val_loss: 0.5239 - val_acc: 0.8710\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9212\n",
      "Epoch 00110: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2467 - acc: 0.9212 - val_loss: 0.5376 - val_acc: 0.8712\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9232\n",
      "Epoch 00111: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 0.2434 - acc: 0.9231 - val_loss: 0.5400 - val_acc: 0.8670\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9236\n",
      "Epoch 00112: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 9s 258us/sample - loss: 0.2397 - acc: 0.9237 - val_loss: 0.5473 - val_acc: 0.8696\n",
      "Epoch 113/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9232\n",
      "Epoch 00113: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2404 - acc: 0.9230 - val_loss: 0.5441 - val_acc: 0.8698\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9248\n",
      "Epoch 00114: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2380 - acc: 0.9247 - val_loss: 0.5337 - val_acc: 0.8733\n",
      "Epoch 115/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9242\n",
      "Epoch 00115: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 9s 258us/sample - loss: 0.2409 - acc: 0.9244 - val_loss: 0.5494 - val_acc: 0.8730\n",
      "Epoch 116/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9213\n",
      "Epoch 00116: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2421 - acc: 0.9214 - val_loss: 0.5408 - val_acc: 0.8647\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9255\n",
      "Epoch 00117: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2324 - acc: 0.9255 - val_loss: 0.5376 - val_acc: 0.8733\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9253\n",
      "Epoch 00118: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2342 - acc: 0.9252 - val_loss: 0.5389 - val_acc: 0.8744\n",
      "Epoch 119/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9256\n",
      "Epoch 00119: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2344 - acc: 0.9256 - val_loss: 0.5623 - val_acc: 0.8661\n",
      "Epoch 120/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9263\n",
      "Epoch 00120: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2304 - acc: 0.9263 - val_loss: 0.5504 - val_acc: 0.8733\n",
      "Epoch 121/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9251\n",
      "Epoch 00121: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2318 - acc: 0.9250 - val_loss: 0.5344 - val_acc: 0.8679\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9267\n",
      "Epoch 00122: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2273 - acc: 0.9267 - val_loss: 0.5430 - val_acc: 0.8714\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9268\n",
      "Epoch 00123: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2264 - acc: 0.9267 - val_loss: 0.5502 - val_acc: 0.8728\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9268\n",
      "Epoch 00124: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2286 - acc: 0.9268 - val_loss: 0.5394 - val_acc: 0.8710\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9290\n",
      "Epoch 00125: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2217 - acc: 0.9290 - val_loss: 0.5422 - val_acc: 0.8712\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9267\n",
      "Epoch 00126: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2244 - acc: 0.9268 - val_loss: 0.5434 - val_acc: 0.8728\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9289\n",
      "Epoch 00127: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2207 - acc: 0.9288 - val_loss: 0.5426 - val_acc: 0.8747\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9293\n",
      "Epoch 00128: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2201 - acc: 0.9292 - val_loss: 0.5410 - val_acc: 0.8740\n",
      "Epoch 129/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9311\n",
      "Epoch 00129: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2123 - acc: 0.9311 - val_loss: 0.5553 - val_acc: 0.8747\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9287\n",
      "Epoch 00130: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2174 - acc: 0.9286 - val_loss: 0.5429 - val_acc: 0.8717\n",
      "Epoch 131/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9295\n",
      "Epoch 00131: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2164 - acc: 0.9293 - val_loss: 0.5537 - val_acc: 0.8712\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9298\n",
      "Epoch 00132: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2160 - acc: 0.9298 - val_loss: 0.5412 - val_acc: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9331\n",
      "Epoch 00133: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2107 - acc: 0.9332 - val_loss: 0.5451 - val_acc: 0.8737\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9317\n",
      "Epoch 00134: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2115 - acc: 0.9317 - val_loss: 0.5528 - val_acc: 0.8705\n",
      "Epoch 135/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9319\n",
      "Epoch 00135: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2116 - acc: 0.9318 - val_loss: 0.5518 - val_acc: 0.8735\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9312\n",
      "Epoch 00136: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2113 - acc: 0.9313 - val_loss: 0.5611 - val_acc: 0.8717\n",
      "Epoch 137/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9311\n",
      "Epoch 00137: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2118 - acc: 0.9312 - val_loss: 0.5538 - val_acc: 0.8726\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9342\n",
      "Epoch 00138: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2045 - acc: 0.9341 - val_loss: 0.5493 - val_acc: 0.8737\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9329\n",
      "Epoch 00139: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2069 - acc: 0.9330 - val_loss: 0.5491 - val_acc: 0.8728\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9326\n",
      "Epoch 00140: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2076 - acc: 0.9325 - val_loss: 0.5531 - val_acc: 0.8707\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9359\n",
      "Epoch 00141: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2006 - acc: 0.9359 - val_loss: 0.5569 - val_acc: 0.8719\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9333\n",
      "Epoch 00142: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2080 - acc: 0.9333 - val_loss: 0.5420 - val_acc: 0.8737\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9327\n",
      "Epoch 00143: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2055 - acc: 0.9327 - val_loss: 0.5498 - val_acc: 0.8754\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9363\n",
      "Epoch 00144: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1929 - acc: 0.9364 - val_loss: 0.5477 - val_acc: 0.8749\n",
      "Epoch 145/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9341\n",
      "Epoch 00145: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2002 - acc: 0.9340 - val_loss: 0.5531 - val_acc: 0.8742\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9347\n",
      "Epoch 00146: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2003 - acc: 0.9347 - val_loss: 0.5568 - val_acc: 0.8763\n",
      "Epoch 147/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9352\n",
      "Epoch 00147: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1986 - acc: 0.9352 - val_loss: 0.5656 - val_acc: 0.8696\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9360\n",
      "Epoch 00148: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1981 - acc: 0.9360 - val_loss: 0.5569 - val_acc: 0.8742\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9345\n",
      "Epoch 00149: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1997 - acc: 0.9345 - val_loss: 0.5551 - val_acc: 0.8735\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9362\n",
      "Epoch 00150: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1954 - acc: 0.9363 - val_loss: 0.5573 - val_acc: 0.8714\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9371\n",
      "Epoch 00151: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1910 - acc: 0.9371 - val_loss: 0.5692 - val_acc: 0.8719\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9374\n",
      "Epoch 00152: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1935 - acc: 0.9374 - val_loss: 0.5464 - val_acc: 0.8719\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9358\n",
      "Epoch 00153: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1952 - acc: 0.9358 - val_loss: 0.5525 - val_acc: 0.8700\n",
      "Epoch 154/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9372\n",
      "Epoch 00154: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1946 - acc: 0.9372 - val_loss: 0.5759 - val_acc: 0.8642\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9382\n",
      "Epoch 00155: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1902 - acc: 0.9382 - val_loss: 0.5585 - val_acc: 0.8719\n",
      "Epoch 156/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9376\n",
      "Epoch 00156: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1916 - acc: 0.9375 - val_loss: 0.5746 - val_acc: 0.8707\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9377\n",
      "Epoch 00157: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1924 - acc: 0.9377 - val_loss: 0.5530 - val_acc: 0.8779\n",
      "Epoch 158/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9388\n",
      "Epoch 00158: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1893 - acc: 0.9390 - val_loss: 0.5640 - val_acc: 0.8712\n",
      "Epoch 159/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9411\n",
      "Epoch 00159: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1834 - acc: 0.9410 - val_loss: 0.5571 - val_acc: 0.8735\n",
      "Epoch 160/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9408\n",
      "Epoch 00160: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1853 - acc: 0.9409 - val_loss: 0.5658 - val_acc: 0.8726\n",
      "Epoch 161/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9394\n",
      "Epoch 00161: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1871 - acc: 0.9394 - val_loss: 0.5689 - val_acc: 0.8717\n",
      "Epoch 162/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9413\n",
      "Epoch 00162: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1819 - acc: 0.9411 - val_loss: 0.5605 - val_acc: 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9406\n",
      "Epoch 00163: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1850 - acc: 0.9406 - val_loss: 0.5600 - val_acc: 0.8705\n",
      "Epoch 164/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9421\n",
      "Epoch 00164: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1787 - acc: 0.9420 - val_loss: 0.5643 - val_acc: 0.8710\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9383\n",
      "Epoch 00165: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1868 - acc: 0.9383 - val_loss: 0.5662 - val_acc: 0.8703\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9402\n",
      "Epoch 00166: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1821 - acc: 0.9403 - val_loss: 0.5640 - val_acc: 0.8691\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9415\n",
      "Epoch 00167: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1798 - acc: 0.9414 - val_loss: 0.5649 - val_acc: 0.8749\n",
      "Epoch 168/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9408\n",
      "Epoch 00168: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1824 - acc: 0.9408 - val_loss: 0.5577 - val_acc: 0.8728\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9435\n",
      "Epoch 00169: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1772 - acc: 0.9435 - val_loss: 0.5583 - val_acc: 0.8721\n",
      "Epoch 170/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9432\n",
      "Epoch 00170: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1733 - acc: 0.9432 - val_loss: 0.5551 - val_acc: 0.8756\n",
      "Epoch 171/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9424\n",
      "Epoch 00171: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1746 - acc: 0.9424 - val_loss: 0.5620 - val_acc: 0.8726\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9447\n",
      "Epoch 00172: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1727 - acc: 0.9448 - val_loss: 0.5624 - val_acc: 0.8768\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9432\n",
      "Epoch 00173: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1747 - acc: 0.9432 - val_loss: 0.5515 - val_acc: 0.8768\n",
      "Epoch 174/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9427\n",
      "Epoch 00174: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1735 - acc: 0.9427 - val_loss: 0.5664 - val_acc: 0.8742\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9419\n",
      "Epoch 00175: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1755 - acc: 0.9419 - val_loss: 0.5516 - val_acc: 0.8777\n",
      "Epoch 176/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9455\n",
      "Epoch 00176: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1715 - acc: 0.9454 - val_loss: 0.5616 - val_acc: 0.8742\n",
      "Epoch 177/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9437\n",
      "Epoch 00177: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1762 - acc: 0.9437 - val_loss: 0.5656 - val_acc: 0.8744\n",
      "Epoch 178/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9439\n",
      "Epoch 00178: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1703 - acc: 0.9438 - val_loss: 0.5573 - val_acc: 0.8772\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9424\n",
      "Epoch 00179: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1751 - acc: 0.9425 - val_loss: 0.5564 - val_acc: 0.8754\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9443\n",
      "Epoch 00180: val_loss did not improve from 0.51751\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1688 - acc: 0.9442 - val_loss: 0.5822 - val_acc: 0.8714\n",
      "\n",
      "1D_CNN_DO_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 274us/sample - loss: 0.6292 - acc: 0.8239\n",
      "Loss: 0.6292192858698955 Accuracy: 0.8238837\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3652 - acc: 0.1938\n",
      "Epoch 00001: val_loss improved from inf to 2.00466, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/001-2.0047.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 2.3647 - acc: 0.1941 - val_loss: 2.0047 - val_acc: 0.3739\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.8197 - acc: 0.3998\n",
      "Epoch 00002: val_loss improved from 2.00466 to 1.51414, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/002-1.5141.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 1.8197 - acc: 0.3999 - val_loss: 1.5141 - val_acc: 0.5355\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5386 - acc: 0.5029\n",
      "Epoch 00003: val_loss improved from 1.51414 to 1.31186, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/003-1.3119.hdf5\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 1.5384 - acc: 0.5030 - val_loss: 1.3119 - val_acc: 0.6066\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.5682\n",
      "Epoch 00004: val_loss improved from 1.31186 to 1.11763, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/004-1.1176.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 1.3525 - acc: 0.5683 - val_loss: 1.1176 - val_acc: 0.6862\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2146 - acc: 0.6180\n",
      "Epoch 00005: val_loss improved from 1.11763 to 1.00092, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/005-1.0009.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 1.2139 - acc: 0.6181 - val_loss: 1.0009 - val_acc: 0.7079\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1032 - acc: 0.6555\n",
      "Epoch 00006: val_loss improved from 1.00092 to 0.89001, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/006-0.8900.hdf5\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 1.1031 - acc: 0.6555 - val_loss: 0.8900 - val_acc: 0.7447\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0216 - acc: 0.6842\n",
      "Epoch 00007: val_loss improved from 0.89001 to 0.84522, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/007-0.8452.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 1.0212 - acc: 0.6843 - val_loss: 0.8452 - val_acc: 0.7494\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.7066\n",
      "Epoch 00008: val_loss improved from 0.84522 to 0.78502, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/008-0.7850.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.9442 - acc: 0.7068 - val_loss: 0.7850 - val_acc: 0.7631\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7250\n",
      "Epoch 00009: val_loss improved from 0.78502 to 0.74233, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/009-0.7423.hdf5\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.8861 - acc: 0.7251 - val_loss: 0.7423 - val_acc: 0.7871\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7410\n",
      "Epoch 00010: val_loss improved from 0.74233 to 0.68376, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/010-0.6838.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.8351 - acc: 0.7410 - val_loss: 0.6838 - val_acc: 0.8078\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7542\n",
      "Epoch 00011: val_loss improved from 0.68376 to 0.67494, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/011-0.6749.hdf5\n",
      "36805/36805 [==============================] - 10s 275us/sample - loss: 0.7929 - acc: 0.7544 - val_loss: 0.6749 - val_acc: 0.8025\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7663\n",
      "Epoch 00012: val_loss improved from 0.67494 to 0.63825, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/012-0.6383.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.7527 - acc: 0.7663 - val_loss: 0.6383 - val_acc: 0.8123\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7185 - acc: 0.7780\n",
      "Epoch 00013: val_loss improved from 0.63825 to 0.59286, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/013-0.5929.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.7186 - acc: 0.7779 - val_loss: 0.5929 - val_acc: 0.8239\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6849 - acc: 0.7883\n",
      "Epoch 00014: val_loss improved from 0.59286 to 0.58509, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/014-0.5851.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.6844 - acc: 0.7885 - val_loss: 0.5851 - val_acc: 0.8279\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7950\n",
      "Epoch 00015: val_loss improved from 0.58509 to 0.54553, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/015-0.5455.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.6572 - acc: 0.7952 - val_loss: 0.5455 - val_acc: 0.8416\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.8029\n",
      "Epoch 00016: val_loss improved from 0.54553 to 0.52241, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/016-0.5224.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 0.6345 - acc: 0.8030 - val_loss: 0.5224 - val_acc: 0.8479\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8109\n",
      "Epoch 00017: val_loss improved from 0.52241 to 0.50881, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/017-0.5088.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.6071 - acc: 0.8109 - val_loss: 0.5088 - val_acc: 0.8537\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8164\n",
      "Epoch 00018: val_loss improved from 0.50881 to 0.48178, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/018-0.4818.hdf5\n",
      "36805/36805 [==============================] - 10s 273us/sample - loss: 0.5899 - acc: 0.8163 - val_loss: 0.4818 - val_acc: 0.8663\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.8250\n",
      "Epoch 00019: val_loss did not improve from 0.48178\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.5609 - acc: 0.8252 - val_loss: 0.4852 - val_acc: 0.8609\n",
      "Epoch 20/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8315\n",
      "Epoch 00020: val_loss improved from 0.48178 to 0.46771, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/020-0.4677.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.5435 - acc: 0.8318 - val_loss: 0.4677 - val_acc: 0.8651\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8387\n",
      "Epoch 00021: val_loss improved from 0.46771 to 0.42373, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/021-0.4237.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.5200 - acc: 0.8387 - val_loss: 0.4237 - val_acc: 0.8828\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8453\n",
      "Epoch 00022: val_loss improved from 0.42373 to 0.40814, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/022-0.4081.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.4945 - acc: 0.8452 - val_loss: 0.4081 - val_acc: 0.8873\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8523\n",
      "Epoch 00023: val_loss improved from 0.40814 to 0.39657, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/023-0.3966.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.4770 - acc: 0.8524 - val_loss: 0.3966 - val_acc: 0.8889\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8555\n",
      "Epoch 00024: val_loss improved from 0.39657 to 0.37916, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/024-0.3792.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.4572 - acc: 0.8555 - val_loss: 0.3792 - val_acc: 0.8973\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8632\n",
      "Epoch 00025: val_loss improved from 0.37916 to 0.36768, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/025-0.3677.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.4409 - acc: 0.8631 - val_loss: 0.3677 - val_acc: 0.8989\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8660\n",
      "Epoch 00026: val_loss improved from 0.36768 to 0.36081, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/026-0.3608.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.4307 - acc: 0.8660 - val_loss: 0.3608 - val_acc: 0.9010\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8719\n",
      "Epoch 00027: val_loss improved from 0.36081 to 0.34581, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/027-0.3458.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.4175 - acc: 0.8719 - val_loss: 0.3458 - val_acc: 0.9057\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8726\n",
      "Epoch 00028: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.4035 - acc: 0.8725 - val_loss: 0.3463 - val_acc: 0.9045\n",
      "Epoch 29/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8760\n",
      "Epoch 00029: val_loss improved from 0.34581 to 0.32546, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/029-0.3255.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.3944 - acc: 0.8759 - val_loss: 0.3255 - val_acc: 0.9117\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8815\n",
      "Epoch 00030: val_loss did not improve from 0.32546\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.3781 - acc: 0.8815 - val_loss: 0.3324 - val_acc: 0.9066\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3667 - acc: 0.8849\n",
      "Epoch 00031: val_loss improved from 0.32546 to 0.32116, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/031-0.3212.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.3664 - acc: 0.8850 - val_loss: 0.3212 - val_acc: 0.9117\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8878\n",
      "Epoch 00032: val_loss did not improve from 0.32116\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.3590 - acc: 0.8878 - val_loss: 0.3309 - val_acc: 0.9103\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8895\n",
      "Epoch 00033: val_loss improved from 0.32116 to 0.30740, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/033-0.3074.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.3518 - acc: 0.8895 - val_loss: 0.3074 - val_acc: 0.9143\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8918\n",
      "Epoch 00034: val_loss did not improve from 0.30740\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.3403 - acc: 0.8918 - val_loss: 0.3075 - val_acc: 0.9157\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8938\n",
      "Epoch 00035: val_loss improved from 0.30740 to 0.30513, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/035-0.3051.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.3334 - acc: 0.8938 - val_loss: 0.3051 - val_acc: 0.9175\n",
      "Epoch 36/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8962\n",
      "Epoch 00036: val_loss improved from 0.30513 to 0.28919, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/036-0.2892.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.3287 - acc: 0.8961 - val_loss: 0.2892 - val_acc: 0.9227\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.8991\n",
      "Epoch 00037: val_loss improved from 0.28919 to 0.28582, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/037-0.2858.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.3186 - acc: 0.8991 - val_loss: 0.2858 - val_acc: 0.9245\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9015\n",
      "Epoch 00038: val_loss did not improve from 0.28582\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.3079 - acc: 0.9015 - val_loss: 0.2955 - val_acc: 0.9215\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9014\n",
      "Epoch 00039: val_loss improved from 0.28582 to 0.27633, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/039-0.2763.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.3069 - acc: 0.9014 - val_loss: 0.2763 - val_acc: 0.9297\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9052\n",
      "Epoch 00040: val_loss did not improve from 0.27633\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2971 - acc: 0.9051 - val_loss: 0.2907 - val_acc: 0.9213\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9082\n",
      "Epoch 00041: val_loss improved from 0.27633 to 0.27387, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/041-0.2739.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2885 - acc: 0.9082 - val_loss: 0.2739 - val_acc: 0.9266\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9104\n",
      "Epoch 00042: val_loss did not improve from 0.27387\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2852 - acc: 0.9103 - val_loss: 0.2740 - val_acc: 0.9306\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9112\n",
      "Epoch 00043: val_loss improved from 0.27387 to 0.27287, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/043-0.2729.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2807 - acc: 0.9112 - val_loss: 0.2729 - val_acc: 0.9283\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9134\n",
      "Epoch 00044: val_loss improved from 0.27287 to 0.26820, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/044-0.2682.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.2724 - acc: 0.9133 - val_loss: 0.2682 - val_acc: 0.9290\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9146\n",
      "Epoch 00045: val_loss improved from 0.26820 to 0.26415, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/045-0.2641.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.2730 - acc: 0.9146 - val_loss: 0.2641 - val_acc: 0.9320\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9143\n",
      "Epoch 00046: val_loss did not improve from 0.26415\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2661 - acc: 0.9144 - val_loss: 0.2656 - val_acc: 0.9287\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9153\n",
      "Epoch 00047: val_loss did not improve from 0.26415\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2632 - acc: 0.9153 - val_loss: 0.2766 - val_acc: 0.9245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9152\n",
      "Epoch 00048: val_loss did not improve from 0.26415\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2608 - acc: 0.9151 - val_loss: 0.2663 - val_acc: 0.9334\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9182\n",
      "Epoch 00049: val_loss improved from 0.26415 to 0.26240, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/049-0.2624.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2573 - acc: 0.9182 - val_loss: 0.2624 - val_acc: 0.9287\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9192\n",
      "Epoch 00050: val_loss improved from 0.26240 to 0.25848, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/050-0.2585.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2524 - acc: 0.9191 - val_loss: 0.2585 - val_acc: 0.9334\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9216\n",
      "Epoch 00051: val_loss improved from 0.25848 to 0.25786, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/051-0.2579.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2447 - acc: 0.9216 - val_loss: 0.2579 - val_acc: 0.9306\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9229\n",
      "Epoch 00052: val_loss did not improve from 0.25786\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2390 - acc: 0.9229 - val_loss: 0.2598 - val_acc: 0.9311\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9253\n",
      "Epoch 00053: val_loss improved from 0.25786 to 0.25155, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/053-0.2515.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2364 - acc: 0.9253 - val_loss: 0.2515 - val_acc: 0.9324\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9225\n",
      "Epoch 00054: val_loss did not improve from 0.25155\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2373 - acc: 0.9225 - val_loss: 0.2548 - val_acc: 0.9322\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9267\n",
      "Epoch 00055: val_loss improved from 0.25155 to 0.24977, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/055-0.2498.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2285 - acc: 0.9267 - val_loss: 0.2498 - val_acc: 0.9373\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9258\n",
      "Epoch 00056: val_loss did not improve from 0.24977\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2273 - acc: 0.9258 - val_loss: 0.2579 - val_acc: 0.9327\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9287\n",
      "Epoch 00057: val_loss improved from 0.24977 to 0.24725, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/057-0.2473.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2221 - acc: 0.9286 - val_loss: 0.2473 - val_acc: 0.9343\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9295\n",
      "Epoch 00058: val_loss did not improve from 0.24725\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2205 - acc: 0.9293 - val_loss: 0.2547 - val_acc: 0.9343\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9303\n",
      "Epoch 00059: val_loss did not improve from 0.24725\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2165 - acc: 0.9303 - val_loss: 0.2500 - val_acc: 0.9357\n",
      "Epoch 60/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9290\n",
      "Epoch 00060: val_loss improved from 0.24725 to 0.24492, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/060-0.2449.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2151 - acc: 0.9290 - val_loss: 0.2449 - val_acc: 0.9324\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9312\n",
      "Epoch 00061: val_loss did not improve from 0.24492\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2102 - acc: 0.9312 - val_loss: 0.2550 - val_acc: 0.9317\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9321\n",
      "Epoch 00062: val_loss improved from 0.24492 to 0.24334, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/062-0.2433.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2072 - acc: 0.9321 - val_loss: 0.2433 - val_acc: 0.9350\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9324\n",
      "Epoch 00063: val_loss did not improve from 0.24334\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2052 - acc: 0.9324 - val_loss: 0.2590 - val_acc: 0.9299\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9334\n",
      "Epoch 00064: val_loss did not improve from 0.24334\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2023 - acc: 0.9333 - val_loss: 0.2440 - val_acc: 0.9364\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9352\n",
      "Epoch 00065: val_loss did not improve from 0.24334\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1979 - acc: 0.9352 - val_loss: 0.2484 - val_acc: 0.9373\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9350\n",
      "Epoch 00066: val_loss did not improve from 0.24334\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2003 - acc: 0.9349 - val_loss: 0.2479 - val_acc: 0.9317\n",
      "Epoch 67/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9360\n",
      "Epoch 00067: val_loss did not improve from 0.24334\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1951 - acc: 0.9362 - val_loss: 0.2464 - val_acc: 0.9364\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9370\n",
      "Epoch 00068: val_loss improved from 0.24334 to 0.24314, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/068-0.2431.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1928 - acc: 0.9369 - val_loss: 0.2431 - val_acc: 0.9378\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9366\n",
      "Epoch 00069: val_loss did not improve from 0.24314\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1922 - acc: 0.9366 - val_loss: 0.2434 - val_acc: 0.9378\n",
      "Epoch 70/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9388\n",
      "Epoch 00070: val_loss improved from 0.24314 to 0.24066, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/070-0.2407.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1873 - acc: 0.9385 - val_loss: 0.2407 - val_acc: 0.9376\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9386\n",
      "Epoch 00071: val_loss did not improve from 0.24066\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1858 - acc: 0.9386 - val_loss: 0.2455 - val_acc: 0.9362\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9398\n",
      "Epoch 00072: val_loss did not improve from 0.24066\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1836 - acc: 0.9398 - val_loss: 0.2625 - val_acc: 0.9285\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9404\n",
      "Epoch 00073: val_loss did not improve from 0.24066\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1813 - acc: 0.9405 - val_loss: 0.2530 - val_acc: 0.9362\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9417\n",
      "Epoch 00074: val_loss did not improve from 0.24066\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1785 - acc: 0.9416 - val_loss: 0.2450 - val_acc: 0.9366\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9407\n",
      "Epoch 00075: val_loss improved from 0.24066 to 0.23887, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/075-0.2389.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1782 - acc: 0.9408 - val_loss: 0.2389 - val_acc: 0.9373\n",
      "Epoch 76/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9416\n",
      "Epoch 00076: val_loss improved from 0.23887 to 0.23872, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/076-0.2387.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1758 - acc: 0.9414 - val_loss: 0.2387 - val_acc: 0.9390\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9411\n",
      "Epoch 00077: val_loss improved from 0.23872 to 0.23797, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/077-0.2380.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1785 - acc: 0.9410 - val_loss: 0.2380 - val_acc: 0.9366\n",
      "Epoch 78/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9429\n",
      "Epoch 00078: val_loss did not improve from 0.23797\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1712 - acc: 0.9430 - val_loss: 0.2536 - val_acc: 0.9331\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9445\n",
      "Epoch 00079: val_loss improved from 0.23797 to 0.23317, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/079-0.2332.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1693 - acc: 0.9446 - val_loss: 0.2332 - val_acc: 0.9390\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9447\n",
      "Epoch 00080: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1657 - acc: 0.9447 - val_loss: 0.2348 - val_acc: 0.9385\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9449\n",
      "Epoch 00081: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1660 - acc: 0.9449 - val_loss: 0.2427 - val_acc: 0.9357\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9448\n",
      "Epoch 00082: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1686 - acc: 0.9448 - val_loss: 0.2500 - val_acc: 0.9371\n",
      "Epoch 83/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9461\n",
      "Epoch 00083: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1651 - acc: 0.9460 - val_loss: 0.2425 - val_acc: 0.9387\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9468\n",
      "Epoch 00084: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1609 - acc: 0.9467 - val_loss: 0.2347 - val_acc: 0.9373\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9468\n",
      "Epoch 00085: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1579 - acc: 0.9469 - val_loss: 0.2365 - val_acc: 0.9373\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9474\n",
      "Epoch 00086: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1610 - acc: 0.9474 - val_loss: 0.2364 - val_acc: 0.9399\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9471\n",
      "Epoch 00087: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1571 - acc: 0.9472 - val_loss: 0.2412 - val_acc: 0.9383\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9468\n",
      "Epoch 00088: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1571 - acc: 0.9467 - val_loss: 0.2369 - val_acc: 0.9383\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9484\n",
      "Epoch 00089: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1561 - acc: 0.9484 - val_loss: 0.2447 - val_acc: 0.9352\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9514\n",
      "Epoch 00090: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1493 - acc: 0.9514 - val_loss: 0.2407 - val_acc: 0.9392\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9512\n",
      "Epoch 00091: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1481 - acc: 0.9512 - val_loss: 0.2345 - val_acc: 0.9387\n",
      "Epoch 92/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9497\n",
      "Epoch 00092: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1515 - acc: 0.9496 - val_loss: 0.2357 - val_acc: 0.9425\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9510\n",
      "Epoch 00093: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1459 - acc: 0.9510 - val_loss: 0.2419 - val_acc: 0.9376\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9501\n",
      "Epoch 00094: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1498 - acc: 0.9501 - val_loss: 0.2401 - val_acc: 0.9376\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9489\n",
      "Epoch 00095: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1481 - acc: 0.9488 - val_loss: 0.2453 - val_acc: 0.9373\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9507\n",
      "Epoch 00096: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1460 - acc: 0.9507 - val_loss: 0.2393 - val_acc: 0.9408\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9530\n",
      "Epoch 00097: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1434 - acc: 0.9530 - val_loss: 0.2376 - val_acc: 0.9376\n",
      "Epoch 98/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9530\n",
      "Epoch 00098: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1428 - acc: 0.9530 - val_loss: 0.2387 - val_acc: 0.9390\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9530\n",
      "Epoch 00099: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1398 - acc: 0.9531 - val_loss: 0.2433 - val_acc: 0.9369\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9539\n",
      "Epoch 00100: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1389 - acc: 0.9539 - val_loss: 0.2353 - val_acc: 0.9385\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9526\n",
      "Epoch 00101: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1379 - acc: 0.9525 - val_loss: 0.2459 - val_acc: 0.9369\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9534\n",
      "Epoch 00102: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1358 - acc: 0.9535 - val_loss: 0.2403 - val_acc: 0.9404\n",
      "Epoch 103/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9545\n",
      "Epoch 00103: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1363 - acc: 0.9547 - val_loss: 0.2388 - val_acc: 0.9390\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9566\n",
      "Epoch 00104: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1332 - acc: 0.9565 - val_loss: 0.2481 - val_acc: 0.9401\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9550\n",
      "Epoch 00105: val_loss did not improve from 0.23317\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1336 - acc: 0.9551 - val_loss: 0.2397 - val_acc: 0.9387\n",
      "Epoch 106/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9566\n",
      "Epoch 00106: val_loss improved from 0.23317 to 0.23263, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/106-0.2326.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1311 - acc: 0.9566 - val_loss: 0.2326 - val_acc: 0.9425\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9552\n",
      "Epoch 00107: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1315 - acc: 0.9553 - val_loss: 0.2369 - val_acc: 0.9387\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9567\n",
      "Epoch 00108: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1298 - acc: 0.9567 - val_loss: 0.2399 - val_acc: 0.9401\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9570\n",
      "Epoch 00109: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1289 - acc: 0.9570 - val_loss: 0.2342 - val_acc: 0.9378\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9563\n",
      "Epoch 00110: val_loss improved from 0.23263 to 0.22904, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/110-0.2290.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1302 - acc: 0.9563 - val_loss: 0.2290 - val_acc: 0.9392\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9565\n",
      "Epoch 00111: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1273 - acc: 0.9565 - val_loss: 0.2479 - val_acc: 0.9406\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9585\n",
      "Epoch 00112: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1242 - acc: 0.9585 - val_loss: 0.2350 - val_acc: 0.9399\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9580\n",
      "Epoch 00113: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1251 - acc: 0.9579 - val_loss: 0.2500 - val_acc: 0.9366\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9585\n",
      "Epoch 00114: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1241 - acc: 0.9584 - val_loss: 0.2442 - val_acc: 0.9399\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9584\n",
      "Epoch 00115: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1202 - acc: 0.9585 - val_loss: 0.2407 - val_acc: 0.9399\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9583\n",
      "Epoch 00116: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1222 - acc: 0.9583 - val_loss: 0.2401 - val_acc: 0.9401\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9591\n",
      "Epoch 00117: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1205 - acc: 0.9591 - val_loss: 0.2478 - val_acc: 0.9390\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9605\n",
      "Epoch 00118: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1189 - acc: 0.9605 - val_loss: 0.2396 - val_acc: 0.9408\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9607\n",
      "Epoch 00119: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1185 - acc: 0.9607 - val_loss: 0.2508 - val_acc: 0.9383\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9606\n",
      "Epoch 00120: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1170 - acc: 0.9606 - val_loss: 0.2406 - val_acc: 0.9411\n",
      "Epoch 121/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9598\n",
      "Epoch 00121: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1182 - acc: 0.9598 - val_loss: 0.2420 - val_acc: 0.9418\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9612\n",
      "Epoch 00122: val_loss did not improve from 0.22904\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1154 - acc: 0.9612 - val_loss: 0.2310 - val_acc: 0.9408\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9605\n",
      "Epoch 00123: val_loss improved from 0.22904 to 0.22827, saving model to model/checkpoint/1D_CNN_DO_4_only_conv_checkpoint/123-0.2283.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1145 - acc: 0.9606 - val_loss: 0.2283 - val_acc: 0.9432\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9610\n",
      "Epoch 00124: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1166 - acc: 0.9610 - val_loss: 0.2379 - val_acc: 0.9411\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9618\n",
      "Epoch 00125: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1115 - acc: 0.9618 - val_loss: 0.2390 - val_acc: 0.9446\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9617\n",
      "Epoch 00126: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1151 - acc: 0.9617 - val_loss: 0.2429 - val_acc: 0.9406\n",
      "Epoch 127/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9625\n",
      "Epoch 00127: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1105 - acc: 0.9625 - val_loss: 0.2350 - val_acc: 0.9420\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9616\n",
      "Epoch 00128: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1142 - acc: 0.9617 - val_loss: 0.2398 - val_acc: 0.9427\n",
      "Epoch 129/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9635\n",
      "Epoch 00129: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1086 - acc: 0.9634 - val_loss: 0.2516 - val_acc: 0.9413\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9648\n",
      "Epoch 00130: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1044 - acc: 0.9648 - val_loss: 0.2524 - val_acc: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9618\n",
      "Epoch 00131: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1092 - acc: 0.9618 - val_loss: 0.2395 - val_acc: 0.9422\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9643\n",
      "Epoch 00132: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1051 - acc: 0.9642 - val_loss: 0.2461 - val_acc: 0.9392\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9648\n",
      "Epoch 00133: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1078 - acc: 0.9647 - val_loss: 0.2375 - val_acc: 0.9385\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9645\n",
      "Epoch 00134: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1073 - acc: 0.9645 - val_loss: 0.2320 - val_acc: 0.9425\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9639\n",
      "Epoch 00135: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1070 - acc: 0.9639 - val_loss: 0.2360 - val_acc: 0.9401\n",
      "Epoch 136/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9633\n",
      "Epoch 00136: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1055 - acc: 0.9632 - val_loss: 0.2518 - val_acc: 0.9415\n",
      "Epoch 137/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9654\n",
      "Epoch 00137: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1033 - acc: 0.9654 - val_loss: 0.2420 - val_acc: 0.9415\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9644\n",
      "Epoch 00138: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1066 - acc: 0.9644 - val_loss: 0.2369 - val_acc: 0.9439\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9649\n",
      "Epoch 00139: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1031 - acc: 0.9649 - val_loss: 0.2419 - val_acc: 0.9434\n",
      "Epoch 140/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9660\n",
      "Epoch 00140: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1022 - acc: 0.9660 - val_loss: 0.2375 - val_acc: 0.9406\n",
      "Epoch 141/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9654\n",
      "Epoch 00141: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1019 - acc: 0.9654 - val_loss: 0.2538 - val_acc: 0.9399\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9656\n",
      "Epoch 00142: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1036 - acc: 0.9655 - val_loss: 0.2458 - val_acc: 0.9406\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9645\n",
      "Epoch 00143: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1009 - acc: 0.9645 - val_loss: 0.2455 - val_acc: 0.9420\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9657\n",
      "Epoch 00144: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0995 - acc: 0.9657 - val_loss: 0.2416 - val_acc: 0.9425\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9667\n",
      "Epoch 00145: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0958 - acc: 0.9667 - val_loss: 0.2411 - val_acc: 0.9439\n",
      "Epoch 146/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9662\n",
      "Epoch 00146: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0973 - acc: 0.9661 - val_loss: 0.2427 - val_acc: 0.9429\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9664\n",
      "Epoch 00147: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0991 - acc: 0.9665 - val_loss: 0.2391 - val_acc: 0.9425\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9690\n",
      "Epoch 00148: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0959 - acc: 0.9689 - val_loss: 0.2372 - val_acc: 0.9434\n",
      "Epoch 149/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9678\n",
      "Epoch 00149: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0956 - acc: 0.9677 - val_loss: 0.2547 - val_acc: 0.9427\n",
      "Epoch 150/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9670\n",
      "Epoch 00150: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0957 - acc: 0.9670 - val_loss: 0.2550 - val_acc: 0.9429\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9671\n",
      "Epoch 00151: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0953 - acc: 0.9672 - val_loss: 0.2450 - val_acc: 0.9446\n",
      "Epoch 152/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9672\n",
      "Epoch 00152: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0939 - acc: 0.9672 - val_loss: 0.2464 - val_acc: 0.9432\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9688\n",
      "Epoch 00153: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0925 - acc: 0.9688 - val_loss: 0.2490 - val_acc: 0.9453\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9672\n",
      "Epoch 00154: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0958 - acc: 0.9672 - val_loss: 0.2577 - val_acc: 0.9429\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9682\n",
      "Epoch 00155: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0923 - acc: 0.9682 - val_loss: 0.2415 - val_acc: 0.9432\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9689\n",
      "Epoch 00156: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0909 - acc: 0.9689 - val_loss: 0.2365 - val_acc: 0.9448\n",
      "Epoch 157/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9689\n",
      "Epoch 00157: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0925 - acc: 0.9689 - val_loss: 0.2415 - val_acc: 0.9434\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9679\n",
      "Epoch 00158: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0932 - acc: 0.9679 - val_loss: 0.2472 - val_acc: 0.9420\n",
      "Epoch 159/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9694\n",
      "Epoch 00159: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0895 - acc: 0.9693 - val_loss: 0.2408 - val_acc: 0.9436\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9695\n",
      "Epoch 00160: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.0915 - acc: 0.9695 - val_loss: 0.2396 - val_acc: 0.9408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9692\n",
      "Epoch 00161: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0924 - acc: 0.9691 - val_loss: 0.2511 - val_acc: 0.9413\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9703\n",
      "Epoch 00162: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0899 - acc: 0.9703 - val_loss: 0.2473 - val_acc: 0.9415\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9703\n",
      "Epoch 00163: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0861 - acc: 0.9703 - val_loss: 0.2551 - val_acc: 0.9450\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9704\n",
      "Epoch 00164: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0888 - acc: 0.9704 - val_loss: 0.2499 - val_acc: 0.9406\n",
      "Epoch 165/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9698\n",
      "Epoch 00165: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0885 - acc: 0.9697 - val_loss: 0.2358 - val_acc: 0.9425\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9711\n",
      "Epoch 00166: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0837 - acc: 0.9710 - val_loss: 0.2548 - val_acc: 0.9420\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9701\n",
      "Epoch 00167: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0884 - acc: 0.9701 - val_loss: 0.2534 - val_acc: 0.9425\n",
      "Epoch 168/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9718\n",
      "Epoch 00168: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0842 - acc: 0.9718 - val_loss: 0.2511 - val_acc: 0.9443\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9717\n",
      "Epoch 00169: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0840 - acc: 0.9717 - val_loss: 0.2454 - val_acc: 0.9460\n",
      "Epoch 170/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9706\n",
      "Epoch 00170: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0852 - acc: 0.9706 - val_loss: 0.2655 - val_acc: 0.9420\n",
      "Epoch 171/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9708\n",
      "Epoch 00171: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0871 - acc: 0.9707 - val_loss: 0.2570 - val_acc: 0.9425\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9730\n",
      "Epoch 00172: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0799 - acc: 0.9730 - val_loss: 0.2499 - val_acc: 0.9460\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9730\n",
      "Epoch 00173: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0806 - acc: 0.9730 - val_loss: 0.2436 - val_acc: 0.9420\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9713\n",
      "Epoch 00174: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0854 - acc: 0.9713 - val_loss: 0.2502 - val_acc: 0.9436\n",
      "Epoch 175/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9715\n",
      "Epoch 00175: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0802 - acc: 0.9715 - val_loss: 0.2565 - val_acc: 0.9420\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9708\n",
      "Epoch 00176: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0841 - acc: 0.9708 - val_loss: 0.2494 - val_acc: 0.9434\n",
      "Epoch 177/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9727\n",
      "Epoch 00177: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0811 - acc: 0.9728 - val_loss: 0.2468 - val_acc: 0.9460\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9724\n",
      "Epoch 00178: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0818 - acc: 0.9724 - val_loss: 0.2419 - val_acc: 0.9422\n",
      "Epoch 179/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9728\n",
      "Epoch 00179: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0814 - acc: 0.9728 - val_loss: 0.2593 - val_acc: 0.9441\n",
      "Epoch 180/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9729\n",
      "Epoch 00180: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0786 - acc: 0.9729 - val_loss: 0.2614 - val_acc: 0.9443\n",
      "Epoch 181/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9721\n",
      "Epoch 00181: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0798 - acc: 0.9721 - val_loss: 0.2667 - val_acc: 0.9434\n",
      "Epoch 182/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9724\n",
      "Epoch 00182: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0816 - acc: 0.9724 - val_loss: 0.2416 - val_acc: 0.9455\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9723\n",
      "Epoch 00183: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0800 - acc: 0.9723 - val_loss: 0.2540 - val_acc: 0.9422\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9726\n",
      "Epoch 00184: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0789 - acc: 0.9726 - val_loss: 0.2460 - val_acc: 0.9462\n",
      "Epoch 185/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9729\n",
      "Epoch 00185: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0806 - acc: 0.9729 - val_loss: 0.2401 - val_acc: 0.9439\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9740\n",
      "Epoch 00186: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0769 - acc: 0.9740 - val_loss: 0.2547 - val_acc: 0.9425\n",
      "Epoch 187/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9722\n",
      "Epoch 00187: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0801 - acc: 0.9721 - val_loss: 0.2613 - val_acc: 0.9443\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9729\n",
      "Epoch 00188: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0780 - acc: 0.9729 - val_loss: 0.2496 - val_acc: 0.9418\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9742\n",
      "Epoch 00189: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0753 - acc: 0.9742 - val_loss: 0.2468 - val_acc: 0.9441\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9747\n",
      "Epoch 00190: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.0730 - acc: 0.9747 - val_loss: 0.2531 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9739\n",
      "Epoch 00191: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0770 - acc: 0.9739 - val_loss: 0.2476 - val_acc: 0.9422\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9741\n",
      "Epoch 00192: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0757 - acc: 0.9741 - val_loss: 0.2504 - val_acc: 0.9422\n",
      "Epoch 193/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9752\n",
      "Epoch 00193: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.0738 - acc: 0.9751 - val_loss: 0.2607 - val_acc: 0.9415\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9735\n",
      "Epoch 00194: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.0763 - acc: 0.9735 - val_loss: 0.2481 - val_acc: 0.9460\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9755\n",
      "Epoch 00195: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.0725 - acc: 0.9755 - val_loss: 0.2610 - val_acc: 0.9425\n",
      "Epoch 196/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9748\n",
      "Epoch 00196: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.0737 - acc: 0.9748 - val_loss: 0.2589 - val_acc: 0.9436\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9745\n",
      "Epoch 00197: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0727 - acc: 0.9745 - val_loss: 0.2589 - val_acc: 0.9450\n",
      "Epoch 198/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9746\n",
      "Epoch 00198: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.0726 - acc: 0.9747 - val_loss: 0.2513 - val_acc: 0.9439\n",
      "Epoch 199/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9740\n",
      "Epoch 00199: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.0765 - acc: 0.9740 - val_loss: 0.2538 - val_acc: 0.9450\n",
      "Epoch 200/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9758\n",
      "Epoch 00200: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0722 - acc: 0.9758 - val_loss: 0.2538 - val_acc: 0.9436\n",
      "Epoch 201/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9761\n",
      "Epoch 00201: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0715 - acc: 0.9761 - val_loss: 0.2658 - val_acc: 0.9448\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9735\n",
      "Epoch 00202: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0751 - acc: 0.9735 - val_loss: 0.2729 - val_acc: 0.9432\n",
      "Epoch 203/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9758\n",
      "Epoch 00203: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0706 - acc: 0.9757 - val_loss: 0.2516 - val_acc: 0.9446\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9747\n",
      "Epoch 00204: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0734 - acc: 0.9747 - val_loss: 0.2633 - val_acc: 0.9420\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9761\n",
      "Epoch 00205: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0696 - acc: 0.9760 - val_loss: 0.2554 - val_acc: 0.9434\n",
      "Epoch 206/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9752\n",
      "Epoch 00206: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0710 - acc: 0.9752 - val_loss: 0.2632 - val_acc: 0.9415\n",
      "Epoch 207/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9738\n",
      "Epoch 00207: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0745 - acc: 0.9738 - val_loss: 0.2531 - val_acc: 0.9450\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9756\n",
      "Epoch 00208: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0717 - acc: 0.9756 - val_loss: 0.2629 - val_acc: 0.9467\n",
      "Epoch 209/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9765\n",
      "Epoch 00209: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0669 - acc: 0.9764 - val_loss: 0.2549 - val_acc: 0.9455\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9745\n",
      "Epoch 00210: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0740 - acc: 0.9745 - val_loss: 0.2620 - val_acc: 0.9429\n",
      "Epoch 211/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9755\n",
      "Epoch 00211: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0716 - acc: 0.9756 - val_loss: 0.2604 - val_acc: 0.9439\n",
      "Epoch 212/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9765\n",
      "Epoch 00212: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0677 - acc: 0.9766 - val_loss: 0.2652 - val_acc: 0.9471\n",
      "Epoch 213/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9761\n",
      "Epoch 00213: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.0703 - acc: 0.9761 - val_loss: 0.2601 - val_acc: 0.9434\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9761\n",
      "Epoch 00214: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0687 - acc: 0.9761 - val_loss: 0.2661 - val_acc: 0.9476\n",
      "Epoch 215/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9767\n",
      "Epoch 00215: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0689 - acc: 0.9767 - val_loss: 0.2732 - val_acc: 0.9434\n",
      "Epoch 216/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9765\n",
      "Epoch 00216: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0688 - acc: 0.9765 - val_loss: 0.2549 - val_acc: 0.9439\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9759\n",
      "Epoch 00217: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.0701 - acc: 0.9759 - val_loss: 0.2601 - val_acc: 0.9436\n",
      "Epoch 218/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9771\n",
      "Epoch 00218: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0688 - acc: 0.9770 - val_loss: 0.2500 - val_acc: 0.9453\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9775\n",
      "Epoch 00219: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0648 - acc: 0.9775 - val_loss: 0.2662 - val_acc: 0.9448\n",
      "Epoch 220/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9767\n",
      "Epoch 00220: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0659 - acc: 0.9767 - val_loss: 0.2622 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9782\n",
      "Epoch 00221: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0662 - acc: 0.9782 - val_loss: 0.2646 - val_acc: 0.9434\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9769\n",
      "Epoch 00222: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0680 - acc: 0.9769 - val_loss: 0.2783 - val_acc: 0.9432\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9778\n",
      "Epoch 00223: val_loss did not improve from 0.22827\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0643 - acc: 0.9778 - val_loss: 0.2589 - val_acc: 0.9469\n",
      "\n",
      "1D_CNN_DO_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmX0mM9kXCFvYlDUEAohFXOpSV1wRrdattbVfa6u2/qTWWvttrdZaa2211rZa9etarFulUq0iLqgsgoIg+xKSkH2SSWaf8/vjZAEMECGTkMzzfr0uM7lzl2cuM/eZc8495yqtNUIIIQSApbcDEEIIcfiQpCCEEKKdJAUhhBDtJCkIIYRoJ0lBCCFEO0kKQggh2klSEEII0U6SghBCiHaSFIQQQrSz9XYAX1Zubq4uKirq7TCEEKJPWb58eY3WOu9Ay/W5pFBUVMSyZct6OwwhhOhTlFLburKcVB8JIYRoJ0lBCCFEO0kKQggh2vW5NoXORKNRysrKCIVCvR1Kn+VyuRg8eDB2u723QxFC9KJ+kRTKysrw+XwUFRWhlOrtcPocrTW1tbWUlZUxfPjw3g5HCNGL+kX1USgUIicnRxLCQVJKkZOTIyUtIUT/SAqAJIRDJMdPCAH9KCkcSDweJBzeSSIR7e1QhBDisJUySSGRCBGJVKB1rNu33dDQwIMPPnhQ655++uk0NDR0efnbb7+de+6556D2JYQQB5IySQHaqkcS3b7l/SWFWGz/SWjBggVkZmZ2e0xCCHEwUiYpKGXeqta627c9b948Nm3aRElJCTfddBOLFi1i1qxZzJ49m3HjxgFwzjnnUFpayvjx43n44Yfb1y0qKqKmpoatW7cyduxYrr76asaPH88pp5xCMBjc735XrlzJjBkzKC4u5txzz6W+vh6A+++/n3HjxlFcXMxFF10EwNtvv01JSQklJSVMnjyZpqambj8OQoi+r19ckrq7DRuuJxBY+YX5WsdJJFqwWDwoZf1S2/R6Sxg9+r59vn7XXXexevVqVq40+120aBErVqxg9erV7Zd4PvLII2RnZxMMBpk2bRrnn38+OTk5e8W+gaeffpq//OUvXHjhhTz//PNceuml+9zvZZddxh/+8AeOO+44brvtNn7+859z3333cdddd7FlyxacTmd71dQ999zDAw88wMyZMwkEArhcri91DIQQqSGFSgptz7q/pNCZ6dOn73HN//3338+kSZOYMWMGO3bsYMOGDV9YZ/jw4ZSUlABQWlrK1q1b97l9v99PQ0MDxx13HACXX345ixcvBqC4uJhLLrmE//u//8NmM3l/5syZ3Hjjjdx///00NDS0zxdCiN31uzPDvn7Rx+MttLR8hss1Ers9K+lxpKWltT9ftGgRb7zxBkuWLMHj8XD88cd32ifA6XS2P7darQesPtqXV199lcWLF/PKK69wxx138OmnnzJv3jzOOOMMFixYwMyZM1m4cCFjxow5qO0LIfqvlCkpdLzV7m9o9vl8+62j9/v9ZGVl4fF4WLduHR988MEh7zMjI4OsrCzeeecdAJ544gmOO+44EokEO3bs4IQTTuDXv/41fr+fQCDApk2bmDhxIjfffDPTpk1j3bp1hxyDEKL/6XclhX1p65yVjIbmnJwcZs6cyYQJEzjttNM444wz9nj91FNP5aGHHmLs2LEceeSRzJgxo1v2+9hjj3HNNdfQ0tLCiBEjePTRR4nH41x66aX4/X601nz/+98nMzOTn/70p7z11ltYLBbGjx/Paaed1i0xCCH6F5WMk2QyTZ06Ve99k521a9cyduzY/a6XSERpbl6F0zkUhyM/mSH2WV05jkKIvkkptVxrPfVAy6VQ9VFbS3PfSoJCCNGTUiYpdPRT6P42BSGE6C9SJilISUEIIQ4sZZKCaWhWJOPqIyGE6C9SJikYKilXHwkhRH+RUknBtCtISUEIIfYlpZICWA6bkoLX6/1S84UQoiekWFKQNgUhhNiflEoKpvooOUNnP/DAA+1/t90IJxAIcOKJJzJlyhQmTpzISy+91OVtaq256aabmDBhAhMnTuTZZ58FoKKigmOPPZaSkhImTJjAO++8Qzwe54orrmhf9ne/+123v0chRGrof8NcXH89rPzi0NkArniLGS7V4v5y2ywpgfv2PXT23Llzuf7667n22msBeO6551i4cCEul4sXXniB9PR0ampqmDFjBrNnz+7S/ZD/+c9/snLlSlatWkVNTQ3Tpk3j2GOP5amnnuJrX/saP/nJT4jH47S0tLBy5Up27tzJ6tWrAb7UndyEEGJ3/S8p9ILJkydTVVVFeXk51dXVZGVlMWTIEKLRKLfccguLFy/GYrGwc+dOdu3axYABAw64zXfffZeLL74Yq9VKQUEBxx13HEuXLmXatGlcddVVRKNRzjnnHEpKShgxYgSbN2/muuuu44wzzuCUU07pgXcthOiP+l9S2M8v+nDL54DG4+n+IaPnzJnD/PnzqaysZO7cuQA8+eSTVFdXs3z5cux2O0VFRZ0Omf1lHHvssSxevJhXX32VK664ghtvvJHLLruMVatWsXDhQh566CGee+45Hnnkke54W0KIFJNSbQrJvPpo7ty5PPPMM8yfP585c+YAZsjs/Px87HY7b731Ftu2bevy9mbNmsWzzz5LPB6nurqaxYsXM336dLZt20ZBQQFXX3013/rWt1ixYgU1NTUkEgnOP/98fvnLX7JixYqkvEchRP/X/0oK+6GUStrYR+PHj6epqYlBgwYxcOBAAC655BLOOussJk6cyNSpU7/UTW3OPfdclixZwqRJk1BKcffddzNgwAAee+wxfvOb32C32/F6vTz++OPs3LmTK6+8kkTCvLc777wzKe9RCNH/pczQ2QDB4Gbi8Ra83gnJCq9Pk6Gzhei/ZOjsTkk/BSGE2J+USgrJ6qcghBD9RUolBTMgnpQUhBBiX1IsKciAeEIIsT9JSwpKqSFKqbeUUp8ppdYopX7QyTJKKXW/UmqjUuoTpdSUZMXTuj9AHzaD4gkhxOEmmZekxoAfaq1XKKV8wHKl1Ota6892W+Y0YHTrdBTwp9bHJGnLgZqOO7EJIYRok7SSgta6Qmu9ovV5E7AWGLTXYmcDj2vjAyBTKTUwKQE1N2Mr96Ni0N2NzQ0NDTz44IMHte7pp58uYxUJIQ4bPdKmoJQqAiYDH+710iBgx25/l/HFxNE9IhGstQFUjG5vbN5fUojFYvtdd8GCBWRmZnZrPEIIcbCSnhSUUl7geeB6rXXjQW7j20qpZUqpZdXV1QcbiHnU7f90m3nz5rFp0yZKSkq46aabWLRoEbNmzWL27NmMGzcOgHPOOYfS0lLGjx/Pww8/3L5uUVERNTU1bN26lbFjx3L11Vczfvx4TjnlFILB4Bf29corr3DUUUcxefJkTjrpJHbt2gVAIBDgyiuvZOLEiRQXF/P8888D8NprrzFlyhQmTZrEiSee2K3vWwjR/yS1R7NSyg78C1iotb63k9f/DCzSWj/d+vfnwPFa64p9bfNAPZr3OXJ2PAYtQRJOUPa01j4LXXOAkbPZunUrZ555ZvvQ1YsWLeKMM85g9erVDB8+HIC6ujqys7MJBoNMmzaNt99+m5ycHIqKili2bBmBQIBRo0axbNkySkpKuPDCC5k9ezaXXnrpHvuqr68nMzMTpRR//etfWbt2Lb/97W+5+eabCYfD3NcaaH19PbFYjClTprB48WKGDx/eHsO+SI9mIfqvrvZoTlpDszKX+vwNWNtZQmj1MvA9pdQzmAZm//4SwiFGtNvz5F99NH369PaEAHD//ffzwgsvALBjxw42bNhATk7OHusMHz6ckpISAEpLS9m6desXtltWVsbcuXOpqKggEom07+ONN97gmWeeaV8uKyuLV155hWOPPbZ9mf0lBCGEgORefTQT+AbwqVKq7bf7LcBQAK31Q8AC4HRgI9ACXHmoO93nL/rmEKz9nJZCcBaMxWpNO9Rd7VdaWsf2Fy1axBtvvMGSJUvweDwcf/zxnQ6h7XQ6259brdZOq4+uu+46brzxRmbPns2iRYu4/fbbkxK/ECI1JfPqo3e11kprXay1LmmdFmitH2pNCLRedXSt1nqk1nqi1nrZgbZ70CzmrSpNt/dT8Pl8NDU17fN1v99PVlYWHo+HdevW8cEHHxz0vvx+P4MGmbb4xx57rH3+ySefvMctQevr65kxYwaLFy9my5YtgKnCEkKI/UmdHs2tScHUHHXv1Uc5OTnMnDmTCRMmcNNNN33h9VNPPZVYLMbYsWOZN28eM2bMOOh93X777cyZM4fS0lJyc3Pb5996663U19czYcIEJk2axFtvvUVeXh4PP/ww5513HpMmTWq/+Y8QQuxL6gydHYnAJ58QKgDbwNHYbBlJjLJvkoZmIfovGTp7b7uVFGRQPCGE6FzKJQWVABkUTwghOpc6SWG3zmt9rcpMCCF6SkolBa1UUhqahRCiv0idpABgsbRWH0lJQQghOpNySUEamoUQYt9SKykohUrCgHgHw+v19nYIQgjxBSmVFFRrSUHaFIQQonMplRSwWFBadXv10bx58/YYYuL222/nnnvuIRAIcOKJJzJlyhQmTpzISy+9dMBt7WuI7c6GwN7XcNlCCHGwkjkgXq+4/rXrWVnZ2djZQEsLmjjaZcdicXV5myUDSrjv1H2PnT137lyuv/56rr32WgCee+45Fi5ciMvl4oUXXiA9PZ2amhpmzJjB7NmzW+8V3blHHnlkjyG2zz//fBKJBFdfffUeQ2AD/OIXvyAjI4NPP/0UMOMdCSHEoeh3SeGAtKK72xQmT55MVVUV5eXlVFdXk5WVxZAhQ4hGo9xyyy0sXrwYi8XCzp072bVrFwMGDNjntjobYru6urrTIbA7Gy5bCCEORb9LCvv7Rc+GDcTDTYRHpOHxHNmt+50zZw7z58+nsrKyfeC5J598kurqapYvX47dbqeoqKjTIbPbdHWIbSGESJbUalNovfpI63i3b3ru3Lk888wzzJ8/nzlz5gBmmOv8/HzsdjtvvfUW27Zt2+829jXE9r6GwO5suGwhhDgUqZUUkthPYfz48TQ1NTFo0CAGDhwIwCWXXMKyZcuYOHEijz/+OGPGjNnvNvY1xPa+hsDubLhsIYQ4FKkzdDbA1q0kGupoGWXF652UpAj7Lhk6W4j+S4bO7ozFgtI6KdVHQgjRH6RWUlCqtd9aQoa6EEKITvSbpNClajCLBbSW8Y860deqEYUQydEvkoLL5aK2tvbAJzaLhY5uY1KF1EZrTW1tLS5X1zv0CSH6p37RT2Hw4MGUlZVRXV29/wUbG6G+npAFHM51WCyOngmwD3C5XAwePLi3wxBC9LJ+kRTsdnt7b9/9evBBuPZa3nsehn91EZmZxyU/OCGE6EP6RfVRl7VWj1giEIs19nIwQghx+EmtpOB2A2CNQCzm7+VghBDi8JNaSWG3kkI8LiUFIYTYW8omBSkpCCHEF6VWUmitPrJErFJSEEKITqRWUmgtKdhjHikpCCFEJ1IyKdgkKQghRKdSMinY4y6pPhJCiE6kVlJovyTVKSUFIYToRGolhfbqI6d0XhNCiE6kaFKwE49LSUEIIfaWtKSglHpEKVWllFq9j9ePV0r5lVIrW6fbkhVLu7bqo6iDaFTuZyyEEHtL5oB4fwf+CDy+n2Xe0VqfmcQY9mS3g1LYok7icT+JRERGShVCiN0kraSgtV4M1CVr+wdFKUhLwxayAhCNHmCobSGESDG93aZwtFJqlVLq30qp8T2yx5wcbA3mBjuRSFWP7FIIIfqK3ryfwgpgmNY6oJQ6HXgRGN3ZgkqpbwPfBhg6dOih7TUvD2tDCIBoVJKCEELsrtdKClrrRq11oPX5AsCulMrdx7IPa62naq2n5uXlHdqO8/Kw1DYDUlIQQoi99VpSUEoNUEqp1ufTW2OpTfqOc3Ox1DYAUlIQQoi9Ja36SCn1NHA8kKuUKgN+BtgBtNYPARcA31VKxYAgcJHWWicrnnZ5eVBTh1IOKSkIIcRekpYUtNYXH+D1P2IuWe1ZubmolhZciUIpKQghxF56++qjntfaJuFuzpaSghBC7CV1k0LAJyUFIYTYS+olhVxzgZOz0SMlBSGE2EvqJYXWkoKzyUk0Wk1PtG0LIURfkXpJobWk4PBbSCSCxOPNvRyQEEIcPlIvKWRmgtWKvXXkbGlXEEKIDqmXFCwWyM3FVh8DpFezEELsLvWSAkBuLtZ6M/5RJFLey8EIIcThIzWTQl4e1vogAOHwjl4ORgghDh8pmxRUTQMWi4tQSJKCEEK0Sc2kkJuLqqnB6RwqJQUhhNhNaiaFvDyoq8NpG0w4vL23oxFCiMNGl5KCUuoHSql0ZfxNKbVCKXVKsoNLmtxc0Jq0UL5UHwkhxG66WlK4SmvdCJwCZAHfAO5KWlTJ1j4oXhaRSDmJRLSXAxJCiMNDV5OCan08HXhCa71mt3l9T/ugeF5Ay2WpQgjRqqtJYblS6j+YpLBQKeUDEskLK8nahrpodAIQCkm7ghBCQNdvsvNNoATYrLVuUUplA1cmL6wkay0pOPxWQPoqCCFEm66WFI4GPtdaNyilLgVuBfzJCyvJcnIAsDfEASkpCCFEm64mhT8BLUqpScAPgU3A40mLKtmcTkhPx1LXiM2WLSUFIYRo1dWkENPmxgNnA3/UWj8A+JIXVg/Iy4PqalyuIkKhLb0djRBCHBa6mhSalFI/xlyK+qpSygLYkxdWD8jNhZoa3O5RBIMbejsaIYQ4LHQ1KcwFwpj+CpXAYOA3SYuqJ7SWFNzu0QSDW6SvghBC0MWk0JoIngQylFJnAiGtdd9tUwBTUqiuxuMZDcQJhbb2dkRCCNHrujrMxYXAR8Ac4ELgQ6XUBckMLOny8kz1kWsUgFQhCSEEXe+n8BNgmta6CkAplQe8AcxPVmBJl5cH4TDu+EBAkoIQQkDX2xQsbQmhVe2XWPfw1Nqr2e4HqzWdYHBjLwckhBC9r6slhdeUUguBp1v/ngssSE5IPaS1V7OqqcHtHk1Li5QUhBCiS0lBa32TUup8YGbrrIe11i8kL6weUFBgHisq8IweTWPjR70bjxBCHAa6WlJAa/088HwSY+lZI0aYx82bcRePpqrqOeLxEFarq3fjEkKIXrTfdgGlVJNSqrGTqUkp1dhTQSZFdjZkZsLGjXi9k4AEzc2f9nZUQgjRq/abFLTWPq11eieTT2ud3lNBJoVSMGoUbNyIzzcVgKam5b0clBBC9K6+fQXRoRo1CjZtwukcis2WQ1PTst6OSAghepUkha1bUbEYPl8pgYCUFIQQqS21k8LIkRCPw7Zt+HxTaW5eTTwe6u2ohBCi1yQtKSilHlFKVSmlVu/jdaWUul8ptVEp9YlSakqyYtmnUWaIC9OuUIrWMZqbP+nxMIQQ4nCRzJLC34FT9/P6acDo1unbmBv59Kw9kkJbY7O0KwghUlfSkoLWejFQt59FzgYe18YHQKZSamCy4ulUQQGkpcHGjTidQ7Dbc+UKJCFESuvNNoVBwO73wSxrnddzdrssVSmF11sqSUEIkdK63KO5Nymlvo2pYmLo0KHdu/FRo2DNGgB8vqls334X8XgQq9XdvfsRQvQ4rc2jUp2/lkiYa01isa4/ag3DhoHVClVV4PVCczNEIpCVZaZQCKqroaHBDLPmcJjnfr9Z32o1U9v+lQKXC1paoLHRbMvtNuskEpCebvYxciQUFyf3mPVmUtgJDNnt78Gt875Aa/0w8DDA1KlTdbdGMWoUvPIKxOP4fKVAnObmT0hPP6pbdyPEl6W1mSwWc0KoqIBwGEaPhtpaqKszJ4xEwpx0PB6w2WDHDjPP4zEnsVgMolEzxWKmxtRigfp6s41YzPydSMDmzeaElpkJGRlmfjze+dR2Qtvf1F3L2FrPVDt3mvdhtZrY2h7BzI9Evji53TBwoHmf4bB5f22PunvPJkl3001w993J3UdvJoWXge8ppZ4BjgL8WuuKHo9i1CjzySkrw1dQCpjGZkkKfVskYk4kFos5Gdh2+6RHo7B9e8evsESi41dj2/OGBnMSrq42v9LAzMvKMttrbIRAwGzfZjMnmLIy85pSZhttJ7S2k3IgYNYLBjtO+HtPsZjZVtuJC8wvyFAPXSltsZgE82X2p1THL999TW0n8C+zjN1u3rvVao5LIgGzZoHT2ZFI2h7BxN3ZFAhAZWXH9pzOjke73fz/Wa0dj7s/7+xRa5M8EwmTbAIBU1pwODoSrdNpSgiZmaY0EYuZz05bom17PxaLmbQ2x9zjMZ83h8OUGjIyzH4bG80+BvVABXvSkoJS6mngeCBXKVUG/AywA2itH8IMvX06sBFoAa5MViz7tdsVSM6hX8Vuz5N2hSSJxcwHPRw2J+22X3bBoPlitU21teZk3NmvvnC4YwpGImg0LpsTq9V8ISsroaJS01APtgGf40xvpHlDKS6nlexssNo0Zf6d6EAexJ2dB+poAksMIl5w+SHsg7QqyNoMuyZB3A5K41ReNAli9nrsngCFuV5cZAMQd9RhsUWx2GJY7XGsthievDijhhSQ5nTT7NhE1OrHghWlLDTbt+JKZKNtLQTdmxhkmYLX4SVEA03hJvLTs8jOSRCzBCgvszEqezRjBgwjYQmzJbQcS9yDJZJJfbiW3Lw4jbqCquZduGwuBqeNYlDaMNx2J3EVpD7QQjAWxOFtwe5uIaZaiMZj2C0OnL4AWKIk4jbCQQt1oVriREl3+qhs2Um600ehbyAJYlQHd2GxKArS8sn15OIP+7Fb7IzOGQ1AJB5hZ+NO1tasZcrAKWS6MqlurgZAo/GH/JQ1llERqGB45nDG5I6hMdzIiooVZLmzGJs7ltpgLVmuLKKJKFsbtlLWWMbwzOEU+gopbyonGAsSjoUJxULEdbx9G+tr15PtK2RoxlCsykpdsI6izCICkQCVgUo0msHpg8l2Z+OwOkjoBO/veJ9QLMT4vPEEIgGqW6qJxqOkO9NpDDfisDpIc6TRGG4kqygLpRR10RZGZo0kmoiiteYIVyardq1ic/1mVkcCZLuzGT15NF6Hl21N5Xxa9SlDM4Zic9pYXbWa0oGlDPQNJBKPEIlHiMajROIREsEELrsLd9SNW7uJOCN8sPUDTrWeysV5Fyf1e5q0pKC13m/kWmsNXJus/XfZbklBnXgiPt90/P73ejemLtBaUxmoZIB3AEopmiPNrKxcSY4nhwxnBtUt1XxW/RmDfIOYNmgaVmXlqU+fwmqxMjRjKEt2LGGgbyA57hyCsSDBaJBgLMjKypV8tPMjpg+ajteewc7aekKRODXNtWQ4MxmTPZFos4d1NeuIhu3kxieyMjKf6tgWVCSDAc0nUWNbSaNjPTGrH1DEY4pEOA3qi+Djq6BwOQx9B+wtYA+CpwasYaiaALuKzcm36G1I2FBNw1GhHPBWonN2oRIOrNFMsMaIuneAJY4lmg4JK+7wcGz2BH7XKpSCGJoY4CITZyKLpoQiovxoay1W7His6QQTTeQ4BhFLhInqMGnWDCrDW9AcuF7B48qiKdJEIhEjDGwBCtIKiOs4NS01+1zPqqzEdbzr/9luIApUtv5tARpANSiUUiR0Ys/l93fN32EozZ5Gc7S5/W+bxUYsEet0WbfNTTAW/MJ8u8WOUopIPAJAhjMDf9j/pWOxKMsXj+dByHHn4HP6qG6u3uO9DUkfQmWgkoROUJRZxEufv9Tlbean5VNSUHLIsR1In2hoTqrCQlPW22juvJaVdRJ1da8SCm3D5RrWy8FBLBFjV2AX725/l/9u+S/b/dsp8BawsW4j7+94n6MHH43P6eOtLW8RTUQ73YbD6iDLlcWu5l0H3J8l7sFSWcqK7Y+hLVEIZoG2QDAb0qp4Je1Rs2DUbX5NW6PQVIijZjoqawtr8m/H0TKUjJZS3CqLRAJc7jjWrGZ25S6lYfSFKCyMdMzEax2C0+Im05WDy25j+4BP2RT4B1mubGYfeS12m5UNdevxh/0M9E6hIK2AaCJKQ6gBq8VKUUYRTpuTmpYaovEon9d+jkYzvfBmrBYrwzOH47F7WLR1EcFYkIRO4La5KS4opqyxjMZwI16Hl7KmMtw2N06rk/pQPePyLsfn8BGIBMhwZeAP+clwZTAqexSrKldhtViJJ+KUNZaR6cokPy0fn9OHP+Tnk6pPsGBhQv4EXDYXNoutfbIoC2WNZTRFmhiTO4Ycdw5xHScajzIscxj1wXrsVjsjskawomIFsUSMLFcWXoeX+lA9FmXB5/ARjodZW72WnU2mCW5a4TSiiSiN4UZy3DnYLDby0/IZ6BtIS7SFz2s+pyJQQTgWxm1347F78Ng9uG3mudvuxmaxEY1H8Tq82K12YokYsUSMHHcOdqudxnAjhb5CmsJN7GrehcPqID8tH601Vc1V1LTUkOHKIBgNsrl+M1aLFYfVQY47hyNzj2TpzqVE4hHy0/KxKNMI4HV4GZIxBK/Dy3b/drb7t5NmT2Ns3lj8IT9bG7aS68mlPlSPzWKjKLOITFcmO/w7qAvWMSh9EGn2NJw2Z/vJfGPdRjx2D4PTBxOMBilrLCOu42S6MtlSvwWf08cgn6mDKWssoyHUQCQeIa7jlAwowWP3sLl+M5muTHI9udgtdpoiTfgcPiLxCM3RZtKd6dQF69Ba47K52Fy/GafNlDprW2qZkD+BvDRzEy+tNRWBCoLRILme3PZjFNdx8/8arKc52ozdYsdhdWC3mkeFIhwPE4wGCcVMXd7g9MGozlrMu5nSfaylZerUqXrZsm7uYDZ+PBxxBLzwAs3Nn7F06XiOOOJhCguv7t797CYYDVIfqscf8vPoykfJdmdT6Cvk7a1v47a72ebfxvLy5exq3tX+yyXbnc2IrBHsbNyJ3WrnovEX8eyaZ4lHbYyznEt6/THUBxupDzTTVJ1JYMtYWhzbCBe8RzxjA9ZPvkmopgB8O2H7MZBWjd3bxKB8F4X5bqor3Ngiucw8ykk0HqMg38LoURYyM8Hng0hEUx2ox5fdwrihA3D5mtkeXMvM4VNxtFba1wXryHJldfrhjSVi/GfTfxidPbq9ikEI0TOUUsu11lMPuJwkBeAW7p/6AAAgAElEQVTss2HLFvjkE7TWfPDBUNLTZzB+/D+6ZfNaaxZtXcR7O95jh38Hm+o38d6O99p/AexeXM52Z5PQCfLT8vnKkK8wNH0oWbZCvMHx2CuP5vN1Vtatg127TMPU6tWmrh1Mg1RmJuTkwODBphCUlrZn493QoTB2rGn0GjIEBgzouHpDCNF/dTUpSPURmHaF11+HeBxltZKVdQo1Nf9E6zhKWQ9qk1XNVfxr/b9YVr6Mjys/5oOyDwDI8+QxNGMo3yn9DmNyxxBLxLhg3AWEYxFWrq8muGUyKz+2sP5DWLUVXtxirnppY7OZcAsLza/3738fpk+H0lJzwrfJ/6gQ4hDIKQRgxgy49154+2346lfJzj6VyspH8PuXkJl5zJfe3Oqq1cx6dBYNoQYyXZkUZRbxh9P+wFWTr8Jj9wDmMrXPPoOaGrh3Pjz/PGzebDrm2e3mxD98OBx9tHkcOdL8wh850rwuhBDJIEkB4Mwzzc/uJ59sTQpfQykHNTUvdikpVAYqufXNW8n15FLoK+Tu9+7GbXPz+tWvUzqwtL1+/dNP4ZlnYOlSePPNjuurbTY4/nj40Y/Mr/4JE0zbtxBC9DRJCmC6PJ53HsyfDw88gM2VTlbWidTUvMjIkb/ZZ4u/P+Tn2TXPcvui26kL1hHXcWKJGKOzRzP/wvkUFxSzZg38/e+wZAm8955JAGPGmJ6JJ51k6vbHjTOdaYQQordJUmhzySXw2GOwYAGcdx65ueewfv13aG5eg9c7YY9F64J13LjwRp5b8xzBWJDigmJeu/Q1BqcPpincxGDfMF55BW74gykROBwwZQr87//CtddCdnYvvUchhDgASQptTjjB9C9fuBDOO4+cnNnANVRXz29PCoFIgMXbFnPjwhvZ0rCFb07+JleWXMnUwqkopaivh+f+ms0DD8C2bebqnjvvhG99C3Jze/ftCSFEV0hSaGOzmcTwxhsAOJ0DyMo6kV27HqOo6Db+vfE1Ln/xcmpaasj15PLGN95g1rBZgBla4YknTAKorzftA/feC7Nny9VAQoi+RU5ZuzvpJHjpJdi8meiwIdy3QRFt2srvt53KM+tep7igmKfOe4qZQ2fisXtYvRruuAOee84MbnXKKXDXXTB5cm+/ESGEODiSFHZ34onm8b//5eaha/n7mtexABb1X358zI+57bjbcFpdLFoEv/kN/PvfZuTCH/4QLr/cdIwWQoi+TJLC7saMgcJCXlryd35X/j7XTb+OSwYH2VnxGGcd+0Nqql1861umLTo/H37xC/jud00PYiGE6A8kKexOKWpnn8x3vI9RklfMPafcQyS4lmDtX3nkkQ/5yU9Op6XFtBd897tyGakQov+RpLCblmgLl01YT+0uWGg9D4fVAY5JPPjgs/zjH6czdarmiScUY8b0dqRCCJEcMhQaEI6FeXj5wxzzyDH8u+YD/rA8n0lPvUllpWlm+Mc/LmTOnN/y2mtLJSEIIfo1KSkA896Yx30f3kdRZhEvzH2Bs+1rWP+Tv/PVyTHqG2383/+1MHToL6ioOImcnPm9Ha4QQiRNyicFf8jPXz/+K5dMvIQnzn0CpRSbpozjBC4n2hzlvfdslJR42Lz5WrZvv5OWls/xeI7s7bCFECIpUr766NGVjxKIBLhhxg0opaipgVOvG0XY4ubNo2+lpPXud4MHfx+LxcmOHff0bsBCCJFEKZ0U6oJ13PP+PcwcMpPSwlLicZgzB3bsULx86p+YsPTR9qFMHY4CBgy4ksrKxwmHy3s5ciGESI6UTQpaa7758jepaq7id1/7HWA6pC1aBH/6E3zlGyPNmBXLl7evM2TIj9A6RlnZfb0UtRBCJFfKJoUlZUt4cd2L3PHVO5g2aBrr1sFPf2pKCldcgRnyQim47jr4+GMA3O4R5OfPpbz8T4TDlb0avxBCJEPKJoUPyz4E4LJJlwFw883mtgp//KPJBeTmmlHutm0zCaK5GYCiop+TSITZsuWW3gpdCCGSJmWTwvKK5QzyDaLAW8Dbb8PLL8OPf2yGr2h3ySXmxjt1dSZBAB7PaAYPvoHKykfx+5f0TvBCCJEkKZsUlpUvo7SwFIBf/xoKCuD66ztZcOZMKC2F++8HrQEYNuxWnM6hrFt3GbFYoAejFkKI5ErJpNAUbmJ97XqmDpzKhg1mtNNrrjHVR1+gFPzgB7B2Lbz1FgA2m4+xYx8nGNzE5s039WzwQgiRRCmZFD6u/BiNprSwlAceALsdvvOd/axwwQVmjOynnmqflZl5HIMH/4Dy8j/T1LR8PysLIUTfkZJJYVn5MgCKc0t54gk47zwYOHA/K7jdZqH58yEcbp9dVHQ7dnseGzZ8H63jSY5aCCGSLyWTwtLypQxOH8znKwqoq4OLLurCSl//Ovj95mYKrWy2DEaOvJvGxvfZsOE6dGubgxBC9FUpmRSW7FjC0YOP5p//BI/H3EbzgE48EQoL4RvfMHfXaTVgwOUMGXIz5eV/YufOPyYvaCGE6AEplxTKm8rZ5t/G0YO/wgsvwGmnmcRwQDab6e781a/CbbfBypXtL40YcSdZWSezdevPiEbrkxa7EEIkW8olhSU7TN+CjKajqaiAc8/9EiuPHg1//zs4nfDnP7fPVkoxcuRviMUa2Lbtl90bsBBC9KCUSwrv73gfp9VJ5ceTAdNZ+UvJzoa5c+HJJyHQ0UfB653EwIHfpKzsXnbterIbIxZCiJ6TcklhSdkSphZO5f13HIwZYzqtfWnXXANNTXDfngPjjRr1BzIzj2fduivw+z/onoCFEKIHJTUpKKVOVUp9rpTaqJSa18nrVyilqpVSK1unbyUzHq01H1d+zNTC6bzzDhx77EFu6OijTWnh5z9vHywPwGp1MWHCizgcA/n886tIJML72YgQQhx+kpYUlFJW4AHgNGAccLFSalwniz6rtS5pnf6arHjA3D8hFAthCwylsRGOO+4QNvbgg5CXB2edtUdisNkyOOKIP9PSspZt2+449KCFEKIHJbOkMB3YqLXerLWOAM8AZydxfwdUEagAoHpzIXAIJQUwbQuvvQYWi9nQli3tL+XknEZBwTfYvv1OAoFVhxKyEEL0qGQmhUHAjt3+Lmudt7fzlVKfKKXmK6WGdLYhpdS3lVLLlFLLqqurDzqgiiaTFCrWD2ToUBg8+KA3ZRQXwzvvmIHybrxxj5dGjfodNls269ZdSTzecog7EkKIntHbDc2vAEVa62LgdeCxzhbSWj+stZ6qtZ6al5d30DsrbzK30SxfP5CxYw96M3saNgx+8hN48UUzVsasWVBfj92ew5FH/o1AYBVr1swhkYh20w6FECJ5kpkUdgK7//If3Dqvnda6Vmvd1hr7V6A0ifG0Vx9t+XQgY8Z044ZvvNHcsm3WLPjoI9POEA6Tm3smRxzxEHV1C9i48YZu3KEQQiRHMpPCUmC0Umq4UsoBXAS8vPsCSqndh6GbDaxNYjxUNFXgs6fT0pDGkUd244adTnjuOTP97W/w3ntm8DygsPBqhgz5EeXlD1Be/pdu3KkQQnS/pCUFrXUM+B6wEHOyf05rvUYp9b9Kqdmti31fKbVGKbUK+D5wRbLiASgPlJNpM3moW0sKu/v612Ho0PY7tQEMH34nWVlfY/36a9i166n9rCyEEL0rqW0KWusFWusjtNYjtdZ3tM67TWv9cuvzH2utx2utJ2mtT9Bar0tmPBVNFbhiJil0a0lhdxYLXHopvP46VFS0zrIxYcI/yciYxdq1l7J+/bXEYk1JCkAIIQ5ebzc096iKQAWqqRCf7wD3TzhU3/gGJBLw8MPts6xWD8XFC1pvzPMnPv54JqHQ9iQGIYQQX17KJAWtNRVNFYRrB3LkkeYum0kzZgycfTbcfjs89FD7bKvVw6hRv6O4eCGh0DZWrDiKxsZlSQxECCG+nJRJCv6wn2AsSENZN195tC/PPAOnnw7f/S5ceSW0dPRVyM4+mSlT3kcpJytXzmLr1v8lHg/2QFBCCLF/KZMU2jqu+csGUlTUAzt0ueCll+DWW+Gxx+DCCyHa0VchLW08paUfkp19Blu3/oxVq04mFgvsZ4NCCJF8qZMUWvso0FTIIfR/+3JsNnOXtj/9CV591fRlqO+4CY/DUcCECfMZN+4ZGhuX8OmnpxEOV/RQcEII8UWpkxSa2pLCQHJze3jn3/kO3H+/SQzjx8Ovfw3Nze0v5+fPZdy4p2hqWs6yZZMoK/u9lBqEEL0iZZLC1yd+nX8dWw31I3s+KQBcdx0sWWIaoefNM7f1rKiAUAgwiaG0dBkezxg2bryeDz4YwpYttxGPh3ohWCFEqkqZpKCUItaYCwkbOTm9FMTUqfDmm2acpFWroLAQcnPhrbcASEsbx+TJi5k8eQmZmSewbdsvWLHiKOrr30Rr3UtBCyFSScokBYCaGvPYKyWF3Z19Nrz7rqlGGjwYLrrIdHg76yxYsoSM255mwrLZTJz4LyKRXaxadSIffXQEmzbdRDRa28vBCyH6M1tvB9CTDpukAKbUMHUqnHEGTJ8OL79sGqa/8hXzut1OztKlzJixlaqqZ6iufo6ysvuorHyM0aP/SF7eHFRSO1sIIVJRSpUUamvNlaIeT29Hspvx42H9eti5E1avhltugbffNjfxOeMMrKeexcBb3qZ423cpLV2O0zmUzz6by4oV01mz5kIaGt7t7XcghOhHUiop1NSYUsJh9wN70CDw+Uwbwx13mDu5PfssjBwJTU3wr3/B2WfjffpDpkxYzKTF51D45yoa6hex7t+zWPf+eTJkhhCiW6Rc9dFhUXXUFccdZ0oMAMEgnHMOfPvbWK69lqzWTnAD6s5B/+tfhHNeZMXvX8A+rJjMzGPJzT2PzMzj96xeikbBbu+FNyKEOCSJBJSVmdGXe0DKlRR67cqjQ+F2m97Rf/+7GTbjn/+Eyy9HPf8ilnETcTV6OOpqN8UXbGTAWQ8SmfNVtt5exOrXZrBt6Y3ErrsK7fHAD34AdXW9/W6E+KJYzPz4ORh+P5SXd288+7N6NVxzDdxzz5dft7nZ3L43FjPf6XPPhccf33MZraGy0jy2tJhOr8OG7TEcfzKpvnap49SpU/WyZQc3iNwRR8CUKWZYoj4vHDaXtp55prm89YEHIJFA19eRWLEEa/WeQ3M3TLaRsTIGQGzcUCzHnoy1sAgaG00p4n/+B7xes3BuLrzyijlg48fDjh2masvWWrDcsgWGDIGqKvNB/Z//MdVfB1JTY+43MW6cGf6js2JbPG6+DLaUKsT2jECg4/94f7SGTz+FSMTch9zh6Hht5UrT/nXGGV3bp9amzWzEiI6Sanm5mT9gAFit5v/8zDPNFXnXXmvuZJif37H+ihWmMXD8+I7tVlbCL38Ja9aYm1o5neZknZ9vhq93Os1yfj888oi5I+LNN0NJiZmfSJjlwMR3551mdGOLBX7/e/PZ9Hph+HC44goTY1OT+WF2++0dsf3iF+bmWsXF5jvy2mtw9dWwbp35Xl58sbnkvKLCbOOjj8x3R2vzvXI4TIL4+c9h1y7z/bj3XnOTrpwck0TCYRg1CrZvh8WLzYUpB0EptVxrPbUL/2e6T02lpaX6YGVlaX3ttQe9et8Rj2u9cqXWv/udDt/zU73rpR/pdeu+rT97aqLecpVd101Bx1xoDTrhsOuEw6G1+ZiaKTvbPNpsWk+a1DHvG98wE2h91FFajxzZ8fxnP9P6ssu0vu46rX/6U60ffVTrJUvMVFmp9erVWk+dqrXDobXFonV6utZ33ql1fX1H3OXlWo8fr/XRR2sdDJp5waDWDz6o9Zlnmm1GIl98v4mEedywQeu33+5Yt01jo9axmNZr1mj98staNzfvue6mTVq3tGjd1KT100+b7Wit9ZYtWv/kJ1ovWNCxj84sXar1r35ltnvDDVpfconWr7zS8d7ee0/rW2/VesUKrefN0/rEE83zujoTV2fvZ8cOrUMh8/fGjVofe6zWbrfWgwdr/d//av3qq1q//vqecb36qtYjRpjjGg6beYGAWeaGG8xxv/RSrR94QOvnn9d6504Tc3m5eZ5ImM/OpZd2fBbGjdP6nnu0vvFGrU87rWP+vfd2xLpsmXn/Cxea9T/5ROvjjzfvs+3zk5Vl4nr+ea2V6vh8HXGE1iefbP4+7jjzmtut9Xe/q/Xvf691cXHHPmfO1Pr667WeP99s1+k0n70f/EBrj8d8bgYO1LqgQOvf/EbrDz80xwPM606n1l//utann6611ar1V75iPlcul1nGYjHzCwq0zs/X2uvtiHP378dll5ljVlpq/h40yGzfYjHHq21bbfvOydH6hBNM/PPmaX3WWVqfcYbWL7xgPiOTJ5vlrNaOfXzve1pfdZXWP/yh1m++qXV1tdZFReZ7dpCAZboL59heP8l/2elgk0I0aj5vh3BM+4VEIq79/qV6w4Yb9OL/evRb/0Uve7VIV35vnC6/eZKu+dEsHZtzjvnyXnGF+fL96lfmi5CdbT64V16pdUaGme68s+NEP2SI1pmZHV/6vSeHQ+t//cucnM8808xzOrWeONEkjAEDzAkBtL7gAnMiHTDA/J2XZx6HDjUnqCuvNDEVF5vtTpnSsV+32ySvW281X8a2L2lbHF6v1tOmaX3MMSZmMPspKupYpqDAxNb295QpWt9/v/mylpaa/d5+u9b/+U9HEm179Pk61ms74ew+pad3PB8zRut339W6ocGcMI46SuvCQvNaYaHWF15o4s3MNCf2I4/cc1tf+YrWf/mL1nfcYY5Dbm5HLFOm7HnsTjih4/h2Ng0caP4vQOubbtL6sce0Hjas45iOGaP1j3+s9fnnm3mTJ3ec+Nomp9OcRPPzzWenuFjr3/624//bYtF6+nStH3pI61tu0frUU81n6n/+x3xA160zn7u24zZ5sln27rvNcW+L3243SajNvfea+aNGaf3Vr3bEk5NjfihUVZnPxJAhZrrmGq1LSsyPkKuv1nr9evN5uvhirf3+ju0uXar1j35kEuPf/mb22ZaIt283n/+GBq1razsS64svar1qlUmQy5ebHxz7U19v9lNfr/X/+39a//nPnS9XV3fQ33utu54UUqb6qLralCz/8Af43veSEFgfFI3WUVX1HHV1/6al5XOUUrS0bEApG05nIT7fdLKzv4bLNQyfrxQbaaY4m5FhirqRiCkKV1ebaoDsbLPheBw2bIBNm8xXc+NGSEszVQS7391oxQpTn7p1qykiKwU//SksWGCqBgBOOslcpnv88aZo/qtfwQcfQEGB2efw4TBhgqnWOOYY08/j3/+Gp54y1SUjR5oRasFc5TVihKnL3bTJxF9QAEcfbarLGhrgZz8z8a5da6ogbrjBNPj/6lcd7+Ooo0yV27vvmveXnQ233QZ33WWGMLnmGvPa0qWmDWfYMJg921Q9TJli9ve3v5nj9MADphGxzfHHmw6NkyebOD//HE47zcRVVGSqQ/74RzNcSnU13H23qc5rW/ef/4Rly0y13pYt5kq29etNtcktt5jj3NgImzeb5ZqbIT3dVGEsXWr+L772NbOsUuYYNTRAXl7HZXvRqInhhRfM8bjgAjNM/DvvwPLlZpkf/rCjCgg6qloWLDDVngMGdLzW3GyuE9/9woj6evMZ2/vmJ9GoqUJJS4MZMzrmx+NmuyedZD6fq1ebv+fMSeJtFvuWrlYfpUxSWLvWVGU//bTpQCw619LyORUVfyUU2kFDw5tEo9Wtr1hwuYbichWRnX0aGRmz8HqLsVrTkhNIRYU5WaV1sn2tD3xdcTxulrF007UUsZg5yQ4f3tHesXOn6XQ4Y4Y5iR+MhgZzIt+40SSO3U90XaFb6//z8pJ8O0HR10lS2Ms775gfTf/5D5x8chIC64e0jhMMbiIU2orf/z6h0Caam9cQCHzcuoQFj+cIvN4peL2TsdtziETKycg4hvT0mVgs0lgsxOGiq0khZb61h9UQF32EUlY8niPweI4gO/uU9vnhcDlNTctoalpBIPAxfv87VFU9tde6NhyOQdhsPtLTv0Jm5rE4ncNwuYbidA5CKWtPvx0hRBekTFIYNcpUk/ZQ/49+zeksxOmcTW7u7PZ5kUgNsVgDDkcedXWvEwisIBzeQTRaT1XVU1RUPNy+rMXiwu0+Ard7NFarB4djADZbJqHQdrKzTyU3dzZKpVQXGiEOGylTfSR6TyIRJhjcTDi8g1BoG8Hgepqb1xIKbSIeDxKJVKB1BKvVSzwewGJx43IV4fNNx2bLIBqtIhzeSVraRNLTjyYj42hcrhHEYvUo5cBm68K190KkOKk+EocNi8VJWtpY0tLGdvq61nHi8SAWi4uamhdpbPyAYPBz6upeI5EIYbdn4XAUsmvXE5SXPwiA1eojHm8CrKSljcVq9WGxuPF4jmTgwG/ico0gFNpMIhEiPX0G0WgNiUQEh2OgtHUIsR9SUhB9htZxmps/o7HxfQKBT3C5hhGPNxEIfEIi0UI8HiQQWEEisedwCW0lkNa/cDoH4fEcidM5BJstA49nHE7nIGy2dKzW9NZHH1ZruiQQ0W9ISUH0O0pZ8Xon4vVO3Ocy0WgdtbULiEarcDoHAYr6+jdwu4/AZksnFNpOKLSVlpZ1NDd/RixWTyLRso/9OUhLG4/F4sJqTcflKgISWK0+7PY8bLZM4nE/dnsuXm8JHs/41vUUFouz+w+AED1AkoLoV+z2bAYMuHSPefn5F+5zea0ThEJbiESqiccbicUaiccbicebCId30tz8KVrHiEarCQRWoJSVWKyJRKJ5v3FYrRmkpx+F1tHWKrAc7Pbc1ikPuz2XWKyxtaRyJFarj0hkFzabD59vKqHQNuz2fKzWNGKxemy2LLmpkugRkhRESlPKgts9Erd75JdaLx5vIRZrwGpNJxKpJBBYSUvLZyhlBzSh0DYaGz/EavVgtaYRDu8kEFhFNFpNIhE6QEwOtI6glB2bLYtotAqPZzx5eefidA7B738Pl2s4dnsWodA2LBY3VqsXq9WHzZaO0zmUaLSKaLQGr3cKDkceFosbsBCNVmG35+NwFEiSEZ2SpCDEQTAne3MLP5ttFB7PqC6vG4sFiMVqsVq9RKN1BIMbiMcD2O15RCIVNDZ+hMczhlBoE5FIFW73aOrrF7Jt26+ABHZ7buu9ujUWi4dEIgzEv1T8NlsmHs84PJ6xgCYSKScarcHlGonLVYTF4tpjslo92GzZhMPbiMX8+HylOJ3m+u5otJZ43I/bfQQOx0Ci0V1EItXY7bmt25LTTF8iDc1C9BGxmJ9IpBK3+whiMT9ah7HbzfhCiUSYeDxALFZPKLQNmy0ThyOPpqaPicf9xONB2hJKJFJJS8tampvX0tKytrWj4UDs9mxaWtYTiVSidbhbYrZY0vB6iwmHdxKPN2G355KRMYvm5jVAnPT0o7FYPMRidUQiFcRiTeTnz8Xnm0YwuB6//1283slYrV6CwY3tVW2QAKx4PKMBRSSyi2i0FpstvbXazU0kUo3bPQKLxU0otA2Xa0jyhmXpA2SYCyHEQdM6QSIRJpEIkUiEiMebiUZrcDoLsdkyaGr6mEikHLBgt+dgtXppbv6UaLQOh2MADkcekUg1gcByAoFVOJ1DW6u7ttLQ8DYezzgsFgdNTSvQOoLNlo3TORCtYzQ3r26Pw2JxHbC67cCstJWkHI5BeDymRGOuVAvjcBTicOQRDpehlAOvt6T1CjaFxeJC6xhNTcuw2TJbk5iT5uZPUMqJzzeFUGgLTqcZFywc3kFT03IsFjc+Xynp6dOprn6BaLSGtLSxpKfPaG+TsttzW2Pbk8s1DKfTjGOVSISJRuvROoLTOeSQqvwkKQgh+hytNU1NHxGJVOFwDMTnm0xz8xq0juPxHElLy3oSiRBKWVs7RX4OWHA4BmC35xCLNRKNVrdWx+XQ0rKWRCKE2z2acHgHLS3rCQbXEwrtwOstwW7PIhzeSSSyC6ezkHi8hebm1VitPpRS7SUsr7eEaLSW5uZPAHC5iojHW4hGq7BY0va48MDhGITWYaLR1rF1sGCzZRKLdf2uh3Z7PvF4YI8r4+z2XIYO/TFDhtx4UMdWLkkVQvQ5SinS04/aY57XW9z+3Ocr2eO1zMxjeiSuNqYEFcFqdaF1gmi0prUtaBfR6K72UofWmmBwA37/+2RmHo/bXUQ4vJPGxg+x2bKwWn3EYrV88Ue5pqXlM5qb12KzZWC3Z2OzZQGKpqaPcDgKk/4epaQghBApoKslhaSOOqaUOlUp9blSaqNSal4nrzuVUs+2vv6hUqoomfEIIYTYv6QlBWXGRn4AOA0YB1yslBq312LfBOq11qOA3wG/TlY8QgghDiyZJYXpwEat9WatdQR4Bjh7r2XOBh5rfT4fOFFJjxohhOg1yUwKg4Adu/1d1jqv02W01jHAD+TsvSGl1LeVUsuUUsuqq6v3flkIIUQ36RN3MtFaP6y1nqq1npqXl9fb4QghRL+VzKSwExiy29+DW+d1uoxSygZkALVJjEkIIcR+JDMpLAVGK6WGK6UcwEXAy3st8zJweevzC4A3dV+7RlYIIfqRpHVe01rHlFLfAxZi+nI/orVeo5T6X2CZ1vpl4G/AE0qpjUAdJnEIIYToJX2u85pSqhrYdpCr5wI1B1wqtcgx+SI5JnuS4/FFffGYDNNaH7BRts8lhUOhlFrWlR59qUSOyRfJMdmTHI8v6s/HpE9cfSSEEKJnSFIQ/7+9+wuRqgzjOP799QcpjUwokYhK86KE2ixC0qIIKr3RwEgqiwi8MUjoosSi6L6EoMwicS2pqJQiuiiX2PDCzGT9n5nVhWLuTVgGSenTxfvuaXbX0UGYOcu8vw8MM/ue2eE5D+/ZZ887M88xM6uUVhTeqjuAMcg5Gc05Gc75GK1rc1LUewpmZnZmpZ0pmJnZGRRTFM7WxrsEkn6VtEvSgKRteWySpK8kHcj3l9UdZztJWl0KZpMAAAQHSURBVCNpUNLuhrHT5kDJa3nO7JQ0s77I26dJTl6SdDjPlQFJ8xq2Lc852S/pvnqibh9JV0n6WtJeSXskPZ3Hi5gnRRSFFtt4l+LuiOhp+Djdc0BfREwH+vLP3WwtcP+IsWY5mAtMz7clwKoOxdhpaxmdE4CVea70RMQXAPm4WQTMyL/zRj6+usm/wDMRcQMwC1ia97uIeVJEUaC1Nt6lamxf3gssqDGWtouIb0jfnm/ULAfzgXWRbAEmSprSmUg7p0lOmpkPfBARJyLiF+An0vHVNSLiSERsz4//BPaROjoXMU9KKQqttPEuQQBfSvpe0pI8NjkijuTHvwGT6wmtVs1yUPq8eSovh6xpWFYsKif5apA3A99SyDwppShYMiciZpJOd5dKurNxY25GWPTH0ZyDyipgGtADHAFeqTeczpM0AfgEWBYRfzRu6+Z5UkpRaKWNd9eLiMP5fhDYSDrtPzp0qpvvB+uLsDbNclDsvImIoxFxMiJOAW/z/xJRETmRdCGpIKyPiA15uIh5UkpRaKWNd1eTNF7SJUOPgXuB3QxvX/448Gk9EdaqWQ4+Ax7Lny6ZBRxrWD7oaiPWxB8gzRVIOVkkaZyka0lvrm7tdHztlC8J/A6wLyJebdhUxjyJiCJuwDzgR+AgsKLueGrY/6nAjnzbM5QD0uVP+4ADwCZgUt2xtjkP75OWQ/4hrf0+2SwHgEifWjsI7AJurTv+Dubk3bzPO0l/9KY0PH9Fzsl+YG7d8bchH3NIS0M7gYF8m1fKPPE3ms3MrFLK8pGZmbXARcHMzCouCmZmVnFRMDOziouCmZlVXBTMOkjSXZI+rzsOs2ZcFMzMrOKiYHYakh6VtDVfS2C1pPMlHZe0MvfY75N0eX5uj6QtuXncxoY++9dJ2iRph6Ttkqbll58g6WNJP0han79BazYmuCiYjSDpeuAhYHZE9AAngUeA8cC2iJgB9AMv5l9ZBzwbETeSvtE6NL4eeD0ibgJuJ31rGFLXzWWka3tMBWa3fafMWnRB3QGYjUH3ALcA3+V/4i8iNT87BXyYn/MesEHSpcDEiOjP473AR7nP1JURsREgIv4GyK+3NSIO5Z8HgGuAze3fLbOzc1EwG01Ab0QsHzYovTDieefaI+ZEw+OT+Di0McTLR2aj9QELJV0B1bV5ryYdLwvzcx4GNkfEMeB3SXfk8cVAf6Qrdh2StCC/xjhJF3d0L8zOgf9DMRshIvZKep50lbrzSN1DlwJ/AbflbYOk9x0gtVF+M//R/xl4Io8vBlZLejm/xoMd3A2zc+IuqWYtknQ8IibUHYdZO3n5yMzMKj5TMDOzis8UzMys4qJgZmYVFwUzM6u4KJiZWcVFwczMKi4KZmZW+Q88EA2LJomBfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 294us/sample - loss: 0.2863 - acc: 0.9215\n",
      "Loss: 0.2863192579363737 Accuracy: 0.9214953\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2267 - acc: 0.2684\n",
      "Epoch 00001: val_loss improved from inf to 1.53086, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/001-1.5309.hdf5\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 2.2258 - acc: 0.2688 - val_loss: 1.5309 - val_acc: 0.5355\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4953 - acc: 0.5172\n",
      "Epoch 00002: val_loss improved from 1.53086 to 1.08238, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/002-1.0824.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 1.4948 - acc: 0.5175 - val_loss: 1.0824 - val_acc: 0.6792\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1551 - acc: 0.6354\n",
      "Epoch 00003: val_loss improved from 1.08238 to 0.87475, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/003-0.8748.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 1.1546 - acc: 0.6356 - val_loss: 0.8748 - val_acc: 0.7293\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9932 - acc: 0.6867\n",
      "Epoch 00004: val_loss improved from 0.87475 to 0.72992, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/004-0.7299.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.9928 - acc: 0.6869 - val_loss: 0.7299 - val_acc: 0.7876\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7217\n",
      "Epoch 00005: val_loss improved from 0.72992 to 0.64315, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/005-0.6432.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.8764 - acc: 0.7217 - val_loss: 0.6432 - val_acc: 0.8022\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7483\n",
      "Epoch 00006: val_loss improved from 0.64315 to 0.57640, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/006-0.5764.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.7945 - acc: 0.7483 - val_loss: 0.5764 - val_acc: 0.8258\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7337 - acc: 0.7687\n",
      "Epoch 00007: val_loss improved from 0.57640 to 0.53338, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/007-0.5334.hdf5\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.7337 - acc: 0.7688 - val_loss: 0.5334 - val_acc: 0.8372\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.7886\n",
      "Epoch 00008: val_loss improved from 0.53338 to 0.48100, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/008-0.4810.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.6709 - acc: 0.7887 - val_loss: 0.4810 - val_acc: 0.8558\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6154 - acc: 0.8061\n",
      "Epoch 00009: val_loss improved from 0.48100 to 0.43389, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/009-0.4339.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.6155 - acc: 0.8061 - val_loss: 0.4339 - val_acc: 0.8735\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8190\n",
      "Epoch 00010: val_loss improved from 0.43389 to 0.40631, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/010-0.4063.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.5791 - acc: 0.8190 - val_loss: 0.4063 - val_acc: 0.8796\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8309\n",
      "Epoch 00011: val_loss improved from 0.40631 to 0.37728, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/011-0.3773.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.5334 - acc: 0.8309 - val_loss: 0.3773 - val_acc: 0.8894\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4988 - acc: 0.8435\n",
      "Epoch 00012: val_loss improved from 0.37728 to 0.35622, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/012-0.3562.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.4986 - acc: 0.8434 - val_loss: 0.3562 - val_acc: 0.8947\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8535\n",
      "Epoch 00013: val_loss improved from 0.35622 to 0.34906, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/013-0.3491.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.4673 - acc: 0.8535 - val_loss: 0.3491 - val_acc: 0.8970\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8613\n",
      "Epoch 00014: val_loss improved from 0.34906 to 0.31597, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/014-0.3160.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4424 - acc: 0.8613 - val_loss: 0.3160 - val_acc: 0.9078\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8692\n",
      "Epoch 00015: val_loss improved from 0.31597 to 0.31086, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/015-0.3109.hdf5\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4156 - acc: 0.8693 - val_loss: 0.3109 - val_acc: 0.9059\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8755\n",
      "Epoch 00016: val_loss did not improve from 0.31086\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3957 - acc: 0.8755 - val_loss: 0.3134 - val_acc: 0.9050\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8814\n",
      "Epoch 00017: val_loss improved from 0.31086 to 0.29565, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/017-0.2956.hdf5\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.3757 - acc: 0.8813 - val_loss: 0.2956 - val_acc: 0.9082\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8866\n",
      "Epoch 00018: val_loss improved from 0.29565 to 0.26908, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/018-0.2691.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3581 - acc: 0.8868 - val_loss: 0.2691 - val_acc: 0.9182\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8906\n",
      "Epoch 00019: val_loss did not improve from 0.26908\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3458 - acc: 0.8905 - val_loss: 0.2732 - val_acc: 0.9140\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8964\n",
      "Epoch 00020: val_loss improved from 0.26908 to 0.24737, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/020-0.2474.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3232 - acc: 0.8965 - val_loss: 0.2474 - val_acc: 0.9243\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8985\n",
      "Epoch 00021: val_loss did not improve from 0.24737\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3207 - acc: 0.8985 - val_loss: 0.2547 - val_acc: 0.9224\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9011\n",
      "Epoch 00022: val_loss improved from 0.24737 to 0.23819, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/022-0.2382.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3064 - acc: 0.9011 - val_loss: 0.2382 - val_acc: 0.9301\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9080\n",
      "Epoch 00023: val_loss did not improve from 0.23819\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2915 - acc: 0.9079 - val_loss: 0.2423 - val_acc: 0.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9084\n",
      "Epoch 00024: val_loss improved from 0.23819 to 0.23324, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/024-0.2332.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.2841 - acc: 0.9084 - val_loss: 0.2332 - val_acc: 0.9311\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9104\n",
      "Epoch 00025: val_loss improved from 0.23324 to 0.21627, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/025-0.2163.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2773 - acc: 0.9105 - val_loss: 0.2163 - val_acc: 0.9369\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9154\n",
      "Epoch 00026: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.2659 - acc: 0.9152 - val_loss: 0.2228 - val_acc: 0.9345\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9171\n",
      "Epoch 00027: val_loss improved from 0.21627 to 0.21594, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/027-0.2159.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2577 - acc: 0.9170 - val_loss: 0.2159 - val_acc: 0.9376\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9173\n",
      "Epoch 00028: val_loss did not improve from 0.21594\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2525 - acc: 0.9173 - val_loss: 0.2165 - val_acc: 0.9345\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9220\n",
      "Epoch 00029: val_loss improved from 0.21594 to 0.21314, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/029-0.2131.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2417 - acc: 0.9220 - val_loss: 0.2131 - val_acc: 0.9343\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9254\n",
      "Epoch 00030: val_loss did not improve from 0.21314\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2345 - acc: 0.9254 - val_loss: 0.2190 - val_acc: 0.9369\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00031: val_loss did not improve from 0.21314\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.2251 - acc: 0.9272 - val_loss: 0.2188 - val_acc: 0.9373\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9274\n",
      "Epoch 00032: val_loss improved from 0.21314 to 0.20189, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/032-0.2019.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2222 - acc: 0.9274 - val_loss: 0.2019 - val_acc: 0.9394\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9303\n",
      "Epoch 00033: val_loss did not improve from 0.20189\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2132 - acc: 0.9303 - val_loss: 0.2043 - val_acc: 0.9380\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9313\n",
      "Epoch 00034: val_loss improved from 0.20189 to 0.20008, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/034-0.2001.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2112 - acc: 0.9312 - val_loss: 0.2001 - val_acc: 0.9392\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9338\n",
      "Epoch 00035: val_loss improved from 0.20008 to 0.19619, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/035-0.1962.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2048 - acc: 0.9339 - val_loss: 0.1962 - val_acc: 0.9446\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9370\n",
      "Epoch 00036: val_loss improved from 0.19619 to 0.18768, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/036-0.1877.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1945 - acc: 0.9370 - val_loss: 0.1877 - val_acc: 0.9455\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9354\n",
      "Epoch 00037: val_loss did not improve from 0.18768\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1968 - acc: 0.9354 - val_loss: 0.1952 - val_acc: 0.9418\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9375\n",
      "Epoch 00038: val_loss did not improve from 0.18768\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1882 - acc: 0.9375 - val_loss: 0.2009 - val_acc: 0.9420\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9392\n",
      "Epoch 00039: val_loss did not improve from 0.18768\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1833 - acc: 0.9392 - val_loss: 0.1918 - val_acc: 0.9425\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9403\n",
      "Epoch 00040: val_loss improved from 0.18768 to 0.18743, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/040-0.1874.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1823 - acc: 0.9403 - val_loss: 0.1874 - val_acc: 0.9467\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9415\n",
      "Epoch 00041: val_loss improved from 0.18743 to 0.18353, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/041-0.1835.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1775 - acc: 0.9414 - val_loss: 0.1835 - val_acc: 0.9474\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9421\n",
      "Epoch 00042: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1764 - acc: 0.9421 - val_loss: 0.2010 - val_acc: 0.9432\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9452\n",
      "Epoch 00043: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1670 - acc: 0.9452 - val_loss: 0.1912 - val_acc: 0.9457\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9439\n",
      "Epoch 00044: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1655 - acc: 0.9438 - val_loss: 0.1871 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9469\n",
      "Epoch 00045: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.1582 - acc: 0.9470 - val_loss: 0.1863 - val_acc: 0.9460\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9479\n",
      "Epoch 00046: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1555 - acc: 0.9479 - val_loss: 0.1845 - val_acc: 0.9464\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9479\n",
      "Epoch 00047: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1551 - acc: 0.9479 - val_loss: 0.1907 - val_acc: 0.9455\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9506\n",
      "Epoch 00048: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1480 - acc: 0.9506 - val_loss: 0.1969 - val_acc: 0.9448\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9490\n",
      "Epoch 00049: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1480 - acc: 0.9490 - val_loss: 0.1958 - val_acc: 0.9464\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9511\n",
      "Epoch 00050: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1453 - acc: 0.9511 - val_loss: 0.1967 - val_acc: 0.9453\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9537\n",
      "Epoch 00051: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1380 - acc: 0.9538 - val_loss: 0.1945 - val_acc: 0.9488\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9523\n",
      "Epoch 00052: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1434 - acc: 0.9523 - val_loss: 0.1898 - val_acc: 0.9483\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9553\n",
      "Epoch 00053: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1330 - acc: 0.9552 - val_loss: 0.2103 - val_acc: 0.9439\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9564\n",
      "Epoch 00054: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1282 - acc: 0.9564 - val_loss: 0.1986 - val_acc: 0.9464\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9565\n",
      "Epoch 00055: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1301 - acc: 0.9566 - val_loss: 0.1927 - val_acc: 0.9495\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9574\n",
      "Epoch 00056: val_loss did not improve from 0.18353\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1286 - acc: 0.9574 - val_loss: 0.1844 - val_acc: 0.9502\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9587\n",
      "Epoch 00057: val_loss improved from 0.18353 to 0.18050, saving model to model/checkpoint/1D_CNN_DO_5_only_conv_checkpoint/057-0.1805.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1219 - acc: 0.9588 - val_loss: 0.1805 - val_acc: 0.9495\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9579\n",
      "Epoch 00058: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1231 - acc: 0.9579 - val_loss: 0.1904 - val_acc: 0.9478\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9597\n",
      "Epoch 00059: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1194 - acc: 0.9597 - val_loss: 0.1935 - val_acc: 0.9497\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9609\n",
      "Epoch 00060: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1174 - acc: 0.9609 - val_loss: 0.2026 - val_acc: 0.9446\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9609\n",
      "Epoch 00061: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1153 - acc: 0.9609 - val_loss: 0.1851 - val_acc: 0.9525\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9620\n",
      "Epoch 00062: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1112 - acc: 0.9620 - val_loss: 0.1949 - val_acc: 0.9476\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9624\n",
      "Epoch 00063: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1128 - acc: 0.9624 - val_loss: 0.2022 - val_acc: 0.9478\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9622\n",
      "Epoch 00064: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1118 - acc: 0.9622 - val_loss: 0.2068 - val_acc: 0.9467\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9615\n",
      "Epoch 00065: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1133 - acc: 0.9615 - val_loss: 0.2051 - val_acc: 0.9462\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9642\n",
      "Epoch 00066: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1049 - acc: 0.9642 - val_loss: 0.2072 - val_acc: 0.9476\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9651\n",
      "Epoch 00067: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1058 - acc: 0.9651 - val_loss: 0.2041 - val_acc: 0.9490\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9649\n",
      "Epoch 00068: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1019 - acc: 0.9649 - val_loss: 0.1959 - val_acc: 0.9511\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9657\n",
      "Epoch 00069: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0996 - acc: 0.9657 - val_loss: 0.1900 - val_acc: 0.9502\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9653\n",
      "Epoch 00070: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1012 - acc: 0.9653 - val_loss: 0.1986 - val_acc: 0.9488\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9665\n",
      "Epoch 00071: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0967 - acc: 0.9665 - val_loss: 0.1968 - val_acc: 0.9485\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9674\n",
      "Epoch 00072: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0967 - acc: 0.9674 - val_loss: 0.1974 - val_acc: 0.9506\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9682\n",
      "Epoch 00073: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0927 - acc: 0.9682 - val_loss: 0.1882 - val_acc: 0.9506\n",
      "Epoch 74/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9674\n",
      "Epoch 00074: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0951 - acc: 0.9675 - val_loss: 0.2185 - val_acc: 0.9471\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9693\n",
      "Epoch 00075: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0924 - acc: 0.9692 - val_loss: 0.1994 - val_acc: 0.9515\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9681\n",
      "Epoch 00076: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0923 - acc: 0.9681 - val_loss: 0.2011 - val_acc: 0.9471\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9698\n",
      "Epoch 00077: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0874 - acc: 0.9698 - val_loss: 0.2081 - val_acc: 0.9502\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9692\n",
      "Epoch 00078: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0914 - acc: 0.9692 - val_loss: 0.2006 - val_acc: 0.9499\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9698\n",
      "Epoch 00079: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0906 - acc: 0.9698 - val_loss: 0.1995 - val_acc: 0.9509\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9704\n",
      "Epoch 00080: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0859 - acc: 0.9704 - val_loss: 0.2177 - val_acc: 0.9492\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9719\n",
      "Epoch 00081: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0821 - acc: 0.9719 - val_loss: 0.2046 - val_acc: 0.9513\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9715\n",
      "Epoch 00082: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0826 - acc: 0.9715 - val_loss: 0.2101 - val_acc: 0.9483\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9704\n",
      "Epoch 00083: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0846 - acc: 0.9705 - val_loss: 0.2116 - val_acc: 0.9504\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9745\n",
      "Epoch 00084: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0761 - acc: 0.9745 - val_loss: 0.2095 - val_acc: 0.9522\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9725\n",
      "Epoch 00085: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0819 - acc: 0.9725 - val_loss: 0.2129 - val_acc: 0.9492\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9735\n",
      "Epoch 00086: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0763 - acc: 0.9734 - val_loss: 0.2004 - val_acc: 0.9511\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9732\n",
      "Epoch 00087: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0774 - acc: 0.9733 - val_loss: 0.2084 - val_acc: 0.9483\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9748\n",
      "Epoch 00088: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0736 - acc: 0.9748 - val_loss: 0.2276 - val_acc: 0.9490\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9730\n",
      "Epoch 00089: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0782 - acc: 0.9730 - val_loss: 0.2089 - val_acc: 0.9509\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9756\n",
      "Epoch 00090: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0726 - acc: 0.9755 - val_loss: 0.2124 - val_acc: 0.9476\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9762\n",
      "Epoch 00091: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0716 - acc: 0.9762 - val_loss: 0.2223 - val_acc: 0.9469\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9733\n",
      "Epoch 00092: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0781 - acc: 0.9733 - val_loss: 0.2087 - val_acc: 0.9527\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9764\n",
      "Epoch 00093: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0719 - acc: 0.9764 - val_loss: 0.2090 - val_acc: 0.9506\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9747\n",
      "Epoch 00094: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0718 - acc: 0.9747 - val_loss: 0.2164 - val_acc: 0.9490\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9767\n",
      "Epoch 00095: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0692 - acc: 0.9768 - val_loss: 0.2165 - val_acc: 0.9504\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9750\n",
      "Epoch 00096: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0710 - acc: 0.9750 - val_loss: 0.2217 - val_acc: 0.9495\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9736\n",
      "Epoch 00097: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0797 - acc: 0.9736 - val_loss: 0.2288 - val_acc: 0.9474\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9781\n",
      "Epoch 00098: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0637 - acc: 0.9781 - val_loss: 0.2240 - val_acc: 0.9453\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9780\n",
      "Epoch 00099: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0654 - acc: 0.9780 - val_loss: 0.2188 - val_acc: 0.9504\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9760\n",
      "Epoch 00100: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0681 - acc: 0.9761 - val_loss: 0.2064 - val_acc: 0.9511\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9768\n",
      "Epoch 00101: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0691 - acc: 0.9769 - val_loss: 0.2081 - val_acc: 0.9476\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9784\n",
      "Epoch 00102: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0640 - acc: 0.9784 - val_loss: 0.2281 - val_acc: 0.9490\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9784\n",
      "Epoch 00103: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0631 - acc: 0.9784 - val_loss: 0.2234 - val_acc: 0.9492\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9762\n",
      "Epoch 00104: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0684 - acc: 0.9762 - val_loss: 0.2035 - val_acc: 0.9543\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9779\n",
      "Epoch 00105: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0677 - acc: 0.9778 - val_loss: 0.2022 - val_acc: 0.9515\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9772\n",
      "Epoch 00106: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0642 - acc: 0.9771 - val_loss: 0.2209 - val_acc: 0.9495\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9792\n",
      "Epoch 00107: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0601 - acc: 0.9792 - val_loss: 0.2058 - val_acc: 0.9502\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9784\n",
      "Epoch 00108: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0607 - acc: 0.9784 - val_loss: 0.2056 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9791\n",
      "Epoch 00109: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0605 - acc: 0.9792 - val_loss: 0.2129 - val_acc: 0.9532\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9782\n",
      "Epoch 00110: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0625 - acc: 0.9782 - val_loss: 0.2176 - val_acc: 0.9520\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9787\n",
      "Epoch 00111: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0637 - acc: 0.9787 - val_loss: 0.2136 - val_acc: 0.9506\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9791\n",
      "Epoch 00112: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0605 - acc: 0.9791 - val_loss: 0.2240 - val_acc: 0.9511\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9810\n",
      "Epoch 00113: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0569 - acc: 0.9810 - val_loss: 0.2253 - val_acc: 0.9497\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9810\n",
      "Epoch 00114: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0552 - acc: 0.9810 - val_loss: 0.2320 - val_acc: 0.9509\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9794\n",
      "Epoch 00115: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0584 - acc: 0.9794 - val_loss: 0.2241 - val_acc: 0.9509\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9805\n",
      "Epoch 00116: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0586 - acc: 0.9805 - val_loss: 0.2130 - val_acc: 0.9541\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9823\n",
      "Epoch 00117: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0515 - acc: 0.9823 - val_loss: 0.2298 - val_acc: 0.9518\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9807\n",
      "Epoch 00118: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0568 - acc: 0.9807 - val_loss: 0.2373 - val_acc: 0.9550\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9797\n",
      "Epoch 00119: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0601 - acc: 0.9797 - val_loss: 0.2208 - val_acc: 0.9532\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9825\n",
      "Epoch 00120: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0519 - acc: 0.9826 - val_loss: 0.2475 - val_acc: 0.9506\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9807\n",
      "Epoch 00121: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0571 - acc: 0.9807 - val_loss: 0.2193 - val_acc: 0.9527\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9821\n",
      "Epoch 00122: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0531 - acc: 0.9821 - val_loss: 0.2157 - val_acc: 0.9527\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9814\n",
      "Epoch 00123: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0548 - acc: 0.9814 - val_loss: 0.2363 - val_acc: 0.9518\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9808\n",
      "Epoch 00124: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0552 - acc: 0.9808 - val_loss: 0.2261 - val_acc: 0.9560\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9819\n",
      "Epoch 00125: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0535 - acc: 0.9819 - val_loss: 0.2220 - val_acc: 0.9529\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9823\n",
      "Epoch 00126: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0516 - acc: 0.9823 - val_loss: 0.2501 - val_acc: 0.9478\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9823\n",
      "Epoch 00127: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0529 - acc: 0.9823 - val_loss: 0.2304 - val_acc: 0.9511\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9835\n",
      "Epoch 00128: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0499 - acc: 0.9835 - val_loss: 0.2240 - val_acc: 0.9522\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9818\n",
      "Epoch 00129: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0538 - acc: 0.9819 - val_loss: 0.2282 - val_acc: 0.9506\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9829\n",
      "Epoch 00130: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0514 - acc: 0.9829 - val_loss: 0.2283 - val_acc: 0.9534\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9815\n",
      "Epoch 00131: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0539 - acc: 0.9815 - val_loss: 0.2258 - val_acc: 0.9504\n",
      "Epoch 132/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9836\n",
      "Epoch 00132: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0497 - acc: 0.9836 - val_loss: 0.2121 - val_acc: 0.9541\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9836\n",
      "Epoch 00133: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0487 - acc: 0.9836 - val_loss: 0.2417 - val_acc: 0.9518\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9830\n",
      "Epoch 00134: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0500 - acc: 0.9830 - val_loss: 0.2327 - val_acc: 0.9527\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9828\n",
      "Epoch 00135: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0499 - acc: 0.9829 - val_loss: 0.2357 - val_acc: 0.9522\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9836\n",
      "Epoch 00136: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0487 - acc: 0.9836 - val_loss: 0.2335 - val_acc: 0.9506\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9844\n",
      "Epoch 00137: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0470 - acc: 0.9844 - val_loss: 0.2443 - val_acc: 0.9520\n",
      "Epoch 138/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9828\n",
      "Epoch 00138: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0507 - acc: 0.9828 - val_loss: 0.2332 - val_acc: 0.9518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9848\n",
      "Epoch 00139: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0450 - acc: 0.9848 - val_loss: 0.2184 - val_acc: 0.9532\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9831\n",
      "Epoch 00140: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0498 - acc: 0.9830 - val_loss: 0.2241 - val_acc: 0.9534\n",
      "Epoch 141/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9849\n",
      "Epoch 00141: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0453 - acc: 0.9848 - val_loss: 0.2228 - val_acc: 0.9557\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9849\n",
      "Epoch 00142: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0458 - acc: 0.9849 - val_loss: 0.2204 - val_acc: 0.9548\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9861\n",
      "Epoch 00143: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0413 - acc: 0.9861 - val_loss: 0.2304 - val_acc: 0.9518\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9849\n",
      "Epoch 00144: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0459 - acc: 0.9849 - val_loss: 0.2344 - val_acc: 0.9527\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9842\n",
      "Epoch 00145: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0474 - acc: 0.9842 - val_loss: 0.2341 - val_acc: 0.9511\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9845\n",
      "Epoch 00146: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0468 - acc: 0.9845 - val_loss: 0.2354 - val_acc: 0.9520\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9852\n",
      "Epoch 00147: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0425 - acc: 0.9852 - val_loss: 0.2370 - val_acc: 0.9527\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9857\n",
      "Epoch 00148: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0412 - acc: 0.9857 - val_loss: 0.2345 - val_acc: 0.9527\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9842\n",
      "Epoch 00149: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0455 - acc: 0.9842 - val_loss: 0.2391 - val_acc: 0.9520\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9855\n",
      "Epoch 00150: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0436 - acc: 0.9855 - val_loss: 0.2262 - val_acc: 0.9541\n",
      "Epoch 151/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9849\n",
      "Epoch 00151: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0434 - acc: 0.9850 - val_loss: 0.2369 - val_acc: 0.9525\n",
      "Epoch 152/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9861\n",
      "Epoch 00152: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0417 - acc: 0.9860 - val_loss: 0.2307 - val_acc: 0.9525\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9856\n",
      "Epoch 00153: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0429 - acc: 0.9857 - val_loss: 0.2620 - val_acc: 0.9515\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9866\n",
      "Epoch 00154: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.0403 - acc: 0.9866 - val_loss: 0.2478 - val_acc: 0.9509\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9867\n",
      "Epoch 00155: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.0406 - acc: 0.9867 - val_loss: 0.2476 - val_acc: 0.9520\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9855\n",
      "Epoch 00156: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.0430 - acc: 0.9855 - val_loss: 0.2503 - val_acc: 0.9490\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9855\n",
      "Epoch 00157: val_loss did not improve from 0.18050\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.0414 - acc: 0.9855 - val_loss: 0.2807 - val_acc: 0.9483\n",
      "\n",
      "1D_CNN_DO_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmT2TmeyBhEXCvq8Cooho3TdcEa3W2lptn1YttbWlfbrYp32qtfZXa7X1oVbrUre6FFFa68JSW1EBQUFZBSSB7Otk9rnn98eZhAAJhGWSkPm+X695zcy9d+79zk3mfO85595zldYaIYQQAsDW3QEIIYToOSQpCCGEaCVJQQghRCtJCkIIIVpJUhBCCNFKkoIQQohWkhSEEEK0kqQghBCilSQFIYQQrRzdHcDhKigo0CUlJd0dhhBCHFdWr15drbUuPNRyx11SKCkpYdWqVd0dhhBCHFeUUjs7s5w0HwkhhGglSUEIIUQrSQpCCCFaHXd9Cu2JxWKUlpYSDoe7O5TjlsfjYcCAATidzu4ORQjRjXpFUigtLcXv91NSUoJSqrvDOe5orampqaG0tJTBgwd3dzhCiG7UK5qPwuEw+fn5khCOkFKK/Px8qWkJIXpHUgAkIRwl2X9CCOhFSeFQEokQkUgZlhXr7lCEEKLHSpukYFkhotE9aB0/5uuur6/n97///RF99oILLqC+vr7Ty995553ce++9R7QtIYQ4lLRJCnu/qnXM13ywpBCPHzwJLVmyhJycnGMekxBCHIm0SQpKma+q9bFPCgsWLGDbtm1MmjSJO+64g2XLljFr1izmzJnDmDFjALj00ks58cQTGTt2LAsXLmz9bElJCdXV1ezYsYPRo0dz0003MXbsWM455xxCodBBt7t27VpmzJjBhAkTuOyyy6irqwPg/vvvZ8yYMUyYMIGrr74agOXLlzNp0iQmTZrE5MmTaWpqOub7QQhx/OsVp6S2tWXLfAKBtQdM1zqBZQWx2bwoZT+sdfp8kxg+/L4O5999992sX7+etWvNdpctW8aaNWtYv3596ymejzzyCHl5eYRCIaZNm8YVV1xBfn7+frFv4emnn+aPf/wjV111FS+88ALXXXddh9u9/vrr+d3vfsfs2bP58Y9/zE9/+lPuu+8+7r77brZv347b7W5tmrr33nt58MEHmTlzJoFAAI/Hc1j7QAiRHtKoptDySnfJ9qZPn77POf/3338/EydOZMaMGezatYstW7Yc8JnBgwczadIkAE488UR27NjR4fobGhqor69n9uzZAHzxi19kxYoVAEyYMIFrr72WJ598EofD5P2ZM2dy++23c//991NfX986XQgh2up1JUNHR/SJRJBg8GM8nqE4nbkpjyMzM7P19bJly3jjjTd455138Hq9nH766e1eE+B2u1tf2+32QzYfdeTVV19lxYoVLF68mP/93//lo48+YsGCBVx44YUsWbKEmTNn8tprrzFq1KgjWr8QovdKm5pCKjua/X7/QdvoGxoayM3Nxev1snHjRlauXHnU28zOziY3N5d//etfADzxxBPMnj0by7LYtWsXZ5xxBr/85S9paGggEAiwbds2xo8fz/e+9z2mTZvGxo0bjzoGIUTv0+tqCh1puThL62PffJSfn8/MmTMZN24c559/PhdeeOE+88877zweeughRo8ezciRI5kxY8Yx2e5jjz3G1772NYLBIEOGDOHRRx8lkUhw3XXX0dDQgNaa2267jZycHH70ox+xdOlSbDYbY8eO5fzzzz8mMQgheheVikIylaZOnar3v8nOJ598wujRow/6OcuK0dy8Drf7BFyuPqkM8bjVmf0ohDg+KaVWa62nHmq5NGo+aulpPr6SoBBCdKW0SQqpvE5BCCF6i7RJClJTEEKIQ0ubpGA6mhWpOPtICCF6i7RJCoZKydlHQgjRW6RVUjD9ClJTEEKIjqRVUgBbj6kp+Hy+w5ouhBBdIc2SgvQpCCHEwaQsKSilBiqlliqlPlZKbVBKfbOdZZRS6n6l1Fal1IdKqSmpisdsLzXNRwsWLODBBx9sfd9yI5xAIMCZZ57JlClTGD9+PIsWLer0OrXW3HHHHYwbN47x48fz7LPPArBnzx5OO+00Jk2axLhx4/jXv/5FIpHghhtuaF32N7/5zTH/jkKI9JDKYS7iwLe11muUUn5gtVLqda31x22WOR8YnnycBPwh+Xzk5s+HtQcOnQ3gSQTNcKm2jMNb56RJcF/HQ2fPmzeP+fPn841vfAOA5557jtdeew2Px8NLL71EVlYW1dXVzJgxgzlz5nTqfsgvvvgia9euZd26dVRXVzNt2jROO+00nnrqKc4991z++7//m0QiQTAYZO3atZSVlbF+/XqAw7qTmxBCtJWypKC13gPsSb5uUkp9AvQH2iaFS4DHtWnoX6mUylFKFSc/m6rIjvkaJ0+eTGVlJbt376aqqorc3FwGDhxILBbjBz/4AStWrMBms1FWVkZFRQVFRUWHXOfbb7/NNddcg91up2/fvsyePZv333+fadOm8eUvf5lYLMall17KpEmTGDJkCJ9++im33norF154Ieecc84x/45CiPTQJQPiKaVKgMnAu/vN6g/savO+NDntyJPCQY7oI8FNaK3JzDz2Q0bPnTuX559/nvLycubNmwfAX/7yF6qqqli9ejVOp5OSkpJ2h8w+HKeddhorVqzg1Vdf5YYbbuD222/n+uuvZ926dbz22ms89NBDPPfcczzyyCPH4msJIdJMyjualVI+4AVgvta68QjXcbNSapVSalVVVdVRRJO6U1LnzZvHM888w/PPP8/cuXMBM2R2nz59cDqdLF26lJ07d3Z6fbNmzeLZZ58lkUhQVVXFihUrmD59Ojt37qRv377cdNNNfOUrX2HNmjVUV1djWRZXXHEFP//5z1mzZk1KvqMQovdLaU1BKeXEJIS/aK1fbGeRMmBgm/cDktP2obVeCCwEM0rqUcSTslNSx44dS1NTE/3796e4uBiAa6+9losvvpjx48czderUw7qpzWWXXcY777zDxIkTUUpxzz33UFRUxGOPPcavfvUrnE4nPp+Pxx9/nLKyMr70pS9hWSbh3XXXXSn5jkKI3i9lQ2cr05v6GFCrtZ7fwTIXArcAF2A6mO/XWk8/2HqPdOhsgFDoUxKJZny+8Z37EmlGhs4Wovfq7NDZqawpzAS+AHyklGo5HegHwAkAWuuHgCWYhLAVCAJfSmE8mOsUesbFa0II0ROl8uyjt9k7NGlHy2jgG6mKYX8yzIUQQhxc2l3R3FOGuRBCiJ4ozZKC1BSEEOJg0iopmOYjLbUFIYToQFolBbn7mhBCHFxaJYWW+zQf6yak+vp6fv/73x/RZy+44AIZq0gI0WOkVVJoqSkc6+ajgyWFeDx+0M8uWbKEnJycYxqPEEIcqTRLCqmpKSxYsIBt27YxadIk7rjjDpYtW8asWbOYM2cOY8aMAeDSSy/lxBNPZOzYsSxcuLD1syUlJVRXV7Njxw5Gjx7NTTfdxNixYznnnHMIhUIHbGvx4sWcdNJJTJ48mbPOOouKigoAAoEAX/rSlxg/fjwTJkzghRdeAOAf//gHU6ZMYeLEiZx55pnH9HsLIXqfLhkQrysdZORstM7GskZisznpxOjVrQ4xcjZ3330369evZ21yw8uWLWPNmjWsX7+ewYMHA/DII4+Ql5dHKBRi2rRpXHHFFeTn5++zni1btvD000/zxz/+kauuuooXXniB6667bp9lTj31VFauXIlSiocffph77rmHX//61/zsZz8jOzubjz76CIC6ujqqqqq46aabWLFiBYMHD6a2trbzX1oIkZZ6XVLonNR3NE+fPr01IQDcf//9vPTSSwDs2rWLLVu2HJAUBg8ezKRJkwA48cQT2bFjxwHrLS0tZd68eezZs4doNNq6jTfeeINnnnmmdbnc3FwWL17Maaed1rpMXl7eMf2OQojep9clhYMd0cfjzYRCW/F6R2O3Z6Y0jszMvetftmwZb7zxBu+88w5er5fTTz+93SG03W5362u73d5u89Gtt97K7bffzpw5c1i2bBl33nlnSuIXQqSntOxT0PrY9in4/X6ampo6nN/Q0EBubi5er5eNGzeycuXKI95WQ0MD/fv3B+Cxxx5rnX722Wfvc0vQuro6ZsyYwYoVK9i+fTuANB8JIQ4pzZJCaq5TyM/PZ+bMmYwbN4477rjjgPnnnXce8Xic0aNHs2DBAmbMmHHE27rzzjuZO3cuJ554IgUFBa3Tf/jDH1JXV8e4ceOYOHEiS5cupbCwkIULF3L55ZczceLE1pv/CCFER1I2dHaqHM3Q2YlEM8HgJ3g8w3A65TTQ/cnQ2UL0Xp0dOltqCkIIIVqlWVJIzXUKQgjRW6RVUlAqNVc0CyFEb5FWSUFqCkIIcXBplRSUkj4FIYQ4mLRKCqm6TkEIIXqLNEsKPaem4PP5ujsEIYQ4QFolBdN8pKSmIIQQHUirpGAc+/s0L1iwYJ8hJu68807uvfdeAoEAZ555JlOmTGH8+PEsWrTokOvqaIjt9obA7mi4bCGEOFK9bkC8+f+Yz9ryDsbOBhKJAEo5sNk8nV7npKJJ3HdexyPtzZs3j/nz5/ONb3wDgOeee47XXnsNj8fDSy+9RFZWFtXV1cyYMYM5c+a06fA+UHtDbFuW1e4Q2O0Nly2EEEej1yWFQzuMGyl00uTJk6msrGT37t1UVVWRm5vLwIEDicVi/OAHP2DFihXYbDbKysqoqKigqKiow3W1N8R2VVVVu0NgtzdcthBCHI1elxQOdkQPEAh8hN2eSUbGkGO63blz5/L8889TXl7eOvDcX/7yF6qqqli9ejVOp5OSkpJ2h8xu0dkhtoUQIlXSrk9BqWPfpwCmCemZZ57h+eefZ+7cuYAZ5rpPnz44nU6WLl3Kzp07D7qOjobY7mgI7PaGyxZCiKORdknBnH107E9JHTt2LE1NTfTv35/i4mIArr32WlatWsX48eN5/PHHGTVq1EHX0dEQ2x0Ngd3ecNlCCHE00mrobIBgcCOg8HpHpiC645sMnS1E7yVDZ3coNTUFIYToDdIwKaSmT0EIIXqDXpMUOnv0b64RkJrC/qT2JISAXpIUPB4PNTU1nSzYbDLMxX601tTU1ODxdP6CPiFE79QrrlMYMGAApaWlVFVVHXLZWKwGywrhdveKr37MeDweBgwY0N1hCCG6Wa8oGZ1OZ+vVvoeyefPXqap6nkmTKlMclRBCHH96RfPR4bDZPFiWXCUshBDtScOk4MayIt0dhhBC9EgpSwpKqUeUUpVKqfUdzD9dKdWglFqbfPw4VbEA8M47MG8ezsooWkels1kIIdqRyprCn4HzDrHMv7TWk5KP/0lhLFBeDs89h702BoBlRVO6OSGEOB6lLClorVcAtala/2FL3v7SHjSnrUq/ghBCHKi7+xROVkqtU0r9XSk1NqVb8vsBsIdMUtBa+hWEEGJ/3XlK6hpgkNY6oJS6APgbMLy9BZVSNwM3A5xwwglHtrWWmkLYJIVEInhk6xFCiF6s22oKWutGrXUg+XoJ4FRKFXSw7EKt9VSt9dTCwsIj22BLUgiZrxyP1x/ZeoQQohfrtqSglCpSyZsVK6WmJ2OpSdkGk81HjpC5HackBSGEOFDKmo+UUk8DpwMFSqlS4CeAE0Br/RBwJfBfSqk4EAKu1qkclW2/jmZJCkIIcaCUJQWt9TWHmP8A8ECqtn8AlwscDuwhc32CJAUhhDhQd5991HWUAr8fW7O5TiEel/sZCyHE/tInKQD4fKhgFFBSUxBCiHakX1IINONw5EhSEEKIdqRXUvD7oalJkoIQQnQgvZKCzweBQDIpSJ+CEELsL42TgtQUhBBif2maFHIlKQghRDvSKylIn4IQQhxUeiWFNs1HsZj0KQghxP7SLykEgzhUFpbVjGXFujsiIYToUdIrKSQHxXNGvQDE4w3dGY0QQvQ46ZUUkoPiOSMeQMY/EkKI/aVlUnBEXICMfySEEPtLr6TQ0nwUMoPDSk1BCCH2lV5JofWWnHL3NSGEaE9aJgVn2A5IUhBCiP2lV1JINh/Zg3KjHSGEaE96JYVkTUE1x1DKIR3NQgixnzRNCnJPBSGEaE9aJgUZFE8IIdqXXknB6QS3WwbFE0KIDnQqKSilvqmUylLGn5RSa5RS56Q6uJSQG+0IIUSHOltT+LLWuhE4B8gFvgDcnbKoUklutCOEEB3qbFJQyecLgCe01hvaTDu+tN5TQfoUhBBif51NCquVUv/EJIXXlFJ+wEpdWCkkNQUhhOiQo5PL3QhMAj7VWgeVUnnAl1IXVgq1SQqWFSaRCGO3e7o7KiGE6BE6W1M4Gdikta5XSl0H/BA4Pm9GkGw+crn6ABCN7unmgIQQoufobFL4AxBUSk0Evg1sAx5PWVSplKwpeDxDAAiHt3dzQEII0XN0NinEtdYauAR4QGv9IOBPXVgplEwKGRkmKYRCn3ZzQEII0XN0tk+hSSn1fcypqLOUUjbAmbqwUsjvh0AAt3sASjkIhyUpCCFEi87WFOYBEcz1CuXAAOBXKYsqlXw+CIdRCY3HUyI1BSGEaKNTSSGZCP4CZCulLgLCWuvjt08BWvsVwuFt3RuPEEL0IJ0d5uIq4D1gLnAV8K5S6spUBpYyyXsq0NhIRsYQqSkIIUQbne1T+G9gmta6EkApVQi8ATyfqsBSpm9f81xejqdoKPF4LbFYPU5nTvfGJYQQPUBn+xRsLQkhqeYwPtuz9O9vnnfvbj0DSU5LFUIIo7MF+z+UUq8ppW5QSt0AvAosSV1YKdSvn3kuK2tzrYI0IQkhBHS+o/kOYCEwIflYqLX+3sE+o5R6RClVqZRa38F8pZS6Xym1VSn1oVJqyuEGf0T69AG7HcrKyMgYDMi1CkII0aKzfQporV8AXjiMdf8ZeICOr3w+HxiefJyEuWr6pMNY/5Gx2aC4GHbvxuHIxuHIl5qCEEIkHTQpKKWaAN3eLEBrrbM6+qzWeoVSquQgq78EeDx5pfRKpVSOUqpYa536wYj694eyMgA5A0kIIdo4aFLQWqdyKIv+wK4270uT01KfFPr1g02bAPB4htDUtCrlmxRCiONBp5uPupNS6mbgZoATTjjh6FfYvz+89RZgagrV1S9gWTFstuNz5A4hjifRKDgcpiW3rWAQPJ690xMJ81q1czuvRAIiEbAs0Hrf585O239eJAKhEGRmQl4eNDVBZeW+72tqIB4/cD0drbvl2eGAnBxzi/iGBrMtl8vMCwQgFjO3kNfa7J+Wh1LmMx6PeR4zBsaNS+3fpzuTQhkwsM37AclpB9BaL8R0dDN16tT2mrMOT//+5i/T3IzXOwat44RCm8nMHHvUqxaiRUvBFY+D12sKgI8/hp07oaAAcnMhHDYFUShkCsVQyBQEffqY5507obraLOfzwZAhZn3btpn15+ebeeXlplDJzzeP7GzYvBk++MDMt9nM9MJC8/mWbbU8x+PmEp68PPPTaGraW2hVVJj3GRnmHI1QyEzPzDTbjMXMIxqFxkZTkIZCZh94vWadsRjU1UFtrYlHKcjKMgWlzwd79ph5drvZL83Ne/dFS4HodJrPhsNmfelowQK4667UbqM7k8LLwC1KqWcwHcwNXdKfAHtPS929G1/xJAACgbWSFI4TlrVvYdryOhzeexTqdO4tsD77zBRsLYVXPG6Wi0T2FjIdvbbZTKHU3AxVVabQ8vvN+5qavUdy8bj5XNtHIrFv3EqZI8FU6GjdxcWm8I3HTbz19WbZjAxTYLc822ywdKkpuLOzzXeMRs06iorMOqqrzXoyMszyZWV7j3BbHn36mCPZzEwTTzBoCnun0xT2ubkmEUQiJvnU15tEMmsWDBxo9mttrfl8S9xtE4HHYx4ZGWa/22x7axP7v97/+WDTWpJPy9+6psZsv6Bgb0xZWSaxulz7rqej7bV9jsXMd41Gzf51u800pcy+djjMe5vN7CuXa2/Noe3/VF5eav5/2kpZUlBKPQ2cDhQopUqBn5AcWVVr/RDmOocLgK1AkK68k1vLBWxlZXiHzkQpF4HAOvr2vbbLQuitWn7sjY2m8M7PN//Yu3ZBaSns2qWJxSAvT9HUBJ9+agqilh9+czRIINZIKBohGvIQbfITDmQQCSuC4QRhq4lYUzagwFMPA/8DVaOhvoRD3jbcHoWMWvBWgyMMMS8umxePzYvH7iXD4cXlDWPPLsfpDeLygj3ch8aqQjJ8EYqmrMcZzyNePZiCQs3o6eVYxIhFHTjtdjwuBx6XA5cLEu4qEq46Ct0DyHEUURdspCFRTkFJBTl9AnijJdiCfYg5alGuMCU5gyj0Z4MrSGOkgc8qG1CWk+kjSuhX5MDjgYraEIvW/IeQ1cD4IX0o8vfBHu6Dy21hz6zHaXNDKI9gQwZ1dVBSYv7VtdZ81vAZ9eF6wrE4Q/NKKMjMb90tcStOVXMV/9n1Hz6p3kiRry8DswZyQvYJFHgLiCQi1ARr2FSziarmKjwODxnODDwODz6XjxxPDm67m2giisvuIseTQ0InqAvVUReuozZU2/ra7/JT7C+mqrmKcN02hrp89PP3o5+/H30y+1AXqqM6WI1SCpfdhdPmJGbF+LDiQ3bU76AgayAjC0ZyysBTKMkpoayxjM8aPqMx0khCJ8jLMKXm9rrtxKwYQ3OHotGsr1xPJB5hQNYA3A43zdFmEjqBTdmxKRt2mx2UnZjNToayc4Ky0RxrZme4nvrMehqyGqgP19PU0ITb7sbv9lOSU8LArIHErTjRRJQcTw7ZnmxiiRiRRIRoIkokbp4tZeEp9BC34nwcKCcSiJCXkUeWOwuX5cIZd+JyuHA73HgcHmLxGE3BJnI8OQzNG0p/bz421TXXC6csKWitrznEfA18I1XbP6g2F7DZbE4yM8cRCKztllBSJRwPUx4oR2tNbkYulraoD9ezavcq/rXzX1QGKwnFQgzIGsDYwrHmH9GKEwzFqaxvZkPVerbUb6A5FiBuWRQ7xpCTGEFFYy21sTIirt3EbA0QziUe9BO3YiQSGh31Qiz5sCUgowbsMYhkgbsBCj8BTwNUu6BxILbyqbhdCit3O3H/dhKeygO+i9J2HDqTmGoCpfHTn762Ueyw3iZOBIBMey5O5USjcdkycKkMXMqLTdkIUUNjvIZArOmAdUeTj8aD7MtcTy5N0SbiVhyAHE8O4XiYcDx86D9EBFREodHmqqDPko922JQNS+9763PnWid9fX3xODyUNpbu3eaajjfpcXjIy8ijz/o+5Gfks75yPRXNFQd8J0tbBGNBYlb3tMW47C6iiWinllUoiv3FlAfKW/dRS8LoCjZlI9udjc/lI5qI0hhpJBQPdcm2W7jsLr57ynf52ed+ltLtHBcdzcdcm6EuAHy+idTUvNqNARk1wRreKX0He/LopTZUy9barbz+6etsrd1Ksb8Yt93NtrptBGNBhuYOpchXhN1mpynSxGcNn1EbqiVmxQ5aYDkSPhyh/sTDHhL+FWh3O3dWbewHFRMgPAJsCcr7rIe8v6Oihbii/VDVQ9DhbLy59Xh9TbgcXpwOhXaEsOyVJFQQUHisAuzKi+WsJTsjk/FF15HvzaOxOUJZeBsf172LTdkYnDOYwTlzKMkpIS8jD7fDTSQeoTHSSGOkkUA0QLbH/Cg/KP+ADZUb+HrJV7loxEVsrd3KR5UfoZPtJ+FEmGAsSDAWJGElyPeOJj8jnwJvQeuzx+EhFA8RjAVpjjab51gzbrubYn8xPpcPrTW7m3azsXojuRm5TCmeQk2whg/KP8Dv8jMkdwgehzn6i1txEjpB3IpjaYtCbyE5nhxKG0spayojLyOPIl8RRb4ivE4vO+p3UNlcSYG3AKfNyc6GndSF6shyZ7UecYbjYTbXbKY8UE44Huai4Rdx1pCz6OfvR1WwisrmSiqbK7ErO9mebKKJKLWhWmpDtdQEa6horqAqWMXZQ8/mlAGn0NfXF5uy8Wndp2yr3YbT7iTDkYHX6SXbk830/tOZ0HcCVc1VfNbwGbsad1ETrMHj8JDtyWZE/giKfEVEE9HWpNgUaaI+XE8kEWkt5OtCddhtdvIy8sj15JrnjFxyPDk0RhrZ3bSbAm8BA7IGELfilAfK2d20m8rmSnI8ORR6CwGIWTGiiSgKxciCka0F8sbqjbz92dtsr9vO0LyhDM4ZTLYnu/U3o7WmJKcEp93J1tqtAIzrMw6v00tpYymxRAyv04vD5iChEySsBJa2Wl8ntHmf6cxs/Vv4XL59jtS11lQ2V/JZw2e4HW6cNif14XoaI4047U7cdjcuuwuX3Rz9KxSRRASbslHkK8Lj8FAbqqUx0kgsEWv9rtFElFAshMPmwO/2UxeqY2vtVlPLi4c5ZeApx67A6YDSqWrkTJGpU6fqVauO8hRSrU1D3le+AvfdR2npb9m6dT4nn7wHt7vo2ATa7mY1H1V+RJGviCx3Fn9e+2eeWf8Mma5MElaCt7a/dcCRj0IxpXgK4/uOpyJQQTgeoV/GEGxWBp81bqOiqZrGQIJocybx6kEEa/OIBl0QzoFAEWgFGXVgOSDix1YzlkGuKQwd7KCkBGJxTUWgnEx/nMICB30KHBQVuOmbk4XPZzoBMzPNc0sba3tngwghejal1Gqt9dRDLZeeNQWlTG2htaawt7PZ7T7vqFa9Zs8aGsINrUdGfpef3U27eX/3+/z23d/yYcWHgKnih+NhxvUZRyAaIBQP8c2TvsmckXNoanDy0foEzkQe7mgxjZU5bFkJ298xl1e0l8eHDYNRo0w7cmGhyXk2mynEhwwx0zIzzbNjn7+6AoqP6jsLIXqP9EwKsM9VzZmZEwBobl5Hfv7hJYXGSCOf1n3K1tqtPPDeAyzfubzDZccUjuEPF/6BYCzIttptnF9yJc7S09m1S1FWBaUfwPc2wMqVBxb8hYVw0klwxRV7zwZxOMz0adPMGR1CCHG00jcp9OsHb78NgNOZi9s9qNOdzRWBCn7//u95ZcsrrC1f29rxVewr5r5z72Ni0cTWsy0awg0U+4sZVTCKzMAEli+zseNDWLcGFr5rTrkDU3np29cc6d95J5xzjin4MzJUeCbCAAAgAElEQVTMdK83BftACCH2k75JoaX5SGtQCp9vIoHAug4XrwvV8crmV1iydQkvfvIisUSMWYNm8aPTfsT4PuMZkDWAiUUT8Tg8rZ+przcXTv/zMfjv183pl2Da58ePhzvugDPPNE0/xcXm3GQhhOhO6Z0UYjFzVVNREX7/FGpqFhOL1eF05u6z6Jo9a5jz9BzKmsrok9mHGyffyPwZ8xmRP+KA1dbUwLPPwlNPwTvvmHP1/X444wz41rfg7LNh+PADL/EXQoieIH2TwvDh5nnLFigqIifnc8Cd1Ncvo7DwstbF/rbxb1z74rXkZ+Sz/IblnHrCqQdcRGJZ8Mor8Oij8OqrJteMGwc//KFJAiedZK5OFEKIni59k8KI5FH+5s0waxZZWSdhs3mpq3uTwsLL0Frzq//8igVvLGBa/2ksunoRRb59T1e1LHjySfjFL8xZQUVFcOutcP31MHFiN3wnIYQ4SumbFEpKzOH75s0A2GwucnJmU1//JhsqN/D9N7/P4s2LmTd2Ho9e8igZzox9Pv6f/8Btt8Hq1TBpEjz9NFx55f6newohxPElfVu27XbTw5tMCgC5uWey8JONTHhoAst3Luees+7h6Sue3ich7NoFn/88zJxpRqZ88klYswauvloSghDi+JfexdiIEfskhWe2V/KnHXDZsOksvGwxBd6C1nnBINx7L9x9tzlh6Uc/gu99z1wQJoQQvYUkhb//HSse4/5VD/Ldpfcwq9DFTycN3SchbN4Ml1wCGzfC3Llwzz2m9UkIIXqbtE8KFc4on//TbN4qf4eLR1zMnWPdNDW8idYJlLLz2mswb57pfvjnP83ZREII0Vulb58CwMiR/PgM+HfFKhZetJBFVy9iQNGVRKPl1Ncv59ln4aKLTK1g1SpJCEKI3i+tk0JoyECeHQdznRO56cSbUEqRnz8Huz2L3/3uM665Bk45BZYvh0GDujtaIYRIvbRuPlpc9y4NHvhiVf/WaXZ7BosWPcyvfz2X889P8MILdjIyDrISIYToRdK6pvDYh48zIOTkjA3B1ml33QW//vVcPve5p/m//3tBEoIQIq2kbVIoD5Tz2tbX+EJoBPbNWwBYtAh+8AP4/Oc1P/3pAurqHu/mKIUQomulbVJ4fN3jJHSC6/M/Bzt38vH7zVx3nbk3wcMPK4qLr6G29h9EowfeM1gIIXqrtEwKcSvOg+8/yBklZzDqlDkktOKG62JkZMCLL5p7GBQVfQFIUFn5THeHK4QQXSYtk8LLm17ms4bPuO2k2+Dkk3nI9g3e35zDb38LAwaYZTIzx+LzTaKi4snuDVYIIbpQWiaF3777WwZlD+LiERezpzGTH6hfcFb2e1x99b7L9e37BZqa3icY3NQ9gQohRBdLu6SwrnwdK3au4Jbpt2C32bnrLghpD79vuh7VHNhn2T59rgFsUlsQQqSNtEsKb25/E4AvTPgCTU3w5z/DvDMqGW5tgn//e59l3e5icnPPoqLiSXTyPsxCCNGbpV1S2FS9ifyMfPr6+vLEE9DUBLf8d44Z93rZsgOW79v3OsLhHTQ0/KfrgxVCiC6Wdklhc+1mRhaMRGt44AGYOhWmn+4156IuX37A8gUFl2GzeamoeKIbohVCiK6VdklhU/UmRuSPYNky+OQTuOUWUAo44wx47z2ord1neYfDR2Hh5VRVPUciEe6WmIUQoqukVVJoijSxJ7CHkfkjef1102J01VXJmZdcAokELF58wOf69r2OeLye2tolXRuwEEJ0sbRKCptrzF3WRuSPYN06GD2avWMbTZsGAwfCCy8c8LmcnDNxuYrYs+dPXRitEEJ0vbRMCiPzR7JuHUyc2GamUnD55eZOOk1N+3zOZnPQv/8t1NYuoa7uzS6MWAghulZaJYVNNZtQKHL0UMrKYMKE/Ra44gqIRGDJgc1EAwZ8G49nCFu23IplxbomYCGE6GJplRQ212ymJKeEzR97gP1qCmDuqNOnT7tNSHa7h2HDfksw+AllZb/rgmiFEKLrpV1SaOlPgHaSgt1uaguLF0N5+QGfLyi4iNzcc9m58xckEs2pD1gIIbpY2iQFrTWbaja19if07WseB/jWtyAahV/9qt31lJT8iHi8RjqdhRC9UtokhfJAOYFogBH5I/jww3b6E1oMHw7XXQd/+EO7tYXs7JlkZ89i1657saxoaoMWQoguljZJYVONGel0WO5INmxop+morR/9yNQW7rmn3dknnPB9IpFdVFQ8lYJIhRCi+6Q0KSilzlNKbVJKbVVKLWhn/g1KqSql1Nrk4yupiqUp0kR/f3/s9SOIRA6RFIYNg2uvhf/7P2hoOGB2Xt55+HxT2LHjx8TjgXZWIIQQx6eUJQWllB14EDgfGANco5Qa086iz2qtJyUfD6cqnotHXkzp7aVUbzsBOEjzUYtbb4VgEJ44cMwjpRTDh/+OSGQXO3bceeyDFUKIbpLKmsJ0YKvW+lOtdRR4BrgkhdvrlNJS81xScogFp041Vzn//veg9QGzs7NPobj4JkpL7yMQWHfM4xRCiO6QyqTQH9jV5n1pctr+rlBKfaiUel4pNbC9FSmlblZKrVJKraqqqjqqoCorwe0Gv78TC//Xf5lR81asaHf2kCF343TmsXHjl+WCNiFEr9DdHc2LgRKt9QTgdeCx9hbSWi/UWk/VWk8tLCw8qg1WVprr05TqxMLz5kFOjqkttMPpzGPEiIcIBNawc+fPjiouIYToCVKZFMqAtkf+A5LTWmmta7TWkeTbh4ETUxgPsDcpdIrXCzfeaK5w3rmz3UUKCy+nb9/r2bnzFzQ0rDx2gQohRDdIZVJ4HxiulBqslHIBVwMvt11AKVXc5u0c4JMUxgOYpNDuRWsd+eY3TbXiN7/pcJHhw+/H7R7Ahg1XEA7v6nA5IYTo6VKWFLTWceAW4DVMYf+c1nqDUup/lFJzkovdppTaoJRaB9wG3JCqeFpUVBxGTQHMcNrXXAMPPwx1de0u4nBkM378YhKJAB99dCHxeOOxCVYIIbpYSvsUtNZLtNYjtNZDtdb/m5z2Y631y8nX39daj9VaT9Ran6G13pjaeA6z+ajFd74Dzc0d9i0A+HzjGTv2eYLBT/j448+jtXV0wQohRDfo7o7mLtXYaC5UPuykMGECXHgh3HWXORupA3l5ZzNs2G+prX2VHTt+cnTBCiFEN0irpFBZaZ4POymAubrZ64UrrzS1hg706/dfFBXdyM6dP6e8/MAL34QQoidLy6RwWB3NLfr3h6eeMjWFr32t3QvawFztPGLEg+TknMHGjV+krOzBIw9YCCG6WFolhYoK83xENQWAs86CO++EJ5+EP/6xw8VsNjfjxy8hP/9itmy5hR07/gfdQRIRQoieJK2SwlE1H7X44Q/hnHPgtttgzZoOF7PbPYwd+wJ9+36RHTt+wtat86XzWQjR46VlUigoOIqV2GymplBYCBdcABs2HGRRB6NGPcKAAd+irOx+NmyYK6OqCiF6tLRLCrm54HId5YoKC+H1102COP10+Oc/IZFod1GlbAwd+muGDv0N1dV/44MPTqapae1RBiCEEKmRdknhqJqO2ho1CpYvh4wMOPdc0xH93HPtLqqUYuDA+UyY8A+i0XJWr57Cpk03S61BCNHjpFVSqKg4wjOPOjJ8uDkb6bnnTFK48Ub47LMOF8/LO5vp07cwYMB89ux5hPXrLyaRCB3DgIQQ4uikVVI4pjWFFpmZMHcuPP88WBZ89asdnq4K4HTmMGzY/2P06Mepr1/O+vWXUFX1Es3NHfdNCCFEV3F0dwBdKSVJocXgweaK529+0wy5fdZZpp+hstKMnTRixD6L9+37eSwrxKZNX6Wu7vXktOsYNux+nM7cFAUphBAHlzZJIRaD2toUJgWAW26BjRvhr381jxa/+Q088wycd94+ixcX30hBweWEwzuorv4bn332C+rq3mLkyIfJzz8/hYEKIUT70qb5qLraPKc0KdhsZtC8ykrYuhXKymD7dnPvzwsvNIlhP05nLn7/ZAYP/ilTpryLw5HLRx9dwCef3EBj47ty0ZsQokulTVJouZr5mHY0d0QpGDoU+vUzCeHf/4aZM+FLX4LVqzv8mN8/halTV3PCCQuorHyKNWtmsHLlILZu/RaNje93QeBCiB5Ja3PB7NKlKd9U2iSFY3I185HKzDQd0YWFMGeOaUaaPBn+9a8DFrXZ3AwZchennFLJqFGP4/NNoqzs96xZM50PP7yApqaOk4oQopd67DH43e/gvfdSvqm0SQp1deYAvluSApgNL1pkRlqtqICaGrjsMti2zYznvX37Pos7nTkUFX2B8eNfZubMKoYM+SWNjStZvXoqq1ZNZdeu/0dT0xq0bv+iOSHEcURrM9Dm1VfD7t37ztu2DW691Vwo+53vpDwUdby1WU+dOlWvWrXqiD4bj5tmf1tPSIVbt8JJJ5mL35qbob4evvxleOABc8l1MAh+/z4ficXqqah4nPLyRwkEzFXRLld/Bg36PkVFN2K3e7rjmwhx9EIh87//hS9AUdGhl//HP8xw9g88YK4R6izLMkeHSrU//+OPTbNvTk7n13ks/PnPpnnZZjO/+yuuAIfDdIa+9x4EAvDhh+ZOkEdIKbVaaz31kMulU1LocZYvN4lg5kwz/sb990NxMTQ0QDgMF18M8+ebI4T9hMO7aGhYQVnZH2hs/DcORx4FBZfSt+915OScjuron16IrlJVZQ56fL59p8diZmiY11+Hyy+HWbPg+uvNmGKnnALLloHTaZbds8fcCnfuXDOKAMCWLTB1qrlr1gknmHWNHHnoeCIRmD3bHIl/7Wvm4tNt20zhm5cHTzxhtp2dDbffbh77x76/xkZTWG/ZYt5rbQ7w6urMIxqFAQNMYf/ee2C3w89+ZvocH33UnIwybZr5/pMnw8KF5rT2Dz80p7Tn5ZnT3b/zHfjc5w5n7x9AksLx6O9/N2cvDRliagt//rM5Uvj2t+Huu2HTJtM5csop4HYDoLWmvv4tysv/THX1yyQSjWRmjqew8EoyMoaSnT0Lj+eE7v1eovMefNCMvnvfffvWFHfsMAXlwY6K43FzIsPf/gZLlpgmy1NPNQcdJ510QM2Tt9+Gn/wEBg0yheV115lCq4VlmWbOwsJDx21Z5v/3gw9MYfj226YQ9HrNUe8tt8D06Wb61VebwrDliP2CC+CVV8zzkiWmMP71r2HnTjjzTFNwK2XO4Js92xTepaXwpz+Zi0UbG811QaNGmcK0tNTEk5lpCuTZs832v/tds19nzICVKw/8DsXFpkBeudLswyFDzG8wMxP+8x/zm+vTxzxiMVNLeeEFs639KWVqGw6HSY5g4quoMPHm5ZnpNpv5vM8HH31kTkxJkc4mBbTWx9XjxBNP1GkjFNL6G9/QGrTOyjLPLa9PP13rmTO1PuccrW+7TesbbtBW/35ag7YUumYq+u0X0UuX2vRHH12qS0sf0JWVz+tQaEd3f6t9lZdr3djY3VF0LBjUevZsrb/6Va2j0UMv/9ZbWk+erPX/+39ax2JaRyJar1mj9T//qfVLL2n99ttaf/CB1s89p/WTT5r5WmudSGg9f/7ev/HEiVq/847Wf/2r1pddprVSWrtcWv/kJ1rv2aN1aanWTz+t9eWXaz1jhtbTpmnt85nP2u1an3GGWYdSe6fdfLPWlZVme6++qnVGhtbFxVoXFJhlrrxS63DYzK+v1/rss7V2OLT++c/Nd9Ha7IO//13r22/X+pRTtJ40Sevrr9d6woS9sWdkaD11qtY/+5nZZna2mX7aaWZ9w4ZpvWiR1tXVWs+bZ+ZdcYXWlqX1LbeY9yecoHVhofnsq69q/cMfaj1ggJmnlNZLlph4Pv3UfKakRGun08Rz+eVaz52r9XnnaT1qlPnMsGHm+dZbzee2bjV/h8ZG8103bjS/txbLl5t1tnyn9h7Z2Vp/5zsmvm3btN6xQ+vt27WuqzN/zxbh8N7/8epq85ueM0frpUu1bmjQ+sUXtV658vD/Nw8TsEp3ooyVmsLx4KmnYPFiOOMM09768svmIjmXyzQ1bdpkjmJajpaam9EPPojOz6bxoiFEd6wh6osQGAaBYcCYMfQd+BX69bsZuz3TbGP3bnOUOWNG+0eGzc3w1ltmGxkZx+Z7rVgBF11kvsePfgQ339y5dW/aZE7Nu/LKveOgW5Y5wlu82ByhbttmmuQ8HigvN81xQ4fClCmm3Xrq1L3tys3N5oi4oMDsx3DYHOEqZY4wH0zePe/ss82RZHm5adN+7TUYPdoc+Q4fbvqJ7rjDHFk2NJimjepq0z/UkWHDzBXwL75oxtGaP9/cr+Oqq0w7Mpjv8fWvm5MRnnpq38/36wdjx5pYhw83TTFnnQX5+WZ+Q4PZLy+/bJomnM69/zdTppjvUVBgLrD89rfh5JPNOpYsMf9jp51m/u6DB5uj2+3bzVWgbjeceCJkZcHatWZ73/ueiTtZi23V1GTOnLn3XtMU+uijpokGTPH6zjtmXW63aW558EHzv1hTA7/4hWlWaVFVZWIfNmzfbWht/gfa1nRa/O1v5u/Yt685PdzTyb63pibTb1FUZOLW2tTUKyvN/8w555jvf5yQ5qN00vI3bNuP8MEH5ge6Ywe6uBhqqlFBM/ie5VDUTdVUnZ9J5i4HeW82krndrEO73airrjLtl5Mnw5gxpqp/ySWmal5QYArvU0818/LyTNVXKdNZuGyZaUZYssQ0efj9porc3Gx+9P37myE/Ro0yfSiDBplpb75pfqyzZpnCft68vQVHIGBO6V2/3hQWy5aZ6SUl5lS9f/8bHnrIDEbocMDEiWb9jY0mpqIiUxBu2QLvv28K/eHDzanB4TA8/fTeArhFSYlJWA88YJozxo2Dm27aO0R6URGcf74pED/4YO/nzj0Xnn3WfJ+HHjJxnHqqKby9XlOoNTWZQq2szLQVb9xomgS//nW49lqznq1bTawjRpj93JIsly83zQx2u0kGp57a+TMnPvnEfB+73ezzr31t7z4G0yzz/e+bGP1+s1/OOstcdPnMM+a75+ebfoBzz+184drCsrrvLI9YzGx//4SVRiQpiL1tnTab+UFv3WoKsffew3rqMWzlNWgFoWn9qJwWoH5wI4VvQ9/XbTiazWe1y26OLh0O4nd+D+eb75r237bsdnPEFAqZQjYjwySV8eNNMkgkzNFzOGwKwo8+MgX05MnmaLugwBR2ixaZhLJpk1nHSSeZwnnRItNO7fGYwvyqq8yR/o037j1976yz4ItfNNeBHOzoraHBFNovvWS2CeZI/+STzZFpNGq+7z/+YWoyEyfCu++awmTrVnPkX1Bgjpxbjkp37jRHj/G46TR0HMboMfG4+W6dabfvSlp3fIaOOC5JUhAHF4+bTr/hw6F/fywrTn39Uhoa/kVT/fvYPy3H/XE5rg3luOpg53UQGgg5OWfQz3st3q0R3DuacDbbTEHb0GCOxs85xzQ5HKoZqKFhby2iLa3NEfITT8CqVSZBtJyfPWPGvsvv2WOaRC691BTehysSMYmzo1g3bjSFdUtTjBDHMUkK4piIxepobv6QeLyB5uYN7N79ByKRXa3zfb5J5OdfTEbGCNzuYpRy4Xb3IyNjaDdGLYTYnyQFkRKWFSMQWEssVkMw+AlVVX+lsfGdA5bLzBxHVtZMlLJjt/vxeErw+yfj909HKUU0Wo3dnondfow6rYUQByVJQXSZRCJEOLyTWKwCy4oRDH5MVdULBIMfAxCPN6B1DAC3exB2u5dg8BMcjjwGDJhPcfGXcbsP46pUIcRhk6QgegytLSKR3dTXv0VV1V/ROk529iwaG1dSU7MYALd7IDabm2i0EpvNjcORR3b2yeTnX4zdnoVlBXG7B+L1jpLahRBHQJKCOC40N2+gru4NGhvfBTROZx+0jhGNllNfv5R4vH6/TyiczgJcrmJcriLc7v5kZo7D6x1FIhEgkWjCZvOilJN4vB6bzUl+/iU4nV08lo0QPUxnk0La3HlN9EyZmWPJzBzb7jzLitHU9B5aW9hsGYTD2wkGPyES2U00uodotJzm5g8pL3/0oNuw2TxkZ89G6zg2m6s1ibjdA7DZMojFarDbffj9UyV5iLQnSUH0WDabk+zsma3vs7LaP8iJRisJhbZgt2fjcPhJJIJoHcXhyCUaraC8/BEaG99NJoBq6ureROtou+uy2801Di5XHzIzJ6KUg2h0NzabF49nEHa7D6XsZGSMICtrGrFYNc3NHwMau92H1zsKn2/i3ivFhTjOSFIQxz2Xqw8uV/s3yvB4TiAra9o+0ywrRiSyi0hkN5YVwunMJxarpanpPWKxKrTWRCKlNDevS66/H7FYNYHAahKJEFrH0TpykIhseL2jyMwcj1J2LCtINFqJZUUoLLyCvLxzaWxcSTi8naysk/H5JmNZIZRy4PEMQSkbweBmtI4k17H32oxgcEsyKQ3pcOuJRJD6+qX4/VNxubriVoOiN5E+BSEOk9aaUGgLTU2rcDr7kJk5DpvNmbyWYz1NTWsIBNbQ3LwBUNhsHlyuvlhWhMbGf7euRykHWsf3WbfN5kEpN4lEAwAORz4+3wTsdj/B4EZCoc0AZGQMx+sdCdhxOvNwuweidYJIpJSamkXE4/U4nYWMHv0kOTmziUariMUqicWqiEarSCQa8fkm4vdPxWZzo3WCQOBDmps/QiknDkcWXu8YPJ5B+yQlcfySjmYheqBgcDONje+SlTUDj2cQjY3vEQxuxG73YVlhmpvXY1lB/P5pKOWkvv5NgsEtJBIBXK4iCgouRmuL2tp/EI3uQesEsVg10egeWjrhc3M/R0HB5ezY8VOCwQ2HiMiO3e5D6ziW1XzgXLufzMxxeDyDSCRCWFYziUSQRKI5+dq8t9sz8HgG4/EMISPDPHs8g7GsIKHQFuLxJsDC55tMbu7ZWFYzDQ3/IRj8mHB4J9nZMykouJxodA+BwFo8nkF4vWPQOt56AoFlRXA683E6C7DZXO1+G60ttI5hs5kxjizLnAptszmP4q/WO0hSECKNWFYMpez7HNUnEkHKyh5E6yhOZ2Hrw+Xqg82WQVPTagKBNSQSTWht4fdPw+83ZUY8XkNz84bW2kMkUobd7sVuz8Rmy0xeeOhtfZ1IBAiHtxMKfZq84r2dewwk2WwZWFaozftMLKu53ZpTRxyOHOx2f2vsHk8JNpuH5uYNWFYzTmcBSjmJRstRyklm5lg8niHY7T4gQTzeAGiUcmOzebDZPDgc2cl+qN0Eg5ta70cSiZTR3LwOt3sQfv9klHIDunWdweBGmps/JBarIZFoxmZzYrdn4/NNJDNzLHa7F8uK0ty8nmi0AperLw5HDqYWmYHTmY9S9uRn3QckvHg8gFI27HZvJ/8b2idJQQjRLSwrSiSyi1DoU+z2TDIyhuN05qF1gvr6pdTUvILLVUR29mnJTnkf9fUrqKl5mYyMYfj9JxIO7yIY3IjN5sZu9+Nw+FHKRSxWQyxmmsLi8SYcDnNiQCj0KZYVJDNzHE5nIdHobiwrits9EMsKEwh8QCRSSiLRUsBmo5QNywpjWREsK0Q83oBlBbHbs/F6RxAMbiKRaATMdTSmZrZ/0rJxsAQIJJOI1XoBZwdLARpQuFxF2O3+1lpgS1OizeZh4MDvMXjwnUfwV+khSUEpdR7wW8AOPKy1vnu/+W7gceBEoAaYp7XecbB1SlIQQqRKIhHGZnOjlMKy4gSDG3C7B+B05pNIhAmFNiVvRhOnuXkdweAWMjPH4PNNweXqm2yKiyVPTFhLMLgpea2Nwu+fjNs9INmn04DWGssKEotVo3Ucu92PZQUJh3eSSDSjlB2nswC3ewBaW8TjNWRnz6ag4KIj+m7dfp2CUsoOPAicDZQC7yulXtZaf9xmsRuBOq31MKXU1cAvgXmpikkIIQ7Gbt97jwibzYHPN3GfeW3fd3SKNGTgcGQd9AyxniyVpxVMB7ZqrT/V5qTwZ4BL9lvmEuCx5OvngTOV3HFeCCG6TSqTQn9gV5v3pclp7S6jTWNdAyCD1wshRDc5Lk5AVkrdrJRapZRaVVVV1d3hCCFEr5XKpFAGDGzzfkByWrvLKKUcQDamw3kfWuuFWuupWuuphT3ttoVCCNGLpDIpvA8MV0oNVkq5gKuBl/db5mXgi8nXVwJv6ePtHFkhhOhFUnb2kdY6rpS6BXgNc0rqI1rrDUqp/wFWaa1fBv4EPKGU2grUYhKHEEKIbpLSAfG01kuAJftN+3Gb12FgbipjEEII0XnHRUezEEKIrnHcDXOhlKoCdh7hxwuA6mMYzrEksR2Znhwb9Oz4JLYjc7zGNkhrfcgzdY67pHA0lFKrOnOZd3eQ2I5MT44NenZ8EtuR6e2xSfOREEKIVpIUhBBCtEq3pLCwuwM4CIntyPTk2KBnxyexHZleHVta9SkIIYQ4uHSrKQghhDiItEkKSqnzlFKblFJblVILujmWgUqppUqpj5VSG5RS30xOz1NKva6U2pJ8zu3GGO1KqQ+UUq8k3w9WSr2b3H/PJocu6Y64cpRSzyulNiqlPlFKndxT9ptS6lvJv+d6pdTTSilPd+03pdQjSqlKpdT6NtPa3U/KuD8Z44dKqSndENuvkn/TD5VSLymlctrM+34ytk1KqXO7OrY2876tlNJKqYLk+27fb8nptyb33Qal1D1tph/ZfjN3EerdD8wwG9uAIYALWAeM6cZ4ioEpydd+YDMwBrgHWJCcvgD4ZTfGeDvwFPBK8v1zwNXJ1w8B/9VNcT0GfCX52gXk9IT9hhkGfjuQ0WZ/3dBd+w04DZgCrG8zrd39BFwA/B1zT8gZwLvdENs5gCP5+pdtYhuT/L26gcHJ37G9K2NLTh+IGbJnJ1DQg/bbGcAbgDv5vs/R7rcu+9F05wM4GXitzfvvA9/v7rjaxLMIc4e6TUBxcloxsKmb4hkAvAl8Dngl+Tp14ZQAAAUFSURBVE9f3eZHu8/+7MK4spMFr9pverfvN/beGyQPM3zMK8C53bnfgJL9CpB29xPwf8A17S3XVbHtN+8y4C/J1/v8VpMF88ldHRvmJmATgR1tkkK37zfMQcdZ7Sx3xPstXZqPOnPDn26hlCoBJgPvAn211nuSs8qBvt0U1n3Ad9l7R/J8oF7vvWt5d+2/wUAV8GiyaethpVQmPWC/aa3LgHuBz4A9mBtGraZn7LcWHe2nnvb7+DLmCBx6QGxKqUuAMq31uv1mdXtswAhgVrKJcrlSatrRxpYuSaFHUkr5gBeA+VrrxrbztEnvXX5qmFLqIqBSa726q7fdCQ5M9fn/t3c3r3FVYRzHvz+pFmvFVrRYLdgXXxBBI4oEq1BaF1WkuFAUY33BpRs3IjW+oH+A4qLYLlxUG0QrsXQpRgl0obGE1Ep9K1pwFrUKUqyi1Pq4eM7cjKkhQ8fMvZDfBwYy995cnnmSM8/cc8+c83pE3AT8RnaDVGrM23Jyedk1wOXABcDmfsfRrbryNBdJw8BfwEjdsQBIWgI8C7ww17E1WURenQ4CTwPvSr0tabxQikI3C/70laRzyYIwEhGjZfOPklaW/SuB4zWEth7YIukoua72RuA1YJlyISSoL38toBURn5bn75FFogl5uxP4PiJ+iohTwCiZyybkrW22PDWifUh6DLgHGCpFC+qPbR1Z6A+WNrEKmJR0WQNig2wTo5EmyKv7S3qJbaEUhW4W/OmbUsnfAL6MiFc6dnUuOvQoea+hryJiW0SsiojVZJ4+iogh4GNyIaQ6YzsG/CDp2rJpE3CYBuSN7DYalLSk/H3bsdWetw6z5Wkf8EgZTTMInOjoZuoLSZvJLsstEfF7x659wIOSFktaA1wNTPQrrog4FBErImJ1aRMtcpDIMRqQN2AvebMZSdeQgy9+ppe8zedNkSY9yJEC35B34YdrjuV28tL9c2CqPO4m++7HgG/JEQUX1xznBqZHH60t/1RHgD2U0Q41xDQAHCi52wssb0regJeAr4AvgLfIkR+15A14m7y3cYp8I3titjyRAwm2l7ZxCLilhtiOkH3g7fawo+P44RLb18Bd/Y5txv6jTN9obkLezgN2l/+5SWBjr3nzN5rNzKyyULqPzMysCy4KZmZWcVEwM7OKi4KZmVVcFMzMrOKiYNZHkjaozDxr1kQuCmZmVnFRMPsPkh6WNCFpStJO5foSJyW9WuatH5N0aTl2QNInHWsBtNcpuErSh5IOSpqUtK6cfqmm14QY6XWuGrP/k4uC2QySrgMeANZHxABwGhgiJ7k7EBHXA+PAi+VX3gSeiYgbyG+2trePANsj4kbgNvLbqJCz4j5Fznm/lpwjyawRFs19iNmCswm4GfisfIg/n5w87m/gnXLMbmBU0kXAsogYL9t3AXskXQhcERHvA0TEHwDlfBMR0SrPp8g58vfP/8sym5uLgtmZBOyKiG3/2ig9P+O4s50j5s+On0/jdmgN4u4jszONAfdJWgHV2sZXku2lPePpQ8D+iDgB/CLpjrJ9KzAeEb8CLUn3lnMsLnPzmzWaP6GYzRARhyU9B3wg6RxyVsonyUV9bi37jpP3HSCnod5R3vS/Ax4v27cCOyW9XM5xfx9fhtlZ8SypZl2SdDIiltYdh9l8cveRmZlVfKVgZmYVXymYmVnFRcHMzCouCmZmVnFRMDOziouCmZlVXBTMzKzyD6Lq/v29KPP0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 307us/sample - loss: 0.2600 - acc: 0.9277\n",
      "Loss: 0.2600180074123826 Accuracy: 0.92772585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_DO_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_DO_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_DO_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,456\n",
      "Trainable params: 511,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 270us/sample - loss: 1.6133 - acc: 0.4889\n",
      "Loss: 1.6133275957741346 Accuracy: 0.4888889\n",
      "\n",
      "1D_CNN_DO_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,648\n",
      "Trainable params: 257,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 303us/sample - loss: 1.1853 - acc: 0.6449\n",
      "Loss: 1.1853097205840415 Accuracy: 0.6448598\n",
      "\n",
      "1D_CNN_DO_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,688\n",
      "Trainable params: 140,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 296us/sample - loss: 0.6292 - acc: 0.8239\n",
      "Loss: 0.6292192858698955 Accuracy: 0.8238837\n",
      "\n",
      "1D_CNN_DO_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 123,856\n",
      "Trainable params: 123,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 331us/sample - loss: 0.2863 - acc: 0.9215\n",
      "Loss: 0.2863192579363737 Accuracy: 0.9214953\n",
      "\n",
      "1D_CNN_DO_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 288,848\n",
      "Trainable params: 288,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 334us/sample - loss: 0.2600 - acc: 0.9277\n",
      "Loss: 0.2600180074123826 Accuracy: 0.92772585\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_DO_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
