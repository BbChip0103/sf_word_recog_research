{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 65,537,552\n",
      "Trainable params: 65,537,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 22,173,456\n",
      "Trainable params: 22,173,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 7,936,016\n",
      "Trainable params: 7,936,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 3,410,192\n",
      "Trainable params: 3,410,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,552,784\n",
      "Trainable params: 1,552,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0393 - acc: 0.3297\n",
      "Epoch 00001: val_loss improved from inf to 1.42635, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/001-1.4264.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 2.0392 - acc: 0.3297 - val_loss: 1.4264 - val_acc: 0.5539\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3612 - acc: 0.5603\n",
      "Epoch 00002: val_loss improved from 1.42635 to 1.12642, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/002-1.1264.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.3610 - acc: 0.5603 - val_loss: 1.1264 - val_acc: 0.6497\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.6579\n",
      "Epoch 00003: val_loss improved from 1.12642 to 0.93626, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/003-0.9363.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 1.0875 - acc: 0.6579 - val_loss: 0.9363 - val_acc: 0.7179\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7230\n",
      "Epoch 00004: val_loss improved from 0.93626 to 0.71591, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/004-0.7159.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.9058 - acc: 0.7230 - val_loss: 0.7159 - val_acc: 0.7987\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7670 - acc: 0.7674\n",
      "Epoch 00005: val_loss improved from 0.71591 to 0.64603, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/005-0.6460.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.7670 - acc: 0.7674 - val_loss: 0.6460 - val_acc: 0.8141\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6622 - acc: 0.8006\n",
      "Epoch 00006: val_loss improved from 0.64603 to 0.64024, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/006-0.6402.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.6622 - acc: 0.8006 - val_loss: 0.6402 - val_acc: 0.8001\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.8213\n",
      "Epoch 00007: val_loss improved from 0.64024 to 0.49202, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/007-0.4920.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.5965 - acc: 0.8213 - val_loss: 0.4920 - val_acc: 0.8588\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8466\n",
      "Epoch 00008: val_loss improved from 0.49202 to 0.43989, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/008-0.4399.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.5170 - acc: 0.8466 - val_loss: 0.4399 - val_acc: 0.8768\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8576\n",
      "Epoch 00009: val_loss did not improve from 0.43989\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.4714 - acc: 0.8576 - val_loss: 0.5264 - val_acc: 0.8477\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8708\n",
      "Epoch 00010: val_loss improved from 0.43989 to 0.39733, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/010-0.3973.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.4296 - acc: 0.8708 - val_loss: 0.3973 - val_acc: 0.8910\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8808\n",
      "Epoch 00011: val_loss improved from 0.39733 to 0.36654, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/011-0.3665.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3977 - acc: 0.8808 - val_loss: 0.3665 - val_acc: 0.8977\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8930\n",
      "Epoch 00012: val_loss improved from 0.36654 to 0.35243, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/012-0.3524.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3553 - acc: 0.8930 - val_loss: 0.3524 - val_acc: 0.9024\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9010\n",
      "Epoch 00013: val_loss improved from 0.35243 to 0.34868, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/013-0.3487.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3303 - acc: 0.9010 - val_loss: 0.3487 - val_acc: 0.9033\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9074\n",
      "Epoch 00014: val_loss did not improve from 0.34868\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3064 - acc: 0.9074 - val_loss: 0.3580 - val_acc: 0.9031\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9139\n",
      "Epoch 00015: val_loss improved from 0.34868 to 0.34758, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/015-0.3476.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2843 - acc: 0.9138 - val_loss: 0.3476 - val_acc: 0.9054\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9173\n",
      "Epoch 00016: val_loss improved from 0.34758 to 0.31751, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/016-0.3175.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2703 - acc: 0.9173 - val_loss: 0.3175 - val_acc: 0.9164\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9248\n",
      "Epoch 00017: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2450 - acc: 0.9248 - val_loss: 0.3357 - val_acc: 0.9108\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9281\n",
      "Epoch 00018: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2305 - acc: 0.9281 - val_loss: 0.3454 - val_acc: 0.9073\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9301\n",
      "Epoch 00019: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2227 - acc: 0.9301 - val_loss: 0.3344 - val_acc: 0.9175\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9335\n",
      "Epoch 00020: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2052 - acc: 0.9335 - val_loss: 0.3396 - val_acc: 0.9182\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9401\n",
      "Epoch 00021: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1889 - acc: 0.9401 - val_loss: 0.3249 - val_acc: 0.9185\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9413\n",
      "Epoch 00022: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1850 - acc: 0.9413 - val_loss: 0.3242 - val_acc: 0.9178\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9457\n",
      "Epoch 00023: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1744 - acc: 0.9457 - val_loss: 0.3348 - val_acc: 0.9147\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9460\n",
      "Epoch 00024: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1649 - acc: 0.9460 - val_loss: 0.3410 - val_acc: 0.9185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9511\n",
      "Epoch 00025: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1524 - acc: 0.9511 - val_loss: 0.3201 - val_acc: 0.9201\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9529\n",
      "Epoch 00026: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1442 - acc: 0.9529 - val_loss: 0.3439 - val_acc: 0.9196\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9558\n",
      "Epoch 00027: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1382 - acc: 0.9558 - val_loss: 0.3368 - val_acc: 0.9196\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9583\n",
      "Epoch 00028: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1302 - acc: 0.9583 - val_loss: 0.4072 - val_acc: 0.9064\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9606\n",
      "Epoch 00029: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1244 - acc: 0.9605 - val_loss: 0.3550 - val_acc: 0.9092\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9602\n",
      "Epoch 00030: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1233 - acc: 0.9602 - val_loss: 0.3399 - val_acc: 0.9224\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9628\n",
      "Epoch 00031: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1137 - acc: 0.9628 - val_loss: 0.3456 - val_acc: 0.9222\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9627\n",
      "Epoch 00032: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1148 - acc: 0.9627 - val_loss: 0.3275 - val_acc: 0.9255\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9661\n",
      "Epoch 00033: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1037 - acc: 0.9661 - val_loss: 0.3611 - val_acc: 0.9285\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9661\n",
      "Epoch 00034: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1033 - acc: 0.9661 - val_loss: 0.3475 - val_acc: 0.9276\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9692\n",
      "Epoch 00035: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0917 - acc: 0.9692 - val_loss: 0.3597 - val_acc: 0.9266\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9700\n",
      "Epoch 00036: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0932 - acc: 0.9700 - val_loss: 0.3742 - val_acc: 0.9217\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9701\n",
      "Epoch 00037: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0899 - acc: 0.9701 - val_loss: 0.3920 - val_acc: 0.9129\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9698\n",
      "Epoch 00038: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0922 - acc: 0.9698 - val_loss: 0.3299 - val_acc: 0.9264\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9745\n",
      "Epoch 00039: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0814 - acc: 0.9745 - val_loss: 0.3686 - val_acc: 0.9259\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9727\n",
      "Epoch 00040: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0826 - acc: 0.9727 - val_loss: 0.3568 - val_acc: 0.9248\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9746\n",
      "Epoch 00041: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0770 - acc: 0.9746 - val_loss: 0.3854 - val_acc: 0.9271\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9761\n",
      "Epoch 00042: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0742 - acc: 0.9761 - val_loss: 0.3568 - val_acc: 0.9297\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9755\n",
      "Epoch 00043: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0741 - acc: 0.9755 - val_loss: 0.3619 - val_acc: 0.9278\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9746\n",
      "Epoch 00044: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0756 - acc: 0.9746 - val_loss: 0.3617 - val_acc: 0.9257\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9765\n",
      "Epoch 00045: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0750 - acc: 0.9766 - val_loss: 0.3938 - val_acc: 0.9269\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9795\n",
      "Epoch 00046: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0651 - acc: 0.9795 - val_loss: 0.3820 - val_acc: 0.9287\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9807\n",
      "Epoch 00047: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0622 - acc: 0.9807 - val_loss: 0.4026 - val_acc: 0.9308\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9790\n",
      "Epoch 00048: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0673 - acc: 0.9791 - val_loss: 0.3881 - val_acc: 0.9248\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9796\n",
      "Epoch 00049: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0629 - acc: 0.9796 - val_loss: 0.4067 - val_acc: 0.9250\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9793\n",
      "Epoch 00050: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0634 - acc: 0.9793 - val_loss: 0.3804 - val_acc: 0.9292\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9795\n",
      "Epoch 00051: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0624 - acc: 0.9794 - val_loss: 0.3821 - val_acc: 0.9292\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9806\n",
      "Epoch 00052: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0600 - acc: 0.9806 - val_loss: 0.4649 - val_acc: 0.9250\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9809\n",
      "Epoch 00053: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0580 - acc: 0.9809 - val_loss: 0.3823 - val_acc: 0.9259\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9832\n",
      "Epoch 00054: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0524 - acc: 0.9832 - val_loss: 0.4114 - val_acc: 0.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9820\n",
      "Epoch 00055: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0564 - acc: 0.9820 - val_loss: 0.4303 - val_acc: 0.9264\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9822\n",
      "Epoch 00056: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0540 - acc: 0.9822 - val_loss: 0.3965 - val_acc: 0.9315\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9820\n",
      "Epoch 00057: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0566 - acc: 0.9820 - val_loss: 0.3898 - val_acc: 0.9262\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9832\n",
      "Epoch 00058: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0515 - acc: 0.9832 - val_loss: 0.3944 - val_acc: 0.9297\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9849\n",
      "Epoch 00059: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0486 - acc: 0.9849 - val_loss: 0.4074 - val_acc: 0.9257\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9851\n",
      "Epoch 00060: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0463 - acc: 0.9851 - val_loss: 0.4068 - val_acc: 0.9341\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9841\n",
      "Epoch 00061: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0502 - acc: 0.9841 - val_loss: 0.3746 - val_acc: 0.9297\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9850\n",
      "Epoch 00062: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0471 - acc: 0.9850 - val_loss: 0.3712 - val_acc: 0.9273\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9848\n",
      "Epoch 00063: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0475 - acc: 0.9848 - val_loss: 0.4000 - val_acc: 0.9331\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9856\n",
      "Epoch 00064: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0445 - acc: 0.9856 - val_loss: 0.4034 - val_acc: 0.9252\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9863\n",
      "Epoch 00065: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0450 - acc: 0.9863 - val_loss: 0.4056 - val_acc: 0.9217\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9852\n",
      "Epoch 00066: val_loss did not improve from 0.31751\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0452 - acc: 0.9852 - val_loss: 0.4075 - val_acc: 0.9280\n",
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.3904 - acc: 0.8866\n",
      "Loss: 0.39037198481901414 Accuracy: 0.88660437\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1621 - acc: 0.2878\n",
      "Epoch 00001: val_loss improved from inf to 1.62771, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/001-1.6277.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 2.1620 - acc: 0.2878 - val_loss: 1.6277 - val_acc: 0.4505\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4567 - acc: 0.5158\n",
      "Epoch 00002: val_loss improved from 1.62771 to 1.07026, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/002-1.0703.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.4566 - acc: 0.5159 - val_loss: 1.0703 - val_acc: 0.6862\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1065 - acc: 0.6444\n",
      "Epoch 00003: val_loss improved from 1.07026 to 0.75397, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/003-0.7540.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.1065 - acc: 0.6444 - val_loss: 0.7540 - val_acc: 0.7850\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8387 - acc: 0.7401\n",
      "Epoch 00004: val_loss improved from 0.75397 to 0.55245, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/004-0.5525.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.8387 - acc: 0.7401 - val_loss: 0.5525 - val_acc: 0.8479\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6497 - acc: 0.8012\n",
      "Epoch 00005: val_loss improved from 0.55245 to 0.40917, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/005-0.4092.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.6497 - acc: 0.8012 - val_loss: 0.4092 - val_acc: 0.8852\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8368\n",
      "Epoch 00006: val_loss improved from 0.40917 to 0.36873, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/006-0.3687.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.5369 - acc: 0.8368 - val_loss: 0.3687 - val_acc: 0.9008\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8572\n",
      "Epoch 00007: val_loss improved from 0.36873 to 0.35179, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/007-0.3518.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4636 - acc: 0.8572 - val_loss: 0.3518 - val_acc: 0.8984\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8721\n",
      "Epoch 00008: val_loss improved from 0.35179 to 0.30798, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/008-0.3080.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.4142 - acc: 0.8721 - val_loss: 0.3080 - val_acc: 0.9108\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8866\n",
      "Epoch 00009: val_loss improved from 0.30798 to 0.26918, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/009-0.2692.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3690 - acc: 0.8866 - val_loss: 0.2692 - val_acc: 0.9285\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8983\n",
      "Epoch 00010: val_loss improved from 0.26918 to 0.25802, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/010-0.2580.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3352 - acc: 0.8984 - val_loss: 0.2580 - val_acc: 0.9262\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.9038\n",
      "Epoch 00011: val_loss improved from 0.25802 to 0.25477, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/011-0.2548.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3129 - acc: 0.9038 - val_loss: 0.2548 - val_acc: 0.9317\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9118\n",
      "Epoch 00012: val_loss improved from 0.25477 to 0.22277, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/012-0.2228.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2843 - acc: 0.9118 - val_loss: 0.2228 - val_acc: 0.9408\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9174\n",
      "Epoch 00013: val_loss improved from 0.22277 to 0.21978, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/013-0.2198.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.2638 - acc: 0.9174 - val_loss: 0.2198 - val_acc: 0.9399\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9208\n",
      "Epoch 00014: val_loss improved from 0.21978 to 0.21710, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/014-0.2171.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2534 - acc: 0.9209 - val_loss: 0.2171 - val_acc: 0.9406\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9254\n",
      "Epoch 00015: val_loss improved from 0.21710 to 0.20132, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/015-0.2013.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2348 - acc: 0.9254 - val_loss: 0.2013 - val_acc: 0.9418\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9303\n",
      "Epoch 00016: val_loss did not improve from 0.20132\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.2242 - acc: 0.9303 - val_loss: 0.2097 - val_acc: 0.9434\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9348\n",
      "Epoch 00017: val_loss improved from 0.20132 to 0.18760, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/017-0.1876.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2071 - acc: 0.9348 - val_loss: 0.1876 - val_acc: 0.9471\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9383\n",
      "Epoch 00018: val_loss did not improve from 0.18760\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1970 - acc: 0.9383 - val_loss: 0.1977 - val_acc: 0.9464\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9413\n",
      "Epoch 00019: val_loss improved from 0.18760 to 0.18154, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/019-0.1815.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1844 - acc: 0.9413 - val_loss: 0.1815 - val_acc: 0.9492\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9436\n",
      "Epoch 00020: val_loss did not improve from 0.18154\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1754 - acc: 0.9436 - val_loss: 0.1862 - val_acc: 0.9499\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9447\n",
      "Epoch 00021: val_loss did not improve from 0.18154\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1715 - acc: 0.9447 - val_loss: 0.1874 - val_acc: 0.9474\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9471\n",
      "Epoch 00022: val_loss did not improve from 0.18154\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1631 - acc: 0.9471 - val_loss: 0.1866 - val_acc: 0.9485\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9497\n",
      "Epoch 00023: val_loss improved from 0.18154 to 0.17595, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/023-0.1759.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1563 - acc: 0.9497 - val_loss: 0.1759 - val_acc: 0.9541\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9535\n",
      "Epoch 00024: val_loss did not improve from 0.17595\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1443 - acc: 0.9535 - val_loss: 0.1987 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9548\n",
      "Epoch 00025: val_loss did not improve from 0.17595\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1360 - acc: 0.9548 - val_loss: 0.1870 - val_acc: 0.9511\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9555\n",
      "Epoch 00026: val_loss did not improve from 0.17595\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1390 - acc: 0.9555 - val_loss: 0.1955 - val_acc: 0.9455\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9568\n",
      "Epoch 00027: val_loss improved from 0.17595 to 0.17431, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/027-0.1743.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1313 - acc: 0.9568 - val_loss: 0.1743 - val_acc: 0.9592\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9582\n",
      "Epoch 00028: val_loss did not improve from 0.17431\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1272 - acc: 0.9582 - val_loss: 0.1791 - val_acc: 0.9541\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9599\n",
      "Epoch 00029: val_loss improved from 0.17431 to 0.17221, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/029-0.1722.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1197 - acc: 0.9598 - val_loss: 0.1722 - val_acc: 0.9550\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9619\n",
      "Epoch 00030: val_loss did not improve from 0.17221\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1152 - acc: 0.9619 - val_loss: 0.1789 - val_acc: 0.9571\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9626\n",
      "Epoch 00031: val_loss did not improve from 0.17221\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1122 - acc: 0.9626 - val_loss: 0.2058 - val_acc: 0.9539\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9659\n",
      "Epoch 00032: val_loss did not improve from 0.17221\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1027 - acc: 0.9659 - val_loss: 0.1742 - val_acc: 0.9576\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9660\n",
      "Epoch 00033: val_loss improved from 0.17221 to 0.16233, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/033-0.1623.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1023 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9597\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9667\n",
      "Epoch 00034: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0999 - acc: 0.9667 - val_loss: 0.1655 - val_acc: 0.9560\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9659\n",
      "Epoch 00035: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0998 - acc: 0.9659 - val_loss: 0.1884 - val_acc: 0.9502\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9705\n",
      "Epoch 00036: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0879 - acc: 0.9705 - val_loss: 0.1762 - val_acc: 0.9557\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9714\n",
      "Epoch 00037: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0865 - acc: 0.9714 - val_loss: 0.1733 - val_acc: 0.9585\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9705\n",
      "Epoch 00038: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0870 - acc: 0.9705 - val_loss: 0.1807 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9708\n",
      "Epoch 00039: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0853 - acc: 0.9708 - val_loss: 0.1828 - val_acc: 0.9557\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9724\n",
      "Epoch 00040: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0824 - acc: 0.9724 - val_loss: 0.1757 - val_acc: 0.9583\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9742\n",
      "Epoch 00041: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0774 - acc: 0.9742 - val_loss: 0.1849 - val_acc: 0.9557\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9745\n",
      "Epoch 00042: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0761 - acc: 0.9745 - val_loss: 0.1837 - val_acc: 0.9560\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9740\n",
      "Epoch 00043: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0772 - acc: 0.9739 - val_loss: 0.1688 - val_acc: 0.9583\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9736\n",
      "Epoch 00044: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0770 - acc: 0.9736 - val_loss: 0.1735 - val_acc: 0.9592\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9765\n",
      "Epoch 00045: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0671 - acc: 0.9765 - val_loss: 0.1965 - val_acc: 0.9481\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9779\n",
      "Epoch 00046: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0639 - acc: 0.9779 - val_loss: 0.1657 - val_acc: 0.9604\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9771\n",
      "Epoch 00047: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0666 - acc: 0.9771 - val_loss: 0.1885 - val_acc: 0.9590\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9778\n",
      "Epoch 00048: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0643 - acc: 0.9778 - val_loss: 0.1864 - val_acc: 0.9592\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9795\n",
      "Epoch 00049: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0610 - acc: 0.9795 - val_loss: 0.1703 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9807\n",
      "Epoch 00050: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0581 - acc: 0.9807 - val_loss: 0.1897 - val_acc: 0.9606\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9795\n",
      "Epoch 00051: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0593 - acc: 0.9795 - val_loss: 0.1937 - val_acc: 0.9571\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9810\n",
      "Epoch 00052: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0556 - acc: 0.9810 - val_loss: 0.1876 - val_acc: 0.9555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9790\n",
      "Epoch 00053: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0606 - acc: 0.9790 - val_loss: 0.1658 - val_acc: 0.9581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9810\n",
      "Epoch 00054: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0529 - acc: 0.9810 - val_loss: 0.2450 - val_acc: 0.9543\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9825\n",
      "Epoch 00055: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0530 - acc: 0.9825 - val_loss: 0.1899 - val_acc: 0.9588\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9824\n",
      "Epoch 00056: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0518 - acc: 0.9824 - val_loss: 0.1924 - val_acc: 0.9574\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9833\n",
      "Epoch 00057: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0524 - acc: 0.9833 - val_loss: 0.2029 - val_acc: 0.9567\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9822\n",
      "Epoch 00058: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0519 - acc: 0.9822 - val_loss: 0.2205 - val_acc: 0.9567\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9835\n",
      "Epoch 00059: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0480 - acc: 0.9835 - val_loss: 0.2115 - val_acc: 0.9562\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9838\n",
      "Epoch 00060: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0473 - acc: 0.9838 - val_loss: 0.2072 - val_acc: 0.9576\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9834\n",
      "Epoch 00061: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0496 - acc: 0.9834 - val_loss: 0.2004 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9842\n",
      "Epoch 00062: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0474 - acc: 0.9842 - val_loss: 0.1828 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9841\n",
      "Epoch 00063: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0485 - acc: 0.9841 - val_loss: 0.2157 - val_acc: 0.9592\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9854\n",
      "Epoch 00064: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0436 - acc: 0.9854 - val_loss: 0.1943 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9857\n",
      "Epoch 00065: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0432 - acc: 0.9857 - val_loss: 0.2033 - val_acc: 0.9564\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9844\n",
      "Epoch 00066: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0453 - acc: 0.9844 - val_loss: 0.1855 - val_acc: 0.9585\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9844\n",
      "Epoch 00067: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0476 - acc: 0.9844 - val_loss: 0.2039 - val_acc: 0.9602\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9859\n",
      "Epoch 00068: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0429 - acc: 0.9859 - val_loss: 0.2017 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9864\n",
      "Epoch 00069: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0391 - acc: 0.9864 - val_loss: 0.1957 - val_acc: 0.9583\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9865\n",
      "Epoch 00070: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0375 - acc: 0.9865 - val_loss: 0.2369 - val_acc: 0.9555\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9861\n",
      "Epoch 00071: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0408 - acc: 0.9861 - val_loss: 0.2352 - val_acc: 0.9569\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9874\n",
      "Epoch 00072: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0378 - acc: 0.9874 - val_loss: 0.1945 - val_acc: 0.9609\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9876\n",
      "Epoch 00073: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0392 - acc: 0.9876 - val_loss: 0.2136 - val_acc: 0.9592\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9888\n",
      "Epoch 00074: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0340 - acc: 0.9888 - val_loss: 0.2013 - val_acc: 0.9592\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9881\n",
      "Epoch 00075: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0348 - acc: 0.9881 - val_loss: 0.1852 - val_acc: 0.9599\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9868\n",
      "Epoch 00076: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0388 - acc: 0.9868 - val_loss: 0.1967 - val_acc: 0.9590\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9885\n",
      "Epoch 00077: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0340 - acc: 0.9885 - val_loss: 0.2014 - val_acc: 0.9571\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9883\n",
      "Epoch 00078: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0340 - acc: 0.9883 - val_loss: 0.2416 - val_acc: 0.9532\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9900\n",
      "Epoch 00079: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0315 - acc: 0.9900 - val_loss: 0.2452 - val_acc: 0.9581\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9876\n",
      "Epoch 00080: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0376 - acc: 0.9876 - val_loss: 0.2307 - val_acc: 0.9576\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9889\n",
      "Epoch 00081: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0326 - acc: 0.9889 - val_loss: 0.2099 - val_acc: 0.9583\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9883\n",
      "Epoch 00082: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0343 - acc: 0.9883 - val_loss: 0.2237 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9878\n",
      "Epoch 00083: val_loss did not improve from 0.16233\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0360 - acc: 0.9878 - val_loss: 0.1949 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1977 - acc: 0.9423\n",
      "Loss: 0.1976632449423908 Accuracy: 0.9422638\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1577 - acc: 0.2901\n",
      "Epoch 00001: val_loss improved from inf to 1.29534, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/001-1.2953.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 2.1577 - acc: 0.2901 - val_loss: 1.2953 - val_acc: 0.5970\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2236 - acc: 0.6009\n",
      "Epoch 00002: val_loss improved from 1.29534 to 0.74195, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/002-0.7419.hdf5\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 1.2234 - acc: 0.6010 - val_loss: 0.7419 - val_acc: 0.7780\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.7214\n",
      "Epoch 00003: val_loss improved from 0.74195 to 0.58523, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/003-0.5852.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.8682 - acc: 0.7213 - val_loss: 0.5852 - val_acc: 0.8248\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.7843\n",
      "Epoch 00004: val_loss improved from 0.58523 to 0.38659, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/004-0.3866.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6705 - acc: 0.7843 - val_loss: 0.3866 - val_acc: 0.8863\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8304\n",
      "Epoch 00005: val_loss improved from 0.38659 to 0.30661, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/005-0.3066.hdf5\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.5374 - acc: 0.8303 - val_loss: 0.3066 - val_acc: 0.9103\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8604\n",
      "Epoch 00006: val_loss did not improve from 0.30661\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.4479 - acc: 0.8605 - val_loss: 0.3135 - val_acc: 0.9022\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8787\n",
      "Epoch 00007: val_loss improved from 0.30661 to 0.23114, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/007-0.2311.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.3859 - acc: 0.8787 - val_loss: 0.2311 - val_acc: 0.9345\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8933\n",
      "Epoch 00008: val_loss improved from 0.23114 to 0.20167, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/008-0.2017.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.3387 - acc: 0.8933 - val_loss: 0.2017 - val_acc: 0.9406\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9038\n",
      "Epoch 00009: val_loss improved from 0.20167 to 0.20058, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/009-0.2006.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.3061 - acc: 0.9038 - val_loss: 0.2006 - val_acc: 0.9380\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9115\n",
      "Epoch 00010: val_loss improved from 0.20058 to 0.17555, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/010-0.1756.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.2814 - acc: 0.9115 - val_loss: 0.1756 - val_acc: 0.9474\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9173\n",
      "Epoch 00011: val_loss did not improve from 0.17555\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.2610 - acc: 0.9173 - val_loss: 0.1835 - val_acc: 0.9448\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9224\n",
      "Epoch 00012: val_loss improved from 0.17555 to 0.15427, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/012-0.1543.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2424 - acc: 0.9224 - val_loss: 0.1543 - val_acc: 0.9525\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9301\n",
      "Epoch 00013: val_loss did not improve from 0.15427\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.2162 - acc: 0.9301 - val_loss: 0.1686 - val_acc: 0.9488\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9348\n",
      "Epoch 00014: val_loss did not improve from 0.15427\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.2048 - acc: 0.9348 - val_loss: 0.1550 - val_acc: 0.9536\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9379\n",
      "Epoch 00015: val_loss did not improve from 0.15427\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1970 - acc: 0.9379 - val_loss: 0.1601 - val_acc: 0.9562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9402\n",
      "Epoch 00016: val_loss improved from 0.15427 to 0.13631, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/016-0.1363.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1840 - acc: 0.9402 - val_loss: 0.1363 - val_acc: 0.9578\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9438\n",
      "Epoch 00017: val_loss improved from 0.13631 to 0.13105, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/017-0.1310.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1765 - acc: 0.9438 - val_loss: 0.1310 - val_acc: 0.9590\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9454\n",
      "Epoch 00018: val_loss did not improve from 0.13105\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.1668 - acc: 0.9454 - val_loss: 0.1422 - val_acc: 0.9590\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9498\n",
      "Epoch 00019: val_loss did not improve from 0.13105\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1540 - acc: 0.9498 - val_loss: 0.1365 - val_acc: 0.9602\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9520\n",
      "Epoch 00020: val_loss improved from 0.13105 to 0.12222, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/020-0.1222.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1497 - acc: 0.9520 - val_loss: 0.1222 - val_acc: 0.9632\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9528\n",
      "Epoch 00021: val_loss did not improve from 0.12222\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.1408 - acc: 0.9528 - val_loss: 0.1335 - val_acc: 0.9639\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9557\n",
      "Epoch 00022: val_loss did not improve from 0.12222\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.1339 - acc: 0.9557 - val_loss: 0.1337 - val_acc: 0.9576\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9586\n",
      "Epoch 00023: val_loss improved from 0.12222 to 0.12023, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/023-0.1202.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.1280 - acc: 0.9586 - val_loss: 0.1202 - val_acc: 0.9641\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9604\n",
      "Epoch 00024: val_loss did not improve from 0.12023\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1230 - acc: 0.9604 - val_loss: 0.1244 - val_acc: 0.9653\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9629\n",
      "Epoch 00025: val_loss did not improve from 0.12023\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.1133 - acc: 0.9629 - val_loss: 0.1322 - val_acc: 0.9634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9646\n",
      "Epoch 00026: val_loss did not improve from 0.12023\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 0.1107 - acc: 0.9646 - val_loss: 0.1352 - val_acc: 0.9630\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9648\n",
      "Epoch 00027: val_loss improved from 0.12023 to 0.11844, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/027-0.1184.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.1075 - acc: 0.9648 - val_loss: 0.1184 - val_acc: 0.9655\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9665\n",
      "Epoch 00028: val_loss did not improve from 0.11844\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1008 - acc: 0.9665 - val_loss: 0.1212 - val_acc: 0.9648\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9674\n",
      "Epoch 00029: val_loss improved from 0.11844 to 0.11818, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/029-0.1182.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0993 - acc: 0.9674 - val_loss: 0.1182 - val_acc: 0.9665\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9682\n",
      "Epoch 00030: val_loss did not improve from 0.11818\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0962 - acc: 0.9682 - val_loss: 0.1325 - val_acc: 0.9665\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9715\n",
      "Epoch 00031: val_loss improved from 0.11818 to 0.11022, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/031-0.1102.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0862 - acc: 0.9715 - val_loss: 0.1102 - val_acc: 0.9697\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9703\n",
      "Epoch 00032: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0886 - acc: 0.9703 - val_loss: 0.1201 - val_acc: 0.9651\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9719\n",
      "Epoch 00033: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0850 - acc: 0.9719 - val_loss: 0.1220 - val_acc: 0.9644\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9725\n",
      "Epoch 00034: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0841 - acc: 0.9725 - val_loss: 0.1111 - val_acc: 0.9681\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9727\n",
      "Epoch 00035: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0802 - acc: 0.9727 - val_loss: 0.1454 - val_acc: 0.9632\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9747\n",
      "Epoch 00036: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0771 - acc: 0.9747 - val_loss: 0.1260 - val_acc: 0.9653\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9753\n",
      "Epoch 00037: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0739 - acc: 0.9753 - val_loss: 0.1300 - val_acc: 0.9674\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9756\n",
      "Epoch 00038: val_loss did not improve from 0.11022\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0721 - acc: 0.9756 - val_loss: 0.1235 - val_acc: 0.9695\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9776\n",
      "Epoch 00039: val_loss improved from 0.11022 to 0.10413, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/039-0.1041.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0695 - acc: 0.9776 - val_loss: 0.1041 - val_acc: 0.9688\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9786\n",
      "Epoch 00040: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0640 - acc: 0.9786 - val_loss: 0.1378 - val_acc: 0.9632\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9762\n",
      "Epoch 00041: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0697 - acc: 0.9762 - val_loss: 0.1229 - val_acc: 0.9690\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9797\n",
      "Epoch 00042: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0604 - acc: 0.9797 - val_loss: 0.1180 - val_acc: 0.9706\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9791\n",
      "Epoch 00043: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0609 - acc: 0.9791 - val_loss: 0.1401 - val_acc: 0.9679\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9796\n",
      "Epoch 00044: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0607 - acc: 0.9796 - val_loss: 0.1176 - val_acc: 0.9681\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9813\n",
      "Epoch 00045: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.0548 - acc: 0.9813 - val_loss: 0.1073 - val_acc: 0.9713\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9810\n",
      "Epoch 00046: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0547 - acc: 0.9810 - val_loss: 0.1187 - val_acc: 0.9681\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9811\n",
      "Epoch 00047: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0551 - acc: 0.9811 - val_loss: 0.1341 - val_acc: 0.9693\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9812\n",
      "Epoch 00048: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0536 - acc: 0.9813 - val_loss: 0.1264 - val_acc: 0.9693\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9827\n",
      "Epoch 00049: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0505 - acc: 0.9827 - val_loss: 0.1229 - val_acc: 0.9709\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9839\n",
      "Epoch 00050: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0477 - acc: 0.9839 - val_loss: 0.1398 - val_acc: 0.9683\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9837\n",
      "Epoch 00051: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0480 - acc: 0.9837 - val_loss: 0.1363 - val_acc: 0.9700\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9841\n",
      "Epoch 00052: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0456 - acc: 0.9841 - val_loss: 0.1318 - val_acc: 0.9695\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9847\n",
      "Epoch 00053: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0460 - acc: 0.9847 - val_loss: 0.1354 - val_acc: 0.9688\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9853\n",
      "Epoch 00054: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0448 - acc: 0.9853 - val_loss: 0.1618 - val_acc: 0.9616\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9866\n",
      "Epoch 00055: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0406 - acc: 0.9866 - val_loss: 0.1504 - val_acc: 0.9662\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9841\n",
      "Epoch 00056: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0455 - acc: 0.9841 - val_loss: 0.1430 - val_acc: 0.9669\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9860\n",
      "Epoch 00057: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0420 - acc: 0.9860 - val_loss: 0.1527 - val_acc: 0.9665\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9861\n",
      "Epoch 00058: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0418 - acc: 0.9861 - val_loss: 0.1192 - val_acc: 0.9711\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9870\n",
      "Epoch 00059: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0379 - acc: 0.9870 - val_loss: 0.1553 - val_acc: 0.9653\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9871\n",
      "Epoch 00060: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0391 - acc: 0.9871 - val_loss: 0.1515 - val_acc: 0.9667\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9871\n",
      "Epoch 00061: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0396 - acc: 0.9871 - val_loss: 0.1442 - val_acc: 0.9690\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9880\n",
      "Epoch 00062: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0358 - acc: 0.9880 - val_loss: 0.1373 - val_acc: 0.9674\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9877\n",
      "Epoch 00063: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0376 - acc: 0.9877 - val_loss: 0.1263 - val_acc: 0.9697\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9873\n",
      "Epoch 00064: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0373 - acc: 0.9873 - val_loss: 0.1400 - val_acc: 0.9709\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9887\n",
      "Epoch 00065: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0326 - acc: 0.9887 - val_loss: 0.1280 - val_acc: 0.9697\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9893\n",
      "Epoch 00066: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0335 - acc: 0.9893 - val_loss: 0.1795 - val_acc: 0.9625\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9881\n",
      "Epoch 00067: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0348 - acc: 0.9881 - val_loss: 0.1513 - val_acc: 0.9695\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0301 - acc: 0.9895 - val_loss: 0.1426 - val_acc: 0.9718\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9890\n",
      "Epoch 00069: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0331 - acc: 0.9890 - val_loss: 0.1541 - val_acc: 0.9690\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 00070: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0311 - acc: 0.9895 - val_loss: 0.1490 - val_acc: 0.9665\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9899\n",
      "Epoch 00071: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0304 - acc: 0.9899 - val_loss: 0.1438 - val_acc: 0.9700\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9897\n",
      "Epoch 00072: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0308 - acc: 0.9897 - val_loss: 0.1520 - val_acc: 0.9660\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9904\n",
      "Epoch 00073: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0288 - acc: 0.9904 - val_loss: 0.1479 - val_acc: 0.9706\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9905\n",
      "Epoch 00074: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 0.0272 - acc: 0.9905 - val_loss: 0.1642 - val_acc: 0.9676\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9900\n",
      "Epoch 00075: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0306 - acc: 0.9900 - val_loss: 0.1528 - val_acc: 0.9697\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9902\n",
      "Epoch 00076: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0293 - acc: 0.9902 - val_loss: 0.1737 - val_acc: 0.9672\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9921\n",
      "Epoch 00077: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.0235 - acc: 0.9921 - val_loss: 0.1426 - val_acc: 0.9725\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9905\n",
      "Epoch 00078: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0274 - acc: 0.9905 - val_loss: 0.1568 - val_acc: 0.9697\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9917\n",
      "Epoch 00079: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0250 - acc: 0.9917 - val_loss: 0.1722 - val_acc: 0.9674\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9914\n",
      "Epoch 00080: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0259 - acc: 0.9914 - val_loss: 0.1739 - val_acc: 0.9676\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9923\n",
      "Epoch 00081: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 0.0233 - acc: 0.9923 - val_loss: 0.1584 - val_acc: 0.9693\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9912\n",
      "Epoch 00082: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 0.0257 - acc: 0.9913 - val_loss: 0.1739 - val_acc: 0.9658\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9909\n",
      "Epoch 00083: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0263 - acc: 0.9909 - val_loss: 0.1660 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9917\n",
      "Epoch 00084: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0257 - acc: 0.9917 - val_loss: 0.1673 - val_acc: 0.9646\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9929\n",
      "Epoch 00085: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0219 - acc: 0.9929 - val_loss: 0.1628 - val_acc: 0.9709\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9901\n",
      "Epoch 00086: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 0.0308 - acc: 0.9901 - val_loss: 0.1473 - val_acc: 0.9695\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9916\n",
      "Epoch 00087: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0265 - acc: 0.9916 - val_loss: 0.1519 - val_acc: 0.9704\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9938\n",
      "Epoch 00088: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0182 - acc: 0.9938 - val_loss: 0.1713 - val_acc: 0.9681\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9917\n",
      "Epoch 00089: val_loss did not improve from 0.10413\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 0.0253 - acc: 0.9917 - val_loss: 0.1500 - val_acc: 0.9681\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1485 - acc: 0.9583\n",
      "Loss: 0.14848895899541406 Accuracy: 0.95825547\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0928 - acc: 0.3263\n",
      "Epoch 00001: val_loss improved from inf to 1.19802, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/001-1.1980.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 2.0928 - acc: 0.3263 - val_loss: 1.1980 - val_acc: 0.6508\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2744 - acc: 0.5905\n",
      "Epoch 00002: val_loss improved from 1.19802 to 0.77553, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/002-0.7755.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 1.2744 - acc: 0.5905 - val_loss: 0.7755 - val_acc: 0.7713\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9745 - acc: 0.6887\n",
      "Epoch 00003: val_loss improved from 0.77553 to 0.58941, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/003-0.5894.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.9745 - acc: 0.6887 - val_loss: 0.5894 - val_acc: 0.8251\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7517 - acc: 0.7627\n",
      "Epoch 00004: val_loss improved from 0.58941 to 0.40357, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/004-0.4036.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.7516 - acc: 0.7627 - val_loss: 0.4036 - val_acc: 0.8796\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8112\n",
      "Epoch 00005: val_loss improved from 0.40357 to 0.36075, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/005-0.3607.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.6091 - acc: 0.8112 - val_loss: 0.3607 - val_acc: 0.8949\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8460\n",
      "Epoch 00006: val_loss improved from 0.36075 to 0.25545, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/006-0.2555.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4979 - acc: 0.8460 - val_loss: 0.2555 - val_acc: 0.9278\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8654\n",
      "Epoch 00007: val_loss improved from 0.25545 to 0.25241, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/007-0.2524.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4384 - acc: 0.8654 - val_loss: 0.2524 - val_acc: 0.9245\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8792\n",
      "Epoch 00008: val_loss improved from 0.25241 to 0.20979, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/008-0.2098.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3886 - acc: 0.8791 - val_loss: 0.2098 - val_acc: 0.9432\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8917\n",
      "Epoch 00009: val_loss did not improve from 0.20979\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3490 - acc: 0.8918 - val_loss: 0.2229 - val_acc: 0.9334\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9050\n",
      "Epoch 00010: val_loss improved from 0.20979 to 0.20889, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/010-0.2089.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3127 - acc: 0.9050 - val_loss: 0.2089 - val_acc: 0.9378\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9083\n",
      "Epoch 00011: val_loss improved from 0.20889 to 0.17282, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/011-0.1728.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2948 - acc: 0.9083 - val_loss: 0.1728 - val_acc: 0.9481\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9178\n",
      "Epoch 00012: val_loss improved from 0.17282 to 0.16596, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/012-0.1660.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2698 - acc: 0.9178 - val_loss: 0.1660 - val_acc: 0.9518\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9211\n",
      "Epoch 00013: val_loss improved from 0.16596 to 0.16157, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/013-0.1616.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2462 - acc: 0.9212 - val_loss: 0.1616 - val_acc: 0.9513\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9282\n",
      "Epoch 00014: val_loss improved from 0.16157 to 0.15033, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/014-0.1503.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2290 - acc: 0.9282 - val_loss: 0.1503 - val_acc: 0.9557\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9310\n",
      "Epoch 00015: val_loss did not improve from 0.15033\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2206 - acc: 0.9310 - val_loss: 0.1617 - val_acc: 0.9527\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9358\n",
      "Epoch 00016: val_loss improved from 0.15033 to 0.14932, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/016-0.1493.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2050 - acc: 0.9358 - val_loss: 0.1493 - val_acc: 0.9590\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9397\n",
      "Epoch 00017: val_loss did not improve from 0.14932\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1882 - acc: 0.9397 - val_loss: 0.1842 - val_acc: 0.9457\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9428\n",
      "Epoch 00018: val_loss improved from 0.14932 to 0.14903, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/018-0.1490.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1806 - acc: 0.9427 - val_loss: 0.1490 - val_acc: 0.9546\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9453\n",
      "Epoch 00019: val_loss improved from 0.14903 to 0.13998, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/019-0.1400.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1719 - acc: 0.9453 - val_loss: 0.1400 - val_acc: 0.9595\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9480\n",
      "Epoch 00020: val_loss improved from 0.13998 to 0.13435, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/020-0.1344.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1660 - acc: 0.9480 - val_loss: 0.1344 - val_acc: 0.9606\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9521\n",
      "Epoch 00021: val_loss improved from 0.13435 to 0.12967, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/021-0.1297.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1508 - acc: 0.9521 - val_loss: 0.1297 - val_acc: 0.9630\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9550\n",
      "Epoch 00022: val_loss did not improve from 0.12967\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1437 - acc: 0.9550 - val_loss: 0.1445 - val_acc: 0.9578\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9548\n",
      "Epoch 00023: val_loss did not improve from 0.12967\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1388 - acc: 0.9548 - val_loss: 0.1420 - val_acc: 0.9623\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9549\n",
      "Epoch 00024: val_loss did not improve from 0.12967\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1453 - acc: 0.9549 - val_loss: 0.1584 - val_acc: 0.9555\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9596\n",
      "Epoch 00025: val_loss improved from 0.12967 to 0.12613, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/025-0.1261.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1233 - acc: 0.9595 - val_loss: 0.1261 - val_acc: 0.9639\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9615\n",
      "Epoch 00026: val_loss did not improve from 0.12613\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1202 - acc: 0.9615 - val_loss: 0.1426 - val_acc: 0.9609\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9628\n",
      "Epoch 00027: val_loss did not improve from 0.12613\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1133 - acc: 0.9628 - val_loss: 0.1261 - val_acc: 0.9627\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9647\n",
      "Epoch 00028: val_loss did not improve from 0.12613\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1066 - acc: 0.9647 - val_loss: 0.1445 - val_acc: 0.9623\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9652\n",
      "Epoch 00029: val_loss improved from 0.12613 to 0.12498, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/029-0.1250.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1047 - acc: 0.9652 - val_loss: 0.1250 - val_acc: 0.9632\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9676\n",
      "Epoch 00030: val_loss did not improve from 0.12498\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0980 - acc: 0.9676 - val_loss: 0.1419 - val_acc: 0.9618\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9656\n",
      "Epoch 00031: val_loss did not improve from 0.12498\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1048 - acc: 0.9656 - val_loss: 0.1481 - val_acc: 0.9616\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9704\n",
      "Epoch 00032: val_loss did not improve from 0.12498\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0919 - acc: 0.9704 - val_loss: 0.1290 - val_acc: 0.9646\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9715\n",
      "Epoch 00033: val_loss improved from 0.12498 to 0.12312, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/033-0.1231.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0877 - acc: 0.9715 - val_loss: 0.1231 - val_acc: 0.9658\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9711\n",
      "Epoch 00034: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0891 - acc: 0.9711 - val_loss: 0.1378 - val_acc: 0.9665\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9731\n",
      "Epoch 00035: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0818 - acc: 0.9730 - val_loss: 0.1441 - val_acc: 0.9641\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9726\n",
      "Epoch 00036: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0835 - acc: 0.9726 - val_loss: 0.1316 - val_acc: 0.9669\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9755\n",
      "Epoch 00037: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0761 - acc: 0.9755 - val_loss: 0.1394 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9746\n",
      "Epoch 00038: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0748 - acc: 0.9746 - val_loss: 0.1236 - val_acc: 0.9665\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9762\n",
      "Epoch 00039: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0735 - acc: 0.9763 - val_loss: 0.1249 - val_acc: 0.9676\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9767\n",
      "Epoch 00040: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0699 - acc: 0.9767 - val_loss: 0.1279 - val_acc: 0.9669\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9790\n",
      "Epoch 00041: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0642 - acc: 0.9790 - val_loss: 0.1380 - val_acc: 0.9646\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9771\n",
      "Epoch 00042: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0677 - acc: 0.9771 - val_loss: 0.1358 - val_acc: 0.9665\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9803\n",
      "Epoch 00043: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0607 - acc: 0.9803 - val_loss: 0.1446 - val_acc: 0.9658\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9798\n",
      "Epoch 00044: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0599 - acc: 0.9798 - val_loss: 0.1416 - val_acc: 0.9700\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9807\n",
      "Epoch 00045: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0581 - acc: 0.9807 - val_loss: 0.1336 - val_acc: 0.9697\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9812\n",
      "Epoch 00046: val_loss did not improve from 0.12312\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0580 - acc: 0.9813 - val_loss: 0.1574 - val_acc: 0.9658\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9816\n",
      "Epoch 00047: val_loss improved from 0.12312 to 0.12264, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/047-0.1226.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0563 - acc: 0.9816 - val_loss: 0.1226 - val_acc: 0.9695\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9826\n",
      "Epoch 00048: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0524 - acc: 0.9826 - val_loss: 0.1592 - val_acc: 0.9695\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9833\n",
      "Epoch 00049: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0517 - acc: 0.9833 - val_loss: 0.1664 - val_acc: 0.9660\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9836\n",
      "Epoch 00050: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0539 - acc: 0.9836 - val_loss: 0.1461 - val_acc: 0.9688\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9822\n",
      "Epoch 00051: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0549 - acc: 0.9822 - val_loss: 0.1723 - val_acc: 0.9606\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9838\n",
      "Epoch 00052: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0487 - acc: 0.9838 - val_loss: 0.1379 - val_acc: 0.9683\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9845\n",
      "Epoch 00053: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0485 - acc: 0.9844 - val_loss: 0.1728 - val_acc: 0.9665\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9832\n",
      "Epoch 00054: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0495 - acc: 0.9831 - val_loss: 0.1590 - val_acc: 0.9667\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9857\n",
      "Epoch 00055: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0445 - acc: 0.9857 - val_loss: 0.1585 - val_acc: 0.9651\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9855\n",
      "Epoch 00056: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0446 - acc: 0.9855 - val_loss: 0.1511 - val_acc: 0.9669\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9855\n",
      "Epoch 00057: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0430 - acc: 0.9855 - val_loss: 0.1660 - val_acc: 0.9693\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9860\n",
      "Epoch 00058: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0424 - acc: 0.9860 - val_loss: 0.1780 - val_acc: 0.9665\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9858\n",
      "Epoch 00059: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.1584 - val_acc: 0.9690\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9863\n",
      "Epoch 00060: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0440 - acc: 0.9863 - val_loss: 0.1671 - val_acc: 0.9690\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9870\n",
      "Epoch 00061: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0380 - acc: 0.9870 - val_loss: 0.1526 - val_acc: 0.9688\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9875\n",
      "Epoch 00062: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0386 - acc: 0.9875 - val_loss: 0.1830 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9885\n",
      "Epoch 00063: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0361 - acc: 0.9885 - val_loss: 0.1684 - val_acc: 0.9665\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9876\n",
      "Epoch 00064: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0375 - acc: 0.9876 - val_loss: 0.1779 - val_acc: 0.9648\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9877\n",
      "Epoch 00065: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0374 - acc: 0.9877 - val_loss: 0.1781 - val_acc: 0.9658\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9881\n",
      "Epoch 00066: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0361 - acc: 0.9881 - val_loss: 0.1701 - val_acc: 0.9683\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9897\n",
      "Epoch 00067: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0322 - acc: 0.9897 - val_loss: 0.1790 - val_acc: 0.9658\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9877\n",
      "Epoch 00068: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0356 - acc: 0.9877 - val_loss: 0.1629 - val_acc: 0.9686\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9893\n",
      "Epoch 00069: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0327 - acc: 0.9893 - val_loss: 0.1541 - val_acc: 0.9713\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9887\n",
      "Epoch 00070: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0328 - acc: 0.9887 - val_loss: 0.1711 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9897\n",
      "Epoch 00071: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0314 - acc: 0.9897 - val_loss: 0.1907 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9887\n",
      "Epoch 00072: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0335 - acc: 0.9888 - val_loss: 0.1863 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9911\n",
      "Epoch 00073: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0268 - acc: 0.9911 - val_loss: 0.2064 - val_acc: 0.9639\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9891\n",
      "Epoch 00074: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0347 - acc: 0.9891 - val_loss: 0.2122 - val_acc: 0.9616\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9904\n",
      "Epoch 00075: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0317 - acc: 0.9904 - val_loss: 0.2212 - val_acc: 0.9662\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9910\n",
      "Epoch 00076: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0287 - acc: 0.9910 - val_loss: 0.1648 - val_acc: 0.9693\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9912\n",
      "Epoch 00077: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0274 - acc: 0.9913 - val_loss: 0.1749 - val_acc: 0.9700\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9894\n",
      "Epoch 00078: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0322 - acc: 0.9894 - val_loss: 0.1746 - val_acc: 0.9688\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9897\n",
      "Epoch 00079: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0331 - acc: 0.9897 - val_loss: 0.1919 - val_acc: 0.9616\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9930\n",
      "Epoch 00080: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0216 - acc: 0.9930 - val_loss: 0.1798 - val_acc: 0.9704\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9904\n",
      "Epoch 00081: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0279 - acc: 0.9904 - val_loss: 0.1889 - val_acc: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9911\n",
      "Epoch 00082: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0277 - acc: 0.9911 - val_loss: 0.1720 - val_acc: 0.9688\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9915\n",
      "Epoch 00083: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0242 - acc: 0.9916 - val_loss: 0.1823 - val_acc: 0.9702\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9920\n",
      "Epoch 00084: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0243 - acc: 0.9920 - val_loss: 0.1836 - val_acc: 0.9686\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9912\n",
      "Epoch 00085: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0285 - acc: 0.9912 - val_loss: 0.1943 - val_acc: 0.9653\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9917\n",
      "Epoch 00086: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0255 - acc: 0.9917 - val_loss: 0.1733 - val_acc: 0.9711\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9919\n",
      "Epoch 00087: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0253 - acc: 0.9919 - val_loss: 0.1909 - val_acc: 0.9662\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9925\n",
      "Epoch 00088: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0217 - acc: 0.9925 - val_loss: 0.2102 - val_acc: 0.9674\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9922\n",
      "Epoch 00089: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0238 - acc: 0.9922 - val_loss: 0.1968 - val_acc: 0.9706\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9918\n",
      "Epoch 00090: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0261 - acc: 0.9918 - val_loss: 0.1939 - val_acc: 0.9679\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9925\n",
      "Epoch 00091: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0244 - acc: 0.9925 - val_loss: 0.1880 - val_acc: 0.9674\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9917\n",
      "Epoch 00092: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0270 - acc: 0.9917 - val_loss: 0.1620 - val_acc: 0.9716\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9939\n",
      "Epoch 00093: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0201 - acc: 0.9939 - val_loss: 0.1881 - val_acc: 0.9697\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9935\n",
      "Epoch 00094: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0213 - acc: 0.9935 - val_loss: 0.1978 - val_acc: 0.9679\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9939\n",
      "Epoch 00095: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0202 - acc: 0.9939 - val_loss: 0.1696 - val_acc: 0.9686\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9929\n",
      "Epoch 00096: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0216 - acc: 0.9929 - val_loss: 0.1814 - val_acc: 0.9702\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9937\n",
      "Epoch 00097: val_loss did not improve from 0.12264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0193 - acc: 0.9937 - val_loss: 0.1962 - val_acc: 0.9713\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1674 - acc: 0.9603\n",
      "Loss: 0.16742254200421453 Accuracy: 0.9603323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1364496     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.3904 - acc: 0.8866\n",
      "Loss: 0.39037198481901414 Accuracy: 0.88660437\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1356432     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1977 - acc: 0.9423\n",
      "Loss: 0.1976632449423908 Accuracy: 0.9422638\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1409808     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1485 - acc: 0.9583\n",
      "Loss: 0.14848895899541406 Accuracy: 0.95825547\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1438544     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1674 - acc: 0.9603\n",
      "Loss: 0.16742254200421453 Accuracy: 0.9603323\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1364496     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.5111 - acc: 0.9097\n",
      "Loss: 0.5111338728387006 Accuracy: 0.9096573\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1356432     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.2211 - acc: 0.9508\n",
      "Loss: 0.2211277675407775 Accuracy: 0.95077884\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1409808     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.2030 - acc: 0.9593\n",
      "Loss: 0.20301731937820708 Accuracy: 0.9592939\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1438544     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.2463 - acc: 0.9595\n",
      "Loss: 0.2462937576544804 Accuracy: 0.95950156\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
