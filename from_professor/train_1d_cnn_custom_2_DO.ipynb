{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 65,537,552\n",
      "Trainable params: 65,537,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 22,173,456\n",
      "Trainable params: 22,173,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 7,936,016\n",
      "Trainable params: 7,936,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 3,410,192\n",
      "Trainable params: 3,410,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,552,784\n",
      "Trainable params: 1,552,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0342 - acc: 0.3335\n",
      "Epoch 00001: val_loss improved from inf to 1.42976, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/001-1.4298.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 2.0341 - acc: 0.3336 - val_loss: 1.4298 - val_acc: 0.5448\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3817 - acc: 0.5525\n",
      "Epoch 00002: val_loss improved from 1.42976 to 1.08362, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/002-1.0836.hdf5\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 1.3817 - acc: 0.5525 - val_loss: 1.0836 - val_acc: 0.6662\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1125 - acc: 0.6515\n",
      "Epoch 00003: val_loss improved from 1.08362 to 0.87089, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/003-0.8709.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 1.1125 - acc: 0.6515 - val_loss: 0.8709 - val_acc: 0.7424\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8997 - acc: 0.7238\n",
      "Epoch 00004: val_loss improved from 0.87089 to 0.78213, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/004-0.7821.hdf5\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.8996 - acc: 0.7238 - val_loss: 0.7821 - val_acc: 0.7689\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7711\n",
      "Epoch 00005: val_loss improved from 0.78213 to 0.70634, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/005-0.7063.hdf5\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.7620 - acc: 0.7711 - val_loss: 0.7063 - val_acc: 0.7990\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.8042\n",
      "Epoch 00006: val_loss improved from 0.70634 to 0.54330, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/006-0.5433.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.6597 - acc: 0.8042 - val_loss: 0.5433 - val_acc: 0.8477\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5785 - acc: 0.8280\n",
      "Epoch 00007: val_loss improved from 0.54330 to 0.50482, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/007-0.5048.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.5785 - acc: 0.8280 - val_loss: 0.5048 - val_acc: 0.8616\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.8464\n",
      "Epoch 00008: val_loss improved from 0.50482 to 0.50080, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/008-0.5008.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.5176 - acc: 0.8464 - val_loss: 0.5008 - val_acc: 0.8651\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8587\n",
      "Epoch 00009: val_loss improved from 0.50080 to 0.43600, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/009-0.4360.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.4773 - acc: 0.8587 - val_loss: 0.4360 - val_acc: 0.8761\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8705\n",
      "Epoch 00010: val_loss improved from 0.43600 to 0.42871, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/010-0.4287.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.4323 - acc: 0.8705 - val_loss: 0.4287 - val_acc: 0.8821\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8809\n",
      "Epoch 00011: val_loss did not improve from 0.42871\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.3968 - acc: 0.8809 - val_loss: 0.4376 - val_acc: 0.8793\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8901\n",
      "Epoch 00012: val_loss did not improve from 0.42871\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.3648 - acc: 0.8902 - val_loss: 0.4295 - val_acc: 0.8805\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8972\n",
      "Epoch 00013: val_loss improved from 0.42871 to 0.36423, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/013-0.3642.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.3371 - acc: 0.8972 - val_loss: 0.3642 - val_acc: 0.8989\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9056\n",
      "Epoch 00014: val_loss improved from 0.36423 to 0.36129, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/014-0.3613.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.3081 - acc: 0.9056 - val_loss: 0.3613 - val_acc: 0.9024\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9121\n",
      "Epoch 00015: val_loss improved from 0.36129 to 0.35390, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/015-0.3539.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2907 - acc: 0.9121 - val_loss: 0.3539 - val_acc: 0.9073\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9162\n",
      "Epoch 00016: val_loss did not improve from 0.35390\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2754 - acc: 0.9162 - val_loss: 0.3600 - val_acc: 0.9008\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9219\n",
      "Epoch 00017: val_loss improved from 0.35390 to 0.34950, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/017-0.3495.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2525 - acc: 0.9219 - val_loss: 0.3495 - val_acc: 0.9099\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9265\n",
      "Epoch 00018: val_loss improved from 0.34950 to 0.33710, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/018-0.3371.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2388 - acc: 0.9265 - val_loss: 0.3371 - val_acc: 0.9078\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9283\n",
      "Epoch 00019: val_loss did not improve from 0.33710\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2280 - acc: 0.9283 - val_loss: 0.3931 - val_acc: 0.9005\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9330\n",
      "Epoch 00020: val_loss improved from 0.33710 to 0.32146, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/020-0.3215.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.2147 - acc: 0.9331 - val_loss: 0.3215 - val_acc: 0.9113\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9370\n",
      "Epoch 00021: val_loss did not improve from 0.32146\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1979 - acc: 0.9370 - val_loss: 0.3284 - val_acc: 0.9229\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9401\n",
      "Epoch 00022: val_loss did not improve from 0.32146\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1877 - acc: 0.9401 - val_loss: 0.3810 - val_acc: 0.9045\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9444\n",
      "Epoch 00023: val_loss did not improve from 0.32146\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1756 - acc: 0.9444 - val_loss: 0.3370 - val_acc: 0.9143\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9465\n",
      "Epoch 00024: val_loss improved from 0.32146 to 0.32006, saving model to model/checkpoint/1D_CNN_custom_2_DO_6_conv_checkpoint/024-0.3201.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1646 - acc: 0.9465 - val_loss: 0.3201 - val_acc: 0.9154\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9497\n",
      "Epoch 00025: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1571 - acc: 0.9497 - val_loss: 0.3383 - val_acc: 0.9178\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9498\n",
      "Epoch 00026: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1525 - acc: 0.9498 - val_loss: 0.3210 - val_acc: 0.9152\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9543\n",
      "Epoch 00027: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1406 - acc: 0.9544 - val_loss: 0.3641 - val_acc: 0.9106\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9585\n",
      "Epoch 00028: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1299 - acc: 0.9584 - val_loss: 0.3995 - val_acc: 0.9061\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9570\n",
      "Epoch 00029: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1322 - acc: 0.9570 - val_loss: 0.3538 - val_acc: 0.9236\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9592\n",
      "Epoch 00030: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1211 - acc: 0.9592 - val_loss: 0.3607 - val_acc: 0.9203\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9616\n",
      "Epoch 00031: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1181 - acc: 0.9616 - val_loss: 0.3806 - val_acc: 0.9131\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9636\n",
      "Epoch 00032: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1101 - acc: 0.9636 - val_loss: 0.3696 - val_acc: 0.9213\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9647\n",
      "Epoch 00033: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.1069 - acc: 0.9647 - val_loss: 0.3693 - val_acc: 0.9234\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9656\n",
      "Epoch 00034: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1057 - acc: 0.9656 - val_loss: 0.3641 - val_acc: 0.9224\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9682\n",
      "Epoch 00035: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0997 - acc: 0.9682 - val_loss: 0.4093 - val_acc: 0.9213\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9687\n",
      "Epoch 00036: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0956 - acc: 0.9687 - val_loss: 0.3858 - val_acc: 0.9131\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9693\n",
      "Epoch 00037: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0928 - acc: 0.9693 - val_loss: 0.3985 - val_acc: 0.9152\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9691\n",
      "Epoch 00038: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0932 - acc: 0.9691 - val_loss: 0.3792 - val_acc: 0.9236\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9728\n",
      "Epoch 00039: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0820 - acc: 0.9728 - val_loss: 0.3898 - val_acc: 0.9192\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9724\n",
      "Epoch 00040: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0871 - acc: 0.9724 - val_loss: 0.3841 - val_acc: 0.9210\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9744\n",
      "Epoch 00041: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0787 - acc: 0.9744 - val_loss: 0.3808 - val_acc: 0.9182\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9746\n",
      "Epoch 00042: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0781 - acc: 0.9746 - val_loss: 0.4279 - val_acc: 0.9150\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9736\n",
      "Epoch 00043: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0803 - acc: 0.9736 - val_loss: 0.3940 - val_acc: 0.9210\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9757\n",
      "Epoch 00044: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0762 - acc: 0.9757 - val_loss: 0.4058 - val_acc: 0.9187\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9764\n",
      "Epoch 00045: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0717 - acc: 0.9764 - val_loss: 0.4054 - val_acc: 0.9245\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9785\n",
      "Epoch 00046: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0688 - acc: 0.9785 - val_loss: 0.4261 - val_acc: 0.9145\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00047: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0698 - acc: 0.9777 - val_loss: 0.3861 - val_acc: 0.9241\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9771\n",
      "Epoch 00048: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0696 - acc: 0.9771 - val_loss: 0.3794 - val_acc: 0.9245\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9782\n",
      "Epoch 00049: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0654 - acc: 0.9782 - val_loss: 0.3956 - val_acc: 0.9201\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9791\n",
      "Epoch 00050: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0666 - acc: 0.9791 - val_loss: 0.4428 - val_acc: 0.9210\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9785\n",
      "Epoch 00051: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0646 - acc: 0.9785 - val_loss: 0.3751 - val_acc: 0.9227\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9810\n",
      "Epoch 00052: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0599 - acc: 0.9810 - val_loss: 0.4197 - val_acc: 0.9224\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9816\n",
      "Epoch 00053: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0576 - acc: 0.9816 - val_loss: 0.4427 - val_acc: 0.9210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9826\n",
      "Epoch 00054: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0562 - acc: 0.9826 - val_loss: 0.4304 - val_acc: 0.9271\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9809\n",
      "Epoch 00055: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0586 - acc: 0.9809 - val_loss: 0.4173 - val_acc: 0.9248\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9827\n",
      "Epoch 00056: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0548 - acc: 0.9827 - val_loss: 0.4436 - val_acc: 0.9192\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9821\n",
      "Epoch 00057: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0543 - acc: 0.9821 - val_loss: 0.4149 - val_acc: 0.9241\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9826\n",
      "Epoch 00058: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0562 - acc: 0.9826 - val_loss: 0.4147 - val_acc: 0.9283\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9835\n",
      "Epoch 00059: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0511 - acc: 0.9835 - val_loss: 0.4114 - val_acc: 0.9266\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 00060: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0505 - acc: 0.9847 - val_loss: 0.4085 - val_acc: 0.9290\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9817\n",
      "Epoch 00061: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0580 - acc: 0.9817 - val_loss: 0.4000 - val_acc: 0.9266\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9854\n",
      "Epoch 00062: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0455 - acc: 0.9854 - val_loss: 0.4279 - val_acc: 0.9262\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9843\n",
      "Epoch 00063: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0493 - acc: 0.9843 - val_loss: 0.4346 - val_acc: 0.9194\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9859\n",
      "Epoch 00064: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0464 - acc: 0.9859 - val_loss: 0.4157 - val_acc: 0.9248\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9840\n",
      "Epoch 00065: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0506 - acc: 0.9840 - val_loss: 0.4292 - val_acc: 0.9266\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9854\n",
      "Epoch 00066: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0473 - acc: 0.9854 - val_loss: 0.3679 - val_acc: 0.9236\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9853\n",
      "Epoch 00067: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0470 - acc: 0.9853 - val_loss: 0.4162 - val_acc: 0.9238\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9857\n",
      "Epoch 00068: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0462 - acc: 0.9857 - val_loss: 0.4173 - val_acc: 0.9269\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9868\n",
      "Epoch 00069: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0448 - acc: 0.9868 - val_loss: 0.4220 - val_acc: 0.9215\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9858\n",
      "Epoch 00070: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0445 - acc: 0.9858 - val_loss: 0.4247 - val_acc: 0.9248\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9866\n",
      "Epoch 00071: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0428 - acc: 0.9866 - val_loss: 0.4459 - val_acc: 0.9255\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9880\n",
      "Epoch 00072: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0382 - acc: 0.9880 - val_loss: 0.4313 - val_acc: 0.9283\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9857\n",
      "Epoch 00073: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0467 - acc: 0.9857 - val_loss: 0.3887 - val_acc: 0.9222\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9874\n",
      "Epoch 00074: val_loss did not improve from 0.32006\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0407 - acc: 0.9874 - val_loss: 0.4115 - val_acc: 0.9304\n",
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.3839 - acc: 0.8906\n",
      "Loss: 0.38391003829048925 Accuracy: 0.8905504\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2048 - acc: 0.2714\n",
      "Epoch 00001: val_loss improved from inf to 1.39608, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/001-1.3961.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 2.2047 - acc: 0.2714 - val_loss: 1.3961 - val_acc: 0.5628\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3817 - acc: 0.5491\n",
      "Epoch 00002: val_loss improved from 1.39608 to 1.03013, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/002-1.0301.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 1.3816 - acc: 0.5491 - val_loss: 1.0301 - val_acc: 0.6909\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0187 - acc: 0.6772\n",
      "Epoch 00003: val_loss improved from 1.03013 to 0.71124, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/003-0.7112.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 1.0187 - acc: 0.6772 - val_loss: 0.7112 - val_acc: 0.7862\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7633 - acc: 0.7624\n",
      "Epoch 00004: val_loss improved from 0.71124 to 0.50515, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/004-0.5051.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.7632 - acc: 0.7625 - val_loss: 0.5051 - val_acc: 0.8549\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6163 - acc: 0.8104\n",
      "Epoch 00005: val_loss improved from 0.50515 to 0.43810, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/005-0.4381.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.6164 - acc: 0.8104 - val_loss: 0.4381 - val_acc: 0.8770\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8385\n",
      "Epoch 00006: val_loss improved from 0.43810 to 0.37762, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/006-0.3776.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.5258 - acc: 0.8386 - val_loss: 0.3776 - val_acc: 0.8919\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8615\n",
      "Epoch 00007: val_loss improved from 0.37762 to 0.32325, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/007-0.3232.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.4530 - acc: 0.8615 - val_loss: 0.3232 - val_acc: 0.9113\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8783\n",
      "Epoch 00008: val_loss did not improve from 0.32325\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3996 - acc: 0.8784 - val_loss: 0.3259 - val_acc: 0.9075\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8870\n",
      "Epoch 00009: val_loss improved from 0.32325 to 0.24571, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/009-0.2457.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3646 - acc: 0.8870 - val_loss: 0.2457 - val_acc: 0.9315\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8964\n",
      "Epoch 00010: val_loss did not improve from 0.24571\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3291 - acc: 0.8964 - val_loss: 0.2627 - val_acc: 0.9273\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9024\n",
      "Epoch 00011: val_loss did not improve from 0.24571\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3133 - acc: 0.9024 - val_loss: 0.2505 - val_acc: 0.9269\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9105\n",
      "Epoch 00012: val_loss improved from 0.24571 to 0.22803, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/012-0.2280.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2882 - acc: 0.9104 - val_loss: 0.2280 - val_acc: 0.9324\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9164\n",
      "Epoch 00013: val_loss improved from 0.22803 to 0.22316, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/013-0.2232.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2638 - acc: 0.9164 - val_loss: 0.2232 - val_acc: 0.9350\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9229\n",
      "Epoch 00014: val_loss improved from 0.22316 to 0.21931, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/014-0.2193.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2438 - acc: 0.9229 - val_loss: 0.2193 - val_acc: 0.9394\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9259\n",
      "Epoch 00015: val_loss improved from 0.21931 to 0.19733, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/015-0.1973.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2316 - acc: 0.9259 - val_loss: 0.1973 - val_acc: 0.9439\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9299\n",
      "Epoch 00016: val_loss did not improve from 0.19733\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2246 - acc: 0.9299 - val_loss: 0.2017 - val_acc: 0.9429\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9328\n",
      "Epoch 00017: val_loss did not improve from 0.19733\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2104 - acc: 0.9328 - val_loss: 0.2188 - val_acc: 0.9427\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9381\n",
      "Epoch 00018: val_loss improved from 0.19733 to 0.19407, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/018-0.1941.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1956 - acc: 0.9381 - val_loss: 0.1941 - val_acc: 0.9481\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9402\n",
      "Epoch 00019: val_loss improved from 0.19407 to 0.18268, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/019-0.1827.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1890 - acc: 0.9403 - val_loss: 0.1827 - val_acc: 0.9497\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9433\n",
      "Epoch 00020: val_loss improved from 0.18268 to 0.16995, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/020-0.1699.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1790 - acc: 0.9432 - val_loss: 0.1699 - val_acc: 0.9513\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9453\n",
      "Epoch 00021: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1721 - acc: 0.9453 - val_loss: 0.1953 - val_acc: 0.9471\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9470\n",
      "Epoch 00022: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1668 - acc: 0.9470 - val_loss: 0.1807 - val_acc: 0.9504\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9497\n",
      "Epoch 00023: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1592 - acc: 0.9497 - val_loss: 0.1804 - val_acc: 0.9518\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9507\n",
      "Epoch 00024: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1503 - acc: 0.9507 - val_loss: 0.1717 - val_acc: 0.9518\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9521\n",
      "Epoch 00025: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1457 - acc: 0.9521 - val_loss: 0.1896 - val_acc: 0.9495\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9559\n",
      "Epoch 00026: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1321 - acc: 0.9559 - val_loss: 0.1931 - val_acc: 0.9502\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9580\n",
      "Epoch 00027: val_loss did not improve from 0.16995\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1275 - acc: 0.9580 - val_loss: 0.1827 - val_acc: 0.9548\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9579\n",
      "Epoch 00028: val_loss improved from 0.16995 to 0.16476, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/028-0.1648.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1291 - acc: 0.9578 - val_loss: 0.1648 - val_acc: 0.9564\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9612\n",
      "Epoch 00029: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1182 - acc: 0.9612 - val_loss: 0.1657 - val_acc: 0.9564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9640\n",
      "Epoch 00030: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1105 - acc: 0.9640 - val_loss: 0.1800 - val_acc: 0.9557\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9608\n",
      "Epoch 00031: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1155 - acc: 0.9608 - val_loss: 0.1865 - val_acc: 0.9520\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9636\n",
      "Epoch 00032: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1079 - acc: 0.9636 - val_loss: 0.1769 - val_acc: 0.9541\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9646\n",
      "Epoch 00033: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1033 - acc: 0.9646 - val_loss: 0.1879 - val_acc: 0.9527\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9648\n",
      "Epoch 00034: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1024 - acc: 0.9648 - val_loss: 0.1720 - val_acc: 0.9520\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9676\n",
      "Epoch 00035: val_loss did not improve from 0.16476\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0950 - acc: 0.9676 - val_loss: 0.1797 - val_acc: 0.9562\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9697\n",
      "Epoch 00036: val_loss improved from 0.16476 to 0.15886, saving model to model/checkpoint/1D_CNN_custom_2_DO_7_conv_checkpoint/036-0.1589.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0931 - acc: 0.9697 - val_loss: 0.1589 - val_acc: 0.9576\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9699\n",
      "Epoch 00037: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0892 - acc: 0.9699 - val_loss: 0.1877 - val_acc: 0.9495\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9681\n",
      "Epoch 00038: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0938 - acc: 0.9681 - val_loss: 0.1785 - val_acc: 0.9567\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9712\n",
      "Epoch 00039: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0834 - acc: 0.9712 - val_loss: 0.1742 - val_acc: 0.9557\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9727\n",
      "Epoch 00040: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0802 - acc: 0.9727 - val_loss: 0.1761 - val_acc: 0.9597\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9748\n",
      "Epoch 00041: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0754 - acc: 0.9748 - val_loss: 0.1815 - val_acc: 0.9562\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9735\n",
      "Epoch 00042: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0764 - acc: 0.9735 - val_loss: 0.1732 - val_acc: 0.9585\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9744\n",
      "Epoch 00043: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0761 - acc: 0.9744 - val_loss: 0.2057 - val_acc: 0.9546\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9768\n",
      "Epoch 00044: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0690 - acc: 0.9768 - val_loss: 0.1872 - val_acc: 0.9588\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9744\n",
      "Epoch 00045: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0732 - acc: 0.9744 - val_loss: 0.1751 - val_acc: 0.9581\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9762\n",
      "Epoch 00046: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0710 - acc: 0.9762 - val_loss: 0.1837 - val_acc: 0.9611\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9778\n",
      "Epoch 00047: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0657 - acc: 0.9778 - val_loss: 0.1944 - val_acc: 0.9569\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9777\n",
      "Epoch 00048: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0646 - acc: 0.9776 - val_loss: 0.2044 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9753\n",
      "Epoch 00049: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0715 - acc: 0.9753 - val_loss: 0.2015 - val_acc: 0.9597\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9784\n",
      "Epoch 00050: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0611 - acc: 0.9784 - val_loss: 0.2014 - val_acc: 0.9557\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9790\n",
      "Epoch 00051: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0576 - acc: 0.9791 - val_loss: 0.1912 - val_acc: 0.9595\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9806\n",
      "Epoch 00052: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0561 - acc: 0.9806 - val_loss: 0.1846 - val_acc: 0.9604\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9804\n",
      "Epoch 00053: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0564 - acc: 0.9804 - val_loss: 0.2028 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9825\n",
      "Epoch 00054: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0509 - acc: 0.9825 - val_loss: 0.2131 - val_acc: 0.9581\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9808\n",
      "Epoch 00055: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0565 - acc: 0.9808 - val_loss: 0.1949 - val_acc: 0.9578\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9834\n",
      "Epoch 00056: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0489 - acc: 0.9834 - val_loss: 0.2090 - val_acc: 0.9590\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9817\n",
      "Epoch 00057: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0528 - acc: 0.9817 - val_loss: 0.1951 - val_acc: 0.9583\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9824\n",
      "Epoch 00058: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0512 - acc: 0.9824 - val_loss: 0.2011 - val_acc: 0.9578\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9836\n",
      "Epoch 00059: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0482 - acc: 0.9836 - val_loss: 0.2130 - val_acc: 0.9534\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9838\n",
      "Epoch 00060: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0462 - acc: 0.9838 - val_loss: 0.2036 - val_acc: 0.9564\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9841\n",
      "Epoch 00061: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0470 - acc: 0.9841 - val_loss: 0.2097 - val_acc: 0.9590\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9838\n",
      "Epoch 00062: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0451 - acc: 0.9838 - val_loss: 0.2003 - val_acc: 0.9599\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9852\n",
      "Epoch 00063: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0426 - acc: 0.9852 - val_loss: 0.2075 - val_acc: 0.9592\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9851\n",
      "Epoch 00064: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0443 - acc: 0.9851 - val_loss: 0.2177 - val_acc: 0.9576\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9851\n",
      "Epoch 00065: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0438 - acc: 0.9851 - val_loss: 0.1997 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9864\n",
      "Epoch 00066: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0404 - acc: 0.9864 - val_loss: 0.2316 - val_acc: 0.9583\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9852\n",
      "Epoch 00067: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0433 - acc: 0.9852 - val_loss: 0.2174 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9850\n",
      "Epoch 00068: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0438 - acc: 0.9850 - val_loss: 0.2173 - val_acc: 0.9562\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9868\n",
      "Epoch 00069: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0382 - acc: 0.9868 - val_loss: 0.2033 - val_acc: 0.9578\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9870\n",
      "Epoch 00070: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0394 - acc: 0.9870 - val_loss: 0.1984 - val_acc: 0.9611\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9869\n",
      "Epoch 00071: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0387 - acc: 0.9869 - val_loss: 0.2067 - val_acc: 0.9539\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9873\n",
      "Epoch 00072: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0383 - acc: 0.9873 - val_loss: 0.2111 - val_acc: 0.9553\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9869\n",
      "Epoch 00073: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0397 - acc: 0.9869 - val_loss: 0.1935 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9869\n",
      "Epoch 00074: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0383 - acc: 0.9869 - val_loss: 0.2054 - val_acc: 0.9613\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9890\n",
      "Epoch 00075: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0329 - acc: 0.9890 - val_loss: 0.2263 - val_acc: 0.9574\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9878\n",
      "Epoch 00076: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0356 - acc: 0.9878 - val_loss: 0.2194 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9896\n",
      "Epoch 00077: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0321 - acc: 0.9896 - val_loss: 0.2285 - val_acc: 0.9520\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9877\n",
      "Epoch 00078: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0362 - acc: 0.9877 - val_loss: 0.2209 - val_acc: 0.9571\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9887\n",
      "Epoch 00079: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0332 - acc: 0.9887 - val_loss: 0.2528 - val_acc: 0.9495\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888\n",
      "Epoch 00080: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0349 - acc: 0.9888 - val_loss: 0.2065 - val_acc: 0.9592\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9895\n",
      "Epoch 00081: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0319 - acc: 0.9895 - val_loss: 0.2349 - val_acc: 0.9555\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9883\n",
      "Epoch 00082: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0353 - acc: 0.9882 - val_loss: 0.2098 - val_acc: 0.9571\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9893\n",
      "Epoch 00083: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0317 - acc: 0.9893 - val_loss: 0.2255 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9896\n",
      "Epoch 00084: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0331 - acc: 0.9896 - val_loss: 0.2351 - val_acc: 0.9557\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9904\n",
      "Epoch 00085: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0302 - acc: 0.9904 - val_loss: 0.2613 - val_acc: 0.9562\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9899\n",
      "Epoch 00086: val_loss did not improve from 0.15886\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0293 - acc: 0.9899 - val_loss: 0.2486 - val_acc: 0.9485\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8nFW9+PHPmX0m+9YlSdu0tHRNm25QBEpZZdGCSykIKopwVUS58ONSERG91yubGwoiKgiILBa5BUEqIKUgVOgKbSl0oUvSJM2ezL6d3x9nZpqmSRraTNJ0vu9Xn1eaebYzT2bO9znro7TWCCGEEACWwU6AEEKIo4cEBSGEECkSFIQQQqRIUBBCCJEiQUEIIUSKBAUhhBApEhSEEEKkSFAQQgiRIkFBCCFEim2wE/BxFRcX64qKisFOhhBCDClr1qxp1FqXHGq7IRcUKioqWL169WAnQwghhhSl1K6+bCfVR0IIIVIkKAghhEiRoCCEECJlyLUpdCcSiVBdXU0wGBzspAxZLpeL8vJy7Hb7YCdFCDGIjomgUF1dTU5ODhUVFSilBjs5Q47WmqamJqqrqxk7duxgJ0cIMYiOieqjYDBIUVGRBITDpJSiqKhISlpCiGMjKAASEI6QXD8hBBxDQeFQYrEAoVAN8XhksJMihBBHrYwJCvF4kHC4Fq37Pyi0trZy3333Hda+559/Pq2trX3e/rbbbuPuu+8+rHMJIcShZExQUMq8Va3j/X7s3oJCNBrtdd8XXniB/Pz8fk+TEEIcjowJCmBN/Iz1+5GXLFnC9u3bqaqq4sYbb2TFihWceuqpLFy4kClTpgBw0UUXMXv2bKZOncoDDzyQ2reiooLGxkZ27tzJ5MmTueqqq5g6dSrnnHMOgUCg1/OuX7+eefPmMX36dD7zmc/Q0tICwD333MOUKVOYPn06l1xyCQCvvfYaVVVVVFVVMXPmTDo6Ovr9Ogghhr5joktqZ1u3XofXu76bNXFiMR8WixulPt7bzs6uYsKEX/S4/vbbb2fjxo2sX2/Ou2LFCtauXcvGjRtTXTwffPBBCgsLCQQCzJ07l8997nMUFRV1SftWHn/8cX73u99x8cUX8/TTT3P55Zf3eN4vfelL/OpXv+K0007j1ltv5Yc//CG/+MUvuP322/noo49wOp2pqqm7776be++9l5NPPhmv14vL5fpY10AIkRkyqKSQpAfkLCeccMIBff7vueceZsyYwbx589izZw9bt249aJ+xY8dSVVUFwOzZs9m5c2ePx29ra6O1tZXTTjsNgC9/+cusXLkSgOnTp3PZZZfxpz/9CZvNBMCTTz6Z66+/nnvuuYfW1tbU60II0dkxlzP0dEcfj0fw+TbgdI7G4RiW9nRkZWWl/r9ixQpefvll3nrrLTweDwsWLOh2TIDT6Uz932q1HrL6qCfPP/88K1eu5LnnnuPHP/4x7733HkuWLOGCCy7ghRde4OSTT2b58uVMmjTpsI4vhDh2ZUxJQSnTpqB1/7cp5OTk9FpH39bWRkFBAR6Phy1btrBq1aojPmdeXh4FBQW8/vrrADz66KOcdtppxONx9uzZw+mnn84dd9xBW1sbXq+X7du3U1lZyU033cTcuXPZsmXLEadBCHHsOeZKCj1LDs7q/95HRUVFnHzyyUybNo3zzjuPCy644ID15557Lvfffz+TJ09m4sSJzJs3r1/O+/DDD/P1r38dv9/PuHHjeOihh4jFYlx++eW0tbWhtebb3/42+fn5fP/73+fVV1/FYrEwdepUzjvvvH5JgxDi2KK0Hpg69v4yZ84c3fUhO++//z6TJ08+5L4dHeuw24twuUanK3lDWl+voxBi6FFKrdFazznUdhlTfQRmrEI6xikIIcSxIqOCghmr0P9tCkIIcazIqKAgJQUhhOhdhgUFKSkIIURvMioogJQUhBCiNxkVFJSypmWcghBCHCvSFhSUUqOUUq8qpTYrpTYppb7TzTZKKXWPUmqbUupdpdSsdKXHnM9COsYpHI7s7OyP9boQQgyEdA5eiwI3aK3XKqVygDVKqZe01ps7bXMeMCGxnAj8JvEzTaSkIIQQvUlbSUFrXau1Xpv4fwfwPlDWZbMLgUe0sQrIV0qNTFeakiWF/h6wt2TJEu69997U78kH4Xi9Xs4880xmzZpFZWUly5Yt6/MxtdbceOONTJs2jcrKSp588kkAamtrmT9/PlVVVUybNo3XX3+dWCzGFVdckdr25z//eb++PyFE5hiQaS6UUhXATODfXVaVAXs6/V6deK32sE923XWwvrups8EeD2PVIbBms3/aiz6oqoJf9Dx19uLFi7nuuuu45pprAHjqqadYvnw5LpeLZ555htzcXBobG5k3bx4LFy7s0/OQ//rXv7J+/Xo2bNhAY2Mjc+fOZf78+fz5z3/mk5/8JN/73veIxWL4/X7Wr19PTU0NGzduBPhYT3ITQojO0h4UlFLZwNPAdVrr9sM8xtXA1QCjRx/BFBVKpWXm7JkzZ7Jv3z727t1LQ0MDBQUFjBo1ikgkws0338zKlSuxWCzU1NRQX1/PiBEjDnnMN954g0svvRSr1crw4cM57bTTeOedd5g7dy5f/epXiUQiXHTRRVRVVTFu3Dh27NjBtddeywUXXMA555zT/29SCJER0hoUlFJ2TEB4TGv91242qQFGdfq9PPHaAbTWDwAPgJn7qNeT9nJHH4s0EgzuJCtrGsrSvw+ZWbRoEUuXLqWuro7FixcD8Nhjj9HQ0MCaNWuw2+1UVFR0O2X2xzF//nxWrlzJ888/zxVXXMH111/Pl770JTZs2MDy5cu5//77eeqpp3jwwQf7420JITJMOnsfKeAPwPta65/1sNmzwJcSvZDmAW1a68OvOjqk5PTZ/d8DafHixTzxxBMsXbqURYsWAWbK7GHDhmG323n11VfZtWtXn4936qmn8uSTTxKLxWhoaGDlypWccMIJ7Nq1i+HDh3PVVVfxta99jbVr19LY2Eg8Hudzn/sc//M//8PatWv7/f0JITJDOksKJwNfBN5TSiUr+W8GRgNore8HXgDOB7YBfuAraUxPWp+pMHXqVDo6OigrK2PkSNNWftlll/HpT3+ayspK5syZ87EeavOZz3yGt956ixkzZqCU4s4772TEiBE8/PDD3HXXXdjtdrKzs3nkkUeoqanhK1/5CvG4CXY/+clP+v39CSEyQ0ZNnR2NegkEtuB2T8Bmy0tXEocsmTpbiGOXTJ3dDdMlNT0lBSGEOBZkWFBIX5uCEEIcCzIqKOx/u1JSEEKI7mRUUJCSghBC9C6jgsL+UcwSFIQQojsZFRTM0AmZFE8IIXqSUUEBks9U6N+SQmtrK/fdd99h7Xv++efLXEVCiKNGBgYFC/3d0NxbUIhGo73u+8ILL5Cfn9+v6RFCiMOVcUEhHY/kXLJkCdu3b6eqqoobb7yRFStWcOqpp7Jw4UKmTJkCwEUXXcTs2bOZOnUqDzzwQGrfiooKGhsb2blzJ5MnT+aqq65i6tSpnHPOOQQCgYPO9dxzz3HiiScyc+ZMzjrrLOrr6wHwer185StfobKykunTp/P0008D8OKLLzJr1ixmzJjBmWee2a/vWwhx7BmQqbMHUi8zZwMQj1cAYPkY4fAQM2dz++23s3HjRtYnTrxixQrWrl3Lxo0bGTt2LAAPPvgghYWFBAIB5s6dy+c+9zmKiooOOM7WrVt5/PHH+d3vfsfFF1/M008/zeWXX37ANqeccgqrVq1CKcXvf/977rzzTn7605/y3//93+Tl5fHee+8B0NLSQkNDA1dddRUrV65k7NixNDc39/1NCyEy0jEXFPpiIKb2OOGEE1IBAeCee+7hmWeeAWDPnj1s3br1oKAwduxYqqqqAJg9ezY7d+486LjV1dUsXryY2tpawuFw6hwvv/wyTzzxRGq7goICnnvuOebPn5/aprCwsF/foxDi2HPMBYXe7ugBAoE6YjEf2dmVaU1HVlZW6v8rVqzg5Zdf5q233sLj8bBgwYJup9B2Op2p/1ut1m6rj6699lquv/56Fi5cyIoVK7jtttvSkn4hRGbKuDaF5CM5+1NOTg4dHR09rm9ra6OgoACPx8OWLVtYtWrVYZ+rra2NsjLzVNOHH3449frZZ599wCNBW1pamDdvHitXruSjjz4CkOojIcQhZVxQSMc4haKiIk4++WSmTZvGjTfeeND6c889l2g0yuTJk1myZAnz5s077HPddtttLFq0iNmzZ1NcXJx6/ZZbbqGlpYVp06YxY8YMXn31VUpKSnjggQf47Gc/y4wZM1IP/xFCiJ5k1NTZAKFQDeFwLdnZs/v0rORMIlNnC3Hskqmze2RN/JSpLoQQoquMCwr7n6kgQUEIIbrKwKCQvkdyCiHEUJdxQWH/W5aSghBCdJVxQUFKCkII0bOMCwpSUhBCiJ5lXFA4WkoK2dnZg3p+IYToTgYGBel9JIQQPcm4oLB/nEL/lRSWLFlywBQTt912G3fffTder5czzzyTWbNmUVlZybJlyw55rJ6m2O5uCuyepssWQojDdcxNiHfdi9exvq6XubOBWKwDpZxYLI4+HbNqRBW/OLfnmfYWL17MddddxzXXXAPAU089xfLly3G5XDzzzDPk5ubS2NjIvHnzWLhwYa8jqbubYjsej3c7BXZ302ULIcSROOaCQt/13/QeM2fOZN++fezdu5eGhgYKCgoYNWoUkUiEm2++mZUrV2KxWKipqaG+vp4RI0b0eKzupthuaGjodgrs7qbLFkKII3HMBYXe7uiTOjrWYbcX4XKN7rfzLlq0iKVLl1JXV5eaeO6xxx6joaGBNWvWYLfbqaio6HbK7KS+TrEthBDpkoFtCqYHUn/3Plq8eDFPPPEES5cuZdGiRYCZ5nrYsGHY7XZeffVVdu3a1esxeppiu6cpsLubLlsIIY5EhgaF/n+mwtSpU+no6KCsrIyRI0cCcNlll7F69WoqKyt55JFHmDRpUq/H6GmK7Z6mwO5uumwhhDgSGTd1NoDP9z5KWfF4ju/v5A1pMnW2EMcumTq7F0pZZJyCEEJ0IyODghmrIHMfCSFEV8dMUPg41WBSUjjYUKtGFEKkxzERFFwuF01NTX3O2Mz8R1JSSNJa09TUhMvlGuykCCEG2TExTqG8vJzq6moaGhr6tH0k0kIs1oHL9X6aUzZ0uFwuysvLBzsZQohBdkwEBbvdnhrt2xcffXQbu3b9kKqqaGrWVCGEEMdI9dHHZbPlABCL+Qc5JUIIcXRJW1BQSj2olNqnlNrYw/oFSqk2pdT6xHJrutLSldVqnmUQi3UM1CmFEGJISGf10R+BXwOP9LLN61rrT6UxDd3aHxS8A31qIYQ4qqWtpKC1Xgk0p+v4R0KCghBCdG+w2xROUkptUEr9XSk1taeNlFJXK6VWK6VW97WHUW+s1mSbggQFIYTobDCDwlpgjNZ6BvAr4P962lBr/YDWeo7Wek5JSckRn1jaFIQQonuDFhS01u1aa2/i/y8AdqVU8UCcW6qPhBCie4MWFJRSI1TiuZRKqRMSaWkaiHNLUBBCiO6lrfeRUupxYAFQrJSqBn4A2AG01vcDnwe+oZSKAgHgEj1AE/BIm4IQQnQvbUFBa33pIdb/GtNldcBZrVkARKPSpiCEEJ0Ndu+jQWGxOFDKISUFIYToIiODAph2BQkKQghxoMwKCl4vxMyU2VZrjgQFIYToInOCwuOPQ04O7NgBJEsK0qYghBCdZU5QGDHC/KyuBiQoCCFEdzInKCQfIJMICnZ7EZHIgAyLEEKIISNzgkJZmflZUwOA01lOKFQ9iAkSQoijT+YEBY8HCgpSJQWns4xIZB/xeGiQEyaEEEePzAkKYEoLnUoKAKHQ3sFMkRBCHFUyKyiUl3cqKSSDQs1gpkgIIY4qmRUUui0pSLuCEEIkZVZQKC+HujqIRCQoCCFENzIvKGgNdXXYbLlYrdkSFIQQopPMCgrJbqmd2hUkKAghxH6ZFRS6DGBzOssJh6WhWQghkjIrKMgANiGE6FVmBYWCAnC7u1Qf1RKPRwc5YUIIcXTIrKCg1AHdUh2OMiBGJFI/uOkSQoijRGYFBehhAJtUIQkhBGRiUJABbEII0aPMCwrl5SYoxOMy1YUQQnSRmUEhHIbGRuz2IpRySklBCCES+hQUlFLfUUrlKuMPSqm1Sqlz0p24tOjULVUphdNZJkFBCCES+lpS+KrWuh04BygAvgjcnrZUpVM3A9gkKAghhNHXoKASP88HHtVab+r02tAiA9iEEKJHfQ0Ka5RS/8AEheVKqRwgnr5kpdHw4WC1dikp1KC1HuSECSHE4LP1cbsrgSpgh9bar5QqBL6SvmSlkdUKI0ceUFLQOkwk0ojDUTLIiRNCiMHV15LCScAHWutWpdTlwC1AW/qSlWYHDGAz1UlShSSEEH0PCr8B/EqpGcANwHbgkbSlKt3KymRUsxBCdKOvQSGqTaX7hcCvtdb3AjnpS1aaJQewIUFBCCE662ubQodS6ruYrqinKqUsgD19yUqz8nLo6ID2dhw5wwGrjGoWQgj6XlJYDIQw4xXqgHLgrrSlKt0OGMBmxekslZKCEELQx6CQCASPAXlKqU8BQa310G1TkAFsQgjRrb5Oc3Ex8DawCLgY+LdS6vPpTFhaHTSATaa6EEII6HubwveAuVrrfQBKqRLgZWBpuhKWVqWl5menkkJT09/RWqPU0ByoLYQQ/aGvbQqWZEBIaPoY+x59XC4oLj6gB1I87iMaHbpDL4QQoj/0NWN/USm1XCl1hVLqCuB54IXedlBKPaiU2qeU2tjDeqWUukcptU0p9a5SatbHS/oRKi+HPXuA/d1Sw2HpgSSEyGx9bWi+EXgAmJ5YHtBa33SI3f4InNvL+vOACYnlaswAuYEzatRBQSEY3DOgSRBCiKNNX9sU0Fo/DTz9MbZfqZSq6GWTC4FHEoPiViml8pVSI7XWtX09xxGpqIDXXgPA5aoAIBjcMSCnFkKIo1WvQUEp1QF0N32oArTWOvcIzl0GdL41r068NnBBob0dWltx5JVitWbj928ZkFMLIY4+WkMoBA4HWDrVocTj4POB3w9uN2RlmXk1O4vHzZI8TjwO0ShEIuan1Wr2dTpBqf3nCoXM+uT+nfeLRCAWM/sml/x8KChI73XoNShorY+KqSyUUldjqpgYPXp0/xx0zBjzc+dOVFUVbvdE/P4P+ufYQvSB1iYDAJNRWCwHZkadt+vogEDAbJ/MNAIBk1H5/eb35DHAZGLt7WYJh01GllwCAWhpMYvPtz+jS2Z2yYwsEjGZVjBofkYiJtNKZnrJn8n/J9NvsZhz+v3mXMHg/veY7NyX/Km1WR8Mmm1jMdMPJLlYrQdu6/OZxes16emclqRkpts5o1XKHCuZvs7HDATMdero2H8ch8OcPxYz5+vK4zHbhMNmSf4d+8JuN2k/HDfdBLen+fFmfa4+SoMaYFSn38sTrx1Ea/0Apk2DOXPm9M+DDyoqzM+dO6GqCo9nIm1t/+qXQ4vBobXJLFpaUrOYpDKPZMahlMn8cnLMz1AIGhuhoQGam/ffEQYCZt9kZtHRATabySjc7v2ZZyRiMoWODmhrg9ZWs292trmry8sz50xmZMlMLZmZd+VymX3y8kym09xs0hcOp+eaWa0m4zsUh8O8/86ZaucgAAdmxHa7yTjdbvOekus7/0xKXtOCApOeUGh/4IrFzPZd/3YjRuy/o++ayXd+b8l1ybTFYvvv6JM8HnPM3FyTjmTADQTMe87ONovHY15Lfh7CYXPn73CYpXMAs1jMvnb7/mucDLDhsNne6TSLzbZ/3877dd43uUyb9vH+vodjMIPCs8C3lFJPACcCbQPWngD7g8KuXQB4PJPYt+/PxGJ+rFbPgCXjWBOL7b9D7egwGaHWENNRQnEf3kCExkZoajKZXWuLldYmG63NdsIBB8VFFoqLTY/haNRkii0tJrNNFreTRe5kZhGNmgy5uRmisRi4WsEeAFsAbCFoGwWhvIMT6/CC3Qcxh1m0BbL2Qc5e7EV7cbgiZKth5FmHk2svIRoK4vO3EdRtROMx3NGRuKOluCzZZGXHGDlhH+WF1eBuJBCwEPDaaeqwEVNBrKWN2LOaKHC1MdpWRKGtjBJXGVm2HML4CGkvIe3FFwzR4Q/jDYSJRhUz3CMozS1ldMFIXFkRfKoOL7X4VQPKHsJiD2OxhXHY7OTYCsmxFZJlzSfbY8HlieHyxLDaNOGgIhBQhIIWLPYIDk8QhzuEskbQcSuxiJ1oyIYFG3a7FafditvuYFThCEYXjMBu219fEtdxWoOttARaaAm20BJooSPcQTgWTi0Adosdu9WO3WLHY/eQ5cjCY/fgtDqxWqxYlRWLshCOhQnFQgSjQRSKAncB+a588l35WJWVmI4Ri8eIxCP4wj78ET++iI+WQAsN/gYafA10hDsYXzieqhFVHFdwHFaL9aA/t9aauI7THmpnW/M2tjVvY3vLdhxWB6U5pYzMHsmwrGE4bU7sFjs2i40CdwHZjuyDjtMSbCEWj1GSVXLQui2NW/iw6UNKskoYkT2CkdkjsVqsB6S9LdhGa7CVtlAbVmVlRPYIRmSPYHj2cCzKQiweI6ZjOK1OnDZnP3wz+y5tQUEp9TiwAChWSlUDPyAxiZ7W+n5Ml9bzgW2An4F+aE9hobnt2LkTAI9nIgCBwFays2cMaFL6m9aaBn8D25u3U9NRgy/swxdJfCA7fTAjsQjDsoZRmlNGsbOMSNDO3rYG6tobaPA10eb30eb34w0FCEUiB9xpqagHSyQPFc4jFsii3RujwxvFG4iAuxHydkP+LsipAWcHWLspLyugMLEAaAvWYAnKO5LoruEopbB52rGUt8PYAFYc2LQbG26s2okFOxZsKCzgrMdtr8Gn6ohz8K3vmKyJTMmfywjXGLa1bWZr+wbqwj13LIgkFh9Qf4jrnWXPIhgNEtN9uOXuquvzC12JpatAYhkEVmWlNKcUj91DU6CJ5kAzcX30PnjRY/cwInsEgUgAf8RPMBokEo8cdpoLXAWMzhvNiOwR1Pvq2dGyg/ZQOwBlOWXMGjmLacOmsa15G6/teo19vn2HOGLfWZWVKSVTmDVyFrNGzuL0itOpHF7Zb8fvTtqCgtb60kOs18A16Tr/ISllSgupoDAJAL9/y6AGhWg8SmuwlSZ/E3va97CnbU/qZ3VHNXva9lDrrcWiLDisDhxWB3aLHaUUFmVBa01New3eiLfHc1jiTlQ0Cx2zEXc2gqWbL0vcCuFsiLgh6oaYHVDmn9Ioux/tbkPndRy0q5t8iu1jGO4cy3DPqWRZc3FYPLgsWTjtdnJzFNk54PFolIoTiUeIxCL4I37qvHXU+eqo66jDoizkOHPIdQ7HbXcTioYIRAMEIgHCsRCRuJdoPEosHmNY1jDKcqdQllNGiacEj92D2+7GYXXwYdOHvF3zNu/sfZn6mnrGF47nlPGzmD7sCoo8Ram72+RxSnNKKc0pxWaxsc+3j32+fTT4G3DZXOQ588hz5WFRFuq8dezt2EttRy0eu4ey3DLKcsoYljWMuI4TjUeJxCO4bC6KPcUUe4rJdebS6G+kpr2G6vZqfBEfOY4csh3ZZDmycNlcqb9rLB5LnWNvx14cVkfqjrIkqwS3zZ3aNhwL0xxopjnQTEuwBTAZitViRaHQaLTWaDR2ix2nzYnL5sJusRONR1NpTV7PmI4Rioao9damPoPBaJBiTzFF7iKKPEUUugvJd+VT4Cog15mL0+ZMfR6Tn+VIPEI4FiYQCeCL+PCFfYRiodQ54jqOw+rAZXPhtDpTpZDWYCstwRbiOp56H11LHAWuAkqySijxlJDlyGJL4xY21G1gQ/0GmgJNuG1u3Da3eZ9We+o4WfYsxheOZ3zheMYVjCMaj1LrrWVvx172+fYRiUVSaW/yN7G7bTe723dT21FLaU4pp4w6hbEFYwFYV7eOtbVreX7r85TllPHJ4z7JaWNOo3J4Jc2BZuq8ddR21BLXcbIcWWTZTdrzXHmpz1I0HqXeW0+dt456Xz1a61RJqiXYwrq6dby47UUe3vAw3z3lu0M3KAwJY8akqo/c7gmA6rfG5iZ/E7e+eivr6tZRmlNKeW45pTmluG3u1B/cH/GbYmyLKcru8+3DG+4+My9xD2OYaxSFtgmM8szH71e0t4XwBsL4ghFCYU0orAmHNeGmT0LLcdAyDtpGQzgbp8oi2+WhKNdDcaGV4mJTWMrOjaJy6olnV+N0RxmeXcLwnBKG5eRTWKgoLDR1vbm5po6z6ywgsXiMQDSAVVlTX7yjdaoQrTWReASH1dHnfaYytd/TkQw6c8vmHnLbicUT+3zckTkjjyRZQ17ybvpw5DhzOL7o+MM+dyQWwWaxpfWzX9sxMLXrmR0UKirgrbcAsFrdOJ2jj7hbalzHeWjdQ9z08k20Bls5efTJbGrYxPLty7vN8POceUwomsDc0rmUuEei/QUEWwro2FdI865y9r4/ih0bymjwOmnosq/FAsOHm6W4GIqKzDK6EsaNg+OOM2P0ko2W3bNhegKXHdb7tVqsB9W5Hq2UUh8rIAjRV3Zr+h8vM1BBX4JCS4tpFc3NxeOZ1OeSQiQW4bH3HuPOf93JPt++VLG+OdDMurp1nDL6FO47/74DinodoY5U0bmxOcaOrU4+WFfImn8qVq+G7dsP7D0xahRMmQLnXm0KNZ0z/tJSGDbM9FQQQoj+ktlZSnKswq5dUFmJxzOR2to3ep0tNRgN8uiGR/nJGz/ho9aPqBpRxaIpi6j3mTrBuI7zxwv/yJdmfAmlFFrDjh3w+uvw5ps5bN6cwwcfmJ43SaNGwZw58IUvwMSJcPzxZsk9kqGBQghxGDI7KHQeq1BZiccziXjcRyhUg8tVntpMa82a2jU8tO4h/rzxz7QGW5lbOpd7zruHCyZccEAACQRg3Tr4+c9h1Sp44w2oTVQFFhRAZSV85jMm8588GWbPNtU/QghxNJCgAJ3GKphGPb9/SyooLN+2nCWvLGF93XpcNhefnfxZrpx5JadXnH5AMHjnHfjVr+DJJ/cPNKqogAUL4NRTzTJlSvcjVoUQ4miR2UGhpMQMYezSLTWGACreAAAgAElEQVQQ+IBN+0by/176f7y47UWOKziO+y+4n8XTFpPvyk/trjU8/TT89KemVJCdDVdeCeecA/PmmVGXQggxlGR2UFDKtCskgoLDMRKrNZs/bFjKD1d/m1xnLj8956dcM/eag0YVrl8P3/62aSuYMAF++Uu44gppBxBCDG2ZHRTA1PEkqo+UUvgs47h97eucMfYMnvjcExR5ig7YvKUFvvtd+N3vTD//3/7WlA66zpoohBBDkQSFMWNg9erUr/dv9RKNx/jtp357UEBYsQK++EXTcHzttfCDH6R/GlshhBhIEhQqKkz/UJ+Pd1o3s2zXDi4dBWNy93cJikTg1lvhjjtMVdG//216DQkhxLFG+sIkeiDpnTu5bvl1lLjzuGw0+P1bATPT5/z5Zg7zK6+EtWslIAghjl0SFBID2J5c+whv7nmTH5z6n2TZTLfUeNxUF73zDjzxhGlHyMoa5PQKIUQaSVCoqCBgg//a+TtmjpjJVXNuABSBwAfceissWwY/+xksXjzYCRVCiPSTNoXhw3l8po098RYeOvsvOOzZuFxjePrpfH78Y1NldO21g51IIYQYGBkfFLRS3DvPytSghzPGngHArl0X8L3vXc0pp8B99x08XbQQQhyrMr766J2977C2KMQ3txempq24++4byMlp5qmn/L1MOS2EEMeejA8K971zH9lxO5e/5QPgzTdh9eqxXHLJHTidrw9y6oQQYmBldFBo8jfxxMYn+KJ1JrnVDRAIcNddUFioueCCR2lpeXmwkyiEEAMqo4PCH9f/kVAsxDfKPwPAlhV1LFsG3/qWYsSImTQ3vzTIKRRCiIGVsUEhruP8ZvVvOGX0KVQefwoAd//ShtMJ3/oWFBScjc+3gXC4fpBTKoQQAydjg8JL219ie8t2vjnnmzBxInsZyaMvj+SrXzUzahcUnA1AS8s/BzmlQggxcDI2KNy/5n6GZQ3js5M/CyUl/DL7FqJxCzfcYNbn5MzEZiugpUWqkIQQmSMjg4Iv7OPFbS+yeOpinDYn7e1wf/DLLMpZzrhxZhulrBQUnElLy0torQc3wUIIMUAyMii8tOMlgtEgF068EIDnn4f2aBbXen9iHrKcUFBwNqFQNX7/B4OVVCGEGFAZGRSWfbCMfFc+88fMN78vg2F5QU6KvwHvvZfarqDgLADpmiqEyBgZFxRi8Rh/+/BvnD/hfOxWO+Ew/P3v8OlPRrCgYd261LZu9zhcrnHSriCEyBgZFxTeqn6LRn8jC49fCJinqbW3w4WXZZvHqHUKCmCqkFpbXyUejwxCaoUQYmBlXFBYtmUZdoud8yacB8Czz4LbDWedraCqqpugcBaxWAcdHe8MRnKFEGJAZVRQ0Fqz7INlnD72dHKduWhtgsI555jAQFUVvPsuRKOpfQoKzgAsNDYuG7R0CyHEQMmooLClcQtbm7emeh2tWwd79sCFFyY2mDkTgkH4YH9vI7u9kOLii6it/T2xmH8QUi2EEAMno4LCsx88C8Cnj/80YHodWSzwqU8lNpg50/zsUoVUXn4d0Wgz9fWPDVRShRBiUGRUUFj2wTJmjZzFqLxR5vdl8IlPmGktAJg0CVyug4JCXt4pZGfPpLr6lzKQTQhxTMuYoFDvrWdV9apU1dGuXbBhAyxc2Gkjmw0qKw8KCkopysuvw+/fREvLKwOYaiGEGFgZExSe3/o8Gp0KCs+amqT97QlJM2eaoNClRDBs2GLs9uFUV/9iAFIrhBCDI2OCwhenf5GVV6xk+vDpgBmwNnEiHH98lw1nzoTWVlOU6MRicVJa+nWam5/H7986QKkWQoiBldagoJQ6Vyn1gVJqm1JqSTfrr1BKNSil1ieWr6UrLXarnVPHnJp6DvPmzTBnTjcb9tDYDFBa+nWUclBT86t0JVMIIQZV2oKCUsoK3AucB0wBLlVKTelm0ye11lWJ5ffpSk9noRDs3g3jx3ezsrLSdElav/6gVU7nCIYNu4S6uoeIRJrSn1AhhBhg6SwpnABs01rv0FqHgSeArjX4g2LHDtNkMGFCNys9HlOv1E1JAWD06JuIxfzs2vW/6U2kEEIMgnQGhTJgT6ffqxOvdfU5pdS7SqmlSqlRaUxPyrZt5me3JQWAWbPgrbfA5ztoVVbWFEaMuIKaml8TCOxMWxqFEGIwDHZD83NAhdZ6OvAS8HB3GymlrlZKrVZKrW5oaDjik25NtBP3GBS+8Q1obITbb+92dUXFD1HKws6dtx5xWoQQ4miSzqBQA3S+8y9PvJaitW7SWocSv/4emN3dgbTWD2it52it55SkRpodvm3bzISoRUU9bHDyyXDZZXDXXaauqQuXq5yysu9QX/8nvN4NR5weIYQ4WqQzKLwDTFBKjVVKOYBLgGc7b6CUGtnp14XA+2lMT8q2bb2UEpLuuMMMZrv++m5Xjx59EzZbPtu339T/CRRCiEGStqCgtY4C3wKWYzL7p7TWm5RSP1JKJccRf1sptUkptQH4NnBFutLT2datfQgKZWXw/e+buTCWLz9otd1ewJgx36OlZbmMchZCHDPUUJvLZ86cOXr16tWHvX8oZDoY3XIL/PCHfdh42jSwWs2U2g7HAatjsSBvvz0Ji8XB7NlrsNlyDjtdQgiRTkqpNVrr7kZnHWCwG5oH3M6dEI/3oaQA4HTCL35hptL++c8PWm21upg8+WECge1s3XpNv6dVCCEGWsYFhUP2POrqggvgM5+B226DDz88aHV+/mlUVNxKff2j1NV123lKCCGGjIwLCskxCt0OXOvJvfeaKbWvvNIUM7oYM+YW8vMX8OGH38Tn29I/CRVCiEGQkUEhL6+X7qjdGTkSfvYzeOMNuP/+g1YrZWXy5MewWj1s3ryYWOzgQW9CCDEUZFxQSPY8SsyL13dXXAFnnw033XTQDKoATmcpkyY9gs+3kfXrTycc3tcv6RVCiIGUcUFh27aPWXWUpBQ88ICZNOk//uOg5y0AFBWdx7Rpz+DzbWTt2k/g92878gQLIcQAyqigEA6b3kd9bmTuqqLCTH2xfDncd1+3mxQXL2TGjFeIRltZt+4TtLe/c7jJFUKIAZdRQeFjdUftyTXXwHnnwQ03mLEL3cjLO4lZs97Eas1m/foFNDX9/QhOKIQQAyejgsJh9TzqSin44x8hPx8uvRT8/m4383iOZ+bMN/F4JvLee5+W7qpCiCEhI4PCEZUUAIYNg0cfNY9vu+GGHjdzOkdQVbWC/PwFbNlyBbt23c5QG0EuhMgstsFOwEDauhVycqAfJlo1PZFuvNHMpNrRAdEotLSY+qk//tHMnQTYbLlMn/4CW7ZcwUcffZeGhqWUll7NsGGXyrQYQoijTsaVFCZMOIzuqD35n/8xI57/8Q/zpLa2NlixAn7ykwM2s1gcTJ78JyZM+A1ah/jww//grbdK2br1WiKRln5KjBBCHLmMmhBvwgTzULUnn+znRHV29dXw8MPmOQxlBz9oTmtNe/sq9u79LfX1f8JuL2bChHsoKVmE6rdoJYQQB5IJ8bqIROCjj46wkbkvvvtdiMVMtVI3lFLk5Z3E5Ml/ZPbsd3A6y9m8eTEbNy4kGNzT7T5CCDFQMiYo7Npl8uojbmQ+lLFj4Utfgt/+Furqet00J2cms2at4rjj7qal5RXefnsye/b8jHg8muZECiFE9zImKPRbz6O+uPlmM1Lupz898PVuquosFhujRt3A3LmbyM8/je3bb2Dt2rm0t789AAkVQogDZUxQyM6GhQvh+OMH4GTjx8MXvmBGPTc0wPr18NWvmqf7zJgBt94Ka9YcECTc7rFUVv6NqVOXEg7vY+3aE9m06WJ8vk0DkGAhxBHp6Oj2pu+wtbfDN78JVVVmvrW33+7f4/dGaz2kltmzZ+sh4f33tVZK69JSrUFrj0frL39Z61NP1dpiMa+NH6/10qVax+MH7BqJtOsdO76vV67M0a++qvSmTZfo9vbVOt5lOyHEUeChh7R2OrW+6qqDvsuHZflyrUeNMvnESSdpbbOZ/KK8XOvf/OawDwus1n3IYzOmpDDgJk0yE+d5PHD33VBdbcYvrFxp2hoefNA8o+Hzn4fTTzddWhNsthzGjv0R8+Z9xOjRS2hsfI41a+awatUYPvzwWzQ3/4NIpHXw3psQ6RAKwd69g52KvotE4Dvfga98BUaMgN/9znzXO9Ma/vxneO213o/V3g5//7uZjfmTn4SsLPjXv+DNN6G+3vRonDXLPA0yzTKqS+pRJxqF3/8evv99aGqC6dNhzBgz8d7o0VBaCiNHEilx05jzLo1tz9PS8g/i8QAATucosrKmk5s7j+HDL8XtPm5w348Qh+vDD0397gcfwAknwBe/CIsX99NI0z6IxeDxx+Hpp01nkWnToLISvF6TOf/rX2aus4oKmDnTLI89Bq++Cv/5n3DHHSbNTz0FS5fCZz8LjY2m2vi558w5rrjCBI3kw1x274Y//AGef97cFMbjYLfD9debJz26XP36FvvaJVWCwtGgtdU8A3rtWjNr386d5sPY2ejR8L//S+ziC2ltfwOfbwNe77t4vRvw+zfh+QjGPTeMwpfaYcECLPf+1uwjMtfu3fDKK+DzweWXm/m6OovFTJvXiBG9H6e+3mRcra3wiU+YO1aHw6yLx00puKPDlI6t1oP3b2w0gzpfecX8zMuD664zpWSbzdwhX3qpyRC/8Q149lnYsMEcq3Oac3PNo3Evu8xkyr2N6wmHTamjuhr27YNTTjHT03Sltcm0v/c92LgRystNeoPBA7ebOtWcc+dO00bo9Zq79gceML0NAQIBOOMMk/Y774T//V9zs3fHHeY633knFBSYmRBWrDDvG+DUU2HBApg/H046ydQupIEEhaFMa/MFrK01y5498KtfmaAxd675sOXmmtf37CH23NNY//kGcaei6QRN4TuAshL83ldx33gPFkf/3nGITqqrzWNaS0vhv/4LJk/+ePtHo+bvumKFuRtVCgoLzZKXZ35PLuPHm8yju8wtadMm08Fh+XLYvn3/69nZpjrzuutMBvWnP5k749pa05h52WUmYx4+3Azo+eADk7k9/zysWnVgI6fbDXPmmCqPDz80mSGYz+QnPmEy4FDI3Flv2GAy0mQa5s83XQE//NDcdZ91lrlbnjED/u//TEkZ4L334C9/gebm/efdvRtefNFU20yaZKpdx441xykuNvv8+9+mUXb79gPT7HSazPs//9Psu2mTCT5Ll5q79AkT4L//GxYtMvvt2GGChNNpMuqCgv3HisfN8bOyzN+9s/p6OPFE0wd+8mRzjWfMMOvefRe+9jV45x2z35VXmiX5ntNMgsKxJh43X+Sbb4aamgPXlZebngpXXYXXVUPj6nvI/e6fKFwVxjfWQtu5o4icNRfb3Pnk5M0jO3smFkunaa927TJflro602vq0kv7/rzSmhpzp1Vba74Q+/aZTOvMM82Xtrj48N5vS4v50iXvSDv76CPzZe36hTyUSMR8mY87ztyVHqlVq8yda0eH+fsEg3DRRSY4nHhi93eyWpsM8R//MMtrr5n9ASZONO+rudncYSYz264qK01wmDHD3MFOmbL/7vRvfzOZ9llnmb/BGWeYEsHdd8MTT5h0am3e//nnm5uMZ581GalS5s49Etl/rtmzTbXOwoWmRPHGG/D66ybzLSw0GezEieacb75p1m3eDBaL6eo3Y4YJOgsWmGPZ7SYNzz5rBni++SZcfLFpY8vKOvQ1b242Gfnjj5s79tYubWtlZab6qbLSlJTLy82EZ488Yurlg0HzeN3aWrP93Llw1VWmaqc/PhNgJllbtsx8J7ve9cdi5vpMnmyu9QCSoHCs8vvNFyo723zgR40yX84uGVA8FsL70C3Yf/UwrvcaUBrCBdB0EjQtcKPPWEBe4WkUPbELz48fBhTquOPM3Yzdbp4ZUV5uvtwWiynGn3CCyeyKi00m9NOfmi9nNGrOX1xsAsLu3SajU8pUNVx9talvdbu7f0/hsLkDXL7cfGE2bzbBxe02d2nz55uM5403zDbbtpk0nX++ufs97zzzZdu0yXT1tdlMYOvcKLdjh3nt7bfNF3XePHNHW1lpMpLycnOXHAqZqgGv1wTJbdtMINmzx9yVzp1rluXLTWZSVmb+HsOGmdLcr39tAtqYMfsz0+xsE0BWrTKZ4J7EyPXjjjOZ9+mnm0xz+PADr0s0ajJwrc37e+89+Oc/TTXMv/51cNAoLoZrrzXP/OguqO/aZTLf0lJzR1xYuH/d1q1m/hevd39GP3Higdv0VUuLqQ/v6e/d2Z495tof7hQvra3mfe3bZ4JjN1PLpDQ0wG9+Y0oAZ58Nn/qUCRAZQoKC2K+hAf3ii8SfW4r6+0tYvAGiWRZCRXGydkPTibDjxgJs46aRv7uAwucayf7HViz+KGiFisdNJh+LmeOVlZkSQlaWKQ5/85swbtz+O59o1BSRX3kFnnnGVI+UlJgM67OfNXeK4bD5Qj/zjLmDbWoyd3TTppkv96RJpmrmtddMANLaZOannw7nnGNKJX/4g/lZXGwmI+x8h1tRAT/6kSn5/OUvJnhYLHDLLSZovfGGudOMxw99/axWc5dcW3vg9qefbo7dOQPu6DCNjc8+Cy+9dGDGPXq0CapnnmkypXHjDvtPSixmSkybNpmluNi0G6SpPloMfRIURPdCIXj5ZXj6aeLr1xD4+gW0nDccn38jfv8HBALbCYf3dwu0WFxkZVWSa51O4UfDyd0cxfbuTtTMmSaj7VzX2h2tTTfcu+4y9dNdOZ1w4YWmvvecc7ovwre2mjv2ysoD7/4jEZP5/vWv5m5z9mxTMtm+3VSzrV27P4CddJIp1XSuv/V6TcZaXW2W+npzh5uTY+7uS0rM3fzo0SZdXq+pf377bRNgvvWt3qsc/H4TGKNREww+bnWXEP1IgoI4bLGYn0BgO17vBrzedXi96+joWE0sZuq+7fZheDyTcLnG4HSOxuOZQGHhBTgch2g/eP99c3fucJjF6TRVUl17xfSHeNzUPd91l+n3/YMf9F+dsRBDkAQF0a+0juHzbaa9/U3a21cRCGwnGNxFKFQDxFDKRkHBWQwbdgn5+afjcJQe2JgthBhUEhTEgIjHo/h879HQ8BT79j1BMLgzscaC01mGw1GK1hHicT+xmA+brYCCgrMpLPwkeXmnYrVKd1khBoIEBTHgtNZ0dLyN1/suodBugsFdhMO1KOXAas3Cas0iGNxNW9sbaB3GYnGTnT2LnJyZZGfPwukcRSi0h2BwJ8HgLuz2QrKzZ5KTMwu3e6KUPIQ4An0NCvItE/1GKUVu7onk5p7Y63axmI/W1tdoaXmJjo7V1NY+RDz+605bWHA6S4lEmlJTeijlxOUajdM5GpdrNHZ7EUrZUcqBxeLC45lEdnYVLtcYeYKdEEdAgoIYcFZrFkVF51NUdD5g2isCgW2EQnsTjdflWCwO4vEogcAHdHSsw+d7l2BwN6HQbpqblxONtqJ1GK2jXY6dh8czAas1F6s1G6s1B7u9GKdzJA7HSKzWHEKhGkKhPYRC1djtJRQUnEl+/mnYbLmDcTmEOKpI9ZEY0rSOE4v58fs34/Wux+tdTyCwg1jMm1g6CIfricd9B+ynlAOns4xwuJZ4PAhYycmZidM5Gru9BIejBIdjBC5XBU7nGFyu0YAFrUPE4yYYKWUBLChlwW4vRqlu5v0R4igh1UciIyhlwWbLJjf3BHJzT+hxu2i0g3C4llisA6ezHLu9BKUsxGJB2tvfoqXlFdrb38Tv30Ik8jqRSBPQh4FtCWY8xzSysqbjdo8jHG4gHK4hFKpB6whWay42Wy5Wax4u1xjc7nG4XMfhdJZisThRyoFSNoLBj/D53sPn20gk0kJe3ikUFJyJyzWqH66WEIcmJQUhuqF1jHB4H8HgLoLBnYRC1QBYLA4sFidgBTQQR+sogcAOfD4za20k0oDVmo3DUYbTWYbF4iQabScWaycabUl04+39e6eUE6s1i2jUTAjndh9PVlYlDsdwHI7h2O0lWK1ZWCwerFY3StnROk4ykDmdZbjd47Fas1LvJxjcTTC4A7u9BI9nMhaLjNvIJFJSEOIIKGXF6RyJ0zmSvLx5fd5Pa008HsRq7Xnen3g8TDC4k0BgB+FwHVqHicdDaB3G6RxNVlYlbvd4lLLg822kpeUVWlpewe/fRGvrq6lA0RcOxwis1hyCwZ1ovX8aEKUcZGVNw+0eTyTSSDhcm6hKi6R6ilmtWdhshdjtRdjtxdhsBVitOdhsOVit2YkndUVTxzX7ZCeWXOz2Amy2fKzWbGKxAPG4j1jMh9YxlLImFhs2WxF2e2GiOm7/NYpGW4jHg2gdS1TX2RPtTRLM0klKCkIMMfF4ONEzy5/IbP2JjNmKUha0jhEK7SEQ2EYgsJ1YrAO3+zjc7vG4XGMJh+sSo9TXEQrtSrShmIZ4i8VBLOZLHNtLJNJMJNJENNpEJNKC1qE0vSsrdnsxVqubSKQpNXq+K6VsuFwVuN0TsFqziceDqYBq3r8tETSsqcCTbAsy1yqA1lEsFhcWixur1Y3DMTJ1fRyOUmKxDqLRFiKRFpSy4nAMw24fjt1eQCTSlOioUEM8HkwETLNoHUu0Y5nAZ7ebgGqzFaF1hEhkH+FwA9FoK3Z7UarUZ7G4iccDxGIBtA4lAm8hVmtWv/akOypKCkqpc4FfYsrav9da395lvRN4BJgNNAGLtdY705kmIYY6i8WB03mo2T1P6nXt8OFfOKxzx+MRYrEOYjEvoFDKjsViT5SQ/KnG/Wi0nWi0lWi0lVisA4vFlajuykpsH0ssESKRJsLheiKRfcTjgUTJwZQeLBY3StlQyko8HiIQ2EEgsJVAYCvxeBCLxYnF4kpUn0USSzSxxIAYWscT27lTSzweJBptJR7309y8vMcgNJiUcmCz5XVqc7JTWno1o0Zdn9bzpi0oKBOi7wXOBqqBd5RSz2qtN3fa7EqgRWs9Xil1CXAHsDhdaRJCHBmLxY7FUojdfhhTah+ltNZEIg2JySBrO1V9FaB1LHGHX08k0ozdXoTTmWwrcidKUo1Eo02JUkpW4g7fmihtNBKJNKGUPVHiGIbNlksk0pw4Zn0iuCUDljMRUJuIRJqJRtsSgS5MPB7G4Rh+6Dd0hNJZUjgB2Ka13gGglHoCuBDoHBQuBG5L/H8p8GullNJDrU5LCDFkKaVwOIbhcPT0RLsJPe5rguP4tKRrsFgOvclhKwP2dPq9OvFat9toMwqpDejjI7+EEEL0t3QGhX6jlLpaKbVaKbW6oaFhsJMjhBDHrHQGhRqg84ib8sRr3W6jlLIBeZgG5wNorR/QWs/RWs8pKSlJU3KFEEKkMyi8A0xQSo1VSjmAS4Bnu2zzLPDlxP8/D/xT2hOEEGLwpK2hWWsdVUp9C1iO6ZL6oNZ6k1LqR8BqrfWzwB+AR5VS24BmTOAQQggxSNI6TkFr/QLwQpfXbu30/yCwKJ1pEEII0XdDoqFZCCHEwJCgIIQQImXIzX2klGoAdh3m7sVAYz8m51gj16d3cn16Jtemd0fD9RmjtT5k980hFxSOhFJqdV8mhMpUcn16J9enZ3JtejeUro9UHwkhhEiRoCCEECIl04LCA4OdgKOcXJ/eyfXpmVyb3g2Z65NRbQpCCCF6l2klBSGEEL3ImKCglDpXKfWBUmqbUmrJYKdnMCmlRimlXlVKbVZKbVJKfSfxeqFS6iWl1NbEz4LBTutgUkpZlVLrlFJ/S/w+Vin178Rn6MnEnF4ZSSmVr5RaqpTaopR6Xyl1knx+DKXUfya+VxuVUo8rpVxD6bOTEUGh01PgzgOmAJcqpaYMbqoGVRS4QWs9BZgHXJO4HkuAV7TWE4BXEr9nsu8A73f6/Q7g51rr8UAL5smBmeqXwIta60nADMx1yvjPj1KqDPg2MEdrPQ0z71vyqZJD4rOTEUGBTk+B0+YJ38mnwGUkrXWt1npt4v8dmC90GeaaPJzY7GHgosFJ4eBTSpUDFwC/T/yugDMwTwiEDL4+Sqk8YD5mQku01mGtdSvy+UmyAe7E4wA8QC1D6LOTKUGhL0+By0hKqQpgJvBvYLjWujaxqg5I/wNhj16/AP4LiCd+LwJaE08IhMz+DI0FGoCHEtVrv1dKZSGfH7TWNcDdwG5MMGgD1jCEPjuZEhREN5RS2cDTwHVa6/bO6xLPtcjIrmlKqU8B+7TWawY7LUcpGzAL+I3Weibgo0tVUaZ+fhLtKBdiAmcpkAWcO6iJ+pgyJSj05SlwGUUpZccEhMe01n9NvFyvlBqZWD8S2DdY6RtkJwMLlVI7MVWNZ2Dq0PMTVQKQ2Z+haqBaa/3vxO9LMUFCPj9wFvCR1rpBax0B/or5PA2Zz06mBIW+PAUuYyTqx/8AvK+1/lmnVZ2fhPdlYNlAp+1ooLX+rta6XGtdgfms/FNrfRnwKuYJgZDZ16cO2KOUmph46UxgM/L5AVNtNE8p5Ul8z5LXZsh8djJm8JpS6nxMPXHyKXA/HuQkDRql1CnA68B77K8zvxnTrvAUMBozE+3FWuvmQUnkUUIptQD4f1rrTymlxmFKDoXAOuByrXVoMNM3WJRSVZhGeAewA/gK5iYz4z8/SqkfAosxvfzWAV/DtCEMic9OxgQFIYQQh5Yp1UdCCCH6QIKCEEKIFAkKQgghUiQoCCGESJGgIIQQIkWCghADSCm1IDnrqhBHIwkKQgghUiQoCNENpdTlSqm3lVLrlVK/TTxbwauU+nlirvxXlFIliW2rlFKrlFLvKqWeST5HQCk1Xin1slJqg1JqrVLquMThszs9i+CxxMhXIY4KEhSE6EIpNRkzIvVkrXUVEAMuw0xutlprPRV4DfhBYpdHgJu01tMxo8STrz8G3Ku1ngF8AjNrJphZaa/DPNtjHGZuHCGOCrZDbyJExjkTmA28k7iJd2Mmd4sDTya2+RPw18SzBXrMN50AAAEGSURBVPK11q8lXn8Y+ItSKgco01o/A6C1DgIkjve21ro68ft6oAJ4I/1vS4hDk6AgxMEU8LDW+rsHvKjU97tsd7hzxHSe8yaGfA/FUUSqj4Q42CvA55VSwyD17OoxmO9LcqbLLwBvaK3bgBal1KmJ178IvJZ4ol21UuqixDGcSinPgL4LIQ6D3KEI0YXWerNS6hbgH0opCxABrsE8TOaExLp9mHYHMFMh35/I9JMzhoIJEL9VSv0ocYxFA/g2hDgsMkuqEH2klPJqrbMHOx1CpJNUHwkhhEiRkoIQQogUKSkIIYRIkaAghBAiRYKCEEKIFAkKQgghUiQoCCGESJGgIIQQIuX/A20p++MgDsr5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2010 - acc: 0.9437\n",
      "Loss: 0.20095838392511212 Accuracy: 0.94371754\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0342 - acc: 0.3336\n",
      "Epoch 00001: val_loss improved from inf to 1.20668, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/001-1.2067.hdf5\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 2.0341 - acc: 0.3336 - val_loss: 1.2067 - val_acc: 0.6203\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1837 - acc: 0.6151\n",
      "Epoch 00002: val_loss improved from 1.20668 to 0.71368, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/002-0.7137.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 1.1836 - acc: 0.6152 - val_loss: 0.7137 - val_acc: 0.7808\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7213\n",
      "Epoch 00003: val_loss improved from 0.71368 to 0.52442, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/003-0.5244.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.8587 - acc: 0.7213 - val_loss: 0.5244 - val_acc: 0.8481\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.7852\n",
      "Epoch 00004: val_loss improved from 0.52442 to 0.36189, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/004-0.3619.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.6607 - acc: 0.7852 - val_loss: 0.3619 - val_acc: 0.8975\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8292\n",
      "Epoch 00005: val_loss improved from 0.36189 to 0.31414, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/005-0.3141.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.5397 - acc: 0.8292 - val_loss: 0.3141 - val_acc: 0.9052\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8585\n",
      "Epoch 00006: val_loss improved from 0.31414 to 0.25369, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/006-0.2537.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.4479 - acc: 0.8585 - val_loss: 0.2537 - val_acc: 0.9259\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8775\n",
      "Epoch 00007: val_loss improved from 0.25369 to 0.22242, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/007-0.2224.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3861 - acc: 0.8775 - val_loss: 0.2224 - val_acc: 0.9322\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8884\n",
      "Epoch 00008: val_loss improved from 0.22242 to 0.20877, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/008-0.2088.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.3517 - acc: 0.8884 - val_loss: 0.2088 - val_acc: 0.9383\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9004\n",
      "Epoch 00009: val_loss improved from 0.20877 to 0.18178, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/009-0.1818.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.3146 - acc: 0.9004 - val_loss: 0.1818 - val_acc: 0.9471\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9103\n",
      "Epoch 00010: val_loss improved from 0.18178 to 0.16725, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/010-0.1673.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.2828 - acc: 0.9103 - val_loss: 0.1673 - val_acc: 0.9471\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9176\n",
      "Epoch 00011: val_loss did not improve from 0.16725\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2635 - acc: 0.9176 - val_loss: 0.1854 - val_acc: 0.9453\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9219\n",
      "Epoch 00012: val_loss improved from 0.16725 to 0.16700, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/012-0.1670.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.2449 - acc: 0.9219 - val_loss: 0.1670 - val_acc: 0.9509\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9288\n",
      "Epoch 00013: val_loss improved from 0.16700 to 0.16438, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/013-0.1644.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.2241 - acc: 0.9288 - val_loss: 0.1644 - val_acc: 0.9522\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9330\n",
      "Epoch 00014: val_loss improved from 0.16438 to 0.14723, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/014-0.1472.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.2103 - acc: 0.9331 - val_loss: 0.1472 - val_acc: 0.9555\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9370\n",
      "Epoch 00015: val_loss did not improve from 0.14723\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.2000 - acc: 0.9370 - val_loss: 0.1520 - val_acc: 0.9562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9408\n",
      "Epoch 00016: val_loss improved from 0.14723 to 0.14376, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/016-0.1438.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1872 - acc: 0.9409 - val_loss: 0.1438 - val_acc: 0.9578\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9432\n",
      "Epoch 00017: val_loss improved from 0.14376 to 0.13661, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/017-0.1366.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1762 - acc: 0.9432 - val_loss: 0.1366 - val_acc: 0.9590\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9471\n",
      "Epoch 00018: val_loss did not improve from 0.13661\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1625 - acc: 0.9472 - val_loss: 0.1530 - val_acc: 0.9564\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9496\n",
      "Epoch 00019: val_loss improved from 0.13661 to 0.12620, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/019-0.1262.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1604 - acc: 0.9496 - val_loss: 0.1262 - val_acc: 0.9620\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9513\n",
      "Epoch 00020: val_loss improved from 0.12620 to 0.12204, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/020-0.1220.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1513 - acc: 0.9513 - val_loss: 0.1220 - val_acc: 0.9637\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9533\n",
      "Epoch 00021: val_loss improved from 0.12204 to 0.12048, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/021-0.1205.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1421 - acc: 0.9533 - val_loss: 0.1205 - val_acc: 0.9644\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9559\n",
      "Epoch 00022: val_loss did not improve from 0.12048\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1363 - acc: 0.9559 - val_loss: 0.1269 - val_acc: 0.9625\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9582\n",
      "Epoch 00023: val_loss did not improve from 0.12048\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1269 - acc: 0.9582 - val_loss: 0.1322 - val_acc: 0.9585\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9599\n",
      "Epoch 00024: val_loss improved from 0.12048 to 0.11109, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/024-0.1111.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1217 - acc: 0.9599 - val_loss: 0.1111 - val_acc: 0.9658\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9610\n",
      "Epoch 00025: val_loss did not improve from 0.11109\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1179 - acc: 0.9610 - val_loss: 0.1143 - val_acc: 0.9658\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9627\n",
      "Epoch 00026: val_loss did not improve from 0.11109\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1131 - acc: 0.9627 - val_loss: 0.1309 - val_acc: 0.9646\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9662\n",
      "Epoch 00027: val_loss did not improve from 0.11109\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1019 - acc: 0.9662 - val_loss: 0.1298 - val_acc: 0.9613\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9652\n",
      "Epoch 00028: val_loss improved from 0.11109 to 0.11013, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/028-0.1101.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.1033 - acc: 0.9652 - val_loss: 0.1101 - val_acc: 0.9667\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9675\n",
      "Epoch 00029: val_loss did not improve from 0.11013\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0991 - acc: 0.9675 - val_loss: 0.1157 - val_acc: 0.9683\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9698\n",
      "Epoch 00030: val_loss did not improve from 0.11013\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0924 - acc: 0.9698 - val_loss: 0.1274 - val_acc: 0.9653\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9688\n",
      "Epoch 00031: val_loss did not improve from 0.11013\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0940 - acc: 0.9688 - val_loss: 0.1139 - val_acc: 0.9655\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9708\n",
      "Epoch 00032: val_loss did not improve from 0.11013\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0871 - acc: 0.9708 - val_loss: 0.1213 - val_acc: 0.9693\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9719\n",
      "Epoch 00033: val_loss did not improve from 0.11013\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0851 - acc: 0.9719 - val_loss: 0.1114 - val_acc: 0.9655\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9725\n",
      "Epoch 00034: val_loss improved from 0.11013 to 0.11009, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/034-0.1101.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0829 - acc: 0.9725 - val_loss: 0.1101 - val_acc: 0.9686\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9728\n",
      "Epoch 00035: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0790 - acc: 0.9728 - val_loss: 0.1114 - val_acc: 0.9679\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9742\n",
      "Epoch 00036: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0747 - acc: 0.9742 - val_loss: 0.1106 - val_acc: 0.9658\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9773\n",
      "Epoch 00037: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0682 - acc: 0.9773 - val_loss: 0.1130 - val_acc: 0.9704\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9761\n",
      "Epoch 00038: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0701 - acc: 0.9761 - val_loss: 0.1136 - val_acc: 0.9700\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9770\n",
      "Epoch 00039: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0688 - acc: 0.9770 - val_loss: 0.1271 - val_acc: 0.9655\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9776\n",
      "Epoch 00040: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0655 - acc: 0.9776 - val_loss: 0.1142 - val_acc: 0.9702\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9772\n",
      "Epoch 00041: val_loss did not improve from 0.11009\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0649 - acc: 0.9772 - val_loss: 0.1179 - val_acc: 0.9697\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9787\n",
      "Epoch 00042: val_loss improved from 0.11009 to 0.10762, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/042-0.1076.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0641 - acc: 0.9787 - val_loss: 0.1076 - val_acc: 0.9713\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9811\n",
      "Epoch 00043: val_loss did not improve from 0.10762\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0556 - acc: 0.9811 - val_loss: 0.1175 - val_acc: 0.9709\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9796\n",
      "Epoch 00044: val_loss improved from 0.10762 to 0.10559, saving model to model/checkpoint/1D_CNN_custom_2_DO_8_conv_checkpoint/044-0.1056.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0579 - acc: 0.9796 - val_loss: 0.1056 - val_acc: 0.9693\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9808\n",
      "Epoch 00045: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0551 - acc: 0.9808 - val_loss: 0.1560 - val_acc: 0.9623\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9806\n",
      "Epoch 00046: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0571 - acc: 0.9806 - val_loss: 0.1488 - val_acc: 0.9665\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9822\n",
      "Epoch 00047: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0517 - acc: 0.9822 - val_loss: 0.1263 - val_acc: 0.9706\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9821\n",
      "Epoch 00048: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0528 - acc: 0.9821 - val_loss: 0.1261 - val_acc: 0.9681\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9839\n",
      "Epoch 00049: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0488 - acc: 0.9839 - val_loss: 0.1524 - val_acc: 0.9704\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9831\n",
      "Epoch 00050: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0503 - acc: 0.9831 - val_loss: 0.1423 - val_acc: 0.9655\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9845\n",
      "Epoch 00051: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0474 - acc: 0.9845 - val_loss: 0.1277 - val_acc: 0.9679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9834\n",
      "Epoch 00052: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0489 - acc: 0.9834 - val_loss: 0.1362 - val_acc: 0.9695\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9842\n",
      "Epoch 00053: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0455 - acc: 0.9842 - val_loss: 0.1323 - val_acc: 0.9683\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9844\n",
      "Epoch 00054: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0465 - acc: 0.9844 - val_loss: 0.1246 - val_acc: 0.9655\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9853\n",
      "Epoch 00055: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0433 - acc: 0.9853 - val_loss: 0.1196 - val_acc: 0.9700\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9854\n",
      "Epoch 00056: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0419 - acc: 0.9854 - val_loss: 0.1317 - val_acc: 0.9681\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9851\n",
      "Epoch 00057: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0447 - acc: 0.9851 - val_loss: 0.1482 - val_acc: 0.9674\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9870\n",
      "Epoch 00058: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0406 - acc: 0.9870 - val_loss: 0.1429 - val_acc: 0.9674\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9869\n",
      "Epoch 00059: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0394 - acc: 0.9869 - val_loss: 0.1209 - val_acc: 0.9730\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9882\n",
      "Epoch 00060: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0348 - acc: 0.9882 - val_loss: 0.1279 - val_acc: 0.9727\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9865\n",
      "Epoch 00061: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0399 - acc: 0.9866 - val_loss: 0.1180 - val_acc: 0.9706\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9877\n",
      "Epoch 00062: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0369 - acc: 0.9877 - val_loss: 0.1186 - val_acc: 0.9734\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9877\n",
      "Epoch 00063: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0372 - acc: 0.9877 - val_loss: 0.1330 - val_acc: 0.9725\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9873\n",
      "Epoch 00064: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0378 - acc: 0.9873 - val_loss: 0.1288 - val_acc: 0.9716\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9881\n",
      "Epoch 00065: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0356 - acc: 0.9881 - val_loss: 0.1174 - val_acc: 0.9709\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887\n",
      "Epoch 00066: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0340 - acc: 0.9887 - val_loss: 0.1473 - val_acc: 0.9697\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9860\n",
      "Epoch 00067: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0431 - acc: 0.9860 - val_loss: 0.1174 - val_acc: 0.9725\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0310 - acc: 0.9895 - val_loss: 0.1257 - val_acc: 0.9700\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9899\n",
      "Epoch 00069: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0299 - acc: 0.9899 - val_loss: 0.1339 - val_acc: 0.9695\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9901\n",
      "Epoch 00070: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0293 - acc: 0.9901 - val_loss: 0.1358 - val_acc: 0.9711\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00071: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0285 - acc: 0.9905 - val_loss: 0.1312 - val_acc: 0.9732\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9899\n",
      "Epoch 00072: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0307 - acc: 0.9899 - val_loss: 0.1435 - val_acc: 0.9693\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9889\n",
      "Epoch 00073: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0339 - acc: 0.9889 - val_loss: 0.1408 - val_acc: 0.9716\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9897\n",
      "Epoch 00074: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0309 - acc: 0.9897 - val_loss: 0.1363 - val_acc: 0.9725\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9904\n",
      "Epoch 00075: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0283 - acc: 0.9904 - val_loss: 0.1605 - val_acc: 0.9667\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9908\n",
      "Epoch 00076: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0288 - acc: 0.9908 - val_loss: 0.1505 - val_acc: 0.9681\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 00077: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.1288 - val_acc: 0.9690\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9910\n",
      "Epoch 00078: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0284 - acc: 0.9910 - val_loss: 0.1233 - val_acc: 0.9695\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9912\n",
      "Epoch 00079: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0261 - acc: 0.9913 - val_loss: 0.1455 - val_acc: 0.9693\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9908\n",
      "Epoch 00080: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0270 - acc: 0.9908 - val_loss: 0.1502 - val_acc: 0.9693\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9925\n",
      "Epoch 00081: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0229 - acc: 0.9925 - val_loss: 0.1427 - val_acc: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9920\n",
      "Epoch 00082: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0236 - acc: 0.9920 - val_loss: 0.1502 - val_acc: 0.9695\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9910\n",
      "Epoch 00083: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0273 - acc: 0.9910 - val_loss: 0.1463 - val_acc: 0.9720\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9924\n",
      "Epoch 00084: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0238 - acc: 0.9924 - val_loss: 0.1272 - val_acc: 0.9718\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9930\n",
      "Epoch 00085: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0212 - acc: 0.9930 - val_loss: 0.1331 - val_acc: 0.9732\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9918\n",
      "Epoch 00086: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0233 - acc: 0.9918 - val_loss: 0.1424 - val_acc: 0.9704\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9921\n",
      "Epoch 00087: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0231 - acc: 0.9921 - val_loss: 0.1657 - val_acc: 0.9709\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9919\n",
      "Epoch 00088: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0243 - acc: 0.9919 - val_loss: 0.1429 - val_acc: 0.9730\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9927\n",
      "Epoch 00089: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0228 - acc: 0.9927 - val_loss: 0.1417 - val_acc: 0.9732\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9925\n",
      "Epoch 00090: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0237 - acc: 0.9925 - val_loss: 0.1561 - val_acc: 0.9704\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9932\n",
      "Epoch 00091: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0190 - acc: 0.9932 - val_loss: 0.1445 - val_acc: 0.9709\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9920\n",
      "Epoch 00092: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0246 - acc: 0.9920 - val_loss: 0.1438 - val_acc: 0.9700\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9929\n",
      "Epoch 00093: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0221 - acc: 0.9929 - val_loss: 0.1564 - val_acc: 0.9704\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9933\n",
      "Epoch 00094: val_loss did not improve from 0.10559\n",
      "36805/36805 [==============================] - 284s 8ms/sample - loss: 0.0199 - acc: 0.9933 - val_loss: 0.1508 - val_acc: 0.9690\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1738 - acc: 0.9574\n",
      "Loss: 0.1738346540741429 Accuracy: 0.9574247\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0626 - acc: 0.3325\n",
      "Epoch 00001: val_loss improved from inf to 1.12985, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/001-1.1299.hdf5\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 2.0626 - acc: 0.3325 - val_loss: 1.1299 - val_acc: 0.6695\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2171 - acc: 0.6158\n",
      "Epoch 00002: val_loss improved from 1.12985 to 0.77843, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/002-0.7784.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 1.2173 - acc: 0.6158 - val_loss: 0.7784 - val_acc: 0.7680\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9432 - acc: 0.7011\n",
      "Epoch 00003: val_loss improved from 0.77843 to 0.56037, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/003-0.5604.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.9432 - acc: 0.7011 - val_loss: 0.5604 - val_acc: 0.8167\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7531 - acc: 0.7633\n",
      "Epoch 00004: val_loss improved from 0.56037 to 0.48601, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/004-0.4860.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.7532 - acc: 0.7633 - val_loss: 0.4860 - val_acc: 0.8558\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6168 - acc: 0.8075\n",
      "Epoch 00005: val_loss improved from 0.48601 to 0.32178, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/005-0.3218.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.6167 - acc: 0.8075 - val_loss: 0.3218 - val_acc: 0.9078\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.8433\n",
      "Epoch 00006: val_loss improved from 0.32178 to 0.29385, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/006-0.2938.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.5090 - acc: 0.8433 - val_loss: 0.2938 - val_acc: 0.9122\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8637\n",
      "Epoch 00007: val_loss improved from 0.29385 to 0.22962, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/007-0.2296.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.4429 - acc: 0.8637 - val_loss: 0.2296 - val_acc: 0.9324\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8836\n",
      "Epoch 00008: val_loss improved from 0.22962 to 0.22055, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/008-0.2206.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.3809 - acc: 0.8836 - val_loss: 0.2206 - val_acc: 0.9359\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8954\n",
      "Epoch 00009: val_loss improved from 0.22055 to 0.18172, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/009-0.1817.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.3401 - acc: 0.8954 - val_loss: 0.1817 - val_acc: 0.9457\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9035\n",
      "Epoch 00010: val_loss did not improve from 0.18172\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.3114 - acc: 0.9035 - val_loss: 0.2072 - val_acc: 0.9357\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9085\n",
      "Epoch 00011: val_loss improved from 0.18172 to 0.17418, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/011-0.1742.hdf5\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.2945 - acc: 0.9085 - val_loss: 0.1742 - val_acc: 0.9515\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9175\n",
      "Epoch 00012: val_loss improved from 0.17418 to 0.15707, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/012-0.1571.hdf5\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.2679 - acc: 0.9175 - val_loss: 0.1571 - val_acc: 0.9499\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9232\n",
      "Epoch 00013: val_loss improved from 0.15707 to 0.15479, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/013-0.1548.hdf5\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.2491 - acc: 0.9232 - val_loss: 0.1548 - val_acc: 0.9525\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9292\n",
      "Epoch 00014: val_loss improved from 0.15479 to 0.13883, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/014-0.1388.hdf5\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.2309 - acc: 0.9292 - val_loss: 0.1388 - val_acc: 0.9562\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9319\n",
      "Epoch 00015: val_loss did not improve from 0.13883\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.2189 - acc: 0.9319 - val_loss: 0.1419 - val_acc: 0.9567\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9364\n",
      "Epoch 00016: val_loss did not improve from 0.13883\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.1994 - acc: 0.9364 - val_loss: 0.1502 - val_acc: 0.9562\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9399\n",
      "Epoch 00017: val_loss did not improve from 0.13883\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.1950 - acc: 0.9399 - val_loss: 0.1399 - val_acc: 0.9604\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9443\n",
      "Epoch 00018: val_loss improved from 0.13883 to 0.13721, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/018-0.1372.hdf5\n",
      "36805/36805 [==============================] - 295s 8ms/sample - loss: 0.1761 - acc: 0.9444 - val_loss: 0.1372 - val_acc: 0.9583\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9453\n",
      "Epoch 00019: val_loss improved from 0.13721 to 0.13376, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/019-0.1338.hdf5\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.1712 - acc: 0.9453 - val_loss: 0.1338 - val_acc: 0.9604\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9495\n",
      "Epoch 00020: val_loss improved from 0.13376 to 0.10899, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/020-0.1090.hdf5\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.1602 - acc: 0.9494 - val_loss: 0.1090 - val_acc: 0.9669\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9506\n",
      "Epoch 00021: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.1557 - acc: 0.9506 - val_loss: 0.1359 - val_acc: 0.9609\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9541\n",
      "Epoch 00022: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.1430 - acc: 0.9541 - val_loss: 0.1124 - val_acc: 0.9674\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9560\n",
      "Epoch 00023: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.1376 - acc: 0.9560 - val_loss: 0.1143 - val_acc: 0.9653\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9592\n",
      "Epoch 00024: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.1273 - acc: 0.9592 - val_loss: 0.1341 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9609\n",
      "Epoch 00025: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.1215 - acc: 0.9609 - val_loss: 0.1353 - val_acc: 0.9637\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9605\n",
      "Epoch 00026: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.1208 - acc: 0.9605 - val_loss: 0.1171 - val_acc: 0.9686\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9632\n",
      "Epoch 00027: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.1143 - acc: 0.9632 - val_loss: 0.1255 - val_acc: 0.9623\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9661\n",
      "Epoch 00028: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.1068 - acc: 0.9661 - val_loss: 0.1266 - val_acc: 0.9634\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9642\n",
      "Epoch 00029: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.1101 - acc: 0.9642 - val_loss: 0.1223 - val_acc: 0.9644\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9669\n",
      "Epoch 00030: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.1020 - acc: 0.9669 - val_loss: 0.1117 - val_acc: 0.9662\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9674\n",
      "Epoch 00031: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.1002 - acc: 0.9674 - val_loss: 0.1295 - val_acc: 0.9651\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9708\n",
      "Epoch 00032: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0891 - acc: 0.9708 - val_loss: 0.1684 - val_acc: 0.9581\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9705\n",
      "Epoch 00033: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 294s 8ms/sample - loss: 0.0901 - acc: 0.9705 - val_loss: 0.1108 - val_acc: 0.9693\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9717\n",
      "Epoch 00034: val_loss improved from 0.10899 to 0.10826, saving model to model/checkpoint/1D_CNN_custom_2_DO_9_conv_checkpoint/034-0.1083.hdf5\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0866 - acc: 0.9717 - val_loss: 0.1083 - val_acc: 0.9679\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9731\n",
      "Epoch 00035: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0822 - acc: 0.9731 - val_loss: 0.1155 - val_acc: 0.9679\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9718\n",
      "Epoch 00036: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0858 - acc: 0.9719 - val_loss: 0.1244 - val_acc: 0.9690\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9762\n",
      "Epoch 00037: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0743 - acc: 0.9763 - val_loss: 0.1416 - val_acc: 0.9669\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9755\n",
      "Epoch 00038: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0744 - acc: 0.9755 - val_loss: 0.1176 - val_acc: 0.9660\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9764\n",
      "Epoch 00039: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0731 - acc: 0.9764 - val_loss: 0.1455 - val_acc: 0.9623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9779\n",
      "Epoch 00040: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0676 - acc: 0.9779 - val_loss: 0.1390 - val_acc: 0.9681\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9774\n",
      "Epoch 00041: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0676 - acc: 0.9774 - val_loss: 0.1313 - val_acc: 0.9672\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9784\n",
      "Epoch 00042: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0641 - acc: 0.9784 - val_loss: 0.1211 - val_acc: 0.9686\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9786\n",
      "Epoch 00043: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0634 - acc: 0.9786 - val_loss: 0.1274 - val_acc: 0.9686\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9800\n",
      "Epoch 00044: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0614 - acc: 0.9800 - val_loss: 0.1187 - val_acc: 0.9681\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9808\n",
      "Epoch 00045: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0585 - acc: 0.9808 - val_loss: 0.1290 - val_acc: 0.9672\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9809\n",
      "Epoch 00046: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0590 - acc: 0.9809 - val_loss: 0.1176 - val_acc: 0.9690\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9805\n",
      "Epoch 00047: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0615 - acc: 0.9805 - val_loss: 0.1623 - val_acc: 0.9686\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9811\n",
      "Epoch 00048: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 0.0610 - acc: 0.9811 - val_loss: 0.1292 - val_acc: 0.9686\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9843\n",
      "Epoch 00049: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0493 - acc: 0.9843 - val_loss: 0.1394 - val_acc: 0.9690\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9829\n",
      "Epoch 00050: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0502 - acc: 0.9829 - val_loss: 0.1380 - val_acc: 0.9681\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9836\n",
      "Epoch 00051: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 292s 8ms/sample - loss: 0.0482 - acc: 0.9836 - val_loss: 0.1555 - val_acc: 0.9688\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9840\n",
      "Epoch 00052: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0468 - acc: 0.9839 - val_loss: 0.1380 - val_acc: 0.9683\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9831\n",
      "Epoch 00053: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0513 - acc: 0.9831 - val_loss: 0.1130 - val_acc: 0.9695\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9860\n",
      "Epoch 00054: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0424 - acc: 0.9860 - val_loss: 0.1483 - val_acc: 0.9665\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9834\n",
      "Epoch 00055: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0512 - acc: 0.9834 - val_loss: 0.1220 - val_acc: 0.9709\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9859\n",
      "Epoch 00056: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0428 - acc: 0.9859 - val_loss: 0.1348 - val_acc: 0.9697\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9850\n",
      "Epoch 00057: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0444 - acc: 0.9850 - val_loss: 0.1358 - val_acc: 0.9695\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9848\n",
      "Epoch 00058: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0468 - acc: 0.9847 - val_loss: 0.1328 - val_acc: 0.9700\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9852\n",
      "Epoch 00059: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0443 - acc: 0.9852 - val_loss: 0.1634 - val_acc: 0.9660\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9875\n",
      "Epoch 00060: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0370 - acc: 0.9875 - val_loss: 0.1415 - val_acc: 0.9653\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9871\n",
      "Epoch 00061: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0385 - acc: 0.9871 - val_loss: 0.1288 - val_acc: 0.9725\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9864\n",
      "Epoch 00062: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0403 - acc: 0.9864 - val_loss: 0.1582 - val_acc: 0.9653\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 00063: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0389 - acc: 0.9870 - val_loss: 0.1499 - val_acc: 0.9660\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9881\n",
      "Epoch 00064: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0361 - acc: 0.9881 - val_loss: 0.1353 - val_acc: 0.9706\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9869\n",
      "Epoch 00065: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0386 - acc: 0.9869 - val_loss: 0.1743 - val_acc: 0.9662\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 00066: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 0.1446 - val_acc: 0.9676\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9878\n",
      "Epoch 00067: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.1517 - val_acc: 0.9697\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9880\n",
      "Epoch 00068: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0386 - acc: 0.9880 - val_loss: 0.1417 - val_acc: 0.9676\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9901\n",
      "Epoch 00069: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0290 - acc: 0.9901 - val_loss: 0.1531 - val_acc: 0.9674\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9896\n",
      "Epoch 00070: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0315 - acc: 0.9896 - val_loss: 0.1417 - val_acc: 0.9727\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9900\n",
      "Epoch 00071: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0316 - acc: 0.9900 - val_loss: 0.1399 - val_acc: 0.9702\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 00072: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0312 - acc: 0.9896 - val_loss: 0.1258 - val_acc: 0.9706\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9893\n",
      "Epoch 00073: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0322 - acc: 0.9893 - val_loss: 0.1596 - val_acc: 0.9700\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9908\n",
      "Epoch 00074: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0287 - acc: 0.9908 - val_loss: 0.1642 - val_acc: 0.9716\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9898\n",
      "Epoch 00075: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0309 - acc: 0.9898 - val_loss: 0.1673 - val_acc: 0.9693\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9887\n",
      "Epoch 00076: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0362 - acc: 0.9887 - val_loss: 0.1569 - val_acc: 0.9695\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9907\n",
      "Epoch 00077: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0281 - acc: 0.9907 - val_loss: 0.1418 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9919\n",
      "Epoch 00078: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0233 - acc: 0.9919 - val_loss: 0.1550 - val_acc: 0.9695\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00079: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0286 - acc: 0.9905 - val_loss: 0.1740 - val_acc: 0.9688\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 00080: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.1755 - val_acc: 0.9730\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 00081: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0256 - acc: 0.9917 - val_loss: 0.1634 - val_acc: 0.9727\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9916\n",
      "Epoch 00082: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 0.0261 - acc: 0.9916 - val_loss: 0.1704 - val_acc: 0.9641\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9908\n",
      "Epoch 00083: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0299 - acc: 0.9907 - val_loss: 0.1642 - val_acc: 0.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9914\n",
      "Epoch 00084: val_loss did not improve from 0.10826\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 0.0270 - acc: 0.9914 - val_loss: 0.1460 - val_acc: 0.9709\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1531 - acc: 0.9574\n",
      "Loss: 0.15306521486297545 Accuracy: 0.9574247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.3839 - acc: 0.8906\n",
      "Loss: 0.38391003829048925 Accuracy: 0.8905504\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2010 - acc: 0.9437\n",
      "Loss: 0.20095838392511212 Accuracy: 0.94371754\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1738 - acc: 0.9574\n",
      "Loss: 0.1738346540741429 Accuracy: 0.9574247\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1531 - acc: 0.9574\n",
      "Loss: 0.15306521486297545 Accuracy: 0.9574247\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,364,496\n",
      "Trainable params: 1,364,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.4690 - acc: 0.9107\n",
      "Loss: 0.4690067165458685 Accuracy: 0.91069573\n",
      "\n",
      "1D_CNN_custom_2_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,356,432\n",
      "Trainable params: 1,356,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2523 - acc: 0.9427\n",
      "Loss: 0.25232962208315496 Accuracy: 0.9426791\n",
      "\n",
      "1D_CNN_custom_2_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,409,808\n",
      "Trainable params: 1,409,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2093 - acc: 0.9580\n",
      "Loss: 0.20932015989902733 Accuracy: 0.95804775\n",
      "\n",
      "1D_CNN_custom_2_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,438,544\n",
      "Trainable params: 1,438,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2158 - acc: 0.9612\n",
      "Loss: 0.2158301523139532 Accuracy: 0.96116304\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
