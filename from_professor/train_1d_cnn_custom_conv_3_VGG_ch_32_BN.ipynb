{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 512000)            2048000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 10,243,504\n",
      "Trainable params: 9,219,376\n",
      "Non-trainable params: 1,024,128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 170656)            682624    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 3,423,088\n",
      "Trainable params: 3,081,520\n",
      "Non-trainable params: 341,568\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 56864)             227456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 1,153,712\n",
      "Trainable params: 1,039,600\n",
      "Non-trainable params: 114,112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 18944)             75776     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 401,776\n",
      "Trainable params: 363,376\n",
      "Non-trainable params: 38,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 12608)             50432     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 294,128\n",
      "Trainable params: 268,144\n",
      "Non-trainable params: 25,984\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 150,384\n",
      "Trainable params: 141,040\n",
      "Non-trainable params: 9,344\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 119,280\n",
      "Trainable params: 115,312\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 126,576\n",
      "Trainable params: 124,144\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 197,744\n",
      "Trainable params: 195,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0716 - acc: 0.3782\n",
      "Epoch 00001: val_loss improved from inf to 1.76027, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv_checkpoint/001-1.7603.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 2.0716 - acc: 0.3782 - val_loss: 1.7603 - val_acc: 0.4556\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3587 - acc: 0.5879\n",
      "Epoch 00002: val_loss improved from 1.76027 to 1.41905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv_checkpoint/002-1.4190.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.3587 - acc: 0.5880 - val_loss: 1.4190 - val_acc: 0.5684\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0879 - acc: 0.6683\n",
      "Epoch 00003: val_loss did not improve from 1.41905\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 1.0882 - acc: 0.6683 - val_loss: 1.6784 - val_acc: 0.5227\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9069 - acc: 0.7244\n",
      "Epoch 00004: val_loss improved from 1.41905 to 1.21745, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv_checkpoint/004-1.2175.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.9072 - acc: 0.7243 - val_loss: 1.2175 - val_acc: 0.6401\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7721\n",
      "Epoch 00005: val_loss did not improve from 1.21745\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.7533 - acc: 0.7721 - val_loss: 1.3345 - val_acc: 0.6017\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.8120\n",
      "Epoch 00006: val_loss did not improve from 1.21745\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.6227 - acc: 0.8119 - val_loss: 1.6078 - val_acc: 0.5672\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8525\n",
      "Epoch 00007: val_loss improved from 1.21745 to 1.18420, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv_checkpoint/007-1.1842.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.5090 - acc: 0.8525 - val_loss: 1.1842 - val_acc: 0.6560\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8788\n",
      "Epoch 00008: val_loss improved from 1.18420 to 1.16652, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv_checkpoint/008-1.1665.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.4276 - acc: 0.8788 - val_loss: 1.1665 - val_acc: 0.6627\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.9078\n",
      "Epoch 00009: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.3484 - acc: 0.9078 - val_loss: 1.2322 - val_acc: 0.6580\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9292\n",
      "Epoch 00010: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2892 - acc: 0.9291 - val_loss: 1.2155 - val_acc: 0.6587\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9451\n",
      "Epoch 00011: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2424 - acc: 0.9451 - val_loss: 1.2216 - val_acc: 0.6683\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9554\n",
      "Epoch 00012: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.2074 - acc: 0.9554 - val_loss: 1.4678 - val_acc: 0.6182\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9657\n",
      "Epoch 00013: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1724 - acc: 0.9657 - val_loss: 1.7582 - val_acc: 0.5865\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9741\n",
      "Epoch 00014: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1465 - acc: 0.9741 - val_loss: 1.3426 - val_acc: 0.6671\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9792\n",
      "Epoch 00015: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1249 - acc: 0.9792 - val_loss: 1.2634 - val_acc: 0.6804\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9810\n",
      "Epoch 00016: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1143 - acc: 0.9810 - val_loss: 1.5634 - val_acc: 0.6203\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9842\n",
      "Epoch 00017: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.1025 - acc: 0.9842 - val_loss: 1.3200 - val_acc: 0.6804\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9876\n",
      "Epoch 00018: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0881 - acc: 0.9876 - val_loss: 1.5033 - val_acc: 0.6466\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9886\n",
      "Epoch 00019: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0794 - acc: 0.9886 - val_loss: 1.3986 - val_acc: 0.6674\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9897\n",
      "Epoch 00020: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0740 - acc: 0.9897 - val_loss: 1.3861 - val_acc: 0.6818\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9907\n",
      "Epoch 00021: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0655 - acc: 0.9907 - val_loss: 1.4352 - val_acc: 0.6737\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9905\n",
      "Epoch 00022: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0665 - acc: 0.9904 - val_loss: 1.5684 - val_acc: 0.6548\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9910\n",
      "Epoch 00023: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0653 - acc: 0.9909 - val_loss: 1.7017 - val_acc: 0.6343\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9902\n",
      "Epoch 00024: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0627 - acc: 0.9901 - val_loss: 1.4344 - val_acc: 0.6660\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9935\n",
      "Epoch 00025: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0485 - acc: 0.9934 - val_loss: 1.4745 - val_acc: 0.6795\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9922\n",
      "Epoch 00026: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0523 - acc: 0.9921 - val_loss: 1.4805 - val_acc: 0.6834\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9939\n",
      "Epoch 00027: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0457 - acc: 0.9939 - val_loss: 1.6105 - val_acc: 0.6485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9931\n",
      "Epoch 00028: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0473 - acc: 0.9931 - val_loss: 1.6106 - val_acc: 0.6608\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9952\n",
      "Epoch 00029: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0400 - acc: 0.9952 - val_loss: 1.5744 - val_acc: 0.6734\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9951\n",
      "Epoch 00030: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0389 - acc: 0.9951 - val_loss: 1.6209 - val_acc: 0.6618\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9943\n",
      "Epoch 00031: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0399 - acc: 0.9943 - val_loss: 1.6405 - val_acc: 0.6653\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9950\n",
      "Epoch 00032: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0385 - acc: 0.9950 - val_loss: 1.5478 - val_acc: 0.6778\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9948\n",
      "Epoch 00033: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0365 - acc: 0.9948 - val_loss: 1.6870 - val_acc: 0.6620\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9954\n",
      "Epoch 00034: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0353 - acc: 0.9954 - val_loss: 1.6258 - val_acc: 0.6681\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9927\n",
      "Epoch 00035: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0434 - acc: 0.9927 - val_loss: 1.7391 - val_acc: 0.6543\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9942\n",
      "Epoch 00036: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0384 - acc: 0.9941 - val_loss: 1.7437 - val_acc: 0.6494\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9917\n",
      "Epoch 00037: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0448 - acc: 0.9917 - val_loss: 1.7338 - val_acc: 0.6664\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9976\n",
      "Epoch 00038: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0250 - acc: 0.9975 - val_loss: 1.6307 - val_acc: 0.6799\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9935\n",
      "Epoch 00039: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0388 - acc: 0.9935 - val_loss: 1.6372 - val_acc: 0.6727\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9962\n",
      "Epoch 00040: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0285 - acc: 0.9961 - val_loss: 1.6122 - val_acc: 0.6888\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9962\n",
      "Epoch 00041: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0280 - acc: 0.9962 - val_loss: 1.6686 - val_acc: 0.6758\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9971\n",
      "Epoch 00042: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0249 - acc: 0.9970 - val_loss: 1.6798 - val_acc: 0.6730\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9964\n",
      "Epoch 00043: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0265 - acc: 0.9964 - val_loss: 1.7065 - val_acc: 0.6730\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9955\n",
      "Epoch 00044: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0306 - acc: 0.9955 - val_loss: 1.7052 - val_acc: 0.6809\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9968\n",
      "Epoch 00045: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0247 - acc: 0.9968 - val_loss: 1.8071 - val_acc: 0.6650\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9935\n",
      "Epoch 00046: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0366 - acc: 0.9935 - val_loss: 1.7721 - val_acc: 0.6611\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9974\n",
      "Epoch 00047: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0224 - acc: 0.9974 - val_loss: 1.7391 - val_acc: 0.6711\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9959\n",
      "Epoch 00048: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0276 - acc: 0.9959 - val_loss: 1.7663 - val_acc: 0.6727\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9958\n",
      "Epoch 00049: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0265 - acc: 0.9957 - val_loss: 1.7711 - val_acc: 0.6720\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9970\n",
      "Epoch 00050: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0238 - acc: 0.9970 - val_loss: 1.7998 - val_acc: 0.6725\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9985\n",
      "Epoch 00051: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0184 - acc: 0.9984 - val_loss: 1.7404 - val_acc: 0.6795\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9952\n",
      "Epoch 00052: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0277 - acc: 0.9952 - val_loss: 1.7874 - val_acc: 0.6681\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9972\n",
      "Epoch 00053: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0212 - acc: 0.9971 - val_loss: 1.7801 - val_acc: 0.6769\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9947\n",
      "Epoch 00054: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0295 - acc: 0.9947 - val_loss: 1.7972 - val_acc: 0.6725\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9986\n",
      "Epoch 00055: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0177 - acc: 0.9986 - val_loss: 1.7856 - val_acc: 0.6732\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9980\n",
      "Epoch 00056: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0186 - acc: 0.9980 - val_loss: 1.8367 - val_acc: 0.6699\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9978\n",
      "Epoch 00057: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0197 - acc: 0.9978 - val_loss: 1.8714 - val_acc: 0.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9977\n",
      "Epoch 00058: val_loss did not improve from 1.16652\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 0.0192 - acc: 0.9977 - val_loss: 1.9065 - val_acc: 0.6625\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 846us/sample - loss: 1.2832 - acc: 0.6264\n",
      "Loss: 1.2831656423313225 Accuracy: 0.6263759\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0126 - acc: 0.3914\n",
      "Epoch 00001: val_loss improved from inf to 1.78512, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/001-1.7851.hdf5\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 2.0127 - acc: 0.3914 - val_loss: 1.7851 - val_acc: 0.4337\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2730 - acc: 0.6146\n",
      "Epoch 00002: val_loss improved from 1.78512 to 1.20399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/002-1.2040.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.2731 - acc: 0.6145 - val_loss: 1.2040 - val_acc: 0.6357\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0313 - acc: 0.6900\n",
      "Epoch 00003: val_loss improved from 1.20399 to 1.06361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/003-1.0636.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0314 - acc: 0.6900 - val_loss: 1.0636 - val_acc: 0.6837\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8767 - acc: 0.7358\n",
      "Epoch 00004: val_loss did not improve from 1.06361\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8767 - acc: 0.7358 - val_loss: 1.0668 - val_acc: 0.6799\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7630 - acc: 0.7715\n",
      "Epoch 00005: val_loss improved from 1.06361 to 0.94760, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/005-0.9476.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7630 - acc: 0.7716 - val_loss: 0.9476 - val_acc: 0.7193\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.8030\n",
      "Epoch 00006: val_loss improved from 0.94760 to 0.93215, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/006-0.9321.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6640 - acc: 0.8030 - val_loss: 0.9321 - val_acc: 0.7275\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8223\n",
      "Epoch 00007: val_loss did not improve from 0.93215\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5899 - acc: 0.8223 - val_loss: 0.9563 - val_acc: 0.7202\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8451\n",
      "Epoch 00008: val_loss did not improve from 0.93215\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5198 - acc: 0.8450 - val_loss: 0.9436 - val_acc: 0.7365\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.8677\n",
      "Epoch 00009: val_loss improved from 0.93215 to 0.92454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv_checkpoint/009-0.9245.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4557 - acc: 0.8677 - val_loss: 0.9245 - val_acc: 0.7435\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8849\n",
      "Epoch 00010: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4026 - acc: 0.8848 - val_loss: 1.0766 - val_acc: 0.7081\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.9015\n",
      "Epoch 00011: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3497 - acc: 0.9014 - val_loss: 0.9866 - val_acc: 0.7270\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9167\n",
      "Epoch 00012: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3070 - acc: 0.9167 - val_loss: 0.9892 - val_acc: 0.7251\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9292\n",
      "Epoch 00013: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2631 - acc: 0.9291 - val_loss: 1.0170 - val_acc: 0.7263\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9400\n",
      "Epoch 00014: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2359 - acc: 0.9400 - val_loss: 1.0565 - val_acc: 0.7198\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9488\n",
      "Epoch 00015: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2080 - acc: 0.9487 - val_loss: 0.9878 - val_acc: 0.7417\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9576\n",
      "Epoch 00016: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1841 - acc: 0.9576 - val_loss: 1.0050 - val_acc: 0.7438\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9644\n",
      "Epoch 00017: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1594 - acc: 0.9644 - val_loss: 1.1632 - val_acc: 0.7121\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9693\n",
      "Epoch 00018: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1454 - acc: 0.9693 - val_loss: 1.0599 - val_acc: 0.7303\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9718\n",
      "Epoch 00019: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1338 - acc: 0.9717 - val_loss: 1.1177 - val_acc: 0.7272\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9736\n",
      "Epoch 00020: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1253 - acc: 0.9736 - val_loss: 1.1263 - val_acc: 0.7254\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9794\n",
      "Epoch 00021: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1087 - acc: 0.9793 - val_loss: 1.0948 - val_acc: 0.7384\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9795\n",
      "Epoch 00022: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1050 - acc: 0.9795 - val_loss: 1.1122 - val_acc: 0.7389\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9846\n",
      "Epoch 00023: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0891 - acc: 0.9845 - val_loss: 1.1685 - val_acc: 0.7272\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9841\n",
      "Epoch 00024: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0871 - acc: 0.9841 - val_loss: 1.1449 - val_acc: 0.7328\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9846\n",
      "Epoch 00025: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0852 - acc: 0.9846 - val_loss: 1.2143 - val_acc: 0.7286\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9848\n",
      "Epoch 00026: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0800 - acc: 0.9848 - val_loss: 1.2107 - val_acc: 0.7263\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9885\n",
      "Epoch 00027: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0681 - acc: 0.9885 - val_loss: 1.2009 - val_acc: 0.7298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9851\n",
      "Epoch 00028: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0772 - acc: 0.9849 - val_loss: 1.1831 - val_acc: 0.7305\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9868\n",
      "Epoch 00029: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0723 - acc: 0.9867 - val_loss: 1.1828 - val_acc: 0.7293\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9881\n",
      "Epoch 00030: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0648 - acc: 0.9880 - val_loss: 1.1860 - val_acc: 0.7389\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9883\n",
      "Epoch 00031: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0642 - acc: 0.9883 - val_loss: 1.2318 - val_acc: 0.7305\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9917\n",
      "Epoch 00032: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0520 - acc: 0.9916 - val_loss: 1.2155 - val_acc: 0.7326\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9906\n",
      "Epoch 00033: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0540 - acc: 0.9906 - val_loss: 1.2124 - val_acc: 0.7338\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9909\n",
      "Epoch 00034: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0540 - acc: 0.9909 - val_loss: 1.2017 - val_acc: 0.7400\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9942\n",
      "Epoch 00035: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0425 - acc: 0.9941 - val_loss: 1.2092 - val_acc: 0.7419\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9911\n",
      "Epoch 00036: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0499 - acc: 0.9911 - val_loss: 1.3005 - val_acc: 0.7293\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9893\n",
      "Epoch 00037: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0558 - acc: 0.9893 - val_loss: 1.3367 - val_acc: 0.7272\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9948\n",
      "Epoch 00038: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0388 - acc: 0.9948 - val_loss: 1.3129 - val_acc: 0.7279\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9923\n",
      "Epoch 00039: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0461 - acc: 0.9923 - val_loss: 1.3421 - val_acc: 0.7242\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9938\n",
      "Epoch 00040: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0411 - acc: 0.9938 - val_loss: 1.3250 - val_acc: 0.7349\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9922\n",
      "Epoch 00041: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0419 - acc: 0.9922 - val_loss: 1.2577 - val_acc: 0.7456\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9920\n",
      "Epoch 00042: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0444 - acc: 0.9919 - val_loss: 1.3613 - val_acc: 0.7296\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9923\n",
      "Epoch 00043: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0420 - acc: 0.9922 - val_loss: 1.3204 - val_acc: 0.7314\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9937\n",
      "Epoch 00044: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0388 - acc: 0.9937 - val_loss: 1.3872 - val_acc: 0.7265\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9912\n",
      "Epoch 00045: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0442 - acc: 0.9912 - val_loss: 1.3583 - val_acc: 0.7289\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9919\n",
      "Epoch 00046: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0426 - acc: 0.9918 - val_loss: 1.4024 - val_acc: 0.7333\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9920\n",
      "Epoch 00047: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0418 - acc: 0.9920 - val_loss: 1.3713 - val_acc: 0.7326\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9968\n",
      "Epoch 00048: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0274 - acc: 0.9968 - val_loss: 1.3504 - val_acc: 0.7354\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9951\n",
      "Epoch 00049: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0327 - acc: 0.9951 - val_loss: 1.4278 - val_acc: 0.7219\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9932\n",
      "Epoch 00050: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0366 - acc: 0.9931 - val_loss: 1.3850 - val_acc: 0.7282\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9953\n",
      "Epoch 00051: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0302 - acc: 0.9952 - val_loss: 1.4314 - val_acc: 0.7233\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9917\n",
      "Epoch 00052: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0443 - acc: 0.9916 - val_loss: 1.3959 - val_acc: 0.7284\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9943\n",
      "Epoch 00053: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0350 - acc: 0.9943 - val_loss: 1.3357 - val_acc: 0.7417\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9950\n",
      "Epoch 00054: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0307 - acc: 0.9950 - val_loss: 1.3348 - val_acc: 0.7347\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9946\n",
      "Epoch 00055: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0307 - acc: 0.9946 - val_loss: 1.4074 - val_acc: 0.7345\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0277 - acc: 0.9959 - val_loss: 1.4178 - val_acc: 0.7317\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9952\n",
      "Epoch 00057: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0296 - acc: 0.9952 - val_loss: 1.4555 - val_acc: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9945\n",
      "Epoch 00058: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0311 - acc: 0.9944 - val_loss: 1.4901 - val_acc: 0.7177\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9922\n",
      "Epoch 00059: val_loss did not improve from 0.92454\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0392 - acc: 0.9922 - val_loss: 1.4005 - val_acc: 0.7340\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 876us/sample - loss: 0.9920 - acc: 0.7115\n",
      "Loss: 0.9919993686527478 Accuracy: 0.71152645\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2999 - acc: 0.3086\n",
      "Epoch 00001: val_loss improved from inf to 1.85221, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/001-1.8522.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 2.2998 - acc: 0.3086 - val_loss: 1.8522 - val_acc: 0.4093\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4470 - acc: 0.5596\n",
      "Epoch 00002: val_loss improved from 1.85221 to 1.23506, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/002-1.2351.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.4470 - acc: 0.5597 - val_loss: 1.2351 - val_acc: 0.6254\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1784 - acc: 0.6428\n",
      "Epoch 00003: val_loss improved from 1.23506 to 1.05150, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/003-1.0515.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1785 - acc: 0.6427 - val_loss: 1.0515 - val_acc: 0.6902\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0112 - acc: 0.6983\n",
      "Epoch 00004: val_loss improved from 1.05150 to 0.95566, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/004-0.9557.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0112 - acc: 0.6983 - val_loss: 0.9557 - val_acc: 0.7212\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8905 - acc: 0.7376\n",
      "Epoch 00005: val_loss improved from 0.95566 to 0.86932, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/005-0.8693.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8907 - acc: 0.7376 - val_loss: 0.8693 - val_acc: 0.7452\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8147 - acc: 0.7643\n",
      "Epoch 00006: val_loss improved from 0.86932 to 0.78437, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/006-0.7844.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8150 - acc: 0.7643 - val_loss: 0.7844 - val_acc: 0.7738\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7871\n",
      "Epoch 00007: val_loss did not improve from 0.78437\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7396 - acc: 0.7872 - val_loss: 0.8532 - val_acc: 0.7570\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6796 - acc: 0.8038\n",
      "Epoch 00008: val_loss improved from 0.78437 to 0.72999, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/008-0.7300.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6796 - acc: 0.8038 - val_loss: 0.7300 - val_acc: 0.7880\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6360 - acc: 0.8162\n",
      "Epoch 00009: val_loss improved from 0.72999 to 0.69355, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/009-0.6936.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6363 - acc: 0.8161 - val_loss: 0.6936 - val_acc: 0.8060\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8308\n",
      "Epoch 00010: val_loss did not improve from 0.69355\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5886 - acc: 0.8307 - val_loss: 0.7061 - val_acc: 0.7983\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8437\n",
      "Epoch 00011: val_loss improved from 0.69355 to 0.65933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/011-0.6593.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5495 - acc: 0.8437 - val_loss: 0.6593 - val_acc: 0.8188\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8549\n",
      "Epoch 00012: val_loss improved from 0.65933 to 0.63855, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/012-0.6385.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5097 - acc: 0.8548 - val_loss: 0.6385 - val_acc: 0.8211\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8644\n",
      "Epoch 00013: val_loss did not improve from 0.63855\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4769 - acc: 0.8644 - val_loss: 0.6610 - val_acc: 0.8157\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8728\n",
      "Epoch 00014: val_loss did not improve from 0.63855\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4422 - acc: 0.8727 - val_loss: 0.6434 - val_acc: 0.8178\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8821\n",
      "Epoch 00015: val_loss did not improve from 0.63855\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4180 - acc: 0.8821 - val_loss: 0.7059 - val_acc: 0.8067\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8876\n",
      "Epoch 00016: val_loss improved from 0.63855 to 0.60604, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/016-0.6060.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3956 - acc: 0.8875 - val_loss: 0.6060 - val_acc: 0.8325\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8960\n",
      "Epoch 00017: val_loss did not improve from 0.60604\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3621 - acc: 0.8960 - val_loss: 0.6211 - val_acc: 0.8262\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.9036\n",
      "Epoch 00018: val_loss improved from 0.60604 to 0.58267, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/018-0.5827.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3391 - acc: 0.9035 - val_loss: 0.5827 - val_acc: 0.8332\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.9074\n",
      "Epoch 00019: val_loss improved from 0.58267 to 0.56784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/019-0.5678.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3310 - acc: 0.9074 - val_loss: 0.5678 - val_acc: 0.8428\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9155\n",
      "Epoch 00020: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3040 - acc: 0.9155 - val_loss: 0.6545 - val_acc: 0.8230\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9193\n",
      "Epoch 00021: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2843 - acc: 0.9193 - val_loss: 0.6488 - val_acc: 0.8253\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9223\n",
      "Epoch 00022: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2732 - acc: 0.9223 - val_loss: 0.5740 - val_acc: 0.8418\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9315\n",
      "Epoch 00023: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2496 - acc: 0.9314 - val_loss: 0.6387 - val_acc: 0.8279\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9330\n",
      "Epoch 00024: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2391 - acc: 0.9330 - val_loss: 0.5934 - val_acc: 0.8400\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9380\n",
      "Epoch 00025: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2241 - acc: 0.9379 - val_loss: 0.5924 - val_acc: 0.8404\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9375\n",
      "Epoch 00026: val_loss did not improve from 0.56784\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2237 - acc: 0.9374 - val_loss: 0.6287 - val_acc: 0.8341\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9452\n",
      "Epoch 00027: val_loss improved from 0.56784 to 0.56723, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/027-0.5672.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2018 - acc: 0.9451 - val_loss: 0.5672 - val_acc: 0.8507\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9493\n",
      "Epoch 00028: val_loss did not improve from 0.56723\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1893 - acc: 0.9493 - val_loss: 0.6085 - val_acc: 0.8421\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9536\n",
      "Epoch 00029: val_loss improved from 0.56723 to 0.56362, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv_checkpoint/029-0.5636.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1743 - acc: 0.9535 - val_loss: 0.5636 - val_acc: 0.8495\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9549\n",
      "Epoch 00030: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1690 - acc: 0.9549 - val_loss: 0.5862 - val_acc: 0.8432\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9600\n",
      "Epoch 00031: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1556 - acc: 0.9600 - val_loss: 0.6412 - val_acc: 0.8290\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9633\n",
      "Epoch 00032: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1435 - acc: 0.9633 - val_loss: 0.6182 - val_acc: 0.8376\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9614\n",
      "Epoch 00033: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1438 - acc: 0.9614 - val_loss: 0.5859 - val_acc: 0.8512\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9642\n",
      "Epoch 00034: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1377 - acc: 0.9642 - val_loss: 0.6063 - val_acc: 0.8428\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9690\n",
      "Epoch 00035: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1244 - acc: 0.9690 - val_loss: 0.6647 - val_acc: 0.8407\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9703\n",
      "Epoch 00036: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1199 - acc: 0.9703 - val_loss: 0.5874 - val_acc: 0.8495\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9719\n",
      "Epoch 00037: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1147 - acc: 0.9718 - val_loss: 0.5782 - val_acc: 0.8491\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9697\n",
      "Epoch 00038: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1164 - acc: 0.9697 - val_loss: 0.6165 - val_acc: 0.8477\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9778\n",
      "Epoch 00039: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0973 - acc: 0.9778 - val_loss: 0.6161 - val_acc: 0.8507\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9779\n",
      "Epoch 00040: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0952 - acc: 0.9779 - val_loss: 0.6932 - val_acc: 0.8355\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9786\n",
      "Epoch 00041: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0913 - acc: 0.9785 - val_loss: 0.6217 - val_acc: 0.8404\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9776\n",
      "Epoch 00042: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0932 - acc: 0.9775 - val_loss: 0.6127 - val_acc: 0.8460\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9759\n",
      "Epoch 00043: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0949 - acc: 0.9759 - val_loss: 0.6712 - val_acc: 0.8439\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0844 - acc: 0.9802 - val_loss: 0.6501 - val_acc: 0.8472\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9813\n",
      "Epoch 00045: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0822 - acc: 0.9813 - val_loss: 0.6574 - val_acc: 0.8411\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9830\n",
      "Epoch 00046: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0744 - acc: 0.9830 - val_loss: 0.6034 - val_acc: 0.8584\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9827\n",
      "Epoch 00047: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0742 - acc: 0.9826 - val_loss: 0.6827 - val_acc: 0.8472\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9791\n",
      "Epoch 00048: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0846 - acc: 0.9791 - val_loss: 0.6620 - val_acc: 0.8446\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9792\n",
      "Epoch 00049: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0835 - acc: 0.9792 - val_loss: 0.6158 - val_acc: 0.8530\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9812\n",
      "Epoch 00050: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0785 - acc: 0.9812 - val_loss: 0.5962 - val_acc: 0.8558\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9881\n",
      "Epoch 00051: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0588 - acc: 0.9881 - val_loss: 0.6425 - val_acc: 0.8516\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9870\n",
      "Epoch 00052: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0604 - acc: 0.9869 - val_loss: 0.6630 - val_acc: 0.8467\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9813\n",
      "Epoch 00053: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0766 - acc: 0.9813 - val_loss: 0.7018 - val_acc: 0.8328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0487 - acc: 0.9916 - val_loss: 0.6723 - val_acc: 0.8400\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9854\n",
      "Epoch 00055: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0649 - acc: 0.9853 - val_loss: 0.6335 - val_acc: 0.8595\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9859\n",
      "Epoch 00056: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0620 - acc: 0.9859 - val_loss: 0.6437 - val_acc: 0.8519\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9883\n",
      "Epoch 00057: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0549 - acc: 0.9883 - val_loss: 0.6872 - val_acc: 0.8451\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9899\n",
      "Epoch 00058: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0489 - acc: 0.9899 - val_loss: 0.6275 - val_acc: 0.8600\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9898\n",
      "Epoch 00059: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0513 - acc: 0.9898 - val_loss: 0.6738 - val_acc: 0.8521\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9855\n",
      "Epoch 00060: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0613 - acc: 0.9855 - val_loss: 0.6879 - val_acc: 0.8486\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9911\n",
      "Epoch 00061: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0459 - acc: 0.9911 - val_loss: 0.6479 - val_acc: 0.8584\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9884\n",
      "Epoch 00062: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0524 - acc: 0.9883 - val_loss: 0.6237 - val_acc: 0.8595\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9849\n",
      "Epoch 00063: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0603 - acc: 0.9848 - val_loss: 0.7080 - val_acc: 0.8430\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9867\n",
      "Epoch 00064: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0567 - acc: 0.9867 - val_loss: 0.6723 - val_acc: 0.8532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9916\n",
      "Epoch 00065: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0428 - acc: 0.9916 - val_loss: 0.6675 - val_acc: 0.8495\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9924\n",
      "Epoch 00066: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0401 - acc: 0.9923 - val_loss: 0.7577 - val_acc: 0.8316\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9874\n",
      "Epoch 00067: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0519 - acc: 0.9874 - val_loss: 0.6635 - val_acc: 0.8567\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9891\n",
      "Epoch 00068: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0478 - acc: 0.9891 - val_loss: 0.7120 - val_acc: 0.8407\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9897\n",
      "Epoch 00069: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0467 - acc: 0.9897 - val_loss: 0.7388 - val_acc: 0.8428\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9932\n",
      "Epoch 00070: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0376 - acc: 0.9931 - val_loss: 0.7078 - val_acc: 0.8470\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9898\n",
      "Epoch 00071: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0486 - acc: 0.9898 - val_loss: 0.6875 - val_acc: 0.8500\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9877\n",
      "Epoch 00072: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0510 - acc: 0.9877 - val_loss: 0.6534 - val_acc: 0.8563\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9938\n",
      "Epoch 00073: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0349 - acc: 0.9938 - val_loss: 0.7241 - val_acc: 0.8409\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9897\n",
      "Epoch 00074: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0478 - acc: 0.9896 - val_loss: 0.7482 - val_acc: 0.8383\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9870\n",
      "Epoch 00075: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0543 - acc: 0.9870 - val_loss: 0.7156 - val_acc: 0.8479\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9937\n",
      "Epoch 00076: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0328 - acc: 0.9937 - val_loss: 0.6929 - val_acc: 0.8549\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9947\n",
      "Epoch 00077: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0308 - acc: 0.9946 - val_loss: 0.7433 - val_acc: 0.8395\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9895\n",
      "Epoch 00078: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0445 - acc: 0.9895 - val_loss: 0.6931 - val_acc: 0.8516\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9928\n",
      "Epoch 00079: val_loss did not improve from 0.56362\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0353 - acc: 0.9927 - val_loss: 0.6574 - val_acc: 0.8626\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 933us/sample - loss: 0.6275 - acc: 0.8264\n",
      "Loss: 0.627503177570034 Accuracy: 0.8263759\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4334 - acc: 0.2532\n",
      "Epoch 00001: val_loss improved from inf to 2.07898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/001-2.0790.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 2.4333 - acc: 0.2532 - val_loss: 2.0790 - val_acc: 0.3371\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5800 - acc: 0.5036\n",
      "Epoch 00002: val_loss improved from 2.07898 to 1.35390, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/002-1.3539.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.5800 - acc: 0.5035 - val_loss: 1.3539 - val_acc: 0.5744\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2664 - acc: 0.6106\n",
      "Epoch 00003: val_loss improved from 1.35390 to 1.11814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/003-1.1181.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.2665 - acc: 0.6105 - val_loss: 1.1181 - val_acc: 0.6569\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0571 - acc: 0.6830\n",
      "Epoch 00004: val_loss improved from 1.11814 to 0.96292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/004-0.9629.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.0573 - acc: 0.6829 - val_loss: 0.9629 - val_acc: 0.7130\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8952 - acc: 0.7362\n",
      "Epoch 00005: val_loss improved from 0.96292 to 0.86680, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/005-0.8668.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8952 - acc: 0.7362 - val_loss: 0.8668 - val_acc: 0.7368\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.7760\n",
      "Epoch 00006: val_loss improved from 0.86680 to 0.69910, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/006-0.6991.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7691 - acc: 0.7760 - val_loss: 0.6991 - val_acc: 0.7943\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6665 - acc: 0.8090\n",
      "Epoch 00007: val_loss improved from 0.69910 to 0.66093, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/007-0.6609.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6667 - acc: 0.8089 - val_loss: 0.6609 - val_acc: 0.8109\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8306\n",
      "Epoch 00008: val_loss improved from 0.66093 to 0.59801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/008-0.5980.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5863 - acc: 0.8305 - val_loss: 0.5980 - val_acc: 0.8244\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8501\n",
      "Epoch 00009: val_loss improved from 0.59801 to 0.50484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/009-0.5048.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5216 - acc: 0.8500 - val_loss: 0.5048 - val_acc: 0.8528\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8632\n",
      "Epoch 00010: val_loss improved from 0.50484 to 0.47773, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/010-0.4777.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4766 - acc: 0.8631 - val_loss: 0.4777 - val_acc: 0.8665\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8769\n",
      "Epoch 00011: val_loss improved from 0.47773 to 0.45142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/011-0.4514.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4255 - acc: 0.8769 - val_loss: 0.4514 - val_acc: 0.8693\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8858\n",
      "Epoch 00012: val_loss improved from 0.45142 to 0.39525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/012-0.3953.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3941 - acc: 0.8858 - val_loss: 0.3953 - val_acc: 0.8870\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8921\n",
      "Epoch 00013: val_loss did not improve from 0.39525\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3647 - acc: 0.8921 - val_loss: 0.4170 - val_acc: 0.8810\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.9021\n",
      "Epoch 00014: val_loss improved from 0.39525 to 0.39281, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/014-0.3928.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3370 - acc: 0.9021 - val_loss: 0.3928 - val_acc: 0.8926\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9098\n",
      "Epoch 00015: val_loss improved from 0.39281 to 0.37358, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/015-0.3736.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3109 - acc: 0.9097 - val_loss: 0.3736 - val_acc: 0.8942\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9159\n",
      "Epoch 00016: val_loss did not improve from 0.37358\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2928 - acc: 0.9159 - val_loss: 0.4544 - val_acc: 0.8668\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9206\n",
      "Epoch 00017: val_loss improved from 0.37358 to 0.32561, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/017-0.3256.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2727 - acc: 0.9206 - val_loss: 0.3256 - val_acc: 0.9085\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9238\n",
      "Epoch 00018: val_loss did not improve from 0.32561\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2613 - acc: 0.9237 - val_loss: 0.3341 - val_acc: 0.9054\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9292\n",
      "Epoch 00019: val_loss did not improve from 0.32561\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2425 - acc: 0.9291 - val_loss: 0.3264 - val_acc: 0.9033\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9324\n",
      "Epoch 00020: val_loss improved from 0.32561 to 0.32262, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/020-0.3226.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2326 - acc: 0.9323 - val_loss: 0.3226 - val_acc: 0.9052\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9367\n",
      "Epoch 00021: val_loss did not improve from 0.32262\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2166 - acc: 0.9366 - val_loss: 0.3678 - val_acc: 0.8901\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9383\n",
      "Epoch 00022: val_loss improved from 0.32262 to 0.30645, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/022-0.3064.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2141 - acc: 0.9383 - val_loss: 0.3064 - val_acc: 0.9126\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9441\n",
      "Epoch 00023: val_loss did not improve from 0.30645\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1917 - acc: 0.9441 - val_loss: 0.3199 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9445\n",
      "Epoch 00024: val_loss did not improve from 0.30645\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1857 - acc: 0.9445 - val_loss: 0.3454 - val_acc: 0.8963\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9504\n",
      "Epoch 00025: val_loss improved from 0.30645 to 0.28748, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/025-0.2875.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1735 - acc: 0.9504 - val_loss: 0.2875 - val_acc: 0.9171\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9512\n",
      "Epoch 00026: val_loss did not improve from 0.28748\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1664 - acc: 0.9512 - val_loss: 0.2935 - val_acc: 0.9147\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9541\n",
      "Epoch 00027: val_loss did not improve from 0.28748\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1595 - acc: 0.9540 - val_loss: 0.3510 - val_acc: 0.9029\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9504\n",
      "Epoch 00028: val_loss improved from 0.28748 to 0.28623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/028-0.2862.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1651 - acc: 0.9503 - val_loss: 0.2862 - val_acc: 0.9175\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9600\n",
      "Epoch 00029: val_loss did not improve from 0.28623\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1412 - acc: 0.9599 - val_loss: 0.3043 - val_acc: 0.9113\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9597\n",
      "Epoch 00030: val_loss improved from 0.28623 to 0.28143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv_checkpoint/030-0.2814.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1412 - acc: 0.9597 - val_loss: 0.2814 - val_acc: 0.9231\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9645\n",
      "Epoch 00031: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1262 - acc: 0.9645 - val_loss: 0.2825 - val_acc: 0.9185\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9662\n",
      "Epoch 00032: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1216 - acc: 0.9661 - val_loss: 0.3244 - val_acc: 0.9089\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9647\n",
      "Epoch 00033: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1242 - acc: 0.9647 - val_loss: 0.2949 - val_acc: 0.9201\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9673\n",
      "Epoch 00034: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1164 - acc: 0.9672 - val_loss: 0.3220 - val_acc: 0.9085\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9695\n",
      "Epoch 00035: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1078 - acc: 0.9695 - val_loss: 0.2933 - val_acc: 0.9222\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9708\n",
      "Epoch 00036: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1032 - acc: 0.9708 - val_loss: 0.3011 - val_acc: 0.9180\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9706\n",
      "Epoch 00037: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1035 - acc: 0.9706 - val_loss: 0.2957 - val_acc: 0.9194\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9751\n",
      "Epoch 00038: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0919 - acc: 0.9751 - val_loss: 0.2886 - val_acc: 0.9234\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9749\n",
      "Epoch 00039: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0916 - acc: 0.9748 - val_loss: 0.2979 - val_acc: 0.9168\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9747\n",
      "Epoch 00040: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0913 - acc: 0.9747 - val_loss: 0.2863 - val_acc: 0.9241\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9760\n",
      "Epoch 00041: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0866 - acc: 0.9759 - val_loss: 0.2990 - val_acc: 0.9257\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9773\n",
      "Epoch 00042: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0829 - acc: 0.9773 - val_loss: 0.3159 - val_acc: 0.9113\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9806\n",
      "Epoch 00043: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0732 - acc: 0.9806 - val_loss: 0.3102 - val_acc: 0.9248\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9807\n",
      "Epoch 00044: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0732 - acc: 0.9807 - val_loss: 0.3075 - val_acc: 0.9199\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9813\n",
      "Epoch 00045: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0709 - acc: 0.9813 - val_loss: 0.3113 - val_acc: 0.9213\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9758\n",
      "Epoch 00046: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0862 - acc: 0.9757 - val_loss: 0.3227 - val_acc: 0.9201\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9841\n",
      "Epoch 00047: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0624 - acc: 0.9841 - val_loss: 0.2956 - val_acc: 0.9229\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9821\n",
      "Epoch 00048: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0675 - acc: 0.9821 - val_loss: 0.3286 - val_acc: 0.9140\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9820\n",
      "Epoch 00049: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0668 - acc: 0.9820 - val_loss: 0.2979 - val_acc: 0.9227\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9854\n",
      "Epoch 00050: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0579 - acc: 0.9853 - val_loss: 0.3291 - val_acc: 0.9124\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9835\n",
      "Epoch 00051: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0611 - acc: 0.9835 - val_loss: 0.3177 - val_acc: 0.9171\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9870\n",
      "Epoch 00052: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0528 - acc: 0.9869 - val_loss: 0.2985 - val_acc: 0.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9822\n",
      "Epoch 00053: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0681 - acc: 0.9822 - val_loss: 0.3082 - val_acc: 0.9250\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9875\n",
      "Epoch 00054: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0517 - acc: 0.9875 - val_loss: 0.3028 - val_acc: 0.9229\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9846\n",
      "Epoch 00055: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0585 - acc: 0.9846 - val_loss: 0.2909 - val_acc: 0.9276\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9904\n",
      "Epoch 00056: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0437 - acc: 0.9903 - val_loss: 0.3060 - val_acc: 0.9234\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9864\n",
      "Epoch 00057: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0539 - acc: 0.9864 - val_loss: 0.2970 - val_acc: 0.9257\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9879\n",
      "Epoch 00058: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0485 - acc: 0.9879 - val_loss: 0.3220 - val_acc: 0.9189\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9889\n",
      "Epoch 00059: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0471 - acc: 0.9889 - val_loss: 0.2978 - val_acc: 0.9255\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9900\n",
      "Epoch 00060: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0432 - acc: 0.9900 - val_loss: 0.3234 - val_acc: 0.9227\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9913\n",
      "Epoch 00061: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0379 - acc: 0.9913 - val_loss: 0.3224 - val_acc: 0.9187\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9871\n",
      "Epoch 00062: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0509 - acc: 0.9871 - val_loss: 0.3395 - val_acc: 0.9187\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9912\n",
      "Epoch 00063: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0387 - acc: 0.9911 - val_loss: 0.3805 - val_acc: 0.9129\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9847\n",
      "Epoch 00064: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0576 - acc: 0.9846 - val_loss: 0.2891 - val_acc: 0.9313\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9864\n",
      "Epoch 00065: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0514 - acc: 0.9864 - val_loss: 0.3028 - val_acc: 0.9280\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9921\n",
      "Epoch 00066: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0352 - acc: 0.9921 - val_loss: 0.3097 - val_acc: 0.9259\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9915\n",
      "Epoch 00067: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0377 - acc: 0.9915 - val_loss: 0.3165 - val_acc: 0.9222\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9942\n",
      "Epoch 00068: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0298 - acc: 0.9942 - val_loss: 0.2959 - val_acc: 0.9313\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9893\n",
      "Epoch 00069: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0424 - acc: 0.9893 - val_loss: 0.3465 - val_acc: 0.9182\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9926\n",
      "Epoch 00070: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0322 - acc: 0.9925 - val_loss: 0.3146 - val_acc: 0.9250\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9919\n",
      "Epoch 00071: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0349 - acc: 0.9919 - val_loss: 0.3266 - val_acc: 0.9276\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9932\n",
      "Epoch 00072: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0317 - acc: 0.9932 - val_loss: 0.3227 - val_acc: 0.9252\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9941\n",
      "Epoch 00073: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0289 - acc: 0.9941 - val_loss: 0.3382 - val_acc: 0.9178\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9885\n",
      "Epoch 00074: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0435 - acc: 0.9885 - val_loss: 0.2995 - val_acc: 0.9285\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9901\n",
      "Epoch 00075: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0383 - acc: 0.9901 - val_loss: 0.3066 - val_acc: 0.9250\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9931\n",
      "Epoch 00076: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0287 - acc: 0.9931 - val_loss: 0.3487 - val_acc: 0.9206\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9913\n",
      "Epoch 00077: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0343 - acc: 0.9913 - val_loss: 0.3328 - val_acc: 0.9206\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9940\n",
      "Epoch 00078: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0269 - acc: 0.9940 - val_loss: 0.2994 - val_acc: 0.9324\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9944\n",
      "Epoch 00079: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0267 - acc: 0.9943 - val_loss: 0.3502 - val_acc: 0.9213\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9921\n",
      "Epoch 00080: val_loss did not improve from 0.28143\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0327 - acc: 0.9920 - val_loss: 0.3773 - val_acc: 0.9166\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 996us/sample - loss: 0.3580 - acc: 0.8989\n",
      "Loss: 0.3579766703048104 Accuracy: 0.8988577\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5245 - acc: 0.2364\n",
      "Epoch 00001: val_loss improved from inf to 2.32138, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/001-2.3214.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/sample - loss: 2.5244 - acc: 0.2364 - val_loss: 2.3214 - val_acc: 0.2791\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5228 - acc: 0.5326\n",
      "Epoch 00002: val_loss improved from 2.32138 to 1.19966, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/002-1.1997.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.5227 - acc: 0.5326 - val_loss: 1.1997 - val_acc: 0.6252\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.6736\n",
      "Epoch 00003: val_loss improved from 1.19966 to 0.89275, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/003-0.8927.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0883 - acc: 0.6735 - val_loss: 0.8927 - val_acc: 0.7391\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7595\n",
      "Epoch 00004: val_loss improved from 0.89275 to 0.69370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/004-0.6937.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8255 - acc: 0.7595 - val_loss: 0.6937 - val_acc: 0.8001\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.8098\n",
      "Epoch 00005: val_loss improved from 0.69370 to 0.57555, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/005-0.5756.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6573 - acc: 0.8097 - val_loss: 0.5756 - val_acc: 0.8353\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8432\n",
      "Epoch 00006: val_loss improved from 0.57555 to 0.47725, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/006-0.4772.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5445 - acc: 0.8431 - val_loss: 0.4772 - val_acc: 0.8635\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8652\n",
      "Epoch 00007: val_loss improved from 0.47725 to 0.43143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/007-0.4314.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4676 - acc: 0.8653 - val_loss: 0.4314 - val_acc: 0.8735\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8846\n",
      "Epoch 00008: val_loss improved from 0.43143 to 0.42050, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/008-0.4205.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3985 - acc: 0.8846 - val_loss: 0.4205 - val_acc: 0.8728\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8970\n",
      "Epoch 00009: val_loss improved from 0.42050 to 0.35722, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/009-0.3572.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3567 - acc: 0.8970 - val_loss: 0.3572 - val_acc: 0.8942\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9081\n",
      "Epoch 00010: val_loss improved from 0.35722 to 0.33750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/010-0.3375.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3170 - acc: 0.9081 - val_loss: 0.3375 - val_acc: 0.9031\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9186\n",
      "Epoch 00011: val_loss improved from 0.33750 to 0.31430, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/011-0.3143.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2834 - acc: 0.9186 - val_loss: 0.3143 - val_acc: 0.9115\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9251\n",
      "Epoch 00012: val_loss improved from 0.31430 to 0.28359, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/012-0.2836.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2591 - acc: 0.9251 - val_loss: 0.2836 - val_acc: 0.9185\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9311\n",
      "Epoch 00013: val_loss did not improve from 0.28359\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2344 - acc: 0.9310 - val_loss: 0.2974 - val_acc: 0.9106\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9352\n",
      "Epoch 00014: val_loss improved from 0.28359 to 0.22772, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/014-0.2277.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2217 - acc: 0.9352 - val_loss: 0.2277 - val_acc: 0.9331\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9424\n",
      "Epoch 00015: val_loss did not improve from 0.22772\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1978 - acc: 0.9424 - val_loss: 0.2573 - val_acc: 0.9192\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9470\n",
      "Epoch 00016: val_loss did not improve from 0.22772\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1822 - acc: 0.9470 - val_loss: 0.2443 - val_acc: 0.9252\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9512\n",
      "Epoch 00017: val_loss did not improve from 0.22772\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1695 - acc: 0.9512 - val_loss: 0.2610 - val_acc: 0.9276\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9533\n",
      "Epoch 00018: val_loss did not improve from 0.22772\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1585 - acc: 0.9533 - val_loss: 0.2730 - val_acc: 0.9154\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9555\n",
      "Epoch 00019: val_loss did not improve from 0.22772\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1486 - acc: 0.9554 - val_loss: 0.2665 - val_acc: 0.9243\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9572\n",
      "Epoch 00020: val_loss improved from 0.22772 to 0.20265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/020-0.2026.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1454 - acc: 0.9572 - val_loss: 0.2026 - val_acc: 0.9366\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9629\n",
      "Epoch 00021: val_loss improved from 0.20265 to 0.19926, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/021-0.1993.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1265 - acc: 0.9629 - val_loss: 0.1993 - val_acc: 0.9422\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9642\n",
      "Epoch 00022: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1202 - acc: 0.9642 - val_loss: 0.2169 - val_acc: 0.9352\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9654\n",
      "Epoch 00023: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1176 - acc: 0.9654 - val_loss: 0.2218 - val_acc: 0.9331\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9693\n",
      "Epoch 00024: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1064 - acc: 0.9693 - val_loss: 0.2211 - val_acc: 0.9348\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9701\n",
      "Epoch 00025: val_loss did not improve from 0.19926\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1053 - acc: 0.9700 - val_loss: 0.2000 - val_acc: 0.9404\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9716\n",
      "Epoch 00026: val_loss improved from 0.19926 to 0.19588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/026-0.1959.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0987 - acc: 0.9716 - val_loss: 0.1959 - val_acc: 0.9411\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9732\n",
      "Epoch 00027: val_loss improved from 0.19588 to 0.19315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/027-0.1931.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0922 - acc: 0.9731 - val_loss: 0.1931 - val_acc: 0.9427\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9749\n",
      "Epoch 00028: val_loss did not improve from 0.19315\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0874 - acc: 0.9749 - val_loss: 0.1941 - val_acc: 0.9460\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9749\n",
      "Epoch 00029: val_loss did not improve from 0.19315\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0846 - acc: 0.9749 - val_loss: 0.2115 - val_acc: 0.9376\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9802\n",
      "Epoch 00030: val_loss improved from 0.19315 to 0.19269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/030-0.1927.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0738 - acc: 0.9802 - val_loss: 0.1927 - val_acc: 0.9376\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9793\n",
      "Epoch 00031: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0725 - acc: 0.9793 - val_loss: 0.1983 - val_acc: 0.9432\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9817\n",
      "Epoch 00032: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0655 - acc: 0.9816 - val_loss: 0.1941 - val_acc: 0.9413\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9810\n",
      "Epoch 00033: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0680 - acc: 0.9809 - val_loss: 0.2242 - val_acc: 0.9366\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9812\n",
      "Epoch 00034: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0655 - acc: 0.9812 - val_loss: 0.2389 - val_acc: 0.9362\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9857\n",
      "Epoch 00035: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0540 - acc: 0.9857 - val_loss: 0.2070 - val_acc: 0.9390\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9842\n",
      "Epoch 00036: val_loss did not improve from 0.19269\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0569 - acc: 0.9842 - val_loss: 0.2505 - val_acc: 0.9366\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9864\n",
      "Epoch 00037: val_loss improved from 0.19269 to 0.19218, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv_checkpoint/037-0.1922.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0498 - acc: 0.9864 - val_loss: 0.1922 - val_acc: 0.9439\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9868\n",
      "Epoch 00038: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0487 - acc: 0.9868 - val_loss: 0.2528 - val_acc: 0.9331\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9884\n",
      "Epoch 00039: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0445 - acc: 0.9884 - val_loss: 0.2218 - val_acc: 0.9380\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9885\n",
      "Epoch 00040: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0440 - acc: 0.9885 - val_loss: 0.2121 - val_acc: 0.9436\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9896\n",
      "Epoch 00041: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0395 - acc: 0.9896 - val_loss: 0.2047 - val_acc: 0.9411\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9893\n",
      "Epoch 00042: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0404 - acc: 0.9893 - val_loss: 0.2017 - val_acc: 0.9457\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9915\n",
      "Epoch 00043: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0336 - acc: 0.9915 - val_loss: 0.1960 - val_acc: 0.9439\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9888\n",
      "Epoch 00044: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0398 - acc: 0.9888 - val_loss: 0.2492 - val_acc: 0.9327\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9896\n",
      "Epoch 00045: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0385 - acc: 0.9896 - val_loss: 0.2184 - val_acc: 0.9380\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0319 - acc: 0.9921 - val_loss: 0.2294 - val_acc: 0.9359\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9910\n",
      "Epoch 00047: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0336 - acc: 0.9910 - val_loss: 0.2367 - val_acc: 0.9350\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9883\n",
      "Epoch 00048: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0411 - acc: 0.9882 - val_loss: 0.2549 - val_acc: 0.9334\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9924\n",
      "Epoch 00049: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0299 - acc: 0.9924 - val_loss: 0.2046 - val_acc: 0.9434\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9948\n",
      "Epoch 00050: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0248 - acc: 0.9948 - val_loss: 0.1996 - val_acc: 0.9457\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9923\n",
      "Epoch 00051: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0287 - acc: 0.9923 - val_loss: 0.2172 - val_acc: 0.9392\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9911\n",
      "Epoch 00052: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0337 - acc: 0.9910 - val_loss: 0.1959 - val_acc: 0.9460\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9921\n",
      "Epoch 00053: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0303 - acc: 0.9921 - val_loss: 0.1990 - val_acc: 0.9474\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9953\n",
      "Epoch 00054: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0225 - acc: 0.9953 - val_loss: 0.2016 - val_acc: 0.9502\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9948\n",
      "Epoch 00055: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0230 - acc: 0.9948 - val_loss: 0.2242 - val_acc: 0.9413\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9908\n",
      "Epoch 00056: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0334 - acc: 0.9908 - val_loss: 0.2218 - val_acc: 0.9420\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9908\n",
      "Epoch 00057: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0331 - acc: 0.9907 - val_loss: 0.2066 - val_acc: 0.9436\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9934\n",
      "Epoch 00058: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0268 - acc: 0.9933 - val_loss: 0.2323 - val_acc: 0.9397\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9921\n",
      "Epoch 00059: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0299 - acc: 0.9921 - val_loss: 0.2024 - val_acc: 0.9467\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9965\n",
      "Epoch 00060: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0164 - acc: 0.9965 - val_loss: 0.2036 - val_acc: 0.9474\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9964\n",
      "Epoch 00061: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0168 - acc: 0.9964 - val_loss: 0.2159 - val_acc: 0.9432\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9919\n",
      "Epoch 00062: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0302 - acc: 0.9919 - val_loss: 0.2196 - val_acc: 0.9432\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9969\n",
      "Epoch 00063: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0151 - acc: 0.9969 - val_loss: 0.2211 - val_acc: 0.9436\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9948\n",
      "Epoch 00064: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0208 - acc: 0.9948 - val_loss: 0.2191 - val_acc: 0.9446\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9929\n",
      "Epoch 00065: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0257 - acc: 0.9929 - val_loss: 0.2177 - val_acc: 0.9411\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9962\n",
      "Epoch 00066: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0171 - acc: 0.9962 - val_loss: 0.2226 - val_acc: 0.9450\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0186 - acc: 0.9952 - val_loss: 0.2229 - val_acc: 0.9415\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9960\n",
      "Epoch 00068: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0164 - acc: 0.9960 - val_loss: 0.2151 - val_acc: 0.9460\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9965\n",
      "Epoch 00069: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0159 - acc: 0.9965 - val_loss: 0.2447 - val_acc: 0.9411\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9950\n",
      "Epoch 00070: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0195 - acc: 0.9949 - val_loss: 0.2261 - val_acc: 0.9467\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9909\n",
      "Epoch 00071: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0311 - acc: 0.9909 - val_loss: 0.2207 - val_acc: 0.9425\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 00072: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0144 - acc: 0.9967 - val_loss: 0.2529 - val_acc: 0.9406\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9927\n",
      "Epoch 00073: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0267 - acc: 0.9927 - val_loss: 0.2161 - val_acc: 0.9455\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9973\n",
      "Epoch 00074: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0127 - acc: 0.9973 - val_loss: 0.2206 - val_acc: 0.9460\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9974\n",
      "Epoch 00075: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0124 - acc: 0.9974 - val_loss: 0.2197 - val_acc: 0.9488\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9917\n",
      "Epoch 00076: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0297 - acc: 0.9917 - val_loss: 0.2122 - val_acc: 0.9478\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9971\n",
      "Epoch 00077: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0127 - acc: 0.9971 - val_loss: 0.2234 - val_acc: 0.9457\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9983\n",
      "Epoch 00078: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0102 - acc: 0.9983 - val_loss: 0.2230 - val_acc: 0.9460\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9981\n",
      "Epoch 00079: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0113 - acc: 0.9980 - val_loss: 0.2215 - val_acc: 0.9427\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9896\n",
      "Epoch 00080: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.2309 - val_acc: 0.9448\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9967\n",
      "Epoch 00081: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0140 - acc: 0.9966 - val_loss: 0.2072 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9927\n",
      "Epoch 00082: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0263 - acc: 0.9927 - val_loss: 0.2107 - val_acc: 0.9474\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9939\n",
      "Epoch 00083: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0217 - acc: 0.9939 - val_loss: 0.2151 - val_acc: 0.9471\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9980\n",
      "Epoch 00084: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0099 - acc: 0.9980 - val_loss: 0.2185 - val_acc: 0.9457\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9931\n",
      "Epoch 00085: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0233 - acc: 0.9931 - val_loss: 0.2205 - val_acc: 0.9471\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9975\n",
      "Epoch 00086: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0107 - acc: 0.9975 - val_loss: 0.2122 - val_acc: 0.9495\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9976\n",
      "Epoch 00087: val_loss did not improve from 0.19218\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0106 - acc: 0.9976 - val_loss: 0.2272 - val_acc: 0.9460\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2376 - acc: 0.9294\n",
      "Loss: 0.23758113803274286 Accuracy: 0.92938733\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1200 - acc: 0.3480\n",
      "Epoch 00001: val_loss improved from inf to 1.84708, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/001-1.8471.hdf5\n",
      "36805/36805 [==============================] - 143s 4ms/sample - loss: 2.1202 - acc: 0.3480 - val_loss: 1.8471 - val_acc: 0.4160\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0787 - acc: 0.6611\n",
      "Epoch 00002: val_loss improved from 1.84708 to 0.79846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/002-0.7985.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0788 - acc: 0.6611 - val_loss: 0.7985 - val_acc: 0.7489\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7427 - acc: 0.7692\n",
      "Epoch 00003: val_loss improved from 0.79846 to 0.57306, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/003-0.5731.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7427 - acc: 0.7691 - val_loss: 0.5731 - val_acc: 0.8213\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8246\n",
      "Epoch 00004: val_loss improved from 0.57306 to 0.47275, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/004-0.4728.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5620 - acc: 0.8245 - val_loss: 0.4728 - val_acc: 0.8477\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.8591\n",
      "Epoch 00005: val_loss improved from 0.47275 to 0.38814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/005-0.3881.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4518 - acc: 0.8591 - val_loss: 0.3881 - val_acc: 0.8791\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8824\n",
      "Epoch 00006: val_loss improved from 0.38814 to 0.35944, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/006-0.3594.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3752 - acc: 0.8824 - val_loss: 0.3594 - val_acc: 0.8896\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9008\n",
      "Epoch 00007: val_loss improved from 0.35944 to 0.33218, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/007-0.3322.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3222 - acc: 0.9008 - val_loss: 0.3322 - val_acc: 0.8935\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9123\n",
      "Epoch 00008: val_loss improved from 0.33218 to 0.29096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/008-0.2910.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2816 - acc: 0.9123 - val_loss: 0.2910 - val_acc: 0.9110\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9233\n",
      "Epoch 00009: val_loss improved from 0.29096 to 0.25975, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/009-0.2597.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2476 - acc: 0.9233 - val_loss: 0.2597 - val_acc: 0.9161\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9313\n",
      "Epoch 00010: val_loss did not improve from 0.25975\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2215 - acc: 0.9313 - val_loss: 0.2654 - val_acc: 0.9173\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9395\n",
      "Epoch 00011: val_loss improved from 0.25975 to 0.24218, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/011-0.2422.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1964 - acc: 0.9395 - val_loss: 0.2422 - val_acc: 0.9271\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9453\n",
      "Epoch 00012: val_loss did not improve from 0.24218\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1766 - acc: 0.9453 - val_loss: 0.2468 - val_acc: 0.9241\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9502\n",
      "Epoch 00013: val_loss improved from 0.24218 to 0.23031, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/013-0.2303.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1614 - acc: 0.9503 - val_loss: 0.2303 - val_acc: 0.9250\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9542\n",
      "Epoch 00014: val_loss did not improve from 0.23031\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1480 - acc: 0.9542 - val_loss: 0.2544 - val_acc: 0.9189\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9561\n",
      "Epoch 00015: val_loss did not improve from 0.23031\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1412 - acc: 0.9561 - val_loss: 0.2343 - val_acc: 0.9297\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9645\n",
      "Epoch 00016: val_loss did not improve from 0.23031\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1198 - acc: 0.9645 - val_loss: 0.2607 - val_acc: 0.9189\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9640\n",
      "Epoch 00017: val_loss improved from 0.23031 to 0.22093, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/017-0.2209.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1185 - acc: 0.9640 - val_loss: 0.2209 - val_acc: 0.9329\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9674\n",
      "Epoch 00018: val_loss improved from 0.22093 to 0.20401, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/018-0.2040.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1059 - acc: 0.9674 - val_loss: 0.2040 - val_acc: 0.9390\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9741\n",
      "Epoch 00019: val_loss did not improve from 0.20401\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0901 - acc: 0.9741 - val_loss: 0.2256 - val_acc: 0.9336\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9735\n",
      "Epoch 00020: val_loss did not improve from 0.20401\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0872 - acc: 0.9735 - val_loss: 0.2073 - val_acc: 0.9373\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9738\n",
      "Epoch 00021: val_loss did not improve from 0.20401\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0861 - acc: 0.9738 - val_loss: 0.2103 - val_acc: 0.9369\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9789\n",
      "Epoch 00022: val_loss did not improve from 0.20401\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0733 - acc: 0.9789 - val_loss: 0.2195 - val_acc: 0.9369\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9765\n",
      "Epoch 00023: val_loss did not improve from 0.20401\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0776 - acc: 0.9764 - val_loss: 0.2063 - val_acc: 0.9357\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9792\n",
      "Epoch 00024: val_loss improved from 0.20401 to 0.19261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv_checkpoint/024-0.1926.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0723 - acc: 0.9791 - val_loss: 0.1926 - val_acc: 0.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9848\n",
      "Epoch 00025: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0566 - acc: 0.9848 - val_loss: 0.2113 - val_acc: 0.9366\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9840\n",
      "Epoch 00026: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0575 - acc: 0.9840 - val_loss: 0.2420 - val_acc: 0.9308\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9860\n",
      "Epoch 00027: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0496 - acc: 0.9859 - val_loss: 0.2341 - val_acc: 0.9329\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9827\n",
      "Epoch 00028: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0602 - acc: 0.9827 - val_loss: 0.2074 - val_acc: 0.9413\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9894\n",
      "Epoch 00029: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0422 - acc: 0.9894 - val_loss: 0.2163 - val_acc: 0.9406\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9903\n",
      "Epoch 00030: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0385 - acc: 0.9903 - val_loss: 0.2015 - val_acc: 0.9450\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9884\n",
      "Epoch 00031: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0411 - acc: 0.9884 - val_loss: 0.2300 - val_acc: 0.9364\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9867\n",
      "Epoch 00032: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0458 - acc: 0.9867 - val_loss: 0.2098 - val_acc: 0.9404\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9904\n",
      "Epoch 00033: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0354 - acc: 0.9904 - val_loss: 0.2359 - val_acc: 0.9371\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9876\n",
      "Epoch 00034: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0442 - acc: 0.9876 - val_loss: 0.2008 - val_acc: 0.9436\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9917\n",
      "Epoch 00035: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0316 - acc: 0.9917 - val_loss: 0.2267 - val_acc: 0.9397\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9927\n",
      "Epoch 00036: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0289 - acc: 0.9927 - val_loss: 0.2249 - val_acc: 0.9411\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9935\n",
      "Epoch 00037: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0264 - acc: 0.9935 - val_loss: 0.2557 - val_acc: 0.9350\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9887\n",
      "Epoch 00038: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0385 - acc: 0.9887 - val_loss: 0.2380 - val_acc: 0.9390\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9939\n",
      "Epoch 00039: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0241 - acc: 0.9939 - val_loss: 0.2257 - val_acc: 0.9404\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9950\n",
      "Epoch 00040: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0220 - acc: 0.9950 - val_loss: 0.2416 - val_acc: 0.9373\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9881\n",
      "Epoch 00041: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0395 - acc: 0.9880 - val_loss: 0.2066 - val_acc: 0.9436\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9939\n",
      "Epoch 00042: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9939 - val_loss: 0.2053 - val_acc: 0.9434\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9945\n",
      "Epoch 00043: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0221 - acc: 0.9944 - val_loss: 0.2064 - val_acc: 0.9455\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9943\n",
      "Epoch 00044: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0219 - acc: 0.9943 - val_loss: 0.2028 - val_acc: 0.9436\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9962\n",
      "Epoch 00045: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0178 - acc: 0.9962 - val_loss: 0.2299 - val_acc: 0.9413\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9941\n",
      "Epoch 00046: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0212 - acc: 0.9941 - val_loss: 0.2373 - val_acc: 0.9415\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9962\n",
      "Epoch 00047: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0156 - acc: 0.9963 - val_loss: 0.2382 - val_acc: 0.9401\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9946\n",
      "Epoch 00048: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0201 - acc: 0.9946 - val_loss: 0.2179 - val_acc: 0.9455\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9946\n",
      "Epoch 00049: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0192 - acc: 0.9946 - val_loss: 0.2326 - val_acc: 0.9436\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9948\n",
      "Epoch 00050: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0206 - acc: 0.9948 - val_loss: 0.2165 - val_acc: 0.9436\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9956\n",
      "Epoch 00051: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0171 - acc: 0.9955 - val_loss: 0.2388 - val_acc: 0.9434\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9898\n",
      "Epoch 00052: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0324 - acc: 0.9898 - val_loss: 0.2298 - val_acc: 0.9462\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9928\n",
      "Epoch 00053: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0244 - acc: 0.9928 - val_loss: 0.2098 - val_acc: 0.9443\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9979\n",
      "Epoch 00054: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0111 - acc: 0.9979 - val_loss: 0.2176 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9981\n",
      "Epoch 00055: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0097 - acc: 0.9981 - val_loss: 0.2235 - val_acc: 0.9453\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9955\n",
      "Epoch 00056: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0179 - acc: 0.9955 - val_loss: 0.2357 - val_acc: 0.9467\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9969\n",
      "Epoch 00057: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0129 - acc: 0.9969 - val_loss: 0.2272 - val_acc: 0.9462\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9956\n",
      "Epoch 00058: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0161 - acc: 0.9956 - val_loss: 0.2584 - val_acc: 0.9397\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 00059: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0141 - acc: 0.9963 - val_loss: 0.2214 - val_acc: 0.9464\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9926\n",
      "Epoch 00060: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0244 - acc: 0.9926 - val_loss: 0.2333 - val_acc: 0.9420\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0121 - acc: 0.9968 - val_loss: 0.2414 - val_acc: 0.9446\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00062: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0165 - acc: 0.9954 - val_loss: 0.2675 - val_acc: 0.9392\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9946\n",
      "Epoch 00063: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0211 - acc: 0.9945 - val_loss: 0.2427 - val_acc: 0.9448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917\n",
      "Epoch 00064: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.2168 - val_acc: 0.9462\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9979\n",
      "Epoch 00065: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0087 - acc: 0.9979 - val_loss: 0.2337 - val_acc: 0.9504\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9934\n",
      "Epoch 00066: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0230 - acc: 0.9933 - val_loss: 0.2207 - val_acc: 0.9511\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9958\n",
      "Epoch 00067: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.2173 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9937\n",
      "Epoch 00068: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0205 - acc: 0.9937 - val_loss: 0.2183 - val_acc: 0.9502\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9980\n",
      "Epoch 00069: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0093 - acc: 0.9980 - val_loss: 0.2269 - val_acc: 0.9481\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 00070: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.2318 - val_acc: 0.9499\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9954\n",
      "Epoch 00071: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0163 - acc: 0.9954 - val_loss: 0.2393 - val_acc: 0.9462\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9979\n",
      "Epoch 00072: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0088 - acc: 0.9979 - val_loss: 0.2321 - val_acc: 0.9485\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
      "Epoch 00073: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.2505 - val_acc: 0.9448\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9970\n",
      "Epoch 00074: val_loss did not improve from 0.19261\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0114 - acc: 0.9969 - val_loss: 0.2540 - val_acc: 0.9413\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XecVOW9+PHPM2VnZmd7AZZeVaQtRcVgwRiNJUGNBY3eRE30mutNYsw1Qb3xZ6omMc00r4VEjSXGctVINNcERI0NEAUURHrZXba32enf3x/PzOwCu8sCO8zCfN+v14GdmTPnfM+Zmef7PM855zlGRFBKKaUAHJkOQCml1MChSUEppVSKJgWllFIpmhSUUkqlaFJQSimVoklBKaVUiiYFpZRSKZoUlFJKpWhSUEopleLKdAD7q6ysTEaPHp3pMJRS6rCyfPnyOhEp39d8h11SGD16NMuWLct0GEopdVgxxmzpy3zafaSUUipFk4JSSqkUTQpKKaVSDrtjCt2JRCJs376dYDCY6VAOW16vl+HDh+N2uzMdilIqg46IpLB9+3by8/MZPXo0xphMh3PYERHq6+vZvn07Y8aMyXQ4SqkMOiK6j4LBIKWlpZoQDpAxhtLSUm1pKaWOjKQAaEI4SLr/lFJwBCWFfYnFOgiFdhCPRzIdilJKDVhZkxTi8SDhcBUi/Z8Umpqa+N3vfndA7z3nnHNoamrq8/y33347d9111wGtSyml9iVrkoIxdlNF4v2+7N6SQjQa7fW9ixYtoqioqN9jUkqpA5E1SaFzU/s/KSxYsIANGzZQWVnJTTfdxJIlSzj55JOZN28exx57LADnn38+M2fOZNKkSdx7772p944ePZq6ujo2b97MxIkTueaaa5g0aRJnnnkmHR0dva535cqVzJ49m6lTp3LBBRfQ2NgIwN13382xxx7L1KlTufTSSwF45ZVXqKyspLKykunTp9Pa2trv+0Epdfg7Ik5J7Wr9+htoa1vZzSsxYrEADocPY/Zvs/PyKpkw4Zc9vn7nnXeyevVqVq60612yZAkrVqxg9erVqVM8Fy5cSElJCR0dHRx33HFceOGFlJaW7hH7eh577DHuu+8+LrnkEp566imuuOKKHtf7hS98gV//+teceuqp3HbbbXz3u9/ll7/8JXfeeSebNm3C4/Gkuqbuuusufvvb3zJnzhza2trwer37tQ+UUtkhi1oKybNr5JCs7fjjj9/tnP+7776badOmMXv2bLZt28b69ev3es+YMWOorKwEYObMmWzevLnH5Tc3N9PU1MSpp54KwBe/+EWWLl0KwNSpU7n88sv505/+hMtlE+CcOXO48cYbufvuu2lqako9r5RSXR1xJUNPNfp4PEx7+/t4PKPIydnn6LEHze/3p/5esmQJL7/8Mm+88Qa5ubnMnTu322sCPB5P6m+n07nP7qOevPDCCyxdupTnn3+eH/7wh6xatYoFCxZw7rnnsmjRIubMmcNLL73EMcccc0DLV0odubKopZC+Ywr5+fm99tE3NzdTXFxMbm4ua9eu5c033zzodRYWFlJcXMyrr74KwMMPP8ypp55KPB5n27ZtnHbaafz4xz+mubmZtrY2NmzYwJQpU/j2t7/Ncccdx9q1aw86BqXUkeeIayn0JJ1nH5WWljJnzhwmT57M2Wefzbnnnrvb62eddRb33HMPEydO5Oijj2b27Nn9st4HH3yQ6667jkAgwNixY/nDH/5ALBbjiiuuoLm5GRHha1/7GkVFRXznO99h8eLFOBwOJk2axNlnn90vMSiljixGJD197MaYEcBDwGBsR/69IvKrPeYxwK+Ac4AAcKWIrOhtubNmzZI9b7Lz4YcfMnHixF7jERHa2paTkzMEj2f4/m5OVujLflRKHZ6MMctFZNa+5ktnSyEKfFNEVhhj8oHlxpj/E5EPusxzNjAhMZ0A/D7xf7+z+ceZlpaCUkodKdJ2TEFEqpK1fhFpBT4Ehu0x23nAQ2K9CRQZYyrSFZMxDk0KSinVi0NyoNkYMxqYDry1x0vDgG1dHm9n78SBMeZaY8wyY8yy2trag4jEQToONCul1JEi7UnBGJMHPAXcICItB7IMEblXRGaJyKzy8gM/ndS2FGIH/H6llDrSpTUpGGPc2ITwiIg83c0sO4ARXR4PTzyXJk60paCUUj1LW1JInFn0APChiPy8h9meA75grNlAs4hUpS8mPaaglFK9SefZR3OAfwNWGWOSgxHdAowEEJF7gEXY01E/xp6SelUa40kkhYFxP4W8vDza2tr6/LxSSh0KaUsKIvIanQMO9TSPANenK4a9aUtBKaV6k0XDXIAxTqD/DzQvWLCA3/72t6nHyRvhtLW1cfrppzNjxgymTJnCs88+2+dligg33XQTkydPZsqUKfz5z38GoKqqilNOOYXKykomT57Mq6++SiwW48orr0zN+4tf/KLft1EplR2OvGEubrgBVnY3dDbkxEO4JALOvP1bZmUl/LLnobPnz5/PDTfcwPXX20bPE088wUsvvYTX6+WZZ56hoKCAuro6Zs+ezbx58/p0P+Snn36alStX8t5771FXV8dxxx3HKaecwqOPPsqnP/1pbr31VmKxGIFAgJUrV7Jjxw5Wr14NsF93clNKqa6OvKSwT4Kwj36t/TR9+nR27drFzp07qa2tpbi4mBEjRhCJRLjllltYunQpDoeDHTt2UFNTw5AhQ/a5zNdee43LLrsMp9PJ4MGDOfXUU3nnnXc47rjjuPrqq4lEIpx//vlUVlYyduxYNm7cyFe/+lXOPfdczjzzzH7cOqVUNjnykkIvNfpIqIpweAd5eTPA9G/P2cUXX8yTTz5JdXU18+fPB+CRRx6htraW5cuX43a7GT16dLdDZu+PU045haVLl/LCCy9w5ZVXcuONN/KFL3yB9957j5deeol77rmHJ554goULF/bHZimlskyWHVNIjpTa/8cV5s+fz+OPP86TTz7JxRdfDNghswcNGoTb7Wbx4sVs2bKlz8s7+eST+fOf/0wsFqO2tpalS5dy/PHHs2XLFgYPHsw111zDl7/8ZVasWEFdXR3xeJwLL7yQH/zgB6xY0euYgkop1aMjr6XQK2fi//4/A2nSpEm0trYybNgwKirs8E2XX345n/3sZ5kyZQqzZs3ar5vaXHDBBbzxxhtMmzYNYww/+clPGDJkCA8++CA//elPcbvd5OXl8dBDD7Fjxw6uuuoq4nG7XXfccUe/b59SKjukbejsdDnQobMBIpEGgsGN5OZOwun0pSvEw5YOna3UkauvQ2dnVfdROu++ppRSR4LsSQqNjbhWbcARSs/d15RS6kiQPUnBGExc7D3g0nABm1JKHQmyJyk47KaauLYUlFKqJ1mXFNCkoJRSPcq6pGAE9ECzUkp1L+uSgm0p9O8xhaamJn73u98d0HvPOeccHatIKTVgZE9ScNoL19LRUugtKUSj0V7fu2jRIoqKivo1HqWUOlDZkxRSB5pNvx9TWLBgARs2bKCyspKbbrqJJUuWcPLJJzNv3jyOPfZYAM4//3xmzpzJpEmTuPfee1PvHT16NHV1dWzevJmJEydyzTXXMGnSJM4880w6Ojr2Wtfzzz/PCSecwPTp0/nUpz5FTU0NAG1tbVx11VVMmTKFqVOn8tRTTwHw4osvMmPGDKZNm8bpp5/er9utlDryHHHDXPQ8crYDWo8m7gZy3KnepL7Yx8jZ3HnnnaxevZqViRUvWbKEFStWsHr1asaMGQPAwoULKSkpoaOjg+OOO44LL7yQ0tLS3Zazfv16HnvsMe677z4uueQSnnrqKa644ord5jnppJN48803McZw//3385Of/ISf/exnfP/736ewsJBVq1YB0NjYSG1tLddccw1Lly5lzJgxNDQ09H2jlVJZ6YhLCj0zqX/t4Nnpdfzxx6cSAsDdd9/NM888A8C2bdtYv379XklhzJgxVFZWAjBz5kw2b96813K3b9/O/PnzqaqqIhwOp9bx8ssv8/jjj6fmKy4u5vnnn+eUU05JzVNSUtKv26iUOvIccUmhtxo9KzcQyReiQ/Px+canNQ6/35/6e8mSJbz88su88cYb5ObmMnfu3G6H0PZ4PKm/nU5nt91HX/3qV7nxxhuZN28eS5Ys4fbbb09L/Eqp7JQ9xxTAHldIwzGF/Px8Wltbe3y9ubmZ4uJicnNzWbt2LW+++eYBr6u5uZlhw4YB8OCDD6aeP+OMM3a7JWhjYyOzZ89m6dKlbNq0CUC7j5RS+5R1ScFI/1+8Vlpaypw5c5g8eTI33XTTXq+fddZZRKNRJk6cyIIFC5g9e/YBr+v222/n4osvZubMmZSVlaWe/+///m8aGxuZPHky06ZNY/HixZSXl3Pvvffyuc99jmnTpqVu/qOUUj3JqqGz+eADYo4wwRE5+P3HpinCw5cOna3UkUuHzu6Ow6HDXCilVC+yKyk4nYmL13SUVKWU6s4Rd/ZRrxwOiAuHW5eZUkodKtnVUkh0H+mAeEop1b2sSwomLoDocQWllOpG1iUF4rbrSJOCUkrtLbuSgtOJkeQtOTObFPLy8jK6fqWU6k52JYXkKHhpuIBNKaWOBFmZFEw/H2xesGDBbkNM3H777dx11120tbVx+umnM2PGDKZMmcKzzz67z2X1NMR2d0Ng9zRctlJKHagj7pTUG168gZXV3Y6dDZEIBIPE3gWHKxdjnH1aZuWQSn55Vs8j7c2fP58bbriB66+/HoAnnniCl156Ca/XyzPPPENBQQF1dXXMnj2befPmYYzpcVndDbEdj8e7HQK7u+GylVLqYBxxSaFXuxXG/XetwvTp09m1axc7d+6ktraW4uJiRowYQSQS4ZZbbmHp0qU4HA527NhBTU0NQ4YM6XFZ3Q2xXVtb2+0Q2N0Nl62UUgfjiEsKvdXoaW6G9etpHwk5xeNwu/uvEL344ot58sknqa6uTg0898gjj1BbW8vy5ctxu92MHj262yGzk/o6xLZSSqWLHlPoJ/Pnz+fxxx/nySef5OKLLwbsMNeDBg3C7XazePFitmzZ0usyehpiu6chsLsbLlsppQ5GViYFe/ZR/45/NGnSJFpbWxk2bBgVFRUAXH755SxbtowpU6bw0EMPccwxx/S6jJ6G2O5pCOzuhstWSqmDkV1DZ3d0wJo1dFSAs3w4OTk99+1nIx06W6kjlw6d3Z0u3Ud6nYJSSu0tbUnBGLPQGLPLGLO6h9fnGmOajTErE9Nt6YolxZk4BVUvXlNKqW6l8+yjPwK/AR7qZZ5XReQz/bEyEen1/H+gs6UgBtGRUndzuHUjKqXSI20tBRFZChySO8V7vV7q6+v3XbAlkoaJm34/0Hw4ExHq6+vxer2ZDkUplWGZvk7hRGPMe8BO4L9EZE13MxljrgWuBRg5cuRerw8fPpzt27dTW1u77zXW1xMLGOKtbbjdeg1AktfrZfjw4ZkOQymVYZlMCiuAUSLSZow5B/hfYEJ3M4rIvcC9YM8+2vN1t9udutp3nz75SWpnR6j67mwmTvzrgcaulFJHpIydfSQiLSLSlvh7EeA2xpSlfcV+P86Qg3i8Pe2rUkqpw03GkoIxZohJHBk2xhyfiKU+7SvOzcUZNMRimhSUUmpPaes+MsY8BswFyowx24H/B7gBROQe4CLgK8aYKNABXCqH4hQYvx9nB5oUlFKqG2lLCiJy2T5e/w32lNVDy+/HUS+aFJRSqhvZdUUz2KQQjOsxBaWU6kamT0k99Px+HB0xYrFQpiNRSqkBJztbCh1R4vEOHepCKaX2kLVJASAWC2Q4GKWUGliyMimYQBgEPa6glFJ7yL6kkJuLicUxET0tVSml9pR9ScHvB8AZ1KSglFJ70qSglFIqJauTgh5TUEqp3WVtUnBoS0EppfaStUlBu4+UUmpvmhSUUkqlZG1ScHToMQWllNpT1iYFbSkopdTesi8p5OYC6I12lFKqG9mXFBItBVcoh3hcxz5SSqmusjcphN3aUlBKqT1kX1JwOsHjwRVyaVJQSqk9ZN9NdsDepzmkxxSUUmpP2ddSAPD7cQUdekqqUkrtIWuTgp59pJRSe8vapODQpKCUUnvJzqSQm4szKJoUlFJqD9mZFPx+HB1xPaaglFJ7yNqk4OyIa0tBKaX2kLVJwQRjxGLtiEimo1FKqQEja5OCoyMCxInHQ5mORimlBoysTQomEAF0+GyllOqqT0nBGPN1Y0yBsR4wxqwwxpyZ7uDSxu/HEYxAXIfPVkqprvraUrhaRFqAM4Fi4N+AO9MWVbol76kQ0qSglFJd9TUpmMT/5wAPi8iaLs8dfrrcfU2TglJKdeprUlhujPk7Nim8ZIzJB+LpCyvNUjfa0WMKSinVVV9HSf0SUAlsFJGAMaYEuCp9YaWZ3pJTKaW61deWwonAOhFpMsZcAfw30Jy+sNIs2X0UhFisNcPBKKXUwNHXpPB7IGCMmQZ8E9gAPJS2qNKtS0shFNqe4WCUUmrg6GtSiIq99Pc84Dci8lsgP31hpVkiKeRE/HR0bMxwMEopNXD09ZhCqzHmZuypqCcbYxyAO31hpVkiKXhi5bQHNSkopVRSX1sK84EQ9nqFamA48NPe3mCMWWiM2WWMWd3D68YYc7cx5mNjzPvGmBn7FfnBSCQFb7RUWwpKKdVFn5JCIhE8AhQaYz4DBEVkX8cU/gic1cvrZwMTEtO12OMWh0aypRAtIhjcjEjskK1aKaUGsr4Oc3EJ8DZwMXAJ8JYx5qLe3iMiS4GGXmY5D3hIrDeBImNMRd/CPkipYwr5iIQJhXYektUqpdRA19djCrcCx4nILgBjTDnwMvDkQax7GLCty+PtieeqDmKZfePxgDG4wz4AgsGNeL0j0r5adfgTgXgcIhGIRu0kAoWF4OimiiUCtbXQ2grhcOeUnw/jxoG7myNz4TDU1Nh1xGKdU5JJjCUQDEJ7OwQC9n+XC/LybJ0nL8/O19oKbW12CofB57PXbvp84PV2Liv5f3L7ROyU3Mbk9hYXw5gxUFLS+Z6u29raCtu2wdat9v+aGht7crnG2NgKC+1UUGCfD4Xs9gSDdh6Px05eLzidnXFEo3Y/l5fbadAguy9ra6G6Gqqq7N9Op31vcsrJsZPbbf/3+6GoyMbgdtsYtm+Hjz+GDRugoQGGDIGhQ+1UVmb3c0tL5z71eOy25OXZfdrc3BlDVZX9TJL7UcSut6ioc/J67bJaWuyUnL8rl8vGl4x7xgw44YQD++72VV+TgiOZEBLqOYQjrBpjrsV2MTFy5Mj+WCD4/bjDHgA6OjZSVHTqwS9X9UkwaAuNnTvtR+FydU7BoP3BJQs7n88WQKWldmpthXXrOqfqalvYRSJ2isftD9Tv75xyczsnr9f+eGtr7VRXZwuQ5A+1uNgup7q6c2ps7CzMI5G9f7hgf7TDhsHw4bYwaWy027h1qy3wuuNy2cRw9NF2vZs3w8aNtnAa6Lf5KCiwySEvzxagjY126mlbwX7WA3G78vLs59pb7AfCmN2naPTgl7lgwcBJCi8aY14CHks8ng8sOsh17wC6Vs+HJ57bi4jcC9wLMGvWrP75Wvn9OEMuwEFQz0BKicVgyxY7gS24nE47JV+Px+3/tbWdtcLt222hWVDQOTmdtgBOTrW1drk7+6m3bvBgWwh7PLZQzs21P75AwBZU7e2dySUQsPHB7jXNsjK7PZs2QVOTLdicTqiosIX78cfbpJSsaebk7F57c7lsQVdTAzt22P2wapUt5GfMgPPPhxEj7OOutdWGBpvU1q61U1OTLWTnzoWxY22C8Xg6973DsXuhKtJZ608mvliss1XQ1mbnyc+3hV5+vl1vR4edAgGbgJPLSv6fXE9ySm5jcqqrs/tq0yabwDo6YNIku33FxXZ/jhxpt3nkSLsPXV1KGRG77uR3oqXFbl+yVeCx9bRUqyEUsoVpMg632xbgdXWwa5f9TrW02M+yosJOgwbZzzS5jI6OzoSe/L+tze7z5GfucsH48TZJjx9vKyA1Nfa7unOnXZ/fb/djQYH9OxzurMC0tdnnkzEMGZLqpd7tt9XS0rnejo7dfy9+/+6tzWRLLVnhiUTsPkq3PiUFEbnJGHMhMCfx1L0i8sxBrvs54D+NMY8DJwDNIpL+rqMkvx8T6MDrHXnEnoEkYguqlSttIZQsYJxO+4Wur7dTXZ1t7q5bZ5vPycKzr3JzbSHg8XQ2h5ub7Q+zoMA20YuKbOF65pm28Bs92hboyRpUsnvC6929lt/RYWNPxurz2Zr1UUfZZe6PWMwWEj5f9109Kv0SjXT8ftstM5Dl5dkk0V+czs7k2Vcu16FJBLuts68zishTwFN9nd8Y8xgwFygzxmwH/h+JaxtE5B5sS+Mc4GMgwKEeS8nvh/Z2vN6xh11Lob3d1mKSU7LPOtm33NZma58rV9oCvzfG2C/p4MEwYQKce64tdMeM2b0vNxaz8zoc9nljbK0wWQvurn85ufyBwuncu/am1J5EhNZwK9F4tMcpFo/hcXnwuXzkunPJdefidXkxPXzhk7f97en1gaTXpGCMaQW6664xgIhIQU/vFZHLelt24grp6/sSZFokkoLPN5a6uucyFsaekl0RySb6pk22y2XbNts1sW2brYX3xO22mzZuHJx3HkyfDpWVtjnb9aCl222byMXFnV1D/anrdz8Si7CleQv1gXqGFQyjIq8Cp2P/VioibG3eSkNHAy6HKzUFo0Gq2qqoaq1iZ+tOOqIdTB40melDpjOuZBwO09kkiMQiNHQ04HP7yM/J3+0H2hRsYn39etY3rCcUDTE4bzCD/IMY7B9MviefUDREMBokGLV9LiMLR+Jz+3qMNy7x1PzBaBCvy0uhp3C37W4NtbKhcQMbGjZQ016DwWCMwWAQhPZwO23hNlrDrXREOvC4PPjdfvw5fvxuP3k5eRR4Csj35JOfk4/L4UIQRARBcBonXpc3NUXiEbY0bWFL8xa2NG2hNlCL1+VNLTPXnbvb/kp9ll3i6oh20BRsoinYRHOomUgsQq47N7UMh3FQH6inNlBLXaCO1nBrKs5CTyF5OXlE49Hd9g2A0+HEYRw4jZNB/kFMKJnAhNIJHFV6FG6Hm3X161hXt4519eto6GhgsH8wFfkVDM0fSqmvlPZIO83BZppDzbSH2xmcN5iRhSMZVTiKEYUjaA42s7lpc2pqCjZ1xhAL0hRsorqtmuq2amraaojEI/v1/Uzup7ycPPw59rMREQKRAO2RdtrD7TgdTspzyyn3l1OeW06+J99+vqFWWkItBKNBin3FDPIPsvMl5i3LLUtNY4vHMiRvyH7Htj96TQoicvgOZbEvXVoKkcguotE2XK68QxpCMAgffGBr9O+91zk1Ne0+36BBtkY+frztcx42zBbygwfbqbzcdtPk5trCvjXUis/tw+XY++ONxqO8uuVV3tz+JhvWbUgVSo3BRoblD2NU0ShGFY5iSN4QmoJN1AZq2dW+i/pAPXGJ2x+uw4nTOPcqQJwOJznOHDxODznOHAKRABsaN7C1eStx6Rxp3WmcDCsYxsjCkYwuGs2YojGMLR7LqMJRCEJrqDVVGH5U/xHvVr/LyuqVNAWb9tycvSQLVID8nHwmlk+kNdRqt6GjPjWfy+GixFdCia8kVYjtr2H5wxhbPDZV6Oxq30VNew272nelCrs9FXoKKfGVEIgEqGmv6dN6ct25+Fw+QrEQ7eH21PYdrCJvEaFoiI5ox36/t8BTQJG3CKdx7lbwAZTmllKWW0apr5Ty3HLawm1sadpCc6iZ1lArbqc7lag8Tg/GGGLxGHGJE41HqWqroiXU0u1683PyGeQfRHVbNe2RAx/h2O/22/W7PHhdXvJz8qnIr2DyoMkM8Q+hNLcUt8ONy+HC7XTjNE7cTneqMuIwDsKxMIFIgI5IR2oftIXbUt9dg9kticckRm17LbsCu6htr2VH6w7yc/LJ99h1e5weGjoaqG6rZlXNKna17yIU2/3o902fuImfnPGTA97uvuhz99ERx++Hujp8vrEABIObyMubkrbVhcOwbBksXQrvvgvvvw8ffWT73cmrJmfcG5ROexPfnDfJ8dZQ7CtiUH4JFcXFFPls7SocD1Mbi1CLramGisfhLhqH1zuUf21bxdItS3l166us2rWKvJw85oyYw8kjT+bkUSfT0NHAM2uf4a8f/ZWGDnv5yGD/YMaVjOPU0adS6itle8t2tjZvZWX1Sna176LQU2hrLf5yRhWNwmmcxCRGLB4jJrFUkzgpJjFC0RBNkSZCsRAep4cTh5/IFVOuYGzxWMpyy9jZupNtLdvY2ryVrc1bWbplKY+uenS3pNGVz+Vj6uCpzJ80n8ohlQzJG0IsHks1491ONxV5tsZYkV+B0zhZU7uGd6ve5d3qd1lbt5bhBcOZO3oug/2DKcstIxgNUt9RT32gnoZgA0WeIo4qPYoJpROYUDIBn9tnC/i2Gmraa2gLt+Fz+VIFWUxibG7anEqob2x7gyJvEYPzBjOxfCKDcgeR78nfreALRoM0dDTQGGykoaMBj9PDhNIJjC8Zz/iS8VTk2Ut04hJHkFStM9edu1vrQkQIxUKpwqcl1JKqaUbjURzGkarVJz+PZI3YYRy29lw0iuEFw/G6vKl1Jgu2PRNOstWR/N/n8lHgKei2pZecp7vWxv4QEWoDtXxU/xEf1X9ELB7jqNKjOLrsaAb7B6daeK2hVqraqqgP1JOXk0eht5BCTyE+t4+athq2NG9ha/NWtjVvo9BbyOii0YwuGs3IwpHkunMPKsZDQURoj7RTF6hLTSMK0n/qvNnzhz3QzZo1S5YtW3bwC7rsMli2jJblj7JixfFMnvy/lJWdd/DLTYhEYMm/2rj/lUUs3/Qxmxu3EfNvhYLtuPytuDxhHO4Q4gzREbfDd7sdbmZUzGBk4Uiagk2pQqQt3EaOMwe3w43b6SYucbY1b9urFuF3+5kzcg6fGP4JatpreHXrq6ze1TnKSLG3mM8c9RkuOOYCTh97OgWeHnv/Uq2CQyEcC6eShMvhIi8nLzUN8g/qtsWjlNo/xpjlIjJrX/Nl76+tyzEFgI6OTQe9yB074LHHhKffWsY7sfuJHvMYeFphNHhHljIydwRHDRlFeUGMcQE3AAAgAElEQVQhOY4cPC4PHqeHEYUjOHH4iUyvmJ6qve1LXOLsaNnBxsaNbGvZxjFlx1A5pHKvArQ+UM+/tv0Lf46fk0eejNvZt3EMD1VCAMhx5qRqzEqpzMrupBAI4HKV4HQW7PcZSNF4lJXVK3mv+n3+9X4NS5fXsKG6BilfA5NX4RIfpxXP58ZPXs1pR8/An9O/p704jIMRhSMYUdh7c7I0t5TPHv3Zfl23UurIld1Job0dYwxe75g+XavwUf1HPLbqMV7b9hpvbHtjtwNdpiCfwpLBHF0xnCtn/Z7LJl9GobcwnVuglFL9LruTQjQK4TA+31gCgbU9zioi/Obt3/Ctl79FKBpipGcq7tVXwfsnMbl0FjdcPZTL5/sO+UUmSinV37I7KUDqtNSGhr8hEsfs0Zde1VrF1c9dzYsfv8jsknNoeeQ+PnhrKJMnwx9/CPPmDawLtJRS6mBoUkgcbI7Hg4TD1Xg89tp7EeGZtc/w73/9d9rCbcxz/Zbnb/gKo0cZ/vQnuPTS9Fz0pZRSmaRJob0db1nyDKSNeDxDWVe3jq+/+HVe2vASU8unU7rsEZ57YiLz58P999sxUZRS6kikSaG9Hd8ImxTqWtZwxzvP84s3f4HP7eNbU37Jk9/+Dz7Y7OZXv4KvflW7ipRSR7bsTQq5iSsa29vxeifRFoXT/3IzW9sauaryKq4bfwdnnzIYrxeWLIE5c3pdmlJKHRGyNykkWwqBAA6Hhwe35rGtrZGX/+1lZg8+nRNPtENQvPKKHXNIKaWygSaFtjaW71zO09vauGR0BZ8cczrz58OaNbBokSYEpVR2yd6kkLitZ+yjdVzXcCclXh9Xj47z4x/DX/4CP/kJfPrTGY5RKaUOsexNCok7kN+z5UmWhd/l7lMu5t03W7nlFuHSSw3/9V+ZDlAppQ697E0KQPUJk7il9AU+NfZTnDPyQqafdwZTpgR54AGfnmWklMpKWX2n2m8eu42QQ/jdSXfw/PPH0dpawh13rEidmKSUUtkma5PCxw0f82j8Pb71Oozf2MLChSMYP/5dpkzph3s1KKXUYSprk8LaOjsA3rkfwb+ermbVKjcXXPAw7e0rMxyZUkplTtYmhQ0NGwAY6x/G718YSUEBXHRRFU1Nr2Q4MqWUypysTQobGzeSl5OHjPskf9l6PF/8IlRUnEAwuIlgcFumw1NKqYzI3qTQtJFxxeP4Q/SLhCWH665oo6joVABtLSilslb2JoXGjYwpGss975/IXBZzbMdy8vKm4nIV0dysSUEplZ2yMinEJc7Gxo2YprFsrsnlP/gdLF+OMU4KC0/WloJSKmtlZVKobqsmGA3y0VtjGTIEzh/6DqxYAUBR0al0dKwnFNqZ4SiVUurQy8qksLFxIwBrXhvHl78M7plTYflyAAoL9biCUip7ZWVSSJ6OSsNYzjwTmDkT1q2Dtjby8ipxOvM1KSilslJWJoWNjRsxGGgeZYfGnjkTRGDlShwOF4WFJ+nBZqVUVsrOpNC0kbzYCPzeHIYMAWbMsC8kupCKik4lEFhLOFyTuSCVUioDsjMpNG4kp30c48cn7rk8dCgMGdLlYPNcAJqalmYuSKWUyoCsTAobGjYQ2TV297uqzZiRaink5c3A4fDrcQWlVNbJuqTQHm6npr2Gtm1jmTChywszZ8KHHybu2eymsHCOHldQSmWdrEsKm5o2ARCv76alEI/De+8B9rhCe/tqwuG6DESplFKZkXVJIXmNAg3j9m4pACyz91NIjoPU3PzqIYxOKaUyK+uSQuoahcY9WgrDh8Mxx8BDD4EI+fnH4XD4aGpakokwlVIqI7IuKWxs3EiOFOAzJVRUdHnBGPj6121L4bXXcDhyKC7+FLW1TxCPRzIWr1JKHUrZlxSaNuLtGMuE8caejtrVF74AJSXw858DUFFxLeFwNfX1zx36QJVSKgPSmhSMMWcZY9YZYz42xizo5vUrjTG1xpiVienL6YwHbPdRvG7c7l1HSbm58JWvwLPPwscfU1p6Nh7PCHbu/J90h6WUUgNC2pKCMcYJ/BY4GzgWuMwYc2w3s/5ZRCoT0/3pigfskNmbmjYR2LHH6ahdXX89uFxw990Y46Si4hoaG/+PQODjdIamlFIDQjpbCscDH4vIRhEJA48D56Vxffu0s3Un4Vh479NRu6qogMsug4ULobGRioovAU6qqu47lKEqpVRGpDMpDAO63ux4e+K5PV1ojHnfGPOkMWZEGuPpPB21sZeWAsA3vgHt7XDffXg8Qykrm0d19ULi8VA6w1NKqYzL9IHm54HRIjIV+D/gwe5mMsZca4xZZoxZVltbe8Ar6xwyu4djCkmVlfDJT8Kvfw2RCEOH/juRSB21tc8c8LqVUupwkM6ksAPoWvMfnnguRUTqRSRZ/b4fmNndgkTkXhGZJSKzysvLDzigjY0bMeLAGx7J0KH7mPkb34Dt2+HJJykuPgOvdww7d95zwOtWSqnDQTqTwjvABGPMGGNMDnApsNu5ncaYrlcKzAM+TGM89nTU8EgmjHXvfTrqns45B44+Gn70I4zY01Obm1+hvX1tOkNUSqmMSltSEJEo8J/AS9jC/gkRWWOM+Z4xZl5itq8ZY9YYY94DvgZcma54INFSaNxH11GSwwG33w6rV8Njj1FRcRXGuKmq0tNTlVJHrrQeUxCRRSJylIiME5EfJp67TUSeS/x9s4hMEpFpInKaiKS1Gr6hYQPBqn0cZO7qkkvs8YXbbiOHYsrLL6Kq6n5CoZ3pDFMppTIm0weaD5nWUCu1gdreT0fdk8MBP/whbNwIDzzAmDHfJx4Ps3HjzWmNVSmlMiVrkkJyyOx9no66p7PPhpNOgu9/H59UMGLEjdTUPERLy1tpiVMppTIpa5JCn09H3ZMxcMcdUFUFv/kNI0feQk5OBevXfxWReFpiVUqpTMmapHBs+bGcFLwDb2DCvk9H3dNJJ9mzke68E1dbjLFjf0xr6zvU1DyclliVUipTsiYpHF12NCUfLGD8iAIcB7LVP/gBNDbCj37E4MGXU1Awm40bFxCNtvZ7rEoplSlZkxQA1q9n/7qOupo+3Q6t/dOfYi66mAl5txEOV7Nlyw/6NUallMqkrEkKsRhs2MD+HWTe0wMPwJ13wqJF5B9/GUe/Ooft235GY+OS/gpTKaUyKmuSwo4dEA4fREsB7JDa3/42vP8+VFZScdvrTP8vDxsXna9DayuljghZkxTWr7f/H1RLIWnCBPjnP+F//of8DS6mX9lM4/UnEGnRi9qUUoe3rEkKkQhMnNhPSQHshW3XXotZ+xHRCz/NsD82ED92HPG/6q07lVKHr6xJCmedBR98AMOH9/OCBw8m5/EXqX/q20RdQRyfPQ/57ndBpJ9XpJRS6Zc1SSHdSj93J9V/+y+qPw3m9tuRr38d4npxm1Lq8KJJoR+NPebHtN39dbZdBObXv0a++AXbb6WUUocJTQr9yBgH4yb8gvhPf8TGL4H50yPEP3ceBAKZDk0ppfpEk0I/M8YwavTN+L5/Px99w2Be+BsyfRosW5bp0JRSap80KaRJRcWXKLnlf1n1czfhpo3IibPtUBnRaKZDUyq7BAL26lXVJ5oU0qisbB5jv/wOq/40il2nxOE730FOOQVuvhkuvRROOAEGD4a5c2Hp0vQEEY/DE0/A4sXpWb5S6VRVZe9nciBaWuCmm6CwEKZOhUWL9KzAPtCkkGZ5edOoPO1dan/1OT64FeKrlyF33WW7k/Lz4dxz4aOP4NRT7XmzXbuZOjrs2Bwf93C1dDgMd98NkybBNdfAh3vc4nrFCjvC6/z58KlPwX33pW9Ds1EkArfdBgUFdhTdZ57REwv6iwg89JC9T/qkSfD4431/bzwODz4IRx0FP/uZvYNiOGx/a2eeCe+9l764+8sHH8Ctt9rx1s45B447DsaMscP4p5uIHFbTzJkz5XAUj8dl69ZfyCsvO+W1V0plx457JR6P2hcDAZGf/lSktFQERI46SqS42P6dnKZNE/n5z0Wqq0ViMZFHHxUZM8a+NmOGiNdr/z73XJFFi0Suu07EGJFBg0Tuu0/k7LPt6//v/4nE4xndF91avVrkwgtFvvAFuy9efFFk5870r7e6WqSjY//ft2aN3e8gctZZIkOH2r+HDBFZsEBk2TL7OXUVj4u89prIf/6nyPe/LxIO98827GnXLpHFi0Uef1zk4YdFFi4U+Z//EXn66QNfZyQi8uSTIn/6k8iHH+69bQdq2zaRlpbdn6uvF7n4Yrs/TzrJTiDy3//d/XrDYfv9efRRkW9/u/NzOeEEkbfftvOEQiK//KVISYn9XXzpSyI1Nf2zDfujqUnk9ddF7r1X5Ic/FPnLX2zsoZCdHntM5JRTbPwul8ioUSIzZ9rv2BVX2PkPELBM+lDGZryQ39/pcE0KSS0t78qKFSfL4sXIO+/MkKam1ztfbG62hcUFF4hcf73Ij34k8sc/itx9t8hxx9mPy+nsTAbTptnCMx63BcF3vytSXt4539e/LtLYaJcdDotcfbV97Utfsj/ynsRiIuvW9d8PvzexmMjPfibi8dgfbLJwTU5z59ofzYFYvVrkpptEXn5570TY0iLyrW+JuN0ixx5rt7c7O3fawvRvfxN55RWRd96xydnjESkrE3nqKTtfJCLy3HMin/2siMNhYy8vF/n850X+8AeRW24RGT3aPu/x2P9POUWkqurAtq2r1laR22+3+yr5+fc0jRxp408WxNGoTSDXXy8yZYrIV75iC63k/opGbWI56qjdl5OXJ3LyyfY7tz+Fa2Oj3Z/XXScydqxdljEiEyaIXHKJrbQMHWo/lzvusOsPhTq/u5/7nP2dvP22/X2cdlrn/gT7vspK+7vp7vvb0CBy4422wC0oELnrLrv85H5cuNBuV1mZTUx//OP+J4/qalsx+/WvbZK6/HKRU08VGT6858/F6RTJz7d/jxkj8uMf2990P9KkMIDF43Gprn5UXn99mCxejKxZ83np6Niy7zd+8IGthc6dK/LQQ91/6QMBkSee6L4gjcdFvvMdSdXAnnyy8wchYgu2hx4SmTjRzjNunMhPfiJSW9t9PNGorTH/4Q+2UPnWt2zB2VvC6WrTJvtjAZHzzuv88dXV2YLqjjtsi8nlEvnmN3evUba02HW9/PLu2yAiEgyK3HabLSCSP7rKSlvLDYXsNg4ZYp+fP9+20AoKRJ59tnMZHR0iP/iBSG5u9z/iz3ym5wK9utqu4/LLOwtph0PkzDPt8y0tNhafzxaA//pX3/bXnuJxm5RGjJBUzfhLX7KF/ksv2c/mo4/sft6+XeT55ztroYWFthAeNMg+9vnsZ+HzdX723/xmZzKYOtV+X95/v/PzPv74ziT35S/b76dIZ8398cft53DFFSKf+ETnPk8mlXnzbO39e9+zFaFk0pw4UWT58r239ec/t/vR6exczrRpIt/4hk1c77+/93ehJ2vXdraejzrKtlD9/s7Hl1/eGa8xItOn23ivvtpWNO64wyal735X5NZb7b76zGf2rtS43Xa7TjrJ7oc77rCVh40b7fdg+XL7Xbj1VrsPFy1KW2VMk8JhIBJplQ0bbpUlSzzyyite2bDhVolEWvb9xoP1wAMiw4bZj7+0VORrX7O1mmTNbcoUmwxOPrnzR3/ZZbYg+Pzn7Y/p+OPtD7vrjzxZCBcX2/kfeaSzpdLVxx/bH1F+vp0WLuy5S6u2VuSaa+wPs6JC5NJLRY4+2j5OrruoSOTKK0VeeEFkyZLOpHbFFbZ74oEHOp9LFvLHHy/y5pt2HZs32yZ6sovi6ac7W2Of+5wttN94Q+Qf/7AF6z//2fcuuFjMFlbdJZCVK+0+d7tFbr5Z5Le/tbE+8ojIn/9sC4sHHhD5/e/t5/Pww7ZluGyZLUyShdrUqbZ231dvvSVy0UW2NnzJJbZLoq3NvtbSYmvHp59u9/HUqTbx9FRQffihyL//e2f35YQJuydjY2zrZO5cW6D+8IciS5f23I3V3GwrGz35+99t99sjj9jke7BeeMEmgbw8Wyh3bSXFYnZff+97ImecYRPQsGG7t0yStXyfT2TSJJF/+zeRX/zCVliSXb0DhCaFw0hHx2ZZs+bzsngx8tprg2X79t9JNBpI70qjUdslcsklIjk59qtw3HG2ttz1i7xqlch//Ict6IuLbSE2a5b9kVx/vS1APvjAvqe52dYmr7yys4bscol88pMiv/qVLWzPOccWFE6nbZ5v3Ni3eN96y9Y2R4ywrYrvfc/+oJ97ztbyCgs7f6SjRtlt6yoWs/NffrlNQnv+WDs6OrsowP7AX375oHZxnzQ02C6n3rp8epry821Nu68ts/3V2tr3Qi3ZfTlvnm0xPvywyLvvHtjxmkMtHt+/fRiP2xZ5R0fvCWyA6WtSMHbew8esWbNk2RF6IVhLy9t8/PGNtLS8jttdxtChX2Ho0P/A4xmS3hU3NMCWLVBZCcb0zzLjcXj7bXj2WTslz4waMgSuvdZOw4b1z7oAQiF4+WXYtAmuvBLy8vZ/GSLwyCP2vParr7b3zzhUmpshGLTbEQzas5hycuzk8YDTaW8HW1dnp6YmOOMMqKg4dDGqw5oxZrmIzNrnfJoUBhYRobl5Kdu2/Zz6+ucxxs2gQfMZPPhyioo+icPhznSIB2b9eltgz51rCzql1CGlSeEIEAisZ/v2X1JT8zCxWCsuVynl5RcyaNB8CgtPweE4hDVZpdRhTZPCESQWC9LQ8CK1tX+mru454vEALlcxJSVnU1r6WUpKzsLtLsp0mEqpAayvSUGrmocBp9NLefn5lJefTywWoKHhb9TVPU9Dwwvs2vUo4MTnG0du7tHk5h6Nz3c0paVn4/H0Y5+9UioraFI4zDiduZSXX0h5+YWIxGhpeYuGhhdpb19DILCOhoa/IxLCmByGDLmKkSO/jc83JtNhK6UOE5oUDmPGOCks/ASFhZ9IPScSIxD4iB077qaqaiFVVfczePDnKSu7AIfDi8ORgzEePJ5hmiyUUnvRYwpHsFBoJ9u2/YydO+8hHt/7Rj8+33hKSs6mpOQsiorm4nTmZiBKpdShoAeaVUok0kQwuBmREPG4nQKBtTQ0vEhT02Li8Q4cDh9lZecxaNDnKSn5NA6Hnjaq1JFEk4Lqk1gsSHPzq9TVPcOuXU8QjdbjcpVQVnY+Xu8oXK5iXK5i3O5iHA4vxuTgcHgwJofc3Ak4nf5Mb4JSqg/07CPVJ06nl5KSMygpOYPx439FY+Pfqal5hLq6p4lGm3p9rzEeiopOpbT0HEpKzsbjGUE4XEU4XE04XEU8HsHrHYnHMxKPpwJjnIdoq5RSB0pbCqpH8XiEaLSJaLSRaLQx0fUURiRMLBagpeUNGhoWEQis3eeyjHHh842nrOxCBg++HL9/4iHYAqVUknYfqUOmo2MTDQ0vEo02kpNTkZiGYIyLUGgbodBWgsGttLa+Q2PjP4A4eXmVlJVdiNOZh0gEkTAiMbzeMfj9U/D7J+JweBCJ0d6+mqamV2lpeR2ns5ChQ68hP39mpjdbqcOKJgU1IIVC1dTWPkFNzSO0tr7dy5xOfL7xhMPVxGLNAOTkDEu0WALk5c1k6NDrGDToYlyuwh6XIiJEIvWEwzsIhXYQDu/C5xtPfv4snE7vbvPG41ECgbU4HF58vnGY/hocUKkBQJOCGvAiEXvMwuFwY4wbMHR0fEx7+yra2t4nEFiD2z2IwsKTKSw8Ca93FLFYCzU1f2Lnzntob1+deL8Pt7sct7scl6uAWKyVaLQ5MTUhEt5r3cbkkJ8/i8LCTxCLBWhtXU57+3vE40EAvN6xlJScRUnJWRQWfgKXqxhjer6luYgQCKylsfHvNDe/Tm7uRMrLL8Lvn9xjchERWlvfoarqfoLBLZSVnUd5+UXk5Aw6yD17aMRiAWKxADk5ZZkORfXBgEgKxpizgF8BTuB+Eblzj9c9wEPATKAemC8im3tbpiYFBbZAbWl5g6ampUQitakpGm3F5SrA5SrE6SzE5SrC46kgJ2cYHs8w3O4yAoEPaG5+nebm12htXYbD4SUvbwb5+TPIy5tBLNZCQ8OLNDb+k3i8PbFGg9Npl+tyFeFyFSSWXwAYmpuXEgptB8DjGUEotAOI4/NNoLz8Qvz+aTideYnJT0vLW1RV3Ud7+/s4HLl4PMPp6PgIcFBcfDplZefhdg/q8p68RFIygANjHImzwsr3GhhRJE402oQxzn22onpOWDHi8VDijLPOZBiLddDQ8Dd27foz9fXPE493kJMzBL9/Knl5U8nLm0lJyZm43SUH/Nn2VTweprV1BcHgBgoKZuPzjdvne0SEWKyFYHALweBmgsFNdHRswuUqZMiQK7u9oDMabaWt7T283tF4PMO63WcisQF/IkXGk4Kxe+gj4AxgO/AOcJmIfNBlnv8AporIdcaYS4ELRGR+b8vVpKD6UzwexhhXt62AeDxEc/NrtLW9l2p1JKdYrIVotIVotBmREPn5J1BS8mmKi8/A5xtNOFxDXd3/Ulv7FI2N/wRiey3fdoFdw6BBl+FyFdDWtppdux5n167HCAY39nkbXK5ScnIGIxIjGq0nEmkA4onXivB6R+P1jsbpLEidGRYK7SQarQeciZZaDsa4EAkTjwcRiSaW7sTtLsXtLsPlKqa9/X1isVbc7nLKyy/G5xtLe/tq2trep719DSIhwElh4RxKSz9DcfGncLmKcTg8OBxewBAIrKW9/X3a2t6jvX0NTqdNih7PCDye4RjjIBKx2xGNNhCPh3E6/ankGIu10tz8Oq2tb6dadgBe7zhKSj5NUdFpiEQIh3cSClUltndH4vHOLonecjj8xOMdgFBSchZDh15HQcFs6usXUVf3dGroGACnswC//1h8vqOIxVoIhbYTCm0nHK7B4xlJUdGpFBXNpahoLl7v6N0SiIgQDG6hrW0Fra0riERq8PmOxu+fjN8/uduEYz/TVmKxZqLRFlyuYrze4X3+bnQ1EJLCicDtIvLpxOObAUTkji7zvJSY5w1jjAuoBsqll6A0KajDTSTSRDhcRSzWlpha8XpHk5c3tdv5RYRQaDuxWEuX97QhEgfi9paJxIhEGgiHa4hEagiHazDGhdtdlphKicfDiRrxJoLBzcRirakTATyeobjdZYjEEgf6I8TjkVThnRwSJRptJRKpS00+3zgGDbqUoqLT9mqhxONRWluX0dDwAnV1z9Pe/l6v+8UWsJOJx0OEQtuIRHbtMYcDt7sEY9zEYu3EYu1ADGNc5OVNp7BwDgUFc/D5xtHc/BoNDS8lLsbsvHrf4fCltjcnZygezzBycobi9Y7A6x2D1zsGt7uMUGg7VVX3U1V1P+HwztT7PZ6RlJd/jqKi0wiFtifGGPuAQGA9bndxIpkNJydnCIHAWpqalhCJ1AFgjBuHIxenMxeHIzdxFl9Dcutxu4tT8yZjtcUggCAS32skghEjvs24cbt1uPTZQEgKFwFniciXE4//DThBRP6zyzyrE/NsTzzekJinrrtlgiYFpQ4XweA2WlreIBYLEI8HUy2Q3NwJ+P1T96pJx2JBwuEdgG39uFwFu7Xg7O0i7fEhh8PT7Trj8RBtbe/hdOaTk1OBy1W4XycMxONR6uufJxD4gJKSs8jLm7Ff7xeJEwh8SFPTEkKhHYltt8denM7cVDel3z8Fp9NHJFJPe/sa2tvX0NHxMbaFl1yfA6czL9Flabsr/f5JB3w69xF18Zox5lrgWoCRI0dmOBqlVF/Y2viIPs/vdHp7PS5gjMEehuyZw+GhoOD4Pq9z7/e7KC+/ALjggN5vjCNRcE/q0/xudylFRadQVHTKAa0vHXo+neLg7QC6fiOGJ57rdp5E91Eh9oDzbkTkXhGZJSKzysvL0xSuUkqpdCaFd4AJxpgxxpgc4FLguT3meQ74YuLvi4B/9nY8QSmlVHqlrftIRKLGmP8EXsKekrpQRNYYY74HLBOR54AHgIeNMR8DDdjEoZRSKkPSekxBRBYBi/Z47rYufweBi9MZg1JKqb5LZ/eRUkqpw4wmBaWUUimaFJRSSqVoUlBKKZVy2I2SaoypBbYc4NvLgB6vlh5ANM7+czjECBpnfzsc4jzUMY4SkX1e6HXYJYWDYYxZ1pfLvDNN4+w/h0OMoHH2t8MhzoEao3YfKaWUStGkoJRSKiXbksK9mQ6gjzTO/nM4xAgaZ387HOIckDFm1TEFpZRSvcu2loJSSqleZE1SMMacZYxZZ4z52BizINPxJBljFhpjdiVuOJR8rsQY83/GmPWJ/4szHOMIY8xiY8wHxpg1xpivD9A4vcaYt40x7yXi/G7i+THGmLcSn/2fE6P2ZpQxxmmMedcY89cBHONmY8wqY8xKY8yyxHMD6jNPxFRkjHnSGLPWGPOhMebEgRanMeboxH5MTi3GmBsGWpyQJUkhcb/o3wJnA8cClxljjs1sVCl/BM7a47kFwD9EZALwj8TjTIoC3xSRY4HZwPWJ/TfQ4gwBnxSRaUAlcJYxZjbwY+AXIjIeaAS+lMEYk74OfNjl8UCMEeA0EanscurkQPvMAX4FvCgixwDTsPt1QMUpIusS+7ESmAkEgGcYYHECyVvcHdkTcCLwUpfHNwM3ZzquLvGMBlZ3ebwOqEj8XQGsy3SMe8T7LHDGQI4TyAVWACdgLxBydfddyFBsw7EFwCeBv2LvvzigYkzEsRko2+O5AfWZY2/MtYnE8dGBGucesZ0JvD5Q48yKlgIwDNjW5fH2xHMD1WARqUr8XQ0MzmQwXRljRgPTgbcYgHEmumVWAruA/wM2AE0iEk3MMhA++18C38LekBeglIEXI4AAfzfGLE/cEhcG3mc+BqgF/pDojrvfGONn4MXZ1aXAY4m/B1yc2ZIUDltiqxAD4hQxY0we8BRwg4i0dH1toMQpIjGxTfThwPHAMRkOaTfGmM8Au0RkeaZj6YOTRGQGttv1epoI5LsAAAOJSURBVGPMbjcSHiCfuQuYAfxeRKYD7ezRBTNA4gQgcaxoHvCXPV8bKHFmS1Loy/2iB5IaY0wFQOL/XRmOB2OMG5sQHhGRpxNPD7g4k0SkCViM7YopStwDHDL/2c8B5hljNgOPY7uQfsXAihEAEdmR+H8Xtv/7eAbeZ74d2C4ibyUeP4lNEgMtzqSzgRUiUpN4PODizJak0Jf7RQ8kXe9d/UVsH37GGGMM9tapH4rIz7u8NNDiLDfGFCX+9mGPe3yITQ4XJWbLaJwicrOIDBeR0djv4T9F5HIGUIwAxhi/MSY/+Te2H3w1A+wzF5FqYJsx5ujEU6cDHzDA4uziMjq7jmAgxpnpgxqH8ODOOcBH2D7mWzMdT5e4HgOqgAi21vMlbB/zP4D1wMtASYZjPAnbrH0fWJmYzhmAcU4F3k3EuRq4LfH8WOBt4GNss92T6c89Eddc4K8DMcZEPO8lpjXJ38xA+8wTMVUCyxKf+/8CxQM0Tj9QDxR2eW7AxalXNCullErJlu4jpZRSfaBJQSmlVIomBaWUUimaFJRSSqVoUlBKKZWiSUGpQ8gYMzc5MqpSA5EmBaWUUimaFJTqhjHmisS9GVYaY/4nMdBemzHmF4l7NfzDGFOemLfSGPOmMeZ9Y8wzyTHxjTHj/397968aZRCFYfx5gyCGgFY2FoJWIlhaKFbegIUiKCmsbexEUATvQdAyYgoR9Aa0WEilIoJgaZXKRkQLLeKxmHFYN0VkIXHB51ftnp0ddopvz/eHOSfJi97f4W2S4336lan6/+t9x7i0EEwK0owkJ4DLwNlqxfW2gKu0HalvquokMAHu9q88Am5W1Sng/VR8Hbhfrb/DGdrOdWhVZm/Qensco9VDkhbCvp2HSP+d87RGKK/7SfwBWqGyn8CTPuYx8CzJQeBQVU16fA142usGHamq5wBV9R2gz/eqqjb7+3e0fhobu78saWcmBWm7AGtVdeuPYHJnZty8NWJ+TL3ewuNQC8TbR9J2L4GLSQ7D6Et8lHa8/K5kegXYqKovwOck53p8FZhU1VdgM8mFPsf+JMt7ugppDp6hSDOq6kOS27SuY0u0CrbXaQ1cTvfPPtGeO0Arefyg/+l/BK71+CrwMMm9PselPVyGNBerpEp/Kcm3qlr5179D2k3ePpIkDV4pSJIGrxQkSYNJQZI0mBQkSYNJQZI0mBQkSYNJQZI0/ALBf/kW42vCuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2672 - acc: 0.9248\n",
      "Loss: 0.26718826258974776 Accuracy: 0.9248183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 18944)             75776     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 401,776\n",
      "Trainable params: 363,376\n",
      "Non-trainable params: 38,400\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 972us/sample - loss: 1.2832 - acc: 0.6264\n",
      "Loss: 1.2831656423313225 Accuracy: 0.6263759\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 12608)             50432     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 294,128\n",
      "Trainable params: 268,144\n",
      "Non-trainable params: 25,984\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 823us/sample - loss: 0.9920 - acc: 0.7115\n",
      "Loss: 0.9919993686527478 Accuracy: 0.71152645\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 150,384\n",
      "Trainable params: 141,040\n",
      "Non-trainable params: 9,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 877us/sample - loss: 0.6275 - acc: 0.8264\n",
      "Loss: 0.627503177570034 Accuracy: 0.8263759\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 119,280\n",
      "Trainable params: 115,312\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 920us/sample - loss: 0.3580 - acc: 0.8989\n",
      "Loss: 0.3579766703048104 Accuracy: 0.8988577\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 126,576\n",
      "Trainable params: 124,144\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2376 - acc: 0.9294\n",
      "Loss: 0.23758113803274286 Accuracy: 0.92938733\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 197,744\n",
      "Trainable params: 195,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2672 - acc: 0.9248\n",
      "Loss: 0.26718826258974776 Accuracy: 0.9248183\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 18944)             75776     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 401,776\n",
      "Trainable params: 363,376\n",
      "Non-trainable params: 38,400\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 965us/sample - loss: 2.1781 - acc: 0.6309\n",
      "Loss: 2.1780878869667726 Accuracy: 0.63094497\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 12608)             50432     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 294,128\n",
      "Trainable params: 268,144\n",
      "Non-trainable params: 25,984\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.5661 - acc: 0.6970\n",
      "Loss: 1.5660618906823274 Accuracy: 0.6969886\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 150,384\n",
      "Trainable params: 141,040\n",
      "Non-trainable params: 9,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7369 - acc: 0.8467\n",
      "Loss: 0.7368653081071216 Accuracy: 0.846729\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 119,280\n",
      "Trainable params: 115,312\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4407 - acc: 0.8984\n",
      "Loss: 0.4406681035785536 Accuracy: 0.8984424\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 126,576\n",
      "Trainable params: 124,144\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2616 - acc: 0.9323\n",
      "Loss: 0.2616408631344822 Accuracy: 0.9322949\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 197,744\n",
      "Trainable params: 195,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3708 - acc: 0.9192\n",
      "Loss: 0.37079391523127986 Accuracy: 0.9192108\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
