{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,248\n",
      "Trainable params: 8,195,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,739,952\n",
      "Trainable params: 2,739,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 925,488\n",
      "Trainable params: 925,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3574 - acc: 0.2385\n",
      "Epoch 00001: val_loss improved from inf to 1.83400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/001-1.8340.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 2.3574 - acc: 0.2385 - val_loss: 1.8340 - val_acc: 0.4146\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7078 - acc: 0.4564\n",
      "Epoch 00002: val_loss improved from 1.83400 to 1.56936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/002-1.5694.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.7077 - acc: 0.4564 - val_loss: 1.5694 - val_acc: 0.4857\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4956 - acc: 0.5249\n",
      "Epoch 00003: val_loss improved from 1.56936 to 1.42370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/003-1.4237.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.4956 - acc: 0.5249 - val_loss: 1.4237 - val_acc: 0.5490\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3489 - acc: 0.5767\n",
      "Epoch 00004: val_loss improved from 1.42370 to 1.32602, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/004-1.3260.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.3489 - acc: 0.5767 - val_loss: 1.3260 - val_acc: 0.5891\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2352 - acc: 0.6156\n",
      "Epoch 00005: val_loss improved from 1.32602 to 1.27202, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/005-1.2720.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.2353 - acc: 0.6156 - val_loss: 1.2720 - val_acc: 0.6054\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1461 - acc: 0.6448\n",
      "Epoch 00006: val_loss improved from 1.27202 to 1.25559, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/006-1.2556.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.1460 - acc: 0.6448 - val_loss: 1.2556 - val_acc: 0.6077\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6710\n",
      "Epoch 00007: val_loss improved from 1.25559 to 1.21154, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/007-1.2115.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.0705 - acc: 0.6710 - val_loss: 1.2115 - val_acc: 0.6224\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9913 - acc: 0.6976\n",
      "Epoch 00008: val_loss did not improve from 1.21154\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9912 - acc: 0.6976 - val_loss: 1.2150 - val_acc: 0.6191\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9267 - acc: 0.7160\n",
      "Epoch 00009: val_loss did not improve from 1.21154\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9266 - acc: 0.7160 - val_loss: 1.2161 - val_acc: 0.6240\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8553 - acc: 0.7388\n",
      "Epoch 00010: val_loss improved from 1.21154 to 1.14439, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/010-1.1444.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8553 - acc: 0.7388 - val_loss: 1.1444 - val_acc: 0.6485\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7580\n",
      "Epoch 00011: val_loss did not improve from 1.14439\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7906 - acc: 0.7579 - val_loss: 1.1819 - val_acc: 0.6378\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.7783\n",
      "Epoch 00012: val_loss did not improve from 1.14439\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7245 - acc: 0.7783 - val_loss: 1.1557 - val_acc: 0.6560\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6763 - acc: 0.7900\n",
      "Epoch 00013: val_loss improved from 1.14439 to 1.13993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv_checkpoint/013-1.1399.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6763 - acc: 0.7901 - val_loss: 1.1399 - val_acc: 0.6664\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.8073\n",
      "Epoch 00014: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6221 - acc: 0.8074 - val_loss: 1.2131 - val_acc: 0.6508\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.8208\n",
      "Epoch 00015: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5709 - acc: 0.8208 - val_loss: 1.1993 - val_acc: 0.6560\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.8335\n",
      "Epoch 00016: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5299 - acc: 0.8334 - val_loss: 1.2417 - val_acc: 0.6641\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.8473\n",
      "Epoch 00017: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4831 - acc: 0.8473 - val_loss: 1.3615 - val_acc: 0.6378\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8588\n",
      "Epoch 00018: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4503 - acc: 0.8588 - val_loss: 1.2938 - val_acc: 0.6585\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8683\n",
      "Epoch 00019: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4165 - acc: 0.8684 - val_loss: 1.3067 - val_acc: 0.6646\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8797\n",
      "Epoch 00020: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3795 - acc: 0.8797 - val_loss: 1.2978 - val_acc: 0.6683\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8864\n",
      "Epoch 00021: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3551 - acc: 0.8863 - val_loss: 1.3865 - val_acc: 0.6578\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8938\n",
      "Epoch 00022: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3292 - acc: 0.8938 - val_loss: 1.3885 - val_acc: 0.6669\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8994\n",
      "Epoch 00023: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3107 - acc: 0.8993 - val_loss: 1.3939 - val_acc: 0.6718\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9107\n",
      "Epoch 00024: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2840 - acc: 0.9106 - val_loss: 1.4410 - val_acc: 0.6683\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9095\n",
      "Epoch 00025: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2760 - acc: 0.9095 - val_loss: 1.4731 - val_acc: 0.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9181\n",
      "Epoch 00026: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2507 - acc: 0.9181 - val_loss: 1.5433 - val_acc: 0.6681\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9221\n",
      "Epoch 00027: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2399 - acc: 0.9222 - val_loss: 1.5672 - val_acc: 0.6611\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9270\n",
      "Epoch 00028: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2243 - acc: 0.9269 - val_loss: 1.6228 - val_acc: 0.6653\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9307\n",
      "Epoch 00029: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2101 - acc: 0.9306 - val_loss: 1.6535 - val_acc: 0.6622\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9345\n",
      "Epoch 00030: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2001 - acc: 0.9345 - val_loss: 1.6067 - val_acc: 0.6646\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9375\n",
      "Epoch 00031: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1925 - acc: 0.9375 - val_loss: 1.6418 - val_acc: 0.6753\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9398\n",
      "Epoch 00032: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1832 - acc: 0.9397 - val_loss: 1.7278 - val_acc: 0.6518\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9395\n",
      "Epoch 00033: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1879 - acc: 0.9395 - val_loss: 1.7075 - val_acc: 0.6755\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9448\n",
      "Epoch 00034: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1682 - acc: 0.9448 - val_loss: 1.7405 - val_acc: 0.6727\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9488\n",
      "Epoch 00035: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1581 - acc: 0.9488 - val_loss: 1.7774 - val_acc: 0.6727\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9507\n",
      "Epoch 00036: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1523 - acc: 0.9507 - val_loss: 1.7696 - val_acc: 0.6674\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9507\n",
      "Epoch 00037: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1539 - acc: 0.9507 - val_loss: 1.6967 - val_acc: 0.6788\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9536\n",
      "Epoch 00038: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1427 - acc: 0.9536 - val_loss: 1.7237 - val_acc: 0.6785\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9547\n",
      "Epoch 00039: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1393 - acc: 0.9547 - val_loss: 1.7752 - val_acc: 0.6795\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9553\n",
      "Epoch 00040: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1358 - acc: 0.9553 - val_loss: 1.8410 - val_acc: 0.6713\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9574\n",
      "Epoch 00041: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1335 - acc: 0.9575 - val_loss: 1.8522 - val_acc: 0.6879\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9593\n",
      "Epoch 00042: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1295 - acc: 0.9594 - val_loss: 1.8624 - val_acc: 0.6818\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9627\n",
      "Epoch 00043: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1186 - acc: 0.9627 - val_loss: 1.8924 - val_acc: 0.6751\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9608\n",
      "Epoch 00044: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1255 - acc: 0.9608 - val_loss: 1.8120 - val_acc: 0.6809\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9638\n",
      "Epoch 00045: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1168 - acc: 0.9638 - val_loss: 1.9081 - val_acc: 0.6702\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9623\n",
      "Epoch 00046: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1165 - acc: 0.9623 - val_loss: 1.8671 - val_acc: 0.6832\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9649\n",
      "Epoch 00047: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1128 - acc: 0.9649 - val_loss: 1.9654 - val_acc: 0.6790\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9638\n",
      "Epoch 00048: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1156 - acc: 0.9638 - val_loss: 1.8869 - val_acc: 0.6846\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9688\n",
      "Epoch 00049: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1015 - acc: 0.9688 - val_loss: 1.9247 - val_acc: 0.6846\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9676\n",
      "Epoch 00050: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1039 - acc: 0.9676 - val_loss: 1.9222 - val_acc: 0.6893\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9684\n",
      "Epoch 00051: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1049 - acc: 0.9684 - val_loss: 1.9523 - val_acc: 0.6867\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9671\n",
      "Epoch 00052: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1064 - acc: 0.9670 - val_loss: 1.9593 - val_acc: 0.6886\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9679\n",
      "Epoch 00053: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1017 - acc: 0.9679 - val_loss: 1.9999 - val_acc: 0.6879\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9693\n",
      "Epoch 00054: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1022 - acc: 0.9694 - val_loss: 1.9641 - val_acc: 0.6909\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9707\n",
      "Epoch 00055: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0937 - acc: 0.9707 - val_loss: 1.9150 - val_acc: 0.6888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9701\n",
      "Epoch 00056: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0947 - acc: 0.9701 - val_loss: 1.9748 - val_acc: 0.6888\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9712\n",
      "Epoch 00057: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0918 - acc: 0.9712 - val_loss: 1.9687 - val_acc: 0.6830\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9732\n",
      "Epoch 00058: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0892 - acc: 0.9732 - val_loss: 1.9409 - val_acc: 0.6893\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9712\n",
      "Epoch 00059: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0922 - acc: 0.9712 - val_loss: 2.0187 - val_acc: 0.6858\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9734\n",
      "Epoch 00060: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0859 - acc: 0.9734 - val_loss: 2.0498 - val_acc: 0.6844\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9741\n",
      "Epoch 00061: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0850 - acc: 0.9741 - val_loss: 2.0111 - val_acc: 0.6802\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9739\n",
      "Epoch 00062: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0853 - acc: 0.9739 - val_loss: 2.0919 - val_acc: 0.6837\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9739\n",
      "Epoch 00063: val_loss did not improve from 1.13993\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0843 - acc: 0.9739 - val_loss: 1.9834 - val_acc: 0.6907\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 507us/sample - loss: 1.2567 - acc: 0.6204\n",
      "Loss: 1.2567360265480272 Accuracy: 0.62035304\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5146 - acc: 0.1818\n",
      "Epoch 00001: val_loss improved from inf to 2.14588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/001-2.1459.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.5145 - acc: 0.1818 - val_loss: 2.1459 - val_acc: 0.3051\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7738 - acc: 0.4393\n",
      "Epoch 00002: val_loss improved from 2.14588 to 1.53595, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/002-1.5359.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.7738 - acc: 0.4393 - val_loss: 1.5359 - val_acc: 0.5036\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4744 - acc: 0.5269\n",
      "Epoch 00003: val_loss improved from 1.53595 to 1.35970, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/003-1.3597.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4743 - acc: 0.5269 - val_loss: 1.3597 - val_acc: 0.5602\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3656 - acc: 0.5633\n",
      "Epoch 00004: val_loss improved from 1.35970 to 1.29794, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/004-1.2979.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.3655 - acc: 0.5633 - val_loss: 1.2979 - val_acc: 0.5847\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2921 - acc: 0.5935\n",
      "Epoch 00005: val_loss improved from 1.29794 to 1.24564, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/005-1.2456.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2920 - acc: 0.5935 - val_loss: 1.2456 - val_acc: 0.6010\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2150 - acc: 0.6231\n",
      "Epoch 00006: val_loss improved from 1.24564 to 1.15893, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/006-1.1589.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2151 - acc: 0.6231 - val_loss: 1.1589 - val_acc: 0.6348\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1437 - acc: 0.6485\n",
      "Epoch 00007: val_loss improved from 1.15893 to 1.13751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/007-1.1375.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1438 - acc: 0.6484 - val_loss: 1.1375 - val_acc: 0.6473\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0825 - acc: 0.6696\n",
      "Epoch 00008: val_loss improved from 1.13751 to 1.09679, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/008-1.0968.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0825 - acc: 0.6696 - val_loss: 1.0968 - val_acc: 0.6615\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0146 - acc: 0.6904\n",
      "Epoch 00009: val_loss improved from 1.09679 to 1.06179, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/009-1.0618.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0147 - acc: 0.6904 - val_loss: 1.0618 - val_acc: 0.6697\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9568 - acc: 0.7098\n",
      "Epoch 00010: val_loss improved from 1.06179 to 1.04021, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/010-1.0402.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9567 - acc: 0.7098 - val_loss: 1.0402 - val_acc: 0.6718\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9113 - acc: 0.7215\n",
      "Epoch 00011: val_loss did not improve from 1.04021\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9114 - acc: 0.7215 - val_loss: 1.0614 - val_acc: 0.6723\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8507 - acc: 0.7399\n",
      "Epoch 00012: val_loss improved from 1.04021 to 0.98622, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/012-0.9862.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8507 - acc: 0.7400 - val_loss: 0.9862 - val_acc: 0.6902\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8094 - acc: 0.7517\n",
      "Epoch 00013: val_loss did not improve from 0.98622\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8095 - acc: 0.7516 - val_loss: 0.9912 - val_acc: 0.6958\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7597 - acc: 0.7693\n",
      "Epoch 00014: val_loss did not improve from 0.98622\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7599 - acc: 0.7693 - val_loss: 1.0514 - val_acc: 0.6648\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7107 - acc: 0.7823\n",
      "Epoch 00015: val_loss did not improve from 0.98622\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7107 - acc: 0.7823 - val_loss: 1.0569 - val_acc: 0.6914\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.7951\n",
      "Epoch 00016: val_loss improved from 0.98622 to 0.97347, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/016-0.9735.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6735 - acc: 0.7951 - val_loss: 0.9735 - val_acc: 0.6958\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.8054\n",
      "Epoch 00017: val_loss did not improve from 0.97347\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6304 - acc: 0.8054 - val_loss: 0.9898 - val_acc: 0.7025\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.8175\n",
      "Epoch 00018: val_loss improved from 0.97347 to 0.95043, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv_checkpoint/018-0.9504.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5984 - acc: 0.8175 - val_loss: 0.9504 - val_acc: 0.7123\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.8272\n",
      "Epoch 00019: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5619 - acc: 0.8272 - val_loss: 1.0098 - val_acc: 0.6972\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8351\n",
      "Epoch 00020: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5251 - acc: 0.8351 - val_loss: 0.9851 - val_acc: 0.7172\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8466\n",
      "Epoch 00021: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4976 - acc: 0.8466 - val_loss: 1.0480 - val_acc: 0.7058\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8549\n",
      "Epoch 00022: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4626 - acc: 0.8550 - val_loss: 1.0128 - val_acc: 0.7154\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8627\n",
      "Epoch 00023: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4331 - acc: 0.8627 - val_loss: 1.0525 - val_acc: 0.7042\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8718\n",
      "Epoch 00024: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4057 - acc: 0.8718 - val_loss: 1.0728 - val_acc: 0.7037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8737\n",
      "Epoch 00025: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3969 - acc: 0.8737 - val_loss: 1.0325 - val_acc: 0.7209\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8875\n",
      "Epoch 00026: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3586 - acc: 0.8875 - val_loss: 1.0521 - val_acc: 0.7275\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8916\n",
      "Epoch 00027: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3387 - acc: 0.8916 - val_loss: 1.1355 - val_acc: 0.7114\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8961\n",
      "Epoch 00028: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3242 - acc: 0.8960 - val_loss: 1.0803 - val_acc: 0.7275\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9015\n",
      "Epoch 00029: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3065 - acc: 0.9015 - val_loss: 1.1669 - val_acc: 0.7186\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9091\n",
      "Epoch 00030: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2847 - acc: 0.9091 - val_loss: 1.1201 - val_acc: 0.7265\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9124\n",
      "Epoch 00031: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2701 - acc: 0.9124 - val_loss: 1.1227 - val_acc: 0.7340\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9148\n",
      "Epoch 00032: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2638 - acc: 0.9148 - val_loss: 1.1518 - val_acc: 0.7256\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9195\n",
      "Epoch 00033: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2499 - acc: 0.9195 - val_loss: 1.2117 - val_acc: 0.7207\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9237\n",
      "Epoch 00034: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2355 - acc: 0.9237 - val_loss: 1.2381 - val_acc: 0.7312\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9306\n",
      "Epoch 00035: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2202 - acc: 0.9306 - val_loss: 1.2045 - val_acc: 0.7354\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9291\n",
      "Epoch 00036: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2198 - acc: 0.9291 - val_loss: 1.2430 - val_acc: 0.7247\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9302\n",
      "Epoch 00037: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2163 - acc: 0.9302 - val_loss: 1.3358 - val_acc: 0.7084\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9342\n",
      "Epoch 00038: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1993 - acc: 0.9342 - val_loss: 1.3224 - val_acc: 0.7249\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9361\n",
      "Epoch 00039: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1994 - acc: 0.9360 - val_loss: 1.2461 - val_acc: 0.7319\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9389\n",
      "Epoch 00040: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1894 - acc: 0.9389 - val_loss: 1.2939 - val_acc: 0.7370\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9443\n",
      "Epoch 00041: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1759 - acc: 0.9443 - val_loss: 1.3113 - val_acc: 0.7312\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9448\n",
      "Epoch 00042: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1736 - acc: 0.9448 - val_loss: 1.4232 - val_acc: 0.7156\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9444\n",
      "Epoch 00043: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1739 - acc: 0.9444 - val_loss: 1.2787 - val_acc: 0.7449\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9466\n",
      "Epoch 00044: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1659 - acc: 0.9466 - val_loss: 1.3205 - val_acc: 0.7326\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9470\n",
      "Epoch 00045: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1671 - acc: 0.9470 - val_loss: 1.3241 - val_acc: 0.7354\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9509\n",
      "Epoch 00046: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1562 - acc: 0.9509 - val_loss: 1.2999 - val_acc: 0.7431\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9536\n",
      "Epoch 00047: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1455 - acc: 0.9536 - val_loss: 1.3909 - val_acc: 0.7277\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9514\n",
      "Epoch 00048: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1542 - acc: 0.9514 - val_loss: 1.3229 - val_acc: 0.7524\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9576\n",
      "Epoch 00049: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1383 - acc: 0.9576 - val_loss: 1.3630 - val_acc: 0.7491\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9576\n",
      "Epoch 00050: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1351 - acc: 0.9576 - val_loss: 1.3866 - val_acc: 0.7365\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9581\n",
      "Epoch 00051: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1339 - acc: 0.9581 - val_loss: 1.3672 - val_acc: 0.7419\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9551\n",
      "Epoch 00052: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1426 - acc: 0.9551 - val_loss: 1.3326 - val_acc: 0.7389\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9590\n",
      "Epoch 00053: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1294 - acc: 0.9591 - val_loss: 1.4010 - val_acc: 0.7396\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9582\n",
      "Epoch 00054: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1336 - acc: 0.9582 - val_loss: 1.4084 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9593\n",
      "Epoch 00055: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1263 - acc: 0.9594 - val_loss: 1.3336 - val_acc: 0.7407\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9637\n",
      "Epoch 00056: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1198 - acc: 0.9636 - val_loss: 1.5184 - val_acc: 0.7328\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9608\n",
      "Epoch 00057: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1252 - acc: 0.9608 - val_loss: 1.3586 - val_acc: 0.7421\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9633\n",
      "Epoch 00058: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1177 - acc: 0.9633 - val_loss: 1.3677 - val_acc: 0.7533\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9639\n",
      "Epoch 00059: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1186 - acc: 0.9639 - val_loss: 1.4821 - val_acc: 0.7375\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9661\n",
      "Epoch 00060: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1152 - acc: 0.9661 - val_loss: 1.4052 - val_acc: 0.7454\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9662\n",
      "Epoch 00061: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1110 - acc: 0.9662 - val_loss: 1.3911 - val_acc: 0.7543\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9657\n",
      "Epoch 00062: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1147 - acc: 0.9657 - val_loss: 1.4989 - val_acc: 0.7375\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9666\n",
      "Epoch 00063: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1080 - acc: 0.9666 - val_loss: 1.4767 - val_acc: 0.7475\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9654\n",
      "Epoch 00064: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1177 - acc: 0.9654 - val_loss: 1.5012 - val_acc: 0.7445\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9702\n",
      "Epoch 00065: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1016 - acc: 0.9702 - val_loss: 1.4086 - val_acc: 0.7552\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9687\n",
      "Epoch 00066: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1058 - acc: 0.9686 - val_loss: 1.4149 - val_acc: 0.7501\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9732\n",
      "Epoch 00067: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0936 - acc: 0.9732 - val_loss: 1.5761 - val_acc: 0.7424\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9671\n",
      "Epoch 00068: val_loss did not improve from 0.95043\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1101 - acc: 0.9671 - val_loss: 1.4305 - val_acc: 0.7496\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 533us/sample - loss: 1.0726 - acc: 0.6845\n",
      "Loss: 1.072585623992195 Accuracy: 0.6845275\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6288 - acc: 0.1287\n",
      "Epoch 00001: val_loss improved from inf to 2.38203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/001-2.3820.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.6288 - acc: 0.1287 - val_loss: 2.3820 - val_acc: 0.2285\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9816 - acc: 0.3593\n",
      "Epoch 00002: val_loss improved from 2.38203 to 1.51741, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/002-1.5174.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.9817 - acc: 0.3593 - val_loss: 1.5174 - val_acc: 0.5176\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5207 - acc: 0.5068\n",
      "Epoch 00003: val_loss improved from 1.51741 to 1.37599, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/003-1.3760.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.5208 - acc: 0.5069 - val_loss: 1.3760 - val_acc: 0.5542\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3808 - acc: 0.5573\n",
      "Epoch 00004: val_loss improved from 1.37599 to 1.36479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/004-1.3648.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3807 - acc: 0.5573 - val_loss: 1.3648 - val_acc: 0.5903\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.5932\n",
      "Epoch 00005: val_loss improved from 1.36479 to 1.18974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/005-1.1897.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2795 - acc: 0.5932 - val_loss: 1.1897 - val_acc: 0.6254\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2119 - acc: 0.6181\n",
      "Epoch 00006: val_loss improved from 1.18974 to 1.14644, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/006-1.1464.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2118 - acc: 0.6181 - val_loss: 1.1464 - val_acc: 0.6415\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1337 - acc: 0.6474\n",
      "Epoch 00007: val_loss improved from 1.14644 to 1.06478, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/007-1.0648.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1336 - acc: 0.6475 - val_loss: 1.0648 - val_acc: 0.6709\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0573 - acc: 0.6738\n",
      "Epoch 00008: val_loss improved from 1.06478 to 0.98510, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/008-0.9851.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0572 - acc: 0.6738 - val_loss: 0.9851 - val_acc: 0.7077\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9812 - acc: 0.6999\n",
      "Epoch 00009: val_loss improved from 0.98510 to 0.89346, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/009-0.8935.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9811 - acc: 0.6999 - val_loss: 0.8935 - val_acc: 0.7389\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9118 - acc: 0.7205\n",
      "Epoch 00010: val_loss improved from 0.89346 to 0.84004, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/010-0.8400.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9118 - acc: 0.7205 - val_loss: 0.8400 - val_acc: 0.7440\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8452 - acc: 0.7438\n",
      "Epoch 00011: val_loss improved from 0.84004 to 0.77127, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/011-0.7713.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8451 - acc: 0.7438 - val_loss: 0.7713 - val_acc: 0.7701\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7611\n",
      "Epoch 00012: val_loss did not improve from 0.77127\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7877 - acc: 0.7611 - val_loss: 0.7808 - val_acc: 0.7622\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7454 - acc: 0.7752\n",
      "Epoch 00013: val_loss improved from 0.77127 to 0.72309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/013-0.7231.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7454 - acc: 0.7752 - val_loss: 0.7231 - val_acc: 0.7843\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6972 - acc: 0.7902\n",
      "Epoch 00014: val_loss improved from 0.72309 to 0.67710, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/014-0.6771.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6972 - acc: 0.7902 - val_loss: 0.6771 - val_acc: 0.8004\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.7988\n",
      "Epoch 00015: val_loss improved from 0.67710 to 0.65378, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/015-0.6538.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6579 - acc: 0.7988 - val_loss: 0.6538 - val_acc: 0.8036\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8073\n",
      "Epoch 00016: val_loss improved from 0.65378 to 0.59782, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/016-0.5978.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6301 - acc: 0.8073 - val_loss: 0.5978 - val_acc: 0.8220\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5924 - acc: 0.8195\n",
      "Epoch 00017: val_loss improved from 0.59782 to 0.57352, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/017-0.5735.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5924 - acc: 0.8195 - val_loss: 0.5735 - val_acc: 0.8328\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8304\n",
      "Epoch 00018: val_loss did not improve from 0.57352\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5630 - acc: 0.8304 - val_loss: 0.5779 - val_acc: 0.8307\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5343 - acc: 0.8373\n",
      "Epoch 00019: val_loss improved from 0.57352 to 0.53474, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/019-0.5347.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5346 - acc: 0.8373 - val_loss: 0.5347 - val_acc: 0.8491\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8448\n",
      "Epoch 00020: val_loss improved from 0.53474 to 0.53307, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/020-0.5331.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5060 - acc: 0.8448 - val_loss: 0.5331 - val_acc: 0.8437\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8521\n",
      "Epoch 00021: val_loss improved from 0.53307 to 0.50455, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/021-0.5046.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4871 - acc: 0.8521 - val_loss: 0.5046 - val_acc: 0.8574\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8592\n",
      "Epoch 00022: val_loss did not improve from 0.50455\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4602 - acc: 0.8592 - val_loss: 0.5165 - val_acc: 0.8551\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8639\n",
      "Epoch 00023: val_loss improved from 0.50455 to 0.47921, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/023-0.4792.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4453 - acc: 0.8639 - val_loss: 0.4792 - val_acc: 0.8642\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8701\n",
      "Epoch 00024: val_loss did not improve from 0.47921\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4260 - acc: 0.8701 - val_loss: 0.4871 - val_acc: 0.8623\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8752\n",
      "Epoch 00025: val_loss did not improve from 0.47921\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4061 - acc: 0.8752 - val_loss: 0.4822 - val_acc: 0.8558\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8800\n",
      "Epoch 00026: val_loss did not improve from 0.47921\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3885 - acc: 0.8800 - val_loss: 0.4964 - val_acc: 0.8526\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8841\n",
      "Epoch 00027: val_loss did not improve from 0.47921\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3770 - acc: 0.8840 - val_loss: 0.5223 - val_acc: 0.8488\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8896\n",
      "Epoch 00028: val_loss improved from 0.47921 to 0.44623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/028-0.4462.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3632 - acc: 0.8896 - val_loss: 0.4462 - val_acc: 0.8758\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8930\n",
      "Epoch 00029: val_loss did not improve from 0.44623\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3486 - acc: 0.8930 - val_loss: 0.4618 - val_acc: 0.8751\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8945\n",
      "Epoch 00030: val_loss did not improve from 0.44623\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3391 - acc: 0.8946 - val_loss: 0.4737 - val_acc: 0.8679\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9011\n",
      "Epoch 00031: val_loss improved from 0.44623 to 0.43345, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/031-0.4334.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3230 - acc: 0.9011 - val_loss: 0.4334 - val_acc: 0.8770\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9034\n",
      "Epoch 00032: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3135 - acc: 0.9034 - val_loss: 0.4601 - val_acc: 0.8740\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9041\n",
      "Epoch 00033: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3092 - acc: 0.9041 - val_loss: 0.4897 - val_acc: 0.8635\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9063\n",
      "Epoch 00034: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2951 - acc: 0.9063 - val_loss: 0.4340 - val_acc: 0.8810\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9112\n",
      "Epoch 00035: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2829 - acc: 0.9112 - val_loss: 0.4563 - val_acc: 0.8754\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9146\n",
      "Epoch 00036: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2724 - acc: 0.9146 - val_loss: 0.4728 - val_acc: 0.8703\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9174\n",
      "Epoch 00037: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2648 - acc: 0.9174 - val_loss: 0.4399 - val_acc: 0.8805\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9204\n",
      "Epoch 00038: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2527 - acc: 0.9204 - val_loss: 0.4924 - val_acc: 0.8791\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9213\n",
      "Epoch 00039: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2485 - acc: 0.9213 - val_loss: 0.4727 - val_acc: 0.8831\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9276\n",
      "Epoch 00040: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2332 - acc: 0.9275 - val_loss: 0.4659 - val_acc: 0.8819\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9289\n",
      "Epoch 00041: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2315 - acc: 0.9289 - val_loss: 0.4618 - val_acc: 0.8786\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9261\n",
      "Epoch 00042: val_loss did not improve from 0.43345\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2340 - acc: 0.9262 - val_loss: 0.4827 - val_acc: 0.8765\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9333\n",
      "Epoch 00043: val_loss improved from 0.43345 to 0.42948, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv_checkpoint/043-0.4295.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2154 - acc: 0.9332 - val_loss: 0.4295 - val_acc: 0.8889\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9343\n",
      "Epoch 00044: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2093 - acc: 0.9343 - val_loss: 0.4795 - val_acc: 0.8803\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9347\n",
      "Epoch 00045: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2069 - acc: 0.9347 - val_loss: 0.4761 - val_acc: 0.8824\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9363\n",
      "Epoch 00046: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1989 - acc: 0.9363 - val_loss: 0.4556 - val_acc: 0.8821\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9373\n",
      "Epoch 00047: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1939 - acc: 0.9373 - val_loss: 0.4513 - val_acc: 0.8849\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9386\n",
      "Epoch 00048: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1923 - acc: 0.9386 - val_loss: 0.4729 - val_acc: 0.8861\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9388\n",
      "Epoch 00049: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1857 - acc: 0.9388 - val_loss: 0.4736 - val_acc: 0.8833\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9428\n",
      "Epoch 00050: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1775 - acc: 0.9428 - val_loss: 0.4782 - val_acc: 0.8854\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9434\n",
      "Epoch 00051: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1751 - acc: 0.9434 - val_loss: 0.4563 - val_acc: 0.8894\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9451\n",
      "Epoch 00052: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1671 - acc: 0.9451 - val_loss: 0.4758 - val_acc: 0.8824\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9475\n",
      "Epoch 00053: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1617 - acc: 0.9475 - val_loss: 0.4868 - val_acc: 0.8849\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9494\n",
      "Epoch 00054: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1572 - acc: 0.9494 - val_loss: 0.5407 - val_acc: 0.8803\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9498\n",
      "Epoch 00055: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1525 - acc: 0.9498 - val_loss: 0.5025 - val_acc: 0.8908\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9510\n",
      "Epoch 00056: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1501 - acc: 0.9510 - val_loss: 0.5175 - val_acc: 0.8866\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9525\n",
      "Epoch 00057: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1468 - acc: 0.9525 - val_loss: 0.4898 - val_acc: 0.8915\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9532\n",
      "Epoch 00058: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1454 - acc: 0.9532 - val_loss: 0.4783 - val_acc: 0.8915\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9555\n",
      "Epoch 00059: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1357 - acc: 0.9555 - val_loss: 0.5989 - val_acc: 0.8735\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9516\n",
      "Epoch 00060: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1472 - acc: 0.9516 - val_loss: 0.4941 - val_acc: 0.8889\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9541\n",
      "Epoch 00061: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1374 - acc: 0.9541 - val_loss: 0.5004 - val_acc: 0.8926\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9571\n",
      "Epoch 00062: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1300 - acc: 0.9571 - val_loss: 0.5179 - val_acc: 0.8882\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9579\n",
      "Epoch 00063: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1268 - acc: 0.9579 - val_loss: 0.4959 - val_acc: 0.8915\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9570\n",
      "Epoch 00064: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1271 - acc: 0.9570 - val_loss: 0.4943 - val_acc: 0.8926\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9593\n",
      "Epoch 00065: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1279 - acc: 0.9593 - val_loss: 0.4978 - val_acc: 0.8938\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9617\n",
      "Epoch 00066: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1183 - acc: 0.9617 - val_loss: 0.5175 - val_acc: 0.8935\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9581\n",
      "Epoch 00067: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1270 - acc: 0.9581 - val_loss: 0.4931 - val_acc: 0.8912\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9625\n",
      "Epoch 00068: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1137 - acc: 0.9625 - val_loss: 0.5262 - val_acc: 0.8954\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9637\n",
      "Epoch 00069: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1098 - acc: 0.9637 - val_loss: 0.5266 - val_acc: 0.8877\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9629\n",
      "Epoch 00070: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1142 - acc: 0.9629 - val_loss: 0.5192 - val_acc: 0.8866\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9648\n",
      "Epoch 00071: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1111 - acc: 0.9648 - val_loss: 0.5057 - val_acc: 0.9003\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9632\n",
      "Epoch 00072: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1107 - acc: 0.9632 - val_loss: 0.4837 - val_acc: 0.8940\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9670\n",
      "Epoch 00073: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0987 - acc: 0.9670 - val_loss: 0.5258 - val_acc: 0.8931\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9667\n",
      "Epoch 00074: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1010 - acc: 0.9667 - val_loss: 0.5142 - val_acc: 0.8931\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9675\n",
      "Epoch 00075: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1012 - acc: 0.9675 - val_loss: 0.5167 - val_acc: 0.8991\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9670\n",
      "Epoch 00076: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1015 - acc: 0.9670 - val_loss: 0.5092 - val_acc: 0.8866\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9685\n",
      "Epoch 00077: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0961 - acc: 0.9685 - val_loss: 0.5256 - val_acc: 0.8959\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9681\n",
      "Epoch 00078: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0969 - acc: 0.9681 - val_loss: 0.5510 - val_acc: 0.8917\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9687\n",
      "Epoch 00079: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0965 - acc: 0.9687 - val_loss: 0.5295 - val_acc: 0.8891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9698\n",
      "Epoch 00080: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0955 - acc: 0.9698 - val_loss: 0.5450 - val_acc: 0.8889\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9703\n",
      "Epoch 00081: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0896 - acc: 0.9703 - val_loss: 0.5636 - val_acc: 0.8891\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9707\n",
      "Epoch 00082: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0917 - acc: 0.9707 - val_loss: 0.5127 - val_acc: 0.8996\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9702\n",
      "Epoch 00083: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0905 - acc: 0.9702 - val_loss: 0.5466 - val_acc: 0.8928\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9724\n",
      "Epoch 00084: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0827 - acc: 0.9724 - val_loss: 0.5216 - val_acc: 0.8987\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9707\n",
      "Epoch 00085: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0883 - acc: 0.9707 - val_loss: 0.5319 - val_acc: 0.8949\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9714\n",
      "Epoch 00086: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0871 - acc: 0.9714 - val_loss: 0.5277 - val_acc: 0.8947\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9726\n",
      "Epoch 00087: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0865 - acc: 0.9726 - val_loss: 0.5505 - val_acc: 0.8915\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9736\n",
      "Epoch 00088: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0827 - acc: 0.9736 - val_loss: 0.5613 - val_acc: 0.8919\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9726\n",
      "Epoch 00089: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0850 - acc: 0.9726 - val_loss: 0.5486 - val_acc: 0.8980\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9718\n",
      "Epoch 00090: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0871 - acc: 0.9718 - val_loss: 0.5180 - val_acc: 0.8977\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9737\n",
      "Epoch 00091: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0814 - acc: 0.9738 - val_loss: 0.5518 - val_acc: 0.8966\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9749\n",
      "Epoch 00092: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0772 - acc: 0.9749 - val_loss: 0.5443 - val_acc: 0.8915\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9751\n",
      "Epoch 00093: val_loss did not improve from 0.42948\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0775 - acc: 0.9750 - val_loss: 0.5908 - val_acc: 0.8873\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmT37Sgh72HeIbGJRsC7UFbcitajVtvqzWpXar9Vqq9hqq1Rra923Fq1rQeuGorYsakUFBAVBIEAIIfs+yWTW8/vjTBYgQIAMk2Se9+t1X0zm3rn3mZtwnrPce67SWiOEEEIAWKIdgBBCiM5DkoIQQohmkhSEEEI0k6QghBCimSQFIYQQzSQpCCGEaCZJQQghRDNJCkIIIZpJUhBCCNHMFu0ADldmZqbOycmJdhhCCNGlrFmzplxr3eNQ23W5pJCTk8Pq1aujHYYQQnQpSqn89mwn3UdCCCGaSVIQQgjRTJKCEEKIZl1uTKEtfr+f3bt309jYGO1QuiyXy0Xfvn2x2+3RDkUIEUXdIins3r2bpKQkcnJyUEpFO5wuR2tNRUUFu3fvZuDAgdEORwgRRd2i+6ixsZGMjAxJCEdIKUVGRoa0tIQQ3SMpAJIQjpKcPyEEdKOkcCjBYANebyGhkD/aoQghRKcVM0khFPLi8xWhdccnherqah599NEj+uxZZ51FdXV1u7efP38+999//xEdSwghDiVmkoJSVgC0DnT4vg+WFAKBgx9vyZIlpKamdnhMQghxJGIoKZgLrbQOdvi+b731VvLy8sjNzeXmm29m+fLlnHTSScyaNYtRo0YBcP755zNx4kRGjx7Nk08+2fzZnJwcysvL2blzJyNHjuSqq65i9OjRzJw5E4/Hc9Djrlu3jqlTpzJu3DguuOACqqqqAHjooYcYNWoU48aN4wc/+AEAK1asIDc3l9zcXI477jjq6uo6/DwIIbq+bnFJamtbt87D7V7XxpoQwWA9FosLpQ7vWvzExFyGDv3LAdffe++9bNiwgXXrzHGXL1/O2rVr2bBhQ/Mlns8++yzp6el4PB4mT57MRRddREZGxj6xb+Wll17iqaee4uKLL2bx4sVceumlBzzu5Zdfzt/+9jdmzJjBHXfcwV133cVf/vIX7r33Xnbs2IHT6Wzumrr//vt55JFHmDZtGm63G5fLdVjnQAgRG2KmpQBNV9foY3K0KVOm7HXN/0MPPcT48eOZOnUqBQUFbN26db/PDBw4kNzcXAAmTpzIzp07D7j/mpoaqqurmTFjBgA/+tGPWLlyJQDjxo1j7ty5/POf/8RmM3l/2rRp3HTTTTz00ENUV1c3vy+EEK11u5LhQDV6rTVu9xocjl44nX0iHkdCQkLz6+XLl/Phhx/y6aefEh8fz8knn9zmPQFOp7P5tdVqPWT30YG88847rFy5krfeeot77rmHr7/+mltvvZWzzz6bJUuWMG3aNJYuXcqIESOOaP9CiO4rZloK5jp8W0QGmpOSkg7aR19TU0NaWhrx8fFs3ryZVatWHfUxU1JSSEtL46OPPgLg+eefZ8aMGYRCIQoKCvjud7/LfffdR01NDW63m7y8PMaOHcstt9zC5MmT2bx581HHIITofiLWUlBK9QOeA3pi+mye1Fr/dZ9tTgbeAHaE33pNa/27yMVkjchAc0ZGBtOmTWPMmDGceeaZnH322XutP+OMM3j88ccZOXIkw4cPZ+rUqR1y3IULF3LNNdfQ0NDAoEGD+Pvf/04wGOTSSy+lpqYGrTU33HADqamp/Pa3v2XZsmVYLBZGjx7NmWee2SExCCG6F6V1ZPrYlVK9gF5a67VKqSRgDXC+1vqbVtucDPyf1vqc9u530qRJet+H7GzatImRI0ce8rP19d+glI34+GHtPVxMae95FEJ0PUqpNVrrSYfaLmLdR1rrIq312vDrOmATEPnO/INQyhaRloIQQnQXx2RMQSmVAxwHfNbG6hOUUuuVUu8qpUYf4PNXK6VWK6VWl5WVHUUckek+EkKI7iLiSUEplQgsBuZprWv3Wb0WGKC1Hg/8Dfh3W/vQWj+ptZ6ktZ7Uo8chnzt9kFhsQMcPNAshRHcR0aSgzF1ii4EXtNav7btea12rtXaHXy8B7EqpzMjFY1oKkRpHEUKIri5iSUGZa0CfATZprf98gG2yw9uhlJoSjqciUjGZi600EIrcIYQQoguL5M1r04DLgK+VUk3zTtwG9AfQWj8OfB/4mVIqAHiAH+gIVuNbJsULNr8WQgjRImJJQWv9MS1zSxxom4eBhyMVw772ninVcawO26bExETcbne73xdCiGMhZu5ohsjOlCqEEN1BjCWFlu6jjnTrrbfyyCOPNP/c9CAct9vNqaeeyoQJExg7dixvvPFGu/eptebmm29mzJgxjB07lldeeQWAoqIipk+fTm5uLmPGjOGjjz4iGAxyxRVXNG/74IMPduj3E0LEjm43IR7z5sG6NqbODoWwBAPEKS8WqwsOZ/rs3Fz4y4Gnzp4zZw7z5s3juuuuA+DVV19l6dKluFwuXn/9dZKTkykvL2fq1KnMmjWrXc9Dfu2111i3bh3r16+nvLycyZMnM336dF588UW+973vcfvttxMMBmloaGDdunUUFhayYcMGgMN6kpsQQrTW/ZLCgYRCqEYvygUdPX32cccdR2lpKXv27KGsrIy0tDT69euH3+/ntttuY+XKlVgsFgoLCykpKSE7O/uQ+/z444+55JJLsFqt9OzZkxkzZvDFF18wefJkfvzjH+P3+zn//PPJzc1l0KBBbN++neuvv56zzz6bmTNnduj3E0LEju6XFA5Uo6+pga1baewPtpSOnz579uzZLFq0iOLiYubMmQPACy+8QFlZGWvWrMFut5OTk9PmlNmHY/r06axcuZJ33nmHK664gptuuonLL7+c9evXs3TpUh5//HFeffVVnn322Y74WkKIGBM7YwpWM55gCVkiMtA8Z84cXn75ZRYtWsTs2bMBM2V2VlYWdrudZcuWkZ+f3+79nXTSSbzyyisEg0HKyspYuXIlU6ZMIT8/n549e3LVVVfx05/+lLVr11JeXk4oFOKiiy7i7rvvZu3atR3+/YQQsaH7tRQOxGLyn9KWiDxTYfTo0dTV1dGnTx969eoFwNy5czn33HMZO3YskyZNOqyH2lxwwQV8+umnjB8/HqUUCxYsIDs7m4ULF/KnP/0Ju91OYmIizz33HIWFhVx55ZWEQuamvD/+8Y8d/v2EELEhYlNnR8oRT53t9cLXX+Pt7SCYGkd8/NAIRtk1ydTZQnRfUZ86u9NpailEqPtICCG6g9hJCuExBaUVMlOqEEK0LXaSQvjeABVS0lIQQogDiK2kYLVCiIgMNAshRHcQO0kBwGpFhQA0Wsv02UIIsa/YSgoWSzgpSGtBCCHaEltJwWqFkLkEtyPHFaqrq3n00UeP6LNnnXWWzFUkhOg0YispWCzND107VkkhEDh4i2TJkiWkpqZ2WCxCCHE0YispWK2o8F2/Hdl9dOutt5KXl0dubi4333wzy5cv56STTmLWrFmMGjUKgPPPP5+JEycyevRonnzyyebP5uTkUF5ezs6dOxk5ciRXXXUVo0ePZubMmXg8nv2O9dZbb3H88cdz3HHHcdppp1FSUgKA2+3myiuvZOzYsYwbN47FixcD8N577zFhwgTGjx/Pqaee2mHfWQjRPXW7aS4ONHM2AI19IBgk6NJYLC7aMYM1cMiZs7n33nvZsGED68IHXr58OWvXrmXDhg0MHDgQgGeffZb09HQ8Hg+TJ0/moosuIiMjY6/9bN26lZdeeomnnnqKiy++mMWLF3PppZfutc2JJ57IqlWrUErx9NNPs2DBAh544AF+//vfk5KSwtdffw1AVVUVZWVlXHXVVaxcuZKBAwdSWVnZvi8shIhZ3S4pHJxqNWt2ZKf3mDJlSnNCAHjooYd4/fXXASgoKGDr1q37JYWBAweSm5sLwMSJE9m5c+d++929ezdz5syhqKgIn8/XfIwPP/yQl19+uXm7tLQ03nrrLaZPn968TXp6eod+RyFE99PtksLBavTsLkeXlOAepnE4euN09o5YHAkJCc2vly9fzocffsinn35KfHw8J598cptTaDudzubXVqu1ze6j66+/nptuuolZs2axfPly5s+fH5H4hRCxKbbGFCwWlNbQwTOlJiUlUVdXd8D1NTU1pKWlER8fz+bNm1m1atURH6umpoY+fcyzIBYuXNj8/umnn77XI0GrqqqYOnUqK1euZMeOHQDSfSSEOKTYSgpNz1TQtg69+igjI4Np06YxZswYbr755v3Wn3HGGQQCAUaOHMmtt97K1KlTj/hY8+fPZ/bs2UycOJHMzMzm93/zm99QVVXFmDFjGD9+PMuWLaNHjx48+eSTXHjhhYwfP7754T9CCHEgsTN1NkBZGeTn0zDEBQ4X8fFDIhRl1yRTZwvRfcnU2W1pninVisyUKoQQ+4utpCDPVBBCiIOKraTQ6pkKMveREELsLzaTgrQUhBCiTbGVFJq6j7QCQjJ9thBC7CO2kkJzS8H8KK0FIYTYW8SSglKqn1JqmVLqG6XURqXUjW1so5RSDymltimlvlJKTYhUPECrgWYz6VE0xxUSExOjdmwhhDiQSE5zEQB+qbVeq5RKAtYopT7QWn/TapszgaHh5XjgsfC/kRFOCpGYPlsIIbqDiLUUtNZFWuu14dd1wCagzz6bnQc8p41VQKpSqlekYkKp8NPXmm7Y65ikcOutt+41xcT8+fO5//77cbvdnHrqqUyYMIGxY8fyxhtvHHJfB5piu60psA80XbYQQhypYzIhnlIqBzgO+GyfVX2AglY/7w6/V3Skx5r33jzWFR9o7mzA7QablaA9EJ4+237IfeZm5/KXMw48096cOXOYN28e1113HQCvvvoqS5cuxeVy8frrr5OcnEx5eTlTp05l1qxZqIPM2d3WFNuhUKjNKbDbmi5bCCGORsSTglIqEVgMzNNa1x7hPq4Grgbo37//0QbU4bNmH3fccZSWlrJnzx7KyspIS0ujX79++P1+brvtNlauXInFYqGwsJCSkhKys7MPuK+2ptguKytrcwrstqbLFkKIoxHRpKBMNXwx8ILW+rU2NikE+rX6uW/4vb1orZ8EngQz99HBjnmwGj0A33yDtttxZ9d06PTZs2fPZtGiRRQXFzdPPPfCCy9QVlbGmjVrsNvt5OTktDlldpP2TrEthBCREsmrjxTwDLBJa/3nA2z2JnB5+CqkqUCN1vqIu47axWpFBYOADa39HbbbOXPm8PLLL7No0SJmz54NmGmus7KysNvtLFu2jPz8/IPu40BTbB9oCuy2pssWQoijEcn7FKYBlwGnKKXWhZezlFLXKKWuCW+zBNgObAOeAq6NYDyGxQKhEBaLrUMvSR09ejR1dXX06dOHXr3MWPncuXNZvXo1Y8eO5bnnnmPEiBEH3ceBptg+0BTYbU2XLYQQRyO2ps4G2L4d6utpGOxAa01CwsEL6lgiU2cL0X3J1NkHEm4pKGXv0O4jIYToDmIvKVitEAxKUhBCiDZ0m6TQ7m4wqzXcUrBhJsWTu5rhMM6fEKJb6xZJweVyUVFR0b6CLTzVhUWbq3GltWASQkVFBS6XK9qhCCGi7Jjc0Rxpffv2Zffu3ZSVlR1647o6qKwktAV8wXIcjs1YLM7IB9nJuVwu+vbtG+0whBBR1i2Sgt1ub77b95BeeAEuvZT6Nf/mi9rzGT16MT16XBjZAIUQoovoFt1HhyUpCQCHLw4An684mtEIIUSnErNJweaxAhZJCkII0UrsJYXww22UuwG7vYckBSGEaCX2kkK4pYDbjcORjc9XEt14hBCiE4ndpFBXF04K0lIQQogmsZcUmp6NLElBCCH2E7tJwe3G4eiJz1cid/MKIURY7CUFqxXi45tbClp7CQRqoh2VEEJ0CrGXFMC0FsJJAeReBSGEaBKbSSEpqbn7CCQpCCFEk9hMCvu0FPx+uSxVCCEgVpNCUpJ0HwkhRBtiNym43dhsaShll6QghBBhsZkUwt1HSqnwZamSFIQQAmI1KYS7jwCZ6kIIIVqJ3aTgdgPIXc1CCNFKbCaFxESTFLTGbpfuIyGEaBKbSSEpCUIhaGgItxRK0ToU7aiEECLqYjcpQPP02RDE76+IakhCCNEZxGZS2GemVJB7FYQQAmI1Kez1TAWZ6kIIIZrEZlLYa/rsppaCXJYqhBCxmRT2efoaSEtBCCEg1pOC243VmojFEidJQQghiGBSUEo9q5QqVUptOMD6k5VSNUqpdeHljkjFsp9WA81mqgu5gU0IIQBsEdz3P4CHgecOss1HWutzIhhD21p1H4Hc1SyEEE0i1lLQWq8EKiO1/6OSnAxOJ+zYAUB8/HDc7rWEQv4oByaEENEV7TGFE5RS65VS7yqlRh+zo9pscMop8M47oDUZGecSCFRRU/PJMQtBCCE6o2gmhbXAAK31eOBvwL8PtKFS6mql1Gql1OqysrKOOfq550JeHmzaRFraTJRyUFHxZsfsWwghuqioJQWtda3W2h1+vQSwK6UyD7Dtk1rrSVrrST169OiYAM4JD2W89RY2WyJpaadSXv4mWuuO2b8QQnRBUUsKSqlspZQKv54SjuXYTUDUrx8cdxy89RYAGRmzaGzMo6Fh0zELQQghOptIXpL6EvApMFwptVsp9ROl1DVKqWvCm3wf2KCUWg88BPxAH+tq+rnnwqefQlkZGRmm5VBeLl1IQojY1a6koJS6USmVrIxnlFJrlVIzD/YZrfUlWuteWmu71rqv1voZrfXjWuvHw+sf1lqP1lqP11pP1Vr/ryO+0GE591wzhfaSJbhcfUlMnCjjCkKImNbelsKPtda1wEwgDbgMuDdiUR0rEyZA797NXUiZmbOorV0l8yAJIWJWe5OCCv97FvC81npjq/e6LovFDDgvXQpeLxkZswBNRcXb0Y5MCCGior1JYY1S6n1MUliqlEoCusejys491zyac/lyEhPH43T2l3EFIUTMam9S+AlwKzBZa90A2IErIxbVsXTqqRAXB888gyouJjNzFlVVHxAM1kc7MiGEOObamxROAL7VWlcrpS4FfgPURC6sYyguDubOhX/9C3r3ZtCst+n3Dw8lxf+MdmRCCHHMtTcpPAY0KKXGA78E8jj4RHddyxNPwJo1cN99WNJ7M/DvUPXf++VGNiFEzGlvUgiE7yE4D3hYa/0IkBS5sI4xi8VcifSrX6FeeQUA52fbqKn5KMqBCSHEsdXepFCnlPo15lLUd5RSFsy4QvfTty960EDSvrJTWPhwtKMRQohjqr1JYQ7gxdyvUAz0Bf4UsaiiTE2fQeoGK2Uli/F6C6MdjhBCHDPtSgrhRPACkKKUOgdo1Fp3nzGFfc2YgbWqkYT8EHv2PBHtaIQQ4php7zQXFwOfA7OBi4HPlFLfj2RgUTV9OgC9t41mz54nCIW8UQ5ICCGOjfZ2H92OuUfhR1rry4EpwG8jF1aUDRwIffuSuTEDv7+UkhK5PFUIERvamxQsWuvSVj9XHMZnux6lYPp0HJ9tITlpKnl5t+DzlUc7KiGEiLj2FuzvKaWWKqWuUEpdAbwDLIlcWJ3AjBmo4mKGW24nGKwhL++X0Y5ICCEirr0DzTcDTwLjwsuTWutbIhlY1IXHFRLWFNOv368oKXmOqqr/RDkoIYSIrHZ3AWmtF2utbwovr0cyqE5h+HDIyoIVKxgw4DfExQ1hy5ZrCAY90Y5MCCEi5qBJQSlVp5SqbWOpU0rVHqsgoyI8rsDKlVitcQwb9jgezzby8++JdmRCCBExB00KWuskrXVyG0uS1jr5WAUZNdOnw65dsHMnaWmnkpU1l927H6CxsSDakQkhRER03yuIOsKMGebfJ8wNbIMG3YPWmp0774hiUEIIETmSFA5m7Fi4/HK49164915crgH07XsDxcULcbu/inZ0QgjR4WzRDqBTUwqefRYCAfj1r8Fmo/+Nv6ao6Gm2b7+FcePejXaEQgjRoSQpHIrVCgsXmsRw883YExIYcM7t5OX9H5WVH5Keflq0IxRCiA4j3UftYbPBP/8JM2fCr35Fb8ccnM4BbN/+K7TuHo+qFkIIkKTQfnY7PPggNDRg/dNfGDToHtzuL2VeJCFEtyJJ4XCMGgWXXQYPP0yWfzpJSZPYvv02gsGGaEcmhBAdQpLC4brzTgiFUPf8gcGD/4zPV0hBwZ+jHZUQQnQISQqHa+BAuPpqePppUit6k5l5Abt23YvXWxztyIQQ4qhJUjgSt99uxhjmz2fQoPvQ2is3tAkhugVJCkeiVy+4/np44QXit3no3fs6ioqewe3eEO3IhBDiqEhSOFK33AIpKXDbbeTk/BabLZm8vP+LdlRCCHFUIpYUlFLPKqVKlVJtVp+V8ZBSaptS6iul1IRIxRIR6elw663wzjvYV33DgAF3UFW1lIqK96IdmRBCHLFIthT+AZxxkPVnAkPDy9XAYxGMJTKuvx5694ZbbqFP72uJixtCXt7/EQoFoh2ZEEIckYglBa31SqDyIJucBzynjVVAqlKqV6TiiYj4eJg/Hz79FMvb7zFo0H00NGykuPiZaEcmhBBHJJpjCn2A1g8m2B1+bz9KqauVUquVUqvLysqOSXDtduWV5iltv/41mannkpJyEjt23EEg0L2fQSSE6J66xIR4WusnMc+IZtKkSTrK4ezNZoM//AEuugj18ssMPu/PrF07mfz8PzB48L3Rjk4I0Q6hEASDZml6rbWZD7NpaWiA2lqoqzPbJCSYzgKXy8yX2bR4vdDYaP71+VqWYBAcjpbFZjNXttvtZr3bDfX15jheb8vnQ+Hp1bSGceNgypTInotoJoVCoF+rn/uG3+t6LrgAxoyB++4jee5XZGdfwe7dD5CdfTkJCaOiHZ0QhyUUAr/fLMGgKRCbCi+v1xSKTYVjbS3U1IDHY6696NEDMjPNuj17oLAQqqpaCspAwBSGTQViY6MpCOvrTaEXH28WpaC0FEpKoKzMxGSxmCUY3Du+pv1ZLGZ/Ho8pWLU271mt5nsFAmb7QMAUtk2FbiBgtu0KbrmleyeFN4GfK6VeBo4HarTWRVGM58gpZX5bl10GS5YwaOYCysvfYMuWn5GbuxylVLQjFJ1cKGQKM7fbFLI1NabAra8377ndpvAKhVoK7abapNdrCkGPxyx+v9nW7zc/NxXefj9kZUHPnqbwbmxsOVZVFVRWmqW+/th+d4fD1LqVavkOYJJMU6wOR0sN3mo1tXO73bxuKuiDQUhLg7g4szStC4VMod+UPKxWcDpbauxN+2m9WCwmnqbWQzBoklVyMiQlmfVNtfrGxpYk17Rvl8v82/o4Fov5HTQlpKbfkd9v1icmtrQ+Wn+2KRYwx480pSOUIpVSLwEnA5lACXAnYAfQWj+uTEn5MOYKpQbgSq316kPtd9KkSXr16kNuduz5/TBkCPTvDx99xJ49T7Nly1WMGPEPsrN/FO3oxGFobIT8/L2b8K0LB5+vpTba2Gh+bvrP7vGY95pqxU3/vTweKC42S1lZSw21dXfDkXI6WwrCuLiWrgmbzRROyclmsVrNsUtKoLzcrEtJMUtamimE09NNoddUWNpsexdeTqdZ37Q0fd7lMomltNTsOynJXJjXp4/ZZ1Mh2VRQNyUup9Mcp7WmxGfrEp3bXYdSao3WetIht4tUUoiUTpsUAB56CG68ET7+GP2dE/jyy5PweLYwZcpm7PaMaEfX7YVCptZbWdlSY/b5oKICtm6FLVtMYV9f31Jwx8dDaqopFN1u2LgRtm9v6cc9EnZ7S00WTC3P6YTsbLP06GHWN9VK4+Jauk0SEloK2qaCt6kGabe3dKHYbC01SWmIivaQpBAN9fUwYAB85zvw5pu43V+xevUEsrOvYMSIp6MdXZcTDJpuj8JC2LULCgpa+pjLy81SVdXS9VFTc/C+4eRkGDTIFLQul1kaGsznq6vNz6NGwejRptEXH793jbmpEHc4zLq4uJYacFOfe1NhL0Rn096kIA20jpSQAD//Odx1F2zcSOLocfTr9wsKCu4nO/syUlNnRDvCYyoUaul39XhM4btlC2zebGrulZUtA5YeT0t3is9n3ms4wGMqUlNNbTsjw/SRDx/e0v2RlmaWppq1w2Fq3UOHms9IrfrIaa3xBX14g168AS9BHUShsCgLVouVREciDqujedtKTyXF7mJsFhuD0wdjs7Rd3Gwu38zrm16nurGaYRnDGJYxjAGpA3DZXNgtduxWOxZlad5vaX0pWyu3sq1yG96Al5mDZzKqx6jmsTt/0M+G0g2EdIheSb3ISsg64LHbUlhbyIr8FWwu30xOag4jM0cyNGMo1Y3V5FXmsb1qO76gj6yELLISskiPS8dqsWJRFizKQqorlYy4DJw25177DYQC5Ffns7VyK/nV+XudP5vFhtPmxGl1kuxMpn9Kf/qn9CfBkXAkv6qjIi2FjlZebloLJ58Mb71FUHv44ouxKGVn0qT1WK2uaEd41LQ2NeviYigqMleZ7Nhhlp07zfvl5abbpq1uGKWgXz9ToDf1d8fFtdTGHY6WrhNHUi05vRPIGWClXz8z8LhvHzRAva8epRQumwuLsuANeCmpL6HYXUy9rx6H1YHT5iTOFkd6XDoZ8RnNBViTkA6xZs8almxdwoayDYzKHMWk3pM4rtdx2Cw23D43dd46/CE/FmVBoQjqINWN1VR5qqhurKYx0NhcaDqsDtLi0khzpeGyucznfXXUemsprS+l2F1MaX0pqa5URvUYxageo8hKyKLWW0tNYw2egIeeCT3pm9yX7MRsNpdvZkX+Clbmr6S8oZzsxGx6JfUixZnCnro97KrZRWFdIf6gv/k7jcgcwc8m/YyLRl2Ew+qgzlvH65tf5/XNr1NQU0BpfSml9aXYrXZ6J/WmT1IfkpxJVDRUUOGpoNJTicfvwRv04gv6Dvm34bQ6SXImUeut3Wt7l83F6B6jGZE5ggR7Ak6bE601/9nxHzaVbwLAbrHjD/kPtOuDGpAygBk5M9hetZ3Ve1bTGGgZpLEoC+lx6aS6UklxppDsTMZqsTYXyBqN1hqNZkfVDvKq8o71CAEBAAAgAElEQVQohn0l2BNw2VwEdZBgKEiDv4GgDh7WPjLiMuid1JteSb3oldiL84afxwUjLziieKT7KJoefthMgTF/Ptx5J5WVH/DVVzPp3/92Bg26O9rRHVIwaLpqtm0zffC7dpl/CwrMsnt3yxUirWUNKYCTf4c1uRSXw0acw0aKI4O+rlEMjB9Dr+QsGlPXUaA/Y0P5OmwWG+lx6aS70hmaMZTpA6YzoZeZAuvtLW/z+OrHWZq3FJvFxoCUAQxMG0hOSg4DUgcwIGUAIR3i410f89Guj/i24tvmOBxWR7sKsCRHEimuFJIcSSQ6EtlZvZOyhjIUigGpA9hVs4tQhJ7B7bQ6yU7MJishi/KGcnZU72jX5yzKQm52Ln2T+1LsLqaorogabw29k3rTL7kffZP74rKZikdIh/hw+4fkVeXRM6EnU/tO5f289/EEPAxIGcDorNFkJWTRI74H/qCfwrpCCusKcfvcZMRlkBGfQbornXh7fHNSdVqdzf/aLLbmAjUQCuD2uan11lLrrSXZmdxckDUGGvm69Gu+KvmKrZVbm5OMP+hnSp8pXDTyIs4fcT7ZidnsqtnF1sqt7KrZhS/owx/04wv60LSUUxlxGQzNGMqQ9CGEdIh3t77L21vf5n8F/2No+lCm9p3KlD5TcNlcFNUVUeQuoqy+jBpvDdWN1dT56gjpUPPSlByUUmQlZDG9/3Rm5MxgTNYYCmoK2FS+ia0VW0l1pTIkfQiD0wfjtDqbE2qlp5KQDqEx56GmsYbyhnIqPBX4gj7TklJWEhwJDE4bzNCMoeSk5uCwOtBaE9Ih/CE/3oAXb9BLdWM1+dX57KrZRX5NPkXuoubvcc3Ea7h9+u1H9DcnSSGatDZ3Oi9cCG+8AbNmsWnTFZSWvsDEiWtITBwXtdC8AS+fF36Oz2uhfE8ye3Yk8XVBPhtrPiM/8Dk1ehdedzzamwDeZKgaBBXDyWQ4/VP7ktMzg5y+Lvr0MTOIZ2dDVs8gSysf5c6PbiMYCjI8cziBUAB/0E9JfQnVjdV7xRBvjyc3OxeFotJTSYWngtL6UgASHYkkOhIpdhfTJ6kPl4+/HIDtVdvZXrWd/Jr85m0BUl2pnNj/RI7vczx2ix1PwENjoJEkRxI9E3uSnZhNoiPRdHsEvHgCHio9lZQ3lFPeUE6tt5Y6Xx1un5vM+EzOHHImMwfPJDM+k3pfPeuK17G+ZD1gkkiSMwm7xY7G/Ge2KiuprlTS4tJIdaUSZ4vDaXPisDrwBrxUNVZR5akyMTmTmveR5Eja61Llel8931Z8S0VDBSmuFFJdqbhsLordxeyu3c2euj0MTB3Iif1PJMWV0u7fd0iHeD/vfR7+/GG+LP6SWcNmcem4S/lOv+/IpdIxRpJCtHk8cNJJphP988/xD+7B55+PxOXKYcKET1EqcqORJe4Snl77NCX1JfRNysFWl8OOXX7+W/gGW3ibgLWuzc856geRGhyCI8GDxVWP31pNqTd/vyZvgj2BzPhMeib2JCshi8LaQr4s/pLvDf4ej539GAPTBjZvq7VmT90eNpZtpNhdzPie4xmdNXq/Pt5idzEr81eyYucKShtKmTt2LucMO6fNvmCP30NBbQH+oJ+RPUY29zcLIQ5MkkJnUFAAEyeaWzy//JKS6tfYtOmH5OT8jpyc3x7VrkM6xIbSDWws3UicPY5EexL5u0I8t/55Pql5mSB+LP4kQvZWCaA+k9SS8xltO5fB/ePI6ldLSs8axuT0ZFrOFHok9NjvOL6gj+1V2/m2/FuK3cVUeCoobyinrKGsufnsD/q57aTbuGTMJVL7FKKTkquPOoN+/eD55+GMM+Dee8m64w4qKt5h5875pKZOP+yrkRoDjby68VXe+PYNVuxcQYWnYv+NvEmw7mf0Lfw54/oNYdjYajKH7mDAYB8XHj+JeNfh/codVgcjMkcwInPEYX1OCNE1SUvhWLjkEnjtNfj6awKDerFmzSSCwTomTVqHw5F10I/6gj521+7m71/+g8e+eJyKxjKSQv1RO0+hdv3JUDSRHtl+cqfUMTrXw4WTT2DimGTi44/NVxNCdA3SfdSZFBfDiBGmK+nDD3HXf8WaNceTmnoy48YtQbXqE19XvI5/rPsHb2x6hxJ3CZ5QuPtHK/j2XPjsBhx7TuG7JyvOPNM0QoYNk+vvhRAHJ91HnUl2Nvzxj3DttfDCCyReeinDMu6m9N2byff/kt659/H8V8/z4P8eZmPFOlTQgd56BlSfjfJk0islk+N7nM53vzOYiddDbi7SEhBCRIS0FI6VUIjikyfxgn0Tfayp5H5ZzOBK+OspKfxxWjyVqgiKcmHtTxlv/QFzL8xg2jRJAEKIjiEthU4kGAry2OrHuP17W6kNNALFMA0IWcFSA8UD6LX5Pn46fS5zn7cwfHi0IxZCxCpJChHQGGhka8XW5jsSn/nyGdYWrWVG39MZvPVBXvt3gGrXOlKGbeB0r5U733oDfc/lJMz5tkvc8SyE6L4kKXSw9/Pe5/LXL6ekvqT5vd6JfbhYvcI782bzsUdx3nlwzTXjOfVUsPga0ScsJXSfk8+H3IPD0ZO+fa+P4jcQQsQyuRW0g/iDfm754Ba+98/vkRGfwYsXvsjySz/lnvQ9BO/fxat3Xsz3Zio2boTFi+H00828+LhcqFdewRKwMe4P6WzfeANlZYuj/XWEEDFKkkIHKKwt5KS/n8SC/y3g6glXs+ySLyhceglzpk3l9ht6MXSIhU8+McmgzfGCYcNQf/878eurGH93Kpu++iHV1R8f8+8hhBDSfXSU1hev5+wXz6bGW8Mr338V35ezGTvCPJbwtNPglVdg+vR23EcwezaqvJyUa69l1J+S2HDbuYw/7kOSkiYek+8hhBAgLYWj8u7Wdznx7ycCsHDGxzx63WwuuwwGDoRPP4UPPoAZMw7jxrKf/QzuuYfMpXUMeSjIujUzqKpaFrkvIIQQ+5CkcASCoSALPlnAuS+dy6DUIVxQ/hk/OHk8X30FTzwB//sfTJ16hDv/9a/hl78ke3EdE68KUPDUTMpKZYxBCHFsSFI4THmVeZy88GRu+fAWpqZeQPWfV/LwH/vwwx/Ct9/C1VeHB5CPlFLwpz/Byy8TF+rFuFsC2M74PoUf3UwoFOiw7yGEEG2RpNBO5Q3l3P+/+xn/+Hi+Kv6aE4qe55MbXyXBnsTy5fCPf5hnAHcIpWDOHNSmbwk9uIDkbXZSL7ufdZ9Moq5uXQcdRAgh9idJ4RDe3fouF75yIb0f6M3NH9zMiPhpxP39a7545lLuukuxbp0ZN4gIhwPLvJuxLn6H+F2Kvvd8y9o1k9i582662vQkQoiuQZLCAfiDfq575zrOevEsPin4hKvG3cD3S79izS+WkmHvx2efwR13mIfMR9zpp6PuvJOspY0MXTGBnTt/y+bNVxAKHfo5xEIIcTjkktQ2VDRUMPtfs1m2cxm/nHoz/bbcw11X2qmthV/+Eu6+G1yuYxzUb34Dn3xCrz+uhIn/jy08gc9XxOjRi7DZko9xMKLL8fvNkwAHDYp2JKKTk5bCPr4p+4YpT0/hk4JPuG/qQj741QLm3WBnwgRYvx7uvz8KCQHAaoUXXkBlZtJ77ktM+O/FVJf8hy+/nE5Dw7dRCEh0KQsWmGd65OdHOxJxJIJB+OlP4cMPI34oSQqtLN22lBOeOYF6Xz0PjFnBvZdcTmEhLFpk7jkYPTrKAfboAcuXw4knkvz7Vznxmp6k/msrhb8dQ+28M9DXXw/ffBPlIEWnozUsXGhaC089Fe1oxOHSGm64AZ55Br766lgcT3epZeLEiToSHvn8EW29y6rHPTZO//HRfG2zaT1qlNZ5eRE53NF7912tR47U2vzJ6JAFHbQrHeqRofWGDdGOTnQmn39u/k6SkrTu2VNrrzfaEXU+fr/WixZp/dZbHb/vYFDrr77S+oMPtG5oOPzP33mn+f396ldHFQawWrejjI16IX+4S0cnhUAwoG9YcoNmPvqcF8/Rv19Qq0HrM87Qurq6Qw/V8fx+rdev16GCAl2Y/5j+/J9xujFD6UBmkg5JYhBN5s3T2uHQ+p//NP/lX3012hEdW//5j9ZPPaW1273/uooKre+7T+t+/cy5UUrrt9/ef7vdu83/t/YKBLR+/nmtL7pI68zM5sqbjovT+pxztH7iCa09nv0/98EHWt98s9bPPGOS+YMPms/9+Mdah0LtP34bJCm0Q523Tp/z4jma+eh5787Ti14LaKW0nj378H7/nUVDw3b9zevH68Z0tC/Dob2Lntb600+1XrdO69LSaIcnjpTXq/W112q9ePHhfzYQ0Do7W+sLLjCvBwzQ+pRTOjzETmnHDvO9mwrkHj20/tOfTG3vzTe1vvhirV0us+673zUthQkTtE5MNDV7rU1BvGCB1haL6Tr48MO9jxEK7V24h0Ja//vfZlswyeZHP9L6H//Q+p13tL7+eq0HDjTrxo1radUHg1rfdZdJSkq1xAxan3dehxRInSIpAGcA3wLbgFvbWH8FUAasCy8/PdQ+OyopFNQU6NzHc7XlLot+9PNH9Zo1WsfHaz1lypG18DqLUCioi5f9VnvT9/nDstu1/v3vtfb5OvaAHo/Wn3121LUYcRDXXNPye3zwwcP77Pvvm88tWmR+vuce8/PmzR0fZ1teflnrK67Q+vLLtZ4717y+915TG9+589B/N2631n/8o9Z//7uprbdHfr7Wt99uCvz4eK3/8AetV6zQ+vTTW1oDYGrw11+v9fr1LZ/dvVvr3r1N8szLM4kDtD77bK0HDTKvL7xQ60cf1fqHP2xpYSQmaj1sWEuX7rBhWv/rX21/v1DIfP+sLBPjgw9qfe655nOXXaZ1XZ3WW7aYSsDTT3dYgRT1pABYgTxgEOAA1gOj9tnmCuDhw9lvRyQFf9CvB/11kE78Q6JesmVJ899B//5aFxUd9e47hYbdX+otTx+n19+H3vHAOB34fviPbsIErb/8UutPPtH6jju0PvFE01d5JDWRpUu1HjJEx2SXxLHyzDPm/M6bZwoj0Pr//k/r+npzzs8/X+vhw01XRVsF0BVXaJ2c3FKbLSrS2mbT+he/2Hs7n8/s75RTtB461BR67Rl7yMvT+rrrTAwLFmjd2Gje93i0vuqqlhp6To4pVHv12ruyMniw+T6ffGJqy63V1Wk9ffre248caQr3KVPMMUeMMOdl/nytH3/crGsq9C++WOtdu/be50cfaX3TTWbs4EAVpNWrTTePzWZaCAsWtLQI7rnHJBowLbA5c7T+3e+0vvFGc7zTTjNdQ+2pfBUXm35qMMf6298iWrnqDEnhBGBpq59/Dfx6n22ikhS2VWzTzEc/sfoJrbVpnSUm7l1h6A5CoaDevftRvWJFgl6xIk4XPjRThzLTW/6DWSxajx5tXp9zTtt9rm359lvTxwamABk2zDSJmwoE0TE+/9yMBZx2mknagYApgJtafmAK2fHjzeszzzS15CYNDWZw+cor997vxRdrnZZmLlZ4+GGzz+xss48BA7Q+/viW1088YQrSNWu0/uYbrb/4QuslS7ReuNAUiBaLiWXyZPOZQYO0fvZZrSdOND//+tf7VziqqrT++GNTCH7ve6ZAbKpdv/iiSQ61tabCYrWa99av1/r++832U6eafy++2HQPDR3akgj69zcDszt2HN25f+01rceMMX38+yot1Xrr1o4pwINBk9BXrTr6fR1CZ0gK3weebvXzZfsmgHBSKAK+AhYB/Q6wr6uB1cDq/v37H/XJWbJliWY++uP8j3V9vWnB3XjjUe+202po2K43bfqxXr7coT/+N7roxhG64bn7ta6sNBs8/LD5zz1pkvlj/+gj896NN5oup4ULTTfE3Xe3FEAul1nX2NjSRfGnPx1doKGQqaH+4AemlngkyspMops2zfTt7lv7PJDSUlNz6wyKikxB2LevKZjLy1vWhUJaP/aY6VL68EOTKAIBrf/6V1ODTUzU+ic/0fqVV0yBDvv3gy9btnftOzHRVAreftvsKxTS+r33zN9D6+32XZKTTSuzsNDs9/33WyoZKSnm/LdHVZXWzz1nCmEw+5g0ySSE9rZA3W6tN20y8Ys2dZWkkAE4w6//H/DfQ+23I1oKf/n0L5r56FJ3qX7nHXMWli496t12el5vsd6+/U790Ufpevlym87Lu00HAuH+yjffbGkWNy37/gxan3CC6QNtKgianHWWKSTaGtAuL9f6pZdMwXWg1kRhoan9NR1n7twD18SCQVPQvfba3tv4fGbA0Ok0hSmY7oUHHjA1vt27999nQYHpV3a5TB/vvt0NGzea2mdKiukqmzrVdOXsu11rfr8p1O+80/Rt/+pXZqB41ixTgx4yxAwq1tS0fKa01Gzf6jJjnZVlaujttX27SagpKS376NVr/4IyFDKthBUrTAI60HkOhUxXygcfaP3GG+Z3+MYbWv/vf6by0FZft99vEtL27e2Pu0kwaMYghg83rYemcRDRITpDUjhk99E+21uBmkPttyOSwrVvX6tT703VoVBIX3edKfvaujqsu/J6y/Q33/xIL1uGXrVqiC4ufkkHAvWmib5ggakxFhSYQqGhwQx6/fe/By8IN240NbvrrjM/79xpWhaTJ+99NcVpp+3dTRUKmQI0Lc304z7yiGmBgGk1tFZQYArTnJyW/V1wQcu1w9dfb95buLClYM7N3b9WPGyY1jNmmNqxw2EKoMsuM+smT275YygpMcfq2dPs+5JLTJ+7zWa6TH7yE3POmgrHQMDUeJvGWcCcE6fTfL+xY00Xz8yZZl16uumjvvbalqtgTj3V/A5Wrz7yWq/fb646+/3v2768srPz+825Fx2qMyQFG7AdGNhqoHn0Ptv0avX6AmDVofbbEUnh9OdO15OfnKxDIfN//txzj3qXXVJl5X/0qlXD9LJl6BUrEvTGjXN1efm7OhRqZ5fLvq691hSCrQcHTzjBFOSrVpm+ZovFvFdVZWqTTQNtU6a0XBETDJrC0+Ew/epFRaZQdjhaEsuLL5oWgM1mCuHf/Masu+mmvWMKhbTes8cktUceaRkQPOkk04q45hqTwLTW+vXXzT6uuMIU9FOnmkT1+ed77zM/X+uf/7ylIAdT6PfsaV6PH2/2dbCuq9WrTesKzPf68Y9NYhUiQqKeFEwMnAVsCV+FdHv4vd8Bs8Kv/whsDCeMZcCIQ+2zI5LCgAcH6LmL5+pNm8wZeOyxo95llxUKBXRl5TK9efNV+qOP0vSyZejPPhup9+x5WgeDhzlwXFpqrjQZNsy0Etoa7Fu0yNSyhwwxBW5ioulW2rdWXF5uum2yskxTzmo1V7Ps2y3x8cfm0jEwV54c7fXcTXePNnXjHOzegKIik+juvtu0kC65xGzf3nEMrU0i6C6XvIlOrVMkhUgsR5sUGnwNWs1X+q7ld+kHHjBnoPUFG7EsGPTqoqLn9eefj9fLlqE//rin3rnzD9rnq2r/Tny+Q1+V8d57Jhmcf77pEjqQzz4zXSw//KHpwz6Q4mJzLXvTwPnRCAZN3z+YO12F6CbamxSU2bbrmDRpkl69evURf35D6QbGPjaWly56iafn/YCSEvj66w4MsBvQWlNd/V927VpAVdX7WK1J9O59DX363IDL1bdjDhIIgK0dM7drbZ5Edyw1NMDnn5unJx3rYwsRIUqpNVrrSYfaLuZmSd1SsQWAPq5hrFwJZ50V5YA6IaUUaWmnMn78UiZOXEtGxtkUFDzAqlU5bNhwAZWVH6B16OgO0p6EYII5uuMcifh4OPlkSQgiJsXcQ3aaksLu9UPx+yUpHEpS0nGMGvUSAwf+gT17nqC4+BnKy/+N09mPtLSZpKfPJC3tVOz2jGiHKoToADGXFLZWbCU7MZtlS5NITobvfCfaEXUNcXEDGTz4XgYOvIuyssXhZRHFxc8AFlJSTqJHjwvJzLyw47qYhBDHXMwlhS2VWxiWPowlS2DmTLDbox1R12KxOOnZ84f07PlDQqEAdXWrqaxcQlnZa2zbdiPbtt1IcvIJZGXNoUeP2TidvaMdshDiMMReUqjYwim9Z7GyEE45JdrRdG0Wi42UlKmkpExl4MDf0dDwLWVliyktfZVt2+axbdsvSEqaTErKd0hOPoGUlGk4nX2iHbYQ4iBiKilUN1ZTWl+Ks34YALm5UQ6om4mPH86AAbcxYMBt1NdvpqzsFaqqPmTPnsfZvfsvAKSmnkx29hX06PF9rNaEKEcshNhXTCWFrRVbAfDuGYpSMHZslAPqxhISRpCQcCc5OXcSCvlwu7+isvI9SkoWsnnzFWzZch2pqSeRnHwCycknkJAwFoejB0pZox26EDEttpJCpUkK5d8OY8gQSEyMckAxwmJxkJw8ieTkSQwYcDs1NZ9QWvoiNTUfsXPnfKDpXhkrDkc2cXGDycw8jx49vo/L1T+KkQsRe2IqKWyp2IJCkffFYCZK11FUKKVITT2R1NQTAQgEaqit/RyPZyte7x58vj3U1a0lL++X5OX9kqSkKWRknEN6+pkkJU1AqZi7tUaIYyrmkkL/5Bx2bHPykyuiHY0AsNlSSE8/HTh9r/cbGrZRXm4ufd2580527rwDuz2LtLRTSU2dQUrKDOLjh6PkBjMhOlTMJYWetmHkA+PHRzsacTDx8UPo3/8W+ve/BZ+vlMrK96msfJfq6mWUlr4EgM2WQWLiOBISxpGYOJ60tFNwuQZEOXIhuraYSQpaa7ZUbOE4ywmAXHnUlTgcWWRnX0p29qVorfF4tlFdvYLa2lXU139NUdFThEINAMTHjyQ9/QwSE8fjcPTC4eiFyzUQm00GkIRoj5hJCiX1JdT56vBXDyM9HfrI5fJdklKK+PihxMcPpXfvnwKgdYiGhs1UVi6lsvI9CgsfRWtvq884ycw8l549LyU9/UwsFke0whei04uZpNB0OWrFt8MYP17mOutOlLKQkDCKhIRR9Ov3C4LBRrzeAny+Yny+ImpqPqa09GXKyhZhscRjt2dgtSZgscQTCjUSCFQTCNTgdPahT5+fk519BTZbUrS/lhBRETNJobS+lDhbHDvXDuPsH0Y7GhFJVquruTUBkJV1MYMHP0BV1QdUVi4lGKwlGKwnGGzAYnFhs6Vgs6VQU/M/tm27gR07fkPPnpeRlHQccXFDiYsbjN3eQ1oYIibETFK4aNRFjOACxvxGySBzDLJY7GRknEVGxsGnxa2pWUVh4V8pKnqKPXt8e61TyonNlozT2Y/ExFwSE3NJSBiDy5WD09kXi0Um0hJdX8wkBYCvvzLXuMsgsziQprmcQqEAXu8uPJ6teDx5BAJVBAK1BAI1NDZup6LiTYqLn231SYXDkY3NloLFEofFEo/T2Yv4+BHhxXRvWSzOqH03IdojppLC+vVmVtSRI6MdiejsLBYbcXGDiIsb1OZ6rTU+XxH19RvxegtobNyF11tAMOgmFGogGGzA7V5PWdnrQBAApWzEx48mMXEcdnsPbLZkrNYU4uIGkpiYi9PZX+67EFEXU0lh3TqTEBzSNSyOklIKp7P3IacGD4V8eDx51NdvwO3+Erf7S6qrl+H3VxEK1e+1rc2WSnz8aOLiBuJyDWzulnI6++Bw9MFmS5GkISIuppLC+vVw+umH3k6IjmKxOEhIGElCwkiysmbvtS4UChAM1tLQsAW3ex1u9zoaGjZRXb0Sr/dFILTPvhJwOvvicvXDbs/CZkvBak3GZkvGbs9sXqzWRCwWFxaLC4cjG6s1fp/jenG7vyYubqA8MU/sJ2aSQmkpFBXJncyi87BYbFgs6c3jGK2FQj683t14vYXhZTc+X2Hzex7PZwSDZoxDa98BjgBgJTExl5SU7+Bw9Ka6ejk1NSsJhTwAxMUNJyVlGklJk0lIGENCwhjs9tQIfmvR2cVMUli/3vwrg8yiK7BYHAcd02gtGPTg91fg95fj95cTCjUQCjUSCnloaNhCbe2nFBU9QyjUQHz8SHr1uoqUlO/g8WyntvZ/lJf/e69Bc7u9R7jVkYHNlgZYgBBah9A6EF782GwpJCcfT3Lyd4iPH0lDw2bq6lbjdq8Lz3R7PomJudLl1cXETFKIi4Ozz5aWguh+rNY4rNa+B302tumqqmmzu0hrjddbQH39BurrN+Dx5OH3VxAIVNDYuAsgPDutQikbStlRyobHs42Kirf225/D0YvS0hfJz/8dTmd/UlOn43D0xuHoidWajN9fEp4RtxiLxRlOPulYrQnh52lYsNlSSU39LnFxOR10lkR7Ka31obfqRCZNmqRXr14d7TCEEIDfX0Vt7SoaGjYTHz+SpKSJOBw98PlKqah4m/Lyf+N2r8PnK9mrm8tmS8PhyCYU8hEIVBIIVNPyXI0WcXHDSUs7DYvFSTBYRyBQi9ZeTLllFq1DmPEXM/jfNEhvscShtT+8hFDKGk46ViwWO0o5sFgc4fmxBmG1uo7NSYsSpdQarfWkQ24nSUEIEWlaawKBaoLBWuz2LKzWuH3WBwmFvGgdROsgPl8RVVXvU1n5HtXVKzCth2Ss1qTwvR4K03JRgBWlLGgdwOstxO8vPYIIFU5nH2y2NIJBd/jS4kaUsmOxOLFY4oiLGxSekXccFosLj2cHjY078PtLwwnGicXiDLemzGK1JuFw9MRu74nNlkQgUBceC6olGKxrPpbNloLLNYi4uMG4XANwOHrtd46OliQFIURMCgbraWzcRSjkDbcI7JhxkWCrcRE/oZAPrb14vbvxePLweLYRCNRisyWFr+CKa94mGGzA49lCff0GQqHG5mPZ7Zk4HNloHQiP43hbjbsECAbdtNUCamKuEksgGKxB68Be62y2VOz2rHDCM62hXr2upn//m4/ovLQ3KcTMmIIQIjZYrQkkJETmDtVQKIDHsxWt/eEp2Q8+cWIoFAhfAFBCMFjffAmx1ZoUbvXYmrfzenfT2LidxsZ8fL6i8FIG6PCYjjgi8RIAAAZdSURBVAWns19EvldrkhSEEKKdLBbbYSUci8WG05mN05l9yO3i4nI6xcB6RB94q5Q6Qyn1rVJqm1Lq1jbWO5VSr4TXf6aUyolkPEIIIQ4uYklBmWH+R4AzgVHAJUqpUfts9hOgSms9BHgQuC9S8QghhDi0SLYUpgDbtNbbtbkW7WXgvH22OQ9YGH69CDhVyZ0uQggRNZFMCn2AglY/7w6/1+Y22gy91wAyGYsQQkRJRMcUOopS6mql1Gql1OqysrJohyOEEN1WJJNCIdD6+qm+4ffa3EYpZQNSgIp9d6S1flJrPUlrPalHjx4RClcIIUQkk8IXwFCl1ECllAP4AfDmPtu8Cfwo/Pr7wH91V7ubTgghupGI3aegtQ4opX4OLAWswLNa641Kqd8Bq7XWbwLPAM8rpbYBlZjEIYQQIkq63DQXSqkyIP8IP54JlHdgOF2ZnAtDzoMh58HozudhgNb6kP3vXS4pHA2l1Or2zP0RC+RcGHIeDDkPhpyHLnL1kRBCiGNDkoIQQohmsZYUnox2AJ2InAtDzoMh58GI+fMQU2MKQgghDi7WWgpCCCEOImaSwqGm8e6ulFL9lFLLlFLfKKU2KqVuDL+frpT6QCm1NfxvWrRjPRaUUlal1JdKqbfDPw8MT9u+LTyNuyPaMUaaUipVKfX/27uzUKuqOI7j31/Y4BBZUWJKqRWNpA2EZYVYD1ESPjSRRgS9BWUUTRRR0EMQWQ9RghFGEk1KbxFZSD6k5dCA9lRRNzSFzDKorH49rHWP13sFL4LnXO76fZ7OHu5m7X3/+/z3Xvvs/3pH0jeStkq6rMV4kHRfPSe+lvSGpGNajIfBmkgKwyzjPVr9A9xv+1xgNnB33feHgdW2zwRW1+kW3AtsHTD9DLCklm/fRSnnPtq9ALxv+2xgJuV4NBUPkqYA9wCX2D6f8oLtrbQZD/tpIikwvDLeo5LtbbY31s+/U74AprB/2fLlwILetLB7JE0FrgeW1WkB8yhl26GB4yDpOOAqSjUBbP9t+1cajAdKRYexte7aOGAbjcXDgbSSFIZTxnvUqyPbXQisAybZ3lYXbQcm9ahZ3fQ88CDwX50+EfjV+0ZMbyEupgM7gVdrN9oySeNpLB5s/wQ8C/xASQa7gQ20Fw9DtJIUmidpAvAusNj2bwOX1SKEo/pnaJLmAztsb+h1W3psDHAR8JLtC4E/GNRV1Eg8HE+5O5oOnAKMB67taaNGiFaSwnDKeI9ako6kJIQVtlfW2T9LmlyXTwZ29Kp9XTIHuEHS95Tuw3mUvvWJtfsA2oiLPqDP9ro6/Q4lSbQWD9cA39neaXsvsJISI63FwxCtJIXhlPEelWq/+SvAVtvPDVg0sGz5HcB73W5bN9l+xPZU29Mo//+PbC8EPqaUbYc2jsN24EdJZ9VZVwNbaCweKN1GsyWNq+dI/3FoKh4OpJmX1yRdR+lT7i/j/XSPm9QVkq4APgG+Yl9f+qOU5wpvAadSqs7ebPuXnjSyyyTNBR6wPV/SDMqdwwnAJmCR7b962b7DTdIsysP2o4BvgTspF4hNxYOkJ4FbKL/Q2wTcRXmG0FQ8DNZMUoiIiINrpfsoIiKGIUkhIiI6khQiIqIjSSEiIjqSFCIioiNJIaKLJM3tr9AaMRIlKUREREeSQsQBSFokab2kzZKW1nEY9khaUmvwr5Z0Ul13lqRPJX0paVX/WASSzpD0oaQvJG2UdHrd/IQB4xmsqG/URowISQoRg0g6h/Km6xzbs4B/gYWUommf2z4PWAM8Uf/kNeAh2xdQ3hzvn78CeNH2TOBySjVOKJVqF1PG9phBqbkTMSKMOfgqEc25GrgY+KxexI+lFIj7D3izrvM6sLKOTzDR9po6fznwtqRjgSm2VwHY/hOgbm+97b46vRmYBqw9/LsVcXBJChFDCVhu+5H9ZkqPD1rvUGvEDKyl8y85D2MESfdRxFCrgRslnQyd8axPo5wv/RU0bwPW2t4N7JJ0ZZ1/O7CmjnLXJ2lB3cbRksZ1dS8iDkGuUCIGsb1F0mPAB5KOAPYCd1MGpLm0LttBee4ApcTyy/VLv7/qKJQEsVTSU3UbN3VxNyIOSaqkRgyTpD22J/S6HRGHU7qPIiKiI3cKERHRkTuFiIjoSFKIiIiOJIWIiOhIUoiIiI4khYiI6EhSiIiIjv8BC6NQL20alD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 588us/sample - loss: 0.4972 - acc: 0.8625\n",
      "Loss: 0.49719522729717186 Accuracy: 0.862513\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5918 - acc: 0.1435\n",
      "Epoch 00001: val_loss improved from inf to 2.28798, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/001-2.2880.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.5918 - acc: 0.1435 - val_loss: 2.2880 - val_acc: 0.2867\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0032 - acc: 0.3449\n",
      "Epoch 00002: val_loss improved from 2.28798 to 1.67965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/002-1.6797.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.0031 - acc: 0.3450 - val_loss: 1.6797 - val_acc: 0.4589\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6353 - acc: 0.4560\n",
      "Epoch 00003: val_loss improved from 1.67965 to 1.34162, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/003-1.3416.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.6352 - acc: 0.4561 - val_loss: 1.3416 - val_acc: 0.5688\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4075 - acc: 0.5262\n",
      "Epoch 00004: val_loss improved from 1.34162 to 1.20025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/004-1.2002.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4075 - acc: 0.5262 - val_loss: 1.2002 - val_acc: 0.6233\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2690 - acc: 0.5769\n",
      "Epoch 00005: val_loss improved from 1.20025 to 1.10000, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/005-1.1000.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2690 - acc: 0.5770 - val_loss: 1.1000 - val_acc: 0.6560\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6201\n",
      "Epoch 00006: val_loss did not improve from 1.10000\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1610 - acc: 0.6201 - val_loss: 1.1047 - val_acc: 0.6522\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0662 - acc: 0.6516\n",
      "Epoch 00007: val_loss improved from 1.10000 to 0.94980, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/007-0.9498.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0661 - acc: 0.6517 - val_loss: 0.9498 - val_acc: 0.7149\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9782 - acc: 0.6853\n",
      "Epoch 00008: val_loss improved from 0.94980 to 0.85763, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/008-0.8576.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9783 - acc: 0.6853 - val_loss: 0.8576 - val_acc: 0.7377\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9146 - acc: 0.7071\n",
      "Epoch 00009: val_loss did not improve from 0.85763\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9146 - acc: 0.7071 - val_loss: 0.8585 - val_acc: 0.7417\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8459 - acc: 0.7299\n",
      "Epoch 00010: val_loss improved from 0.85763 to 0.81265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/010-0.8127.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8458 - acc: 0.7299 - val_loss: 0.8127 - val_acc: 0.7552\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7452\n",
      "Epoch 00011: val_loss improved from 0.81265 to 0.70701, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/011-0.7070.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7989 - acc: 0.7452 - val_loss: 0.7070 - val_acc: 0.7876\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7481 - acc: 0.7625\n",
      "Epoch 00012: val_loss improved from 0.70701 to 0.69990, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/012-0.6999.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7481 - acc: 0.7626 - val_loss: 0.6999 - val_acc: 0.7962\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7065 - acc: 0.7751\n",
      "Epoch 00013: val_loss improved from 0.69990 to 0.63397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/013-0.6340.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7064 - acc: 0.7752 - val_loss: 0.6340 - val_acc: 0.8130\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7886\n",
      "Epoch 00014: val_loss did not improve from 0.63397\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6660 - acc: 0.7886 - val_loss: 0.6739 - val_acc: 0.8036\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.8022\n",
      "Epoch 00015: val_loss improved from 0.63397 to 0.60481, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/015-0.6048.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6221 - acc: 0.8023 - val_loss: 0.6048 - val_acc: 0.8178\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8125\n",
      "Epoch 00016: val_loss improved from 0.60481 to 0.54009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/016-0.5401.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5971 - acc: 0.8125 - val_loss: 0.5401 - val_acc: 0.8472\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.8235\n",
      "Epoch 00017: val_loss improved from 0.54009 to 0.50499, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/017-0.5050.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5611 - acc: 0.8235 - val_loss: 0.5050 - val_acc: 0.8563\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8321\n",
      "Epoch 00018: val_loss did not improve from 0.50499\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5319 - acc: 0.8321 - val_loss: 0.5593 - val_acc: 0.8505\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8397\n",
      "Epoch 00019: val_loss improved from 0.50499 to 0.47000, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/019-0.4700.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5072 - acc: 0.8397 - val_loss: 0.4700 - val_acc: 0.8619\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8529\n",
      "Epoch 00020: val_loss improved from 0.47000 to 0.42205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/020-0.4221.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4738 - acc: 0.8529 - val_loss: 0.4221 - val_acc: 0.8863\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8564\n",
      "Epoch 00021: val_loss did not improve from 0.42205\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4546 - acc: 0.8564 - val_loss: 0.4357 - val_acc: 0.8805\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8629\n",
      "Epoch 00022: val_loss did not improve from 0.42205\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4343 - acc: 0.8629 - val_loss: 0.4261 - val_acc: 0.8845\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8719\n",
      "Epoch 00023: val_loss improved from 0.42205 to 0.40947, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/023-0.4095.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4162 - acc: 0.8719 - val_loss: 0.4095 - val_acc: 0.8849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8733\n",
      "Epoch 00024: val_loss improved from 0.40947 to 0.37568, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/024-0.3757.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4041 - acc: 0.8733 - val_loss: 0.3757 - val_acc: 0.8980\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8789\n",
      "Epoch 00025: val_loss did not improve from 0.37568\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3848 - acc: 0.8789 - val_loss: 0.3872 - val_acc: 0.8954\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8857\n",
      "Epoch 00026: val_loss improved from 0.37568 to 0.34843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/026-0.3484.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3680 - acc: 0.8857 - val_loss: 0.3484 - val_acc: 0.9068\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8890\n",
      "Epoch 00027: val_loss did not improve from 0.34843\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3539 - acc: 0.8890 - val_loss: 0.3723 - val_acc: 0.9015\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8917\n",
      "Epoch 00028: val_loss improved from 0.34843 to 0.33702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/028-0.3370.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3470 - acc: 0.8917 - val_loss: 0.3370 - val_acc: 0.9110\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8951\n",
      "Epoch 00029: val_loss improved from 0.33702 to 0.32951, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/029-0.3295.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3345 - acc: 0.8952 - val_loss: 0.3295 - val_acc: 0.9143\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8976\n",
      "Epoch 00030: val_loss did not improve from 0.32951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3233 - acc: 0.8976 - val_loss: 0.3367 - val_acc: 0.9108\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9010\n",
      "Epoch 00031: val_loss did not improve from 0.32951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3092 - acc: 0.9010 - val_loss: 0.3385 - val_acc: 0.9080\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9075\n",
      "Epoch 00032: val_loss did not improve from 0.32951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2965 - acc: 0.9074 - val_loss: 0.3611 - val_acc: 0.9040\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9075\n",
      "Epoch 00033: val_loss improved from 0.32951 to 0.31359, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/033-0.3136.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2936 - acc: 0.9075 - val_loss: 0.3136 - val_acc: 0.9152\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9119\n",
      "Epoch 00034: val_loss improved from 0.31359 to 0.30573, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/034-0.3057.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2783 - acc: 0.9119 - val_loss: 0.3057 - val_acc: 0.9203\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9137\n",
      "Epoch 00035: val_loss improved from 0.30573 to 0.28486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/035-0.2849.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2712 - acc: 0.9137 - val_loss: 0.2849 - val_acc: 0.9215\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9148\n",
      "Epoch 00036: val_loss did not improve from 0.28486\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2690 - acc: 0.9148 - val_loss: 0.2899 - val_acc: 0.9243\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9169\n",
      "Epoch 00037: val_loss improved from 0.28486 to 0.28386, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/037-0.2839.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2635 - acc: 0.9169 - val_loss: 0.2839 - val_acc: 0.9252\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9181\n",
      "Epoch 00038: val_loss did not improve from 0.28386\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2541 - acc: 0.9181 - val_loss: 0.2919 - val_acc: 0.9206\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9217\n",
      "Epoch 00039: val_loss did not improve from 0.28386\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2497 - acc: 0.9217 - val_loss: 0.2874 - val_acc: 0.9250\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9235\n",
      "Epoch 00040: val_loss improved from 0.28386 to 0.27000, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/040-0.2700.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2414 - acc: 0.9235 - val_loss: 0.2700 - val_acc: 0.9306\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9249\n",
      "Epoch 00041: val_loss did not improve from 0.27000\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2341 - acc: 0.9249 - val_loss: 0.2849 - val_acc: 0.9243\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9274\n",
      "Epoch 00042: val_loss did not improve from 0.27000\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2279 - acc: 0.9274 - val_loss: 0.2711 - val_acc: 0.9299\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9291\n",
      "Epoch 00043: val_loss improved from 0.27000 to 0.26517, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/043-0.2652.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2263 - acc: 0.9291 - val_loss: 0.2652 - val_acc: 0.9320\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9299\n",
      "Epoch 00044: val_loss did not improve from 0.26517\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2159 - acc: 0.9299 - val_loss: 0.2679 - val_acc: 0.9331\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9317\n",
      "Epoch 00045: val_loss did not improve from 0.26517\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2094 - acc: 0.9317 - val_loss: 0.2736 - val_acc: 0.9311\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9321\n",
      "Epoch 00046: val_loss did not improve from 0.26517\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2109 - acc: 0.9320 - val_loss: 0.2768 - val_acc: 0.9299\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9334\n",
      "Epoch 00047: val_loss improved from 0.26517 to 0.25213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/047-0.2521.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2074 - acc: 0.9334 - val_loss: 0.2521 - val_acc: 0.9343\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9350\n",
      "Epoch 00048: val_loss did not improve from 0.25213\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1991 - acc: 0.9350 - val_loss: 0.2661 - val_acc: 0.9320\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9377\n",
      "Epoch 00049: val_loss did not improve from 0.25213\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1923 - acc: 0.9377 - val_loss: 0.2764 - val_acc: 0.9276\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9391\n",
      "Epoch 00050: val_loss did not improve from 0.25213\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1898 - acc: 0.9391 - val_loss: 0.2597 - val_acc: 0.9329\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9389\n",
      "Epoch 00051: val_loss improved from 0.25213 to 0.23845, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv_checkpoint/051-0.2385.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1873 - acc: 0.9389 - val_loss: 0.2385 - val_acc: 0.9376\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9392\n",
      "Epoch 00052: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1865 - acc: 0.9392 - val_loss: 0.2645 - val_acc: 0.9299\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9400\n",
      "Epoch 00053: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1818 - acc: 0.9400 - val_loss: 0.2823 - val_acc: 0.9273\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9423\n",
      "Epoch 00054: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1796 - acc: 0.9423 - val_loss: 0.2496 - val_acc: 0.9380\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9438\n",
      "Epoch 00055: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1708 - acc: 0.9438 - val_loss: 0.2603 - val_acc: 0.9394\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9424\n",
      "Epoch 00056: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1733 - acc: 0.9424 - val_loss: 0.2513 - val_acc: 0.9369\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9441\n",
      "Epoch 00057: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1702 - acc: 0.9441 - val_loss: 0.2873 - val_acc: 0.9320\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9449\n",
      "Epoch 00058: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1676 - acc: 0.9448 - val_loss: 0.2618 - val_acc: 0.9359\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9468\n",
      "Epoch 00059: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1637 - acc: 0.9469 - val_loss: 0.2692 - val_acc: 0.9241\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9483\n",
      "Epoch 00060: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1586 - acc: 0.9483 - val_loss: 0.2565 - val_acc: 0.9387\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9472\n",
      "Epoch 00061: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1608 - acc: 0.9472 - val_loss: 0.2507 - val_acc: 0.9331\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9495\n",
      "Epoch 00062: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1565 - acc: 0.9495 - val_loss: 0.2557 - val_acc: 0.9399\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9509\n",
      "Epoch 00063: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1494 - acc: 0.9509 - val_loss: 0.2499 - val_acc: 0.9399\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9521\n",
      "Epoch 00064: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1445 - acc: 0.9521 - val_loss: 0.2538 - val_acc: 0.9362\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9504\n",
      "Epoch 00065: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1478 - acc: 0.9504 - val_loss: 0.2713 - val_acc: 0.9362\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9525\n",
      "Epoch 00066: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1437 - acc: 0.9525 - val_loss: 0.2754 - val_acc: 0.9371\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9531\n",
      "Epoch 00067: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1412 - acc: 0.9531 - val_loss: 0.2599 - val_acc: 0.9380\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9549\n",
      "Epoch 00068: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1381 - acc: 0.9550 - val_loss: 0.2644 - val_acc: 0.9399\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9554\n",
      "Epoch 00069: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1354 - acc: 0.9554 - val_loss: 0.2406 - val_acc: 0.9427\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9557\n",
      "Epoch 00070: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1349 - acc: 0.9557 - val_loss: 0.2651 - val_acc: 0.9387\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9542\n",
      "Epoch 00071: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1365 - acc: 0.9542 - val_loss: 0.2529 - val_acc: 0.9404\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9579\n",
      "Epoch 00072: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1269 - acc: 0.9579 - val_loss: 0.2841 - val_acc: 0.9348\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9579\n",
      "Epoch 00073: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1279 - acc: 0.9579 - val_loss: 0.2577 - val_acc: 0.9408\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9596\n",
      "Epoch 00074: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1239 - acc: 0.9595 - val_loss: 0.2568 - val_acc: 0.9436\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9588\n",
      "Epoch 00075: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1251 - acc: 0.9587 - val_loss: 0.2567 - val_acc: 0.9408\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9603\n",
      "Epoch 00076: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1199 - acc: 0.9603 - val_loss: 0.2511 - val_acc: 0.9441\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9589\n",
      "Epoch 00077: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1219 - acc: 0.9588 - val_loss: 0.2680 - val_acc: 0.9408\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9590\n",
      "Epoch 00078: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1231 - acc: 0.9590 - val_loss: 0.2462 - val_acc: 0.9411\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9614\n",
      "Epoch 00079: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1176 - acc: 0.9614 - val_loss: 0.2504 - val_acc: 0.9436\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9621\n",
      "Epoch 00080: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1147 - acc: 0.9621 - val_loss: 0.2588 - val_acc: 0.9432\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9615\n",
      "Epoch 00081: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1144 - acc: 0.9614 - val_loss: 0.2858 - val_acc: 0.9331\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9590\n",
      "Epoch 00082: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1262 - acc: 0.9590 - val_loss: 0.2502 - val_acc: 0.9441\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9643\n",
      "Epoch 00083: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1075 - acc: 0.9643 - val_loss: 0.2803 - val_acc: 0.9383\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9631\n",
      "Epoch 00084: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1080 - acc: 0.9631 - val_loss: 0.2572 - val_acc: 0.9453\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9618\n",
      "Epoch 00085: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1103 - acc: 0.9619 - val_loss: 0.2600 - val_acc: 0.9441\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9642\n",
      "Epoch 00086: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1094 - acc: 0.9642 - val_loss: 0.2557 - val_acc: 0.9436\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9642\n",
      "Epoch 00087: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1078 - acc: 0.9642 - val_loss: 0.2648 - val_acc: 0.9408\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9650\n",
      "Epoch 00088: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1049 - acc: 0.9650 - val_loss: 0.2828 - val_acc: 0.9432\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9652\n",
      "Epoch 00089: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1022 - acc: 0.9652 - val_loss: 0.2740 - val_acc: 0.9446\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9646\n",
      "Epoch 00090: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1061 - acc: 0.9646 - val_loss: 0.2560 - val_acc: 0.9455\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9674\n",
      "Epoch 00091: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0990 - acc: 0.9674 - val_loss: 0.2641 - val_acc: 0.9457\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9657\n",
      "Epoch 00092: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1015 - acc: 0.9656 - val_loss: 0.2514 - val_acc: 0.9453\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9669\n",
      "Epoch 00093: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0993 - acc: 0.9669 - val_loss: 0.2578 - val_acc: 0.9446\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9670\n",
      "Epoch 00094: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0947 - acc: 0.9670 - val_loss: 0.2720 - val_acc: 0.9429\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9672\n",
      "Epoch 00095: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0974 - acc: 0.9672 - val_loss: 0.2677 - val_acc: 0.9406\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9677\n",
      "Epoch 00096: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0946 - acc: 0.9677 - val_loss: 0.2560 - val_acc: 0.9455\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9671\n",
      "Epoch 00097: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0946 - acc: 0.9671 - val_loss: 0.2735 - val_acc: 0.9432\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9687\n",
      "Epoch 00098: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0903 - acc: 0.9687 - val_loss: 0.2712 - val_acc: 0.9460\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9685\n",
      "Epoch 00099: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0916 - acc: 0.9685 - val_loss: 0.2839 - val_acc: 0.9427\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9684\n",
      "Epoch 00100: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0943 - acc: 0.9684 - val_loss: 0.2725 - val_acc: 0.9434\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9705\n",
      "Epoch 00101: val_loss did not improve from 0.23845\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0897 - acc: 0.9705 - val_loss: 0.2644 - val_acc: 0.9425\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 558us/sample - loss: 0.2752 - acc: 0.9192\n",
      "Loss: 0.2752435742385167 Accuracy: 0.9192108\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6385 - acc: 0.1360\n",
      "Epoch 00001: val_loss improved from inf to 2.35072, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/001-2.3507.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 2.6386 - acc: 0.1359 - val_loss: 2.3507 - val_acc: 0.2455\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1492 - acc: 0.2954\n",
      "Epoch 00002: val_loss improved from 2.35072 to 1.79682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/002-1.7968.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.1492 - acc: 0.2953 - val_loss: 1.7968 - val_acc: 0.4163\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7821 - acc: 0.4218\n",
      "Epoch 00003: val_loss improved from 1.79682 to 1.44617, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/003-1.4462.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.7822 - acc: 0.4217 - val_loss: 1.4462 - val_acc: 0.5362\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4533 - acc: 0.5288\n",
      "Epoch 00004: val_loss improved from 1.44617 to 1.10982, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/004-1.1098.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4533 - acc: 0.5287 - val_loss: 1.1098 - val_acc: 0.6399\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2120 - acc: 0.6060\n",
      "Epoch 00005: val_loss improved from 1.10982 to 0.94287, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/005-0.9429.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2119 - acc: 0.6060 - val_loss: 0.9429 - val_acc: 0.6960\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0394 - acc: 0.6599\n",
      "Epoch 00006: val_loss improved from 0.94287 to 0.79896, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/006-0.7990.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0394 - acc: 0.6599 - val_loss: 0.7990 - val_acc: 0.7438\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8842 - acc: 0.7113\n",
      "Epoch 00007: val_loss improved from 0.79896 to 0.67842, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/007-0.6784.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8841 - acc: 0.7114 - val_loss: 0.6784 - val_acc: 0.7920\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7474\n",
      "Epoch 00008: val_loss improved from 0.67842 to 0.57807, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/008-0.5781.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7840 - acc: 0.7474 - val_loss: 0.5781 - val_acc: 0.8204\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6995 - acc: 0.7743\n",
      "Epoch 00009: val_loss did not improve from 0.57807\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6995 - acc: 0.7743 - val_loss: 0.6497 - val_acc: 0.7815\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.7947\n",
      "Epoch 00010: val_loss improved from 0.57807 to 0.46446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/010-0.4645.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6324 - acc: 0.7948 - val_loss: 0.4645 - val_acc: 0.8581\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.8138\n",
      "Epoch 00011: val_loss improved from 0.46446 to 0.43446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/011-0.4345.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5766 - acc: 0.8137 - val_loss: 0.4345 - val_acc: 0.8642\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8271\n",
      "Epoch 00012: val_loss improved from 0.43446 to 0.40201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/012-0.4020.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5318 - acc: 0.8271 - val_loss: 0.4020 - val_acc: 0.8763\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8361\n",
      "Epoch 00013: val_loss improved from 0.40201 to 0.40096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/013-0.4010.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4997 - acc: 0.8361 - val_loss: 0.4010 - val_acc: 0.8765\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8471\n",
      "Epoch 00014: val_loss improved from 0.40096 to 0.38384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/014-0.3838.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4707 - acc: 0.8471 - val_loss: 0.3838 - val_acc: 0.8838\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8533\n",
      "Epoch 00015: val_loss improved from 0.38384 to 0.36928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/015-0.3693.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4451 - acc: 0.8533 - val_loss: 0.3693 - val_acc: 0.8852\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8632\n",
      "Epoch 00016: val_loss did not improve from 0.36928\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4176 - acc: 0.8633 - val_loss: 0.3700 - val_acc: 0.8807\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8711\n",
      "Epoch 00017: val_loss improved from 0.36928 to 0.31055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/017-0.3106.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3965 - acc: 0.8711 - val_loss: 0.3106 - val_acc: 0.9082\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8783\n",
      "Epoch 00018: val_loss did not improve from 0.31055\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3750 - acc: 0.8783 - val_loss: 0.3250 - val_acc: 0.9008\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8823\n",
      "Epoch 00019: val_loss improved from 0.31055 to 0.28394, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/019-0.2839.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3621 - acc: 0.8822 - val_loss: 0.2839 - val_acc: 0.9147\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8866\n",
      "Epoch 00020: val_loss did not improve from 0.28394\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3454 - acc: 0.8866 - val_loss: 0.2927 - val_acc: 0.9094\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8922\n",
      "Epoch 00021: val_loss improved from 0.28394 to 0.28255, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/021-0.2826.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3290 - acc: 0.8922 - val_loss: 0.2826 - val_acc: 0.9161\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8948\n",
      "Epoch 00022: val_loss improved from 0.28255 to 0.26844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/022-0.2684.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3201 - acc: 0.8948 - val_loss: 0.2684 - val_acc: 0.9175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8971\n",
      "Epoch 00023: val_loss did not improve from 0.26844\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3147 - acc: 0.8971 - val_loss: 0.2889 - val_acc: 0.9166\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9018\n",
      "Epoch 00024: val_loss improved from 0.26844 to 0.26126, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/024-0.2613.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2968 - acc: 0.9018 - val_loss: 0.2613 - val_acc: 0.9210\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9032\n",
      "Epoch 00025: val_loss improved from 0.26126 to 0.25239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/025-0.2524.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2951 - acc: 0.9032 - val_loss: 0.2524 - val_acc: 0.9231\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9061\n",
      "Epoch 00026: val_loss improved from 0.25239 to 0.23951, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/026-0.2395.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2855 - acc: 0.9061 - val_loss: 0.2395 - val_acc: 0.9320\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9105\n",
      "Epoch 00027: val_loss did not improve from 0.23951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2721 - acc: 0.9105 - val_loss: 0.2529 - val_acc: 0.9231\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9115\n",
      "Epoch 00028: val_loss did not improve from 0.23951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2673 - acc: 0.9115 - val_loss: 0.2415 - val_acc: 0.9276\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9146\n",
      "Epoch 00029: val_loss did not improve from 0.23951\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2582 - acc: 0.9147 - val_loss: 0.2600 - val_acc: 0.9231\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9175\n",
      "Epoch 00030: val_loss improved from 0.23951 to 0.22727, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/030-0.2273.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2551 - acc: 0.9175 - val_loss: 0.2273 - val_acc: 0.9304\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9184\n",
      "Epoch 00031: val_loss did not improve from 0.22727\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2492 - acc: 0.9184 - val_loss: 0.2348 - val_acc: 0.9269\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9213\n",
      "Epoch 00032: val_loss improved from 0.22727 to 0.21519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/032-0.2152.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2406 - acc: 0.9213 - val_loss: 0.2152 - val_acc: 0.9371\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9237\n",
      "Epoch 00033: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2329 - acc: 0.9238 - val_loss: 0.2597 - val_acc: 0.9250\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9242\n",
      "Epoch 00034: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2311 - acc: 0.9242 - val_loss: 0.2266 - val_acc: 0.9350\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9258\n",
      "Epoch 00035: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2229 - acc: 0.9257 - val_loss: 0.2626 - val_acc: 0.9262\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9299\n",
      "Epoch 00036: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2155 - acc: 0.9298 - val_loss: 0.2420 - val_acc: 0.9278\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9270\n",
      "Epoch 00037: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2193 - acc: 0.9270 - val_loss: 0.2357 - val_acc: 0.9343\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9294\n",
      "Epoch 00038: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2108 - acc: 0.9294 - val_loss: 0.2168 - val_acc: 0.9380\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9308\n",
      "Epoch 00039: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2080 - acc: 0.9309 - val_loss: 0.2177 - val_acc: 0.9341\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9337\n",
      "Epoch 00040: val_loss did not improve from 0.21519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1999 - acc: 0.9337 - val_loss: 0.2171 - val_acc: 0.9394\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9367\n",
      "Epoch 00041: val_loss improved from 0.21519 to 0.20090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/041-0.2009.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1933 - acc: 0.9367 - val_loss: 0.2009 - val_acc: 0.9394\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9365\n",
      "Epoch 00042: val_loss did not improve from 0.20090\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1912 - acc: 0.9365 - val_loss: 0.2056 - val_acc: 0.9415\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9386\n",
      "Epoch 00043: val_loss improved from 0.20090 to 0.19956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/043-0.1996.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1858 - acc: 0.9386 - val_loss: 0.1996 - val_acc: 0.9436\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9383\n",
      "Epoch 00044: val_loss did not improve from 0.19956\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1857 - acc: 0.9383 - val_loss: 0.2029 - val_acc: 0.9406\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9404\n",
      "Epoch 00045: val_loss improved from 0.19956 to 0.18576, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/045-0.1858.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1804 - acc: 0.9404 - val_loss: 0.1858 - val_acc: 0.9460\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9422\n",
      "Epoch 00046: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1762 - acc: 0.9422 - val_loss: 0.2072 - val_acc: 0.9385\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9420\n",
      "Epoch 00047: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1739 - acc: 0.9420 - val_loss: 0.1944 - val_acc: 0.9455\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9442\n",
      "Epoch 00048: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1662 - acc: 0.9442 - val_loss: 0.1939 - val_acc: 0.9453\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9450\n",
      "Epoch 00049: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1630 - acc: 0.9450 - val_loss: 0.2230 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9452\n",
      "Epoch 00050: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1627 - acc: 0.9452 - val_loss: 0.1881 - val_acc: 0.9429\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9480\n",
      "Epoch 00051: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1544 - acc: 0.9480 - val_loss: 0.2069 - val_acc: 0.9415\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9479\n",
      "Epoch 00052: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1559 - acc: 0.9479 - val_loss: 0.1973 - val_acc: 0.9439\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9496\n",
      "Epoch 00053: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1498 - acc: 0.9497 - val_loss: 0.1958 - val_acc: 0.9415\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9492\n",
      "Epoch 00054: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1499 - acc: 0.9492 - val_loss: 0.1990 - val_acc: 0.9483\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9515\n",
      "Epoch 00055: val_loss did not improve from 0.18576\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1454 - acc: 0.9515 - val_loss: 0.2232 - val_acc: 0.9338\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9507\n",
      "Epoch 00056: val_loss improved from 0.18576 to 0.18041, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/056-0.1804.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1454 - acc: 0.9507 - val_loss: 0.1804 - val_acc: 0.9485\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9523\n",
      "Epoch 00057: val_loss improved from 0.18041 to 0.17469, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv_checkpoint/057-0.1747.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1409 - acc: 0.9523 - val_loss: 0.1747 - val_acc: 0.9520\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9537\n",
      "Epoch 00058: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1370 - acc: 0.9536 - val_loss: 0.1951 - val_acc: 0.9455\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9546\n",
      "Epoch 00059: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1347 - acc: 0.9547 - val_loss: 0.1834 - val_acc: 0.9504\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9552\n",
      "Epoch 00060: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1339 - acc: 0.9552 - val_loss: 0.2227 - val_acc: 0.9450\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9551\n",
      "Epoch 00061: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1319 - acc: 0.9551 - val_loss: 0.1935 - val_acc: 0.9502\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9569\n",
      "Epoch 00062: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1297 - acc: 0.9569 - val_loss: 0.2085 - val_acc: 0.9513\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9583\n",
      "Epoch 00063: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1243 - acc: 0.9583 - val_loss: 0.1915 - val_acc: 0.9483\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9576\n",
      "Epoch 00064: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1248 - acc: 0.9576 - val_loss: 0.2122 - val_acc: 0.9502\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9589\n",
      "Epoch 00065: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1211 - acc: 0.9589 - val_loss: 0.2154 - val_acc: 0.9485\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9589\n",
      "Epoch 00066: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1187 - acc: 0.9589 - val_loss: 0.2052 - val_acc: 0.9457\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9592\n",
      "Epoch 00067: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1195 - acc: 0.9592 - val_loss: 0.1837 - val_acc: 0.9527\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9617\n",
      "Epoch 00068: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1145 - acc: 0.9617 - val_loss: 0.1825 - val_acc: 0.9522\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9604\n",
      "Epoch 00069: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1160 - acc: 0.9604 - val_loss: 0.1987 - val_acc: 0.9509\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9601\n",
      "Epoch 00070: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1152 - acc: 0.9601 - val_loss: 0.1808 - val_acc: 0.9504\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9629\n",
      "Epoch 00071: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1087 - acc: 0.9629 - val_loss: 0.2027 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9631\n",
      "Epoch 00072: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1102 - acc: 0.9631 - val_loss: 0.1855 - val_acc: 0.9509\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9629\n",
      "Epoch 00073: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1091 - acc: 0.9629 - val_loss: 0.2295 - val_acc: 0.9441\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9640\n",
      "Epoch 00074: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1027 - acc: 0.9640 - val_loss: 0.2016 - val_acc: 0.9492\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9637\n",
      "Epoch 00075: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1056 - acc: 0.9637 - val_loss: 0.2034 - val_acc: 0.9520\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9640\n",
      "Epoch 00076: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1050 - acc: 0.9640 - val_loss: 0.1967 - val_acc: 0.9527\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9657\n",
      "Epoch 00077: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0991 - acc: 0.9657 - val_loss: 0.1965 - val_acc: 0.9499\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9666\n",
      "Epoch 00078: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0978 - acc: 0.9666 - val_loss: 0.1962 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9664\n",
      "Epoch 00079: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0998 - acc: 0.9663 - val_loss: 0.1853 - val_acc: 0.9541\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9655\n",
      "Epoch 00080: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0976 - acc: 0.9655 - val_loss: 0.1979 - val_acc: 0.9525\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9679\n",
      "Epoch 00081: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0946 - acc: 0.9679 - val_loss: 0.2087 - val_acc: 0.9495\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9672\n",
      "Epoch 00082: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0969 - acc: 0.9672 - val_loss: 0.2125 - val_acc: 0.9504\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9689\n",
      "Epoch 00083: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0913 - acc: 0.9689 - val_loss: 0.1966 - val_acc: 0.9520\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9691\n",
      "Epoch 00084: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0884 - acc: 0.9691 - val_loss: 0.2151 - val_acc: 0.9492\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9692\n",
      "Epoch 00085: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0887 - acc: 0.9692 - val_loss: 0.2078 - val_acc: 0.9525\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9711\n",
      "Epoch 00086: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0833 - acc: 0.9711 - val_loss: 0.1967 - val_acc: 0.9534\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9697\n",
      "Epoch 00087: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0871 - acc: 0.9697 - val_loss: 0.2418 - val_acc: 0.9513\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9700\n",
      "Epoch 00088: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0854 - acc: 0.9700 - val_loss: 0.2001 - val_acc: 0.9534\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9704\n",
      "Epoch 00089: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0870 - acc: 0.9704 - val_loss: 0.2530 - val_acc: 0.9383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9708\n",
      "Epoch 00090: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0838 - acc: 0.9708 - val_loss: 0.1895 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9711\n",
      "Epoch 00091: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0843 - acc: 0.9711 - val_loss: 0.1824 - val_acc: 0.9536\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9733\n",
      "Epoch 00092: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0783 - acc: 0.9733 - val_loss: 0.2135 - val_acc: 0.9532\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9726\n",
      "Epoch 00093: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0798 - acc: 0.9726 - val_loss: 0.2060 - val_acc: 0.9532\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9728\n",
      "Epoch 00094: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0823 - acc: 0.9728 - val_loss: 0.1948 - val_acc: 0.9529\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9723\n",
      "Epoch 00095: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0828 - acc: 0.9723 - val_loss: 0.1816 - val_acc: 0.9569\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9746\n",
      "Epoch 00096: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0729 - acc: 0.9746 - val_loss: 0.1974 - val_acc: 0.9562\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9734\n",
      "Epoch 00097: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0760 - acc: 0.9734 - val_loss: 0.1994 - val_acc: 0.9571\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9734\n",
      "Epoch 00098: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0767 - acc: 0.9734 - val_loss: 0.1942 - val_acc: 0.9583\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9730\n",
      "Epoch 00099: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0775 - acc: 0.9730 - val_loss: 0.1992 - val_acc: 0.9576\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9748\n",
      "Epoch 00100: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0720 - acc: 0.9748 - val_loss: 0.1851 - val_acc: 0.9576\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9747\n",
      "Epoch 00101: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0717 - acc: 0.9747 - val_loss: 0.2074 - val_acc: 0.9509\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9755\n",
      "Epoch 00102: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0705 - acc: 0.9755 - val_loss: 0.2008 - val_acc: 0.9543\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9763\n",
      "Epoch 00103: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0687 - acc: 0.9763 - val_loss: 0.1925 - val_acc: 0.9550\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9754\n",
      "Epoch 00104: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0694 - acc: 0.9754 - val_loss: 0.2204 - val_acc: 0.9497\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9767\n",
      "Epoch 00105: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0665 - acc: 0.9767 - val_loss: 0.2108 - val_acc: 0.9590\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9763\n",
      "Epoch 00106: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0677 - acc: 0.9763 - val_loss: 0.2004 - val_acc: 0.9576\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9779\n",
      "Epoch 00107: val_loss did not improve from 0.17469\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0664 - acc: 0.9779 - val_loss: 0.2027 - val_acc: 0.9567\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 579us/sample - loss: 0.2247 - acc: 0.9310\n",
      "Loss: 0.22471109002611966 Accuracy: 0.9310488\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5026 - acc: 0.1884\n",
      "Epoch 00001: val_loss improved from inf to 2.02844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/001-2.0284.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.5025 - acc: 0.1885 - val_loss: 2.0284 - val_acc: 0.3699\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9279 - acc: 0.3946\n",
      "Epoch 00002: val_loss improved from 2.02844 to 1.60976, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/002-1.6098.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.9278 - acc: 0.3946 - val_loss: 1.6098 - val_acc: 0.4801\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4845 - acc: 0.5179\n",
      "Epoch 00003: val_loss improved from 1.60976 to 1.14169, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/003-1.1417.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4846 - acc: 0.5179 - val_loss: 1.1417 - val_acc: 0.6296\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1410 - acc: 0.6260\n",
      "Epoch 00004: val_loss improved from 1.14169 to 0.90694, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/004-0.9069.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1411 - acc: 0.6259 - val_loss: 0.9069 - val_acc: 0.7039\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9172 - acc: 0.6952\n",
      "Epoch 00005: val_loss improved from 0.90694 to 0.81720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/005-0.8172.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9171 - acc: 0.6952 - val_loss: 0.8172 - val_acc: 0.7235\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7806 - acc: 0.7408\n",
      "Epoch 00006: val_loss improved from 0.81720 to 0.66942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/006-0.6694.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7806 - acc: 0.7408 - val_loss: 0.6694 - val_acc: 0.7771\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.7744\n",
      "Epoch 00007: val_loss improved from 0.66942 to 0.59242, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/007-0.5924.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6793 - acc: 0.7745 - val_loss: 0.5924 - val_acc: 0.8104\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.7980\n",
      "Epoch 00008: val_loss improved from 0.59242 to 0.49038, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/008-0.4904.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6055 - acc: 0.7981 - val_loss: 0.4904 - val_acc: 0.8395\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8174\n",
      "Epoch 00009: val_loss improved from 0.49038 to 0.45774, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/009-0.4577.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5522 - acc: 0.8174 - val_loss: 0.4577 - val_acc: 0.8535\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8339\n",
      "Epoch 00010: val_loss improved from 0.45774 to 0.40096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/010-0.4010.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5048 - acc: 0.8339 - val_loss: 0.4010 - val_acc: 0.8677\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.8457\n",
      "Epoch 00011: val_loss improved from 0.40096 to 0.39792, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/011-0.3979.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4671 - acc: 0.8457 - val_loss: 0.3979 - val_acc: 0.8710\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8551\n",
      "Epoch 00012: val_loss improved from 0.39792 to 0.35466, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/012-0.3547.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4342 - acc: 0.8552 - val_loss: 0.3547 - val_acc: 0.8875\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8698\n",
      "Epoch 00013: val_loss improved from 0.35466 to 0.33086, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/013-0.3309.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4027 - acc: 0.8698 - val_loss: 0.3309 - val_acc: 0.8924\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8783\n",
      "Epoch 00014: val_loss did not improve from 0.33086\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3717 - acc: 0.8783 - val_loss: 0.3386 - val_acc: 0.8901\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8837\n",
      "Epoch 00015: val_loss did not improve from 0.33086\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3541 - acc: 0.8837 - val_loss: 0.3326 - val_acc: 0.8898\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8876\n",
      "Epoch 00016: val_loss improved from 0.33086 to 0.31399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/016-0.3140.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3409 - acc: 0.8877 - val_loss: 0.3140 - val_acc: 0.8994\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8958\n",
      "Epoch 00017: val_loss improved from 0.31399 to 0.26933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/017-0.2693.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3121 - acc: 0.8958 - val_loss: 0.2693 - val_acc: 0.9124\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.8999\n",
      "Epoch 00018: val_loss improved from 0.26933 to 0.25349, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/018-0.2535.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3001 - acc: 0.8999 - val_loss: 0.2535 - val_acc: 0.9175\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9062\n",
      "Epoch 00019: val_loss improved from 0.25349 to 0.25282, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/019-0.2528.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2852 - acc: 0.9061 - val_loss: 0.2528 - val_acc: 0.9145\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9127\n",
      "Epoch 00020: val_loss did not improve from 0.25282\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2653 - acc: 0.9127 - val_loss: 0.2740 - val_acc: 0.9066\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9130\n",
      "Epoch 00021: val_loss improved from 0.25282 to 0.23441, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/021-0.2344.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2628 - acc: 0.9130 - val_loss: 0.2344 - val_acc: 0.9203\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9189\n",
      "Epoch 00022: val_loss improved from 0.23441 to 0.23046, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/022-0.2305.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2458 - acc: 0.9189 - val_loss: 0.2305 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9221\n",
      "Epoch 00023: val_loss did not improve from 0.23046\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2332 - acc: 0.9222 - val_loss: 0.2728 - val_acc: 0.9087\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9250\n",
      "Epoch 00024: val_loss improved from 0.23046 to 0.21894, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/024-0.2189.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2269 - acc: 0.9250 - val_loss: 0.2189 - val_acc: 0.9306\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9287\n",
      "Epoch 00025: val_loss improved from 0.21894 to 0.21413, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/025-0.2141.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2151 - acc: 0.9287 - val_loss: 0.2141 - val_acc: 0.9306\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9289\n",
      "Epoch 00026: val_loss did not improve from 0.21413\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2125 - acc: 0.9289 - val_loss: 0.2387 - val_acc: 0.9234\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9333\n",
      "Epoch 00027: val_loss did not improve from 0.21413\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2022 - acc: 0.9333 - val_loss: 0.2167 - val_acc: 0.9280\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9337\n",
      "Epoch 00028: val_loss improved from 0.21413 to 0.19694, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/028-0.1969.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1991 - acc: 0.9337 - val_loss: 0.1969 - val_acc: 0.9366\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9385\n",
      "Epoch 00029: val_loss did not improve from 0.19694\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1841 - acc: 0.9385 - val_loss: 0.2139 - val_acc: 0.9308\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9386\n",
      "Epoch 00030: val_loss did not improve from 0.19694\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1794 - acc: 0.9386 - val_loss: 0.2030 - val_acc: 0.9373\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9423\n",
      "Epoch 00031: val_loss did not improve from 0.19694\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1721 - acc: 0.9423 - val_loss: 0.2246 - val_acc: 0.9290\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9436\n",
      "Epoch 00032: val_loss improved from 0.19694 to 0.19398, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/032-0.1940.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1654 - acc: 0.9436 - val_loss: 0.1940 - val_acc: 0.9362\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9489\n",
      "Epoch 00033: val_loss improved from 0.19398 to 0.18962, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/033-0.1896.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1514 - acc: 0.9489 - val_loss: 0.1896 - val_acc: 0.9401\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9486\n",
      "Epoch 00034: val_loss did not improve from 0.18962\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1527 - acc: 0.9486 - val_loss: 0.1984 - val_acc: 0.9364\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9497\n",
      "Epoch 00035: val_loss did not improve from 0.18962\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1523 - acc: 0.9497 - val_loss: 0.2065 - val_acc: 0.9376\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9512\n",
      "Epoch 00036: val_loss did not improve from 0.18962\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1405 - acc: 0.9511 - val_loss: 0.2057 - val_acc: 0.9385\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9529\n",
      "Epoch 00037: val_loss did not improve from 0.18962\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1402 - acc: 0.9528 - val_loss: 0.1991 - val_acc: 0.9399\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9551\n",
      "Epoch 00038: val_loss improved from 0.18962 to 0.18922, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/038-0.1892.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1320 - acc: 0.9551 - val_loss: 0.1892 - val_acc: 0.9413\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9567\n",
      "Epoch 00039: val_loss did not improve from 0.18922\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1290 - acc: 0.9567 - val_loss: 0.1954 - val_acc: 0.9448\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9581\n",
      "Epoch 00040: val_loss did not improve from 0.18922\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1229 - acc: 0.9581 - val_loss: 0.1894 - val_acc: 0.9457\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9614\n",
      "Epoch 00041: val_loss improved from 0.18922 to 0.18835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/041-0.1884.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1158 - acc: 0.9614 - val_loss: 0.1884 - val_acc: 0.9471\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9597\n",
      "Epoch 00042: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1153 - acc: 0.9597 - val_loss: 0.1905 - val_acc: 0.9432\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9603\n",
      "Epoch 00043: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1149 - acc: 0.9603 - val_loss: 0.1942 - val_acc: 0.9422\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9635\n",
      "Epoch 00044: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1056 - acc: 0.9635 - val_loss: 0.2107 - val_acc: 0.9406\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9643\n",
      "Epoch 00045: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1040 - acc: 0.9642 - val_loss: 0.1922 - val_acc: 0.9478\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9655\n",
      "Epoch 00046: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1020 - acc: 0.9655 - val_loss: 0.1947 - val_acc: 0.9464\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9659\n",
      "Epoch 00047: val_loss did not improve from 0.18835\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0993 - acc: 0.9659 - val_loss: 0.2005 - val_acc: 0.9476\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9679\n",
      "Epoch 00048: val_loss improved from 0.18835 to 0.18076, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv_checkpoint/048-0.1808.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0933 - acc: 0.9679 - val_loss: 0.1808 - val_acc: 0.9492\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9685\n",
      "Epoch 00049: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0913 - acc: 0.9685 - val_loss: 0.2360 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9694\n",
      "Epoch 00050: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0910 - acc: 0.9694 - val_loss: 0.1861 - val_acc: 0.9469\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9708\n",
      "Epoch 00051: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0863 - acc: 0.9707 - val_loss: 0.1834 - val_acc: 0.9515\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9711\n",
      "Epoch 00052: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0818 - acc: 0.9711 - val_loss: 0.1963 - val_acc: 0.9476\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9729\n",
      "Epoch 00053: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0781 - acc: 0.9729 - val_loss: 0.1924 - val_acc: 0.9499\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9728\n",
      "Epoch 00054: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0808 - acc: 0.9728 - val_loss: 0.1970 - val_acc: 0.9520\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9732\n",
      "Epoch 00055: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0791 - acc: 0.9732 - val_loss: 0.2258 - val_acc: 0.9462\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9741\n",
      "Epoch 00056: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0749 - acc: 0.9741 - val_loss: 0.2438 - val_acc: 0.9380\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9727\n",
      "Epoch 00057: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0798 - acc: 0.9727 - val_loss: 0.2628 - val_acc: 0.9378\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9742\n",
      "Epoch 00058: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0727 - acc: 0.9742 - val_loss: 0.2159 - val_acc: 0.9420\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9774\n",
      "Epoch 00059: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0673 - acc: 0.9774 - val_loss: 0.2032 - val_acc: 0.9515\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9762\n",
      "Epoch 00060: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0692 - acc: 0.9762 - val_loss: 0.2066 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9779\n",
      "Epoch 00061: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0645 - acc: 0.9779 - val_loss: 0.2164 - val_acc: 0.9471\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9775\n",
      "Epoch 00062: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0635 - acc: 0.9775 - val_loss: 0.2162 - val_acc: 0.9518\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9775\n",
      "Epoch 00063: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0645 - acc: 0.9775 - val_loss: 0.2016 - val_acc: 0.9515\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9799\n",
      "Epoch 00064: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0593 - acc: 0.9799 - val_loss: 0.2122 - val_acc: 0.9506\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9786\n",
      "Epoch 00065: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0619 - acc: 0.9786 - val_loss: 0.2328 - val_acc: 0.9481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9790\n",
      "Epoch 00066: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0595 - acc: 0.9791 - val_loss: 0.2449 - val_acc: 0.9443\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9806\n",
      "Epoch 00067: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0565 - acc: 0.9806 - val_loss: 0.2154 - val_acc: 0.9534\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9814\n",
      "Epoch 00068: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0549 - acc: 0.9814 - val_loss: 0.2159 - val_acc: 0.9536\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9796\n",
      "Epoch 00069: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0588 - acc: 0.9796 - val_loss: 0.2229 - val_acc: 0.9546\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9830\n",
      "Epoch 00070: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0501 - acc: 0.9830 - val_loss: 0.2340 - val_acc: 0.9502\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9803\n",
      "Epoch 00071: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0548 - acc: 0.9803 - val_loss: 0.2344 - val_acc: 0.9490\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9827\n",
      "Epoch 00072: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0503 - acc: 0.9827 - val_loss: 0.2489 - val_acc: 0.9499\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9821\n",
      "Epoch 00073: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0536 - acc: 0.9821 - val_loss: 0.2173 - val_acc: 0.9511\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9838\n",
      "Epoch 00074: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0485 - acc: 0.9838 - val_loss: 0.2703 - val_acc: 0.9425\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9832\n",
      "Epoch 00075: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0502 - acc: 0.9832 - val_loss: 0.2294 - val_acc: 0.9506\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9823\n",
      "Epoch 00076: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0519 - acc: 0.9823 - val_loss: 0.2351 - val_acc: 0.9511\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9830\n",
      "Epoch 00077: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0493 - acc: 0.9830 - val_loss: 0.2318 - val_acc: 0.9481\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9849\n",
      "Epoch 00078: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0438 - acc: 0.9849 - val_loss: 0.2258 - val_acc: 0.9506\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9835\n",
      "Epoch 00079: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0481 - acc: 0.9835 - val_loss: 0.2458 - val_acc: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9840\n",
      "Epoch 00080: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0481 - acc: 0.9840 - val_loss: 0.2493 - val_acc: 0.9478\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9846\n",
      "Epoch 00081: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0434 - acc: 0.9846 - val_loss: 0.2597 - val_acc: 0.9515\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9849\n",
      "Epoch 00082: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0434 - acc: 0.9849 - val_loss: 0.2518 - val_acc: 0.9532\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9855\n",
      "Epoch 00083: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0409 - acc: 0.9855 - val_loss: 0.2333 - val_acc: 0.9562\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9855\n",
      "Epoch 00084: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0432 - acc: 0.9855 - val_loss: 0.2222 - val_acc: 0.9541\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9869\n",
      "Epoch 00085: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0392 - acc: 0.9869 - val_loss: 0.2259 - val_acc: 0.9529\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9866\n",
      "Epoch 00086: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0397 - acc: 0.9866 - val_loss: 0.2232 - val_acc: 0.9534\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9868\n",
      "Epoch 00087: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0387 - acc: 0.9868 - val_loss: 0.2576 - val_acc: 0.9469\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9855\n",
      "Epoch 00088: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0438 - acc: 0.9854 - val_loss: 0.2428 - val_acc: 0.9529\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9861\n",
      "Epoch 00089: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0405 - acc: 0.9861 - val_loss: 0.2384 - val_acc: 0.9529\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9868\n",
      "Epoch 00090: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0390 - acc: 0.9868 - val_loss: 0.2350 - val_acc: 0.9532\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9882\n",
      "Epoch 00091: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0345 - acc: 0.9882 - val_loss: 0.2602 - val_acc: 0.9541\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9871\n",
      "Epoch 00092: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0404 - acc: 0.9871 - val_loss: 0.2660 - val_acc: 0.9518\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9890\n",
      "Epoch 00093: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0326 - acc: 0.9891 - val_loss: 0.2522 - val_acc: 0.9543\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9877\n",
      "Epoch 00094: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0369 - acc: 0.9876 - val_loss: 0.2318 - val_acc: 0.9527\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 00095: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0397 - acc: 0.9870 - val_loss: 0.2422 - val_acc: 0.9532\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9881\n",
      "Epoch 00096: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0379 - acc: 0.9881 - val_loss: 0.2466 - val_acc: 0.9532\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9876\n",
      "Epoch 00097: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0372 - acc: 0.9876 - val_loss: 0.2542 - val_acc: 0.9504\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9889\n",
      "Epoch 00098: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0317 - acc: 0.9889 - val_loss: 0.2527 - val_acc: 0.9513\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 626us/sample - loss: 0.2923 - acc: 0.9223\n",
      "Loss: 0.29233125300917423 Accuracy: 0.9223261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO'\n",
    "    \n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 628us/sample - loss: 1.2567 - acc: 0.6204\n",
      "Loss: 1.2567360265480272 Accuracy: 0.62035304\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 490us/sample - loss: 1.0726 - acc: 0.6845\n",
      "Loss: 1.072585623992195 Accuracy: 0.6845275\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 540us/sample - loss: 0.4972 - acc: 0.8625\n",
      "Loss: 0.49719522729717186 Accuracy: 0.862513\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 548us/sample - loss: 0.2752 - acc: 0.9192\n",
      "Loss: 0.2752435742385167 Accuracy: 0.9192108\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 577us/sample - loss: 0.2247 - acc: 0.9310\n",
      "Loss: 0.22471109002611966 Accuracy: 0.9310488\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 613us/sample - loss: 0.2923 - acc: 0.9223\n",
      "Loss: 0.29233125300917423 Accuracy: 0.9223261\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 551us/sample - loss: 2.2371 - acc: 0.6496\n",
      "Loss: 2.237056231771055 Accuracy: 0.64963657\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 586us/sample - loss: 1.6579 - acc: 0.7090\n",
      "Loss: 1.6578784703960796 Accuracy: 0.70903426\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 605us/sample - loss: 0.6910 - acc: 0.8685\n",
      "Loss: 0.6909672454386362 Accuracy: 0.8685358\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 630us/sample - loss: 0.2775 - acc: 0.9308\n",
      "Loss: 0.2775355306509128 Accuracy: 0.93084115\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_025_DO_8_conv Model\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
