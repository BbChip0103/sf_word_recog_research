{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 75808)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 75808)        303232      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1212944     batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,527,056\n",
      "Trainable params: 1,375,248\n",
      "Non-trainable params: 151,808\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 25248)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 25248)        100992      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           403984      batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 521,136\n",
      "Trainable params: 470,384\n",
      "Non-trainable params: 50,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10464)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 10464)        41856       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           167440      batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 236,016\n",
      "Trainable params: 214,704\n",
      "Non-trainable params: 21,312\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5504)         0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 5504)         22016       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           88080       batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 157,616\n",
      "Trainable params: 146,096\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1792)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 1792)         7168        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           28688       batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 104,176\n",
      "Trainable params: 99,952\n",
      "Non-trainable params: 4,224\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 576)          0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 576)          2304        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           9232        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 100,656\n",
      "Trainable params: 98,736\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1237 - acc: 0.4297\n",
      "Epoch 00001: val_loss improved from inf to 1.79838, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv_checkpoint/001-1.7984.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 2.1241 - acc: 0.4296 - val_loss: 1.7984 - val_acc: 0.4468\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0594 - acc: 0.6929\n",
      "Epoch 00002: val_loss improved from 1.79838 to 1.72925, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv_checkpoint/002-1.7293.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0595 - acc: 0.6929 - val_loss: 1.7293 - val_acc: 0.5483\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6208 - acc: 0.8138\n",
      "Epoch 00003: val_loss improved from 1.72925 to 1.69668, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv_checkpoint/003-1.6967.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6211 - acc: 0.8137 - val_loss: 1.6967 - val_acc: 0.5695\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8816\n",
      "Epoch 00004: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4086 - acc: 0.8815 - val_loss: 1.8201 - val_acc: 0.5632\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9247\n",
      "Epoch 00005: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.2824 - acc: 0.9246 - val_loss: 1.8019 - val_acc: 0.5721\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9495\n",
      "Epoch 00006: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2065 - acc: 0.9495 - val_loss: 1.9807 - val_acc: 0.5614\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9635\n",
      "Epoch 00007: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1614 - acc: 0.9634 - val_loss: 1.9784 - val_acc: 0.5670\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9666\n",
      "Epoch 00008: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1411 - acc: 0.9666 - val_loss: 2.1076 - val_acc: 0.5563\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9700\n",
      "Epoch 00009: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1311 - acc: 0.9698 - val_loss: 2.4855 - val_acc: 0.5316\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9710\n",
      "Epoch 00010: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1272 - acc: 0.9709 - val_loss: 2.6023 - val_acc: 0.5367\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9745\n",
      "Epoch 00011: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1057 - acc: 0.9745 - val_loss: 2.5187 - val_acc: 0.5362\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9834\n",
      "Epoch 00012: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0810 - acc: 0.9834 - val_loss: 2.6887 - val_acc: 0.5425\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9832\n",
      "Epoch 00013: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0838 - acc: 0.9832 - val_loss: 2.6356 - val_acc: 0.5525\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9775\n",
      "Epoch 00014: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.1005 - acc: 0.9775 - val_loss: 2.5658 - val_acc: 0.5479\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9827\n",
      "Epoch 00015: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0728 - acc: 0.9828 - val_loss: 2.7068 - val_acc: 0.5535\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9851\n",
      "Epoch 00016: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0639 - acc: 0.9851 - val_loss: 2.5557 - val_acc: 0.5744\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9878\n",
      "Epoch 00017: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0571 - acc: 0.9878 - val_loss: 2.8433 - val_acc: 0.5490\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9829\n",
      "Epoch 00018: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0764 - acc: 0.9829 - val_loss: 2.7850 - val_acc: 0.5563\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9835\n",
      "Epoch 00019: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0691 - acc: 0.9835 - val_loss: 2.6112 - val_acc: 0.5858\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9828\n",
      "Epoch 00020: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.0720 - acc: 0.9828 - val_loss: 3.0648 - val_acc: 0.5386\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9883\n",
      "Epoch 00021: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0546 - acc: 0.9883 - val_loss: 2.8692 - val_acc: 0.5551\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9913\n",
      "Epoch 00022: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0430 - acc: 0.9913 - val_loss: 3.0128 - val_acc: 0.5488\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9835\n",
      "Epoch 00023: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0690 - acc: 0.9835 - val_loss: 3.1211 - val_acc: 0.5544\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9867\n",
      "Epoch 00024: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0556 - acc: 0.9867 - val_loss: 2.8984 - val_acc: 0.5777\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9862\n",
      "Epoch 00025: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0624 - acc: 0.9862 - val_loss: 3.4250 - val_acc: 0.5181\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9890\n",
      "Epoch 00026: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0496 - acc: 0.9891 - val_loss: 3.0842 - val_acc: 0.5518\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9899\n",
      "Epoch 00027: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0470 - acc: 0.9899 - val_loss: 3.0758 - val_acc: 0.5625\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9896\n",
      "Epoch 00028: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0507 - acc: 0.9896 - val_loss: 3.1702 - val_acc: 0.5511\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9883\n",
      "Epoch 00029: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0507 - acc: 0.9883 - val_loss: 3.3406 - val_acc: 0.5530\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9910\n",
      "Epoch 00030: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0438 - acc: 0.9909 - val_loss: 3.1565 - val_acc: 0.5511\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9823\n",
      "Epoch 00031: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0763 - acc: 0.9822 - val_loss: 3.2808 - val_acc: 0.5537\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9902\n",
      "Epoch 00032: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0461 - acc: 0.9901 - val_loss: 3.2947 - val_acc: 0.5567\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9908\n",
      "Epoch 00033: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0410 - acc: 0.9908 - val_loss: 3.5114 - val_acc: 0.5379\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9928\n",
      "Epoch 00034: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0375 - acc: 0.9928 - val_loss: 3.2876 - val_acc: 0.5635\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9911\n",
      "Epoch 00035: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0436 - acc: 0.9910 - val_loss: 3.3322 - val_acc: 0.5604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9892\n",
      "Epoch 00036: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0499 - acc: 0.9892 - val_loss: 3.2212 - val_acc: 0.5670\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9902\n",
      "Epoch 00037: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0466 - acc: 0.9902 - val_loss: 3.2704 - val_acc: 0.5740\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9905\n",
      "Epoch 00038: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0445 - acc: 0.9905 - val_loss: 3.4199 - val_acc: 0.5465\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9918\n",
      "Epoch 00039: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0410 - acc: 0.9917 - val_loss: 3.1741 - val_acc: 0.5756\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9864\n",
      "Epoch 00040: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0607 - acc: 0.9863 - val_loss: 3.3032 - val_acc: 0.5719\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9901\n",
      "Epoch 00041: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.0474 - acc: 0.9901 - val_loss: 3.5149 - val_acc: 0.5525\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9925\n",
      "Epoch 00042: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0375 - acc: 0.9925 - val_loss: 3.6170 - val_acc: 0.5514\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9937\n",
      "Epoch 00043: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0345 - acc: 0.9937 - val_loss: 3.6992 - val_acc: 0.5439\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9912\n",
      "Epoch 00044: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0442 - acc: 0.9912 - val_loss: 3.2476 - val_acc: 0.5868\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9933\n",
      "Epoch 00045: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0345 - acc: 0.9933 - val_loss: 3.2459 - val_acc: 0.5751\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9923\n",
      "Epoch 00046: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0363 - acc: 0.9923 - val_loss: 4.8494 - val_acc: 0.4906\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9938\n",
      "Epoch 00047: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0319 - acc: 0.9937 - val_loss: 3.5197 - val_acc: 0.5637\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9927\n",
      "Epoch 00048: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0369 - acc: 0.9926 - val_loss: 3.6013 - val_acc: 0.5565\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9915\n",
      "Epoch 00049: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0421 - acc: 0.9915 - val_loss: 3.5932 - val_acc: 0.5639\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9934\n",
      "Epoch 00050: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0364 - acc: 0.9934 - val_loss: 3.6096 - val_acc: 0.5621\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9933\n",
      "Epoch 00051: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0339 - acc: 0.9933 - val_loss: 3.6541 - val_acc: 0.5667\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9926\n",
      "Epoch 00052: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0376 - acc: 0.9926 - val_loss: 4.2809 - val_acc: 0.5099\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9937\n",
      "Epoch 00053: val_loss did not improve from 1.69668\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0322 - acc: 0.9937 - val_loss: 3.2587 - val_acc: 0.5961\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 410us/sample - loss: 1.7964 - acc: 0.5445\n",
      "Loss: 1.7964190778207432 Accuracy: 0.5445483\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9270 - acc: 0.4248\n",
      "Epoch 00001: val_loss improved from inf to 1.70415, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv_checkpoint/001-1.7042.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 1.9262 - acc: 0.4250 - val_loss: 1.7042 - val_acc: 0.4601\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2044 - acc: 0.6380\n",
      "Epoch 00002: val_loss improved from 1.70415 to 1.26230, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv_checkpoint/002-1.2623.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 1.2040 - acc: 0.6383 - val_loss: 1.2623 - val_acc: 0.6219\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9179 - acc: 0.7207\n",
      "Epoch 00003: val_loss improved from 1.26230 to 1.23208, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv_checkpoint/003-1.2321.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.9180 - acc: 0.7206 - val_loss: 1.2321 - val_acc: 0.6329\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7285 - acc: 0.7787\n",
      "Epoch 00004: val_loss did not improve from 1.23208\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7284 - acc: 0.7787 - val_loss: 1.2418 - val_acc: 0.6373\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8238\n",
      "Epoch 00005: val_loss improved from 1.23208 to 1.13888, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv_checkpoint/005-1.1389.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5888 - acc: 0.8238 - val_loss: 1.1389 - val_acc: 0.6762\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8602\n",
      "Epoch 00006: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4747 - acc: 0.8603 - val_loss: 1.1820 - val_acc: 0.6611\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8903\n",
      "Epoch 00007: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3904 - acc: 0.8904 - val_loss: 1.2819 - val_acc: 0.6378\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9138\n",
      "Epoch 00008: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.3237 - acc: 0.9138 - val_loss: 1.1864 - val_acc: 0.6702\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9322\n",
      "Epoch 00009: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.2709 - acc: 0.9322 - val_loss: 1.3676 - val_acc: 0.6410\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9460\n",
      "Epoch 00010: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.2265 - acc: 0.9459 - val_loss: 1.2660 - val_acc: 0.6641\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9564\n",
      "Epoch 00011: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.1939 - acc: 0.9564 - val_loss: 1.4240 - val_acc: 0.6334\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9613\n",
      "Epoch 00012: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1740 - acc: 0.9613 - val_loss: 1.2013 - val_acc: 0.6823\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9691\n",
      "Epoch 00013: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1473 - acc: 0.9690 - val_loss: 1.2866 - val_acc: 0.6823\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9757\n",
      "Epoch 00014: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.1271 - acc: 0.9756 - val_loss: 1.2784 - val_acc: 0.6765\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9787\n",
      "Epoch 00015: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1163 - acc: 0.9787 - val_loss: 1.2697 - val_acc: 0.6862\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9837\n",
      "Epoch 00016: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0988 - acc: 0.9838 - val_loss: 1.2995 - val_acc: 0.6816\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9848\n",
      "Epoch 00017: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0917 - acc: 0.9847 - val_loss: 1.3726 - val_acc: 0.6783\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9852\n",
      "Epoch 00018: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0861 - acc: 0.9851 - val_loss: 1.4746 - val_acc: 0.6548\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9818\n",
      "Epoch 00019: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0903 - acc: 0.9818 - val_loss: 1.3812 - val_acc: 0.6769\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9908\n",
      "Epoch 00020: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0660 - acc: 0.9907 - val_loss: 1.5274 - val_acc: 0.6555\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9918\n",
      "Epoch 00021: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0605 - acc: 0.9918 - val_loss: 1.4861 - val_acc: 0.6781\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9888\n",
      "Epoch 00022: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0653 - acc: 0.9888 - val_loss: 1.6433 - val_acc: 0.6667\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9897\n",
      "Epoch 00023: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0633 - acc: 0.9896 - val_loss: 1.4447 - val_acc: 0.6811\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9916\n",
      "Epoch 00024: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0519 - acc: 0.9917 - val_loss: 1.4990 - val_acc: 0.6758\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9938\n",
      "Epoch 00025: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0452 - acc: 0.9937 - val_loss: 2.1314 - val_acc: 0.5975\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9895\n",
      "Epoch 00026: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0582 - acc: 0.9894 - val_loss: 1.7652 - val_acc: 0.6548\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9924\n",
      "Epoch 00027: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.0489 - acc: 0.9923 - val_loss: 1.6659 - val_acc: 0.6497\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9918\n",
      "Epoch 00028: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0484 - acc: 0.9917 - val_loss: 1.6579 - val_acc: 0.6608\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9931\n",
      "Epoch 00029: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.0427 - acc: 0.9931 - val_loss: 1.6473 - val_acc: 0.6643\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9924\n",
      "Epoch 00030: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0457 - acc: 0.9923 - val_loss: 1.8210 - val_acc: 0.6359\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9920\n",
      "Epoch 00031: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0463 - acc: 0.9920 - val_loss: 1.8293 - val_acc: 0.6490\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9955\n",
      "Epoch 00032: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0337 - acc: 0.9954 - val_loss: 1.6509 - val_acc: 0.6741\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9926- ETA: 1s - loss: 0.0\n",
      "Epoch 00033: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0429 - acc: 0.9926 - val_loss: 1.7170 - val_acc: 0.6615\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9945\n",
      "Epoch 00034: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0371 - acc: 0.9945 - val_loss: 2.2420 - val_acc: 0.5933\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9946\n",
      "Epoch 00035: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0330 - acc: 0.9946 - val_loss: 1.8090 - val_acc: 0.6504\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9953\n",
      "Epoch 00036: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0317 - acc: 0.9952 - val_loss: 2.1945 - val_acc: 0.6101\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9928\n",
      "Epoch 00037: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0395 - acc: 0.9928 - val_loss: 1.7674 - val_acc: 0.6690\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9954\n",
      "Epoch 00038: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0305 - acc: 0.9954 - val_loss: 2.0724 - val_acc: 0.6331\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9967\n",
      "Epoch 00039: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0262 - acc: 0.9966 - val_loss: 1.8031 - val_acc: 0.6601\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9944\n",
      "Epoch 00040: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0337 - acc: 0.9943 - val_loss: 1.8754 - val_acc: 0.6660\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9892\n",
      "Epoch 00041: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0516 - acc: 0.9892 - val_loss: 1.7431 - val_acc: 0.6688\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9972- ETA:\n",
      "Epoch 00042: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0229 - acc: 0.9971 - val_loss: 1.8590 - val_acc: 0.6606\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9944\n",
      "Epoch 00043: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0320 - acc: 0.9944 - val_loss: 1.7285 - val_acc: 0.6774\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9964\n",
      "Epoch 00044: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0257 - acc: 0.9964 - val_loss: 1.9501 - val_acc: 0.6515\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9970\n",
      "Epoch 00045: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0234 - acc: 0.9970 - val_loss: 1.7721 - val_acc: 0.6716\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9956\n",
      "Epoch 00046: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0272 - acc: 0.9955 - val_loss: 2.4011 - val_acc: 0.5998\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9953\n",
      "Epoch 00047: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0277 - acc: 0.9953 - val_loss: 1.7602 - val_acc: 0.6781\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9940\n",
      "Epoch 00048: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0306 - acc: 0.9939 - val_loss: 1.9491 - val_acc: 0.6562\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9934\n",
      "Epoch 00049: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0317 - acc: 0.9934 - val_loss: 1.7728 - val_acc: 0.6783\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9959\n",
      "Epoch 00050: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0259 - acc: 0.9958 - val_loss: 1.7974 - val_acc: 0.6716\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9971\n",
      "Epoch 00051: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0209 - acc: 0.9971 - val_loss: 1.8545 - val_acc: 0.6751\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9928\n",
      "Epoch 00052: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0335 - acc: 0.9928 - val_loss: 1.8767 - val_acc: 0.6674\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9969\n",
      "Epoch 00053: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0212 - acc: 0.9968 - val_loss: 2.0847 - val_acc: 0.6403\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9950\n",
      "Epoch 00054: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0268 - acc: 0.9950 - val_loss: 2.2079 - val_acc: 0.6224\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9963\n",
      "Epoch 00055: val_loss did not improve from 1.13888\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0251 - acc: 0.9963 - val_loss: 1.9776 - val_acc: 0.6639\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 414us/sample - loss: 1.2226 - acc: 0.6407\n",
      "Loss: 1.2226354261053685 Accuracy: 0.6407061\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9077 - acc: 0.4155\n",
      "Epoch 00001: val_loss improved from inf to 1.65186, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/001-1.6519.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.9077 - acc: 0.4156 - val_loss: 1.6519 - val_acc: 0.4621\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2267 - acc: 0.6248\n",
      "Epoch 00002: val_loss improved from 1.65186 to 1.19720, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/002-1.1972.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 1.2267 - acc: 0.6248 - val_loss: 1.1972 - val_acc: 0.6382\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0156 - acc: 0.6924\n",
      "Epoch 00003: val_loss improved from 1.19720 to 1.01292, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/003-1.0129.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 1.0156 - acc: 0.6924 - val_loss: 1.0129 - val_acc: 0.6986\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8797 - acc: 0.7352\n",
      "Epoch 00004: val_loss improved from 1.01292 to 0.91668, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/004-0.9167.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.8797 - acc: 0.7352 - val_loss: 0.9167 - val_acc: 0.7349\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7669\n",
      "Epoch 00005: val_loss improved from 0.91668 to 0.89585, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/005-0.8959.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.7801 - acc: 0.7669 - val_loss: 0.8959 - val_acc: 0.7391\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6987 - acc: 0.7906\n",
      "Epoch 00006: val_loss did not improve from 0.89585\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.6988 - acc: 0.7905 - val_loss: 1.0684 - val_acc: 0.6874\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8120\n",
      "Epoch 00007: val_loss improved from 0.89585 to 0.86404, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/007-0.8640.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.6337 - acc: 0.8120 - val_loss: 0.8640 - val_acc: 0.7519\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5706 - acc: 0.8297\n",
      "Epoch 00008: val_loss improved from 0.86404 to 0.85092, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/008-0.8509.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.5706 - acc: 0.8297 - val_loss: 0.8509 - val_acc: 0.7575\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8498\n",
      "Epoch 00009: val_loss improved from 0.85092 to 0.83771, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/009-0.8377.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.5160 - acc: 0.8497 - val_loss: 0.8377 - val_acc: 0.7552\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8615\n",
      "Epoch 00010: val_loss did not improve from 0.83771\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.4749 - acc: 0.8615 - val_loss: 0.8823 - val_acc: 0.7466\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4358 - acc: 0.8722\n",
      "Epoch 00011: val_loss did not improve from 0.83771\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.4359 - acc: 0.8721 - val_loss: 1.0178 - val_acc: 0.7235\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8881\n",
      "Epoch 00012: val_loss improved from 0.83771 to 0.78226, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/012-0.7823.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.3953 - acc: 0.8881 - val_loss: 0.7823 - val_acc: 0.7782\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8996\n",
      "Epoch 00013: val_loss did not improve from 0.78226\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3569 - acc: 0.8996 - val_loss: 0.9049 - val_acc: 0.7480\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.9082\n",
      "Epoch 00014: val_loss did not improve from 0.78226\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3293 - acc: 0.9081 - val_loss: 0.7969 - val_acc: 0.7815\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9176- E\n",
      "Epoch 00015: val_loss improved from 0.78226 to 0.76525, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/015-0.7652.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.3012 - acc: 0.9175 - val_loss: 0.7652 - val_acc: 0.7934\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9256\n",
      "Epoch 00016: val_loss did not improve from 0.76525\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.2756 - acc: 0.9255 - val_loss: 0.7950 - val_acc: 0.7890\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9345\n",
      "Epoch 00017: val_loss improved from 0.76525 to 0.75423, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv_checkpoint/017-0.7542.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2491 - acc: 0.9344 - val_loss: 0.7542 - val_acc: 0.7973\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9393\n",
      "Epoch 00018: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.2310 - acc: 0.9393 - val_loss: 0.8925 - val_acc: 0.7685\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9436\n",
      "Epoch 00019: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2152 - acc: 0.9436 - val_loss: 0.8205 - val_acc: 0.7829\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9497\n",
      "Epoch 00020: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2023 - acc: 0.9496 - val_loss: 0.8986 - val_acc: 0.7659\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9556\n",
      "Epoch 00021: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1803 - acc: 0.9556 - val_loss: 0.7945 - val_acc: 0.7939\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9595\n",
      "Epoch 00022: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1660 - acc: 0.9595 - val_loss: 0.7649 - val_acc: 0.8043\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9637\n",
      "Epoch 00023: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1537 - acc: 0.9637 - val_loss: 0.8363 - val_acc: 0.7915\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9649\n",
      "Epoch 00024: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1479 - acc: 0.9649 - val_loss: 0.9185 - val_acc: 0.7759\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9670\n",
      "Epoch 00025: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1364 - acc: 0.9669 - val_loss: 0.9442 - val_acc: 0.7694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9718\n",
      "Epoch 00026: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1281 - acc: 0.9718 - val_loss: 0.8564 - val_acc: 0.7939\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9734\n",
      "Epoch 00027: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1180 - acc: 0.9733 - val_loss: 0.7835 - val_acc: 0.8053\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9780\n",
      "Epoch 00028: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1055 - acc: 0.9780 - val_loss: 0.8641 - val_acc: 0.7987\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9763\n",
      "Epoch 00029: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1087 - acc: 0.9763 - val_loss: 0.8295 - val_acc: 0.7994\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9774\n",
      "Epoch 00030: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1055 - acc: 0.9774 - val_loss: 0.9611 - val_acc: 0.7806\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9814\n",
      "Epoch 00031: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0916 - acc: 0.9813 - val_loss: 0.8647 - val_acc: 0.7992\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9822\n",
      "Epoch 00032: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0915 - acc: 0.9821 - val_loss: 0.8514 - val_acc: 0.8025\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9855\n",
      "Epoch 00033: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0790 - acc: 0.9855 - val_loss: 0.9680 - val_acc: 0.7729\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9841\n",
      "Epoch 00034: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0792 - acc: 0.9841 - val_loss: 0.9089 - val_acc: 0.7859\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9856\n",
      "Epoch 00035: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0737 - acc: 0.9856 - val_loss: 0.9473 - val_acc: 0.7806\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9882\n",
      "Epoch 00036: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0679 - acc: 0.9882 - val_loss: 1.0604 - val_acc: 0.7685\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9881\n",
      "Epoch 00037: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0653 - acc: 0.9881 - val_loss: 0.9053 - val_acc: 0.7906\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9891\n",
      "Epoch 00038: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0620 - acc: 0.9890 - val_loss: 0.9624 - val_acc: 0.7773\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9862\n",
      "Epoch 00039: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0703 - acc: 0.9862 - val_loss: 0.9246 - val_acc: 0.7943\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9885\n",
      "Epoch 00040: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0607 - acc: 0.9884 - val_loss: 0.9419 - val_acc: 0.7941\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9893\n",
      "Epoch 00041: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0614 - acc: 0.9892 - val_loss: 0.9576 - val_acc: 0.7908\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9901\n",
      "Epoch 00042: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0565 - acc: 0.9901 - val_loss: 0.8619 - val_acc: 0.8123\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9880\n",
      "Epoch 00043: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0603 - acc: 0.9880 - val_loss: 0.8804 - val_acc: 0.8095\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9915\n",
      "Epoch 00044: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0501 - acc: 0.9915 - val_loss: 1.0385 - val_acc: 0.7801\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9881\n",
      "Epoch 00045: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0599 - acc: 0.9880 - val_loss: 0.9460 - val_acc: 0.7915\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9909\n",
      "Epoch 00046: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0498 - acc: 0.9909 - val_loss: 0.8928 - val_acc: 0.8160\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9930\n",
      "Epoch 00047: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0433 - acc: 0.9930 - val_loss: 1.0061 - val_acc: 0.7878\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9924\n",
      "Epoch 00048: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0461 - acc: 0.9923 - val_loss: 1.0980 - val_acc: 0.7682\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9923\n",
      "Epoch 00049: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0467 - acc: 0.9922 - val_loss: 0.9277 - val_acc: 0.8067\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9906\n",
      "Epoch 00050: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0503 - acc: 0.9905 - val_loss: 0.9894 - val_acc: 0.7843\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9885\n",
      "Epoch 00051: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0546 - acc: 0.9884 - val_loss: 0.8896 - val_acc: 0.8111\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9939\n",
      "Epoch 00052: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0393 - acc: 0.9939 - val_loss: 0.8985 - val_acc: 0.8078\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9939\n",
      "Epoch 00053: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0371 - acc: 0.9939 - val_loss: 0.9711 - val_acc: 0.7936\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9937\n",
      "Epoch 00054: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0389 - acc: 0.9936 - val_loss: 0.9904 - val_acc: 0.7901\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9916\n",
      "Epoch 00055: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0445 - acc: 0.9915 - val_loss: 0.9374 - val_acc: 0.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9931\n",
      "Epoch 00056: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0403 - acc: 0.9931 - val_loss: 0.9973 - val_acc: 0.7939\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9933\n",
      "Epoch 00057: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0383 - acc: 0.9933 - val_loss: 0.9137 - val_acc: 0.8074\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9947\n",
      "Epoch 00058: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0350 - acc: 0.9947 - val_loss: 0.9557 - val_acc: 0.8064\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9954\n",
      "Epoch 00059: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0321 - acc: 0.9953 - val_loss: 0.9695 - val_acc: 0.8001\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9913\n",
      "Epoch 00060: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0446 - acc: 0.9913 - val_loss: 0.9305 - val_acc: 0.8102\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0328 - acc: 0.9946 - val_loss: 0.9360 - val_acc: 0.8043\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9952\n",
      "Epoch 00062: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0309 - acc: 0.9952 - val_loss: 0.9416 - val_acc: 0.8116\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9936\n",
      "Epoch 00063: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0351 - acc: 0.9935 - val_loss: 1.0282 - val_acc: 0.7834\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9910\n",
      "Epoch 00064: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0396 - acc: 0.9910 - val_loss: 0.9819 - val_acc: 0.7978\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9957\n",
      "Epoch 00065: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0277 - acc: 0.9957 - val_loss: 0.9680 - val_acc: 0.8069\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9962\n",
      "Epoch 00066: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0266 - acc: 0.9962 - val_loss: 1.0474 - val_acc: 0.7985\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9930\n",
      "Epoch 00067: val_loss did not improve from 0.75423\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0364 - acc: 0.9930 - val_loss: 0.9675 - val_acc: 0.8067\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 470us/sample - loss: 0.8492 - acc: 0.7556\n",
      "Loss: 0.8491813025989513 Accuracy: 0.75555557\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9420 - acc: 0.4040\n",
      "Epoch 00001: val_loss improved from inf to 1.63147, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/001-1.6315.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.9419 - acc: 0.4040 - val_loss: 1.6315 - val_acc: 0.4533\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2340 - acc: 0.6239\n",
      "Epoch 00002: val_loss improved from 1.63147 to 1.09482, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/002-1.0948.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 1.2341 - acc: 0.6239 - val_loss: 1.0948 - val_acc: 0.6657\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9839 - acc: 0.7068\n",
      "Epoch 00003: val_loss improved from 1.09482 to 0.90408, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/003-0.9041.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.9839 - acc: 0.7067 - val_loss: 0.9041 - val_acc: 0.7291\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7589\n",
      "Epoch 00004: val_loss improved from 0.90408 to 0.81118, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/004-0.8112.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.8236 - acc: 0.7589 - val_loss: 0.8112 - val_acc: 0.7694\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7051 - acc: 0.7964\n",
      "Epoch 00005: val_loss did not improve from 0.81118\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.7051 - acc: 0.7964 - val_loss: 0.8202 - val_acc: 0.7608\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6214 - acc: 0.8215\n",
      "Epoch 00006: val_loss improved from 0.81118 to 0.63885, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/006-0.6388.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.6215 - acc: 0.8214 - val_loss: 0.6388 - val_acc: 0.8211\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8423\n",
      "Epoch 00007: val_loss improved from 0.63885 to 0.59259, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/007-0.5926.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.5494 - acc: 0.8423 - val_loss: 0.5926 - val_acc: 0.8374\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8598\n",
      "Epoch 00008: val_loss improved from 0.59259 to 0.55478, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/008-0.5548.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.4846 - acc: 0.8598 - val_loss: 0.5548 - val_acc: 0.8463\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4423 - acc: 0.8753\n",
      "Epoch 00009: val_loss improved from 0.55478 to 0.52429, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/009-0.5243.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.4423 - acc: 0.8752 - val_loss: 0.5243 - val_acc: 0.8505\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8886\n",
      "Epoch 00010: val_loss improved from 0.52429 to 0.49267, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/010-0.4927.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3965 - acc: 0.8885 - val_loss: 0.4927 - val_acc: 0.8593\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8967\n",
      "Epoch 00011: val_loss did not improve from 0.49267\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.3631 - acc: 0.8967 - val_loss: 0.5720 - val_acc: 0.8411\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.9058\n",
      "Epoch 00012: val_loss improved from 0.49267 to 0.47385, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/012-0.4739.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.3336 - acc: 0.9057 - val_loss: 0.4739 - val_acc: 0.8684\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9144\n",
      "Epoch 00013: val_loss improved from 0.47385 to 0.45325, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/013-0.4533.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3029 - acc: 0.9144 - val_loss: 0.4533 - val_acc: 0.8717\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9219\n",
      "Epoch 00014: val_loss improved from 0.45325 to 0.42787, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/014-0.4279.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2783 - acc: 0.9219 - val_loss: 0.4279 - val_acc: 0.8779\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9296\n",
      "Epoch 00015: val_loss did not improve from 0.42787\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2555 - acc: 0.9296 - val_loss: 0.4599 - val_acc: 0.8705\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9345\n",
      "Epoch 00016: val_loss did not improve from 0.42787\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2359 - acc: 0.9345 - val_loss: 0.4858 - val_acc: 0.8682\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9392\n",
      "Epoch 00017: val_loss improved from 0.42787 to 0.41881, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/017-0.4188.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2186 - acc: 0.9391 - val_loss: 0.4188 - val_acc: 0.8826\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9422\n",
      "Epoch 00018: val_loss did not improve from 0.41881\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2081 - acc: 0.9422 - val_loss: 0.5633 - val_acc: 0.8521\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9485\n",
      "Epoch 00019: val_loss improved from 0.41881 to 0.38922, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/019-0.3892.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1913 - acc: 0.9485 - val_loss: 0.3892 - val_acc: 0.8931\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9518\n",
      "Epoch 00020: val_loss did not improve from 0.38922\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1792 - acc: 0.9518 - val_loss: 0.5546 - val_acc: 0.8458\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9565\n",
      "Epoch 00021: val_loss did not improve from 0.38922\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1636 - acc: 0.9565 - val_loss: 0.4340 - val_acc: 0.8805\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9585\n",
      "Epoch 00022: val_loss did not improve from 0.38922\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1532 - acc: 0.9584 - val_loss: 0.3997 - val_acc: 0.8852\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9612\n",
      "Epoch 00023: val_loss improved from 0.38922 to 0.38855, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/023-0.3885.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1452 - acc: 0.9612 - val_loss: 0.3885 - val_acc: 0.8945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9661\n",
      "Epoch 00024: val_loss improved from 0.38855 to 0.37362, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/024-0.3736.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1334 - acc: 0.9661 - val_loss: 0.3736 - val_acc: 0.8975\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9679\n",
      "Epoch 00025: val_loss did not improve from 0.37362\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1258 - acc: 0.9679 - val_loss: 0.3863 - val_acc: 0.8938\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9689\n",
      "Epoch 00026: val_loss did not improve from 0.37362\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1236 - acc: 0.9688 - val_loss: 0.4014 - val_acc: 0.8887\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9710\n",
      "Epoch 00027: val_loss did not improve from 0.37362\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1143 - acc: 0.9710 - val_loss: 0.3923 - val_acc: 0.8935\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9738\n",
      "Epoch 00028: val_loss did not improve from 0.37362\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1049 - acc: 0.9738 - val_loss: 0.4500 - val_acc: 0.8768\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9762\n",
      "Epoch 00029: val_loss did not improve from 0.37362\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0984 - acc: 0.9762 - val_loss: 0.4065 - val_acc: 0.8935\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9804\n",
      "Epoch 00030: val_loss improved from 0.37362 to 0.36410, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv_checkpoint/030-0.3641.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0892 - acc: 0.9804 - val_loss: 0.3641 - val_acc: 0.8994\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9803\n",
      "Epoch 00031: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0853 - acc: 0.9802 - val_loss: 0.3792 - val_acc: 0.8973\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9812\n",
      "Epoch 00032: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0818 - acc: 0.9811 - val_loss: 0.4233 - val_acc: 0.8987\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9776\n",
      "Epoch 00033: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0934 - acc: 0.9776 - val_loss: 0.3816 - val_acc: 0.8980\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9846\n",
      "Epoch 00034: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0700 - acc: 0.9846 - val_loss: 0.3789 - val_acc: 0.9033\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9857\n",
      "Epoch 00035: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0682 - acc: 0.9857 - val_loss: 0.3879 - val_acc: 0.9031\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9866\n",
      "Epoch 00036: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0631 - acc: 0.9866 - val_loss: 0.3923 - val_acc: 0.9019\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9862\n",
      "Epoch 00037: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0634 - acc: 0.9861 - val_loss: 0.4423 - val_acc: 0.8847\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9867\n",
      "Epoch 00038: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0613 - acc: 0.9866 - val_loss: 0.4023 - val_acc: 0.9017\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9876\n",
      "Epoch 00039: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0572 - acc: 0.9876 - val_loss: 0.3777 - val_acc: 0.9045\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9897\n",
      "Epoch 00040: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0519 - acc: 0.9896 - val_loss: 0.4071 - val_acc: 0.9001\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9842\n",
      "Epoch 00041: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0644 - acc: 0.9842 - val_loss: 0.3966 - val_acc: 0.9036\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9893\n",
      "Epoch 00042: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0529 - acc: 0.9893 - val_loss: 0.4108 - val_acc: 0.9005\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9927\n",
      "Epoch 00043: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0424 - acc: 0.9927 - val_loss: 0.4088 - val_acc: 0.9068\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9924\n",
      "Epoch 00044: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0425 - acc: 0.9924 - val_loss: 0.3958 - val_acc: 0.9005\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9918\n",
      "Epoch 00045: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0437 - acc: 0.9918 - val_loss: 0.4543 - val_acc: 0.8882\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0422 - acc: 0.9921 - val_loss: 0.3820 - val_acc: 0.9031\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9900\n",
      "Epoch 00047: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0477 - acc: 0.9900 - val_loss: 0.4376 - val_acc: 0.8933\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9924\n",
      "Epoch 00048: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0406 - acc: 0.9924 - val_loss: 0.4228 - val_acc: 0.8959\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9924\n",
      "Epoch 00049: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0388 - acc: 0.9924 - val_loss: 0.4371 - val_acc: 0.9005\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9924\n",
      "Epoch 00050: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0397 - acc: 0.9924 - val_loss: 0.3829 - val_acc: 0.9071\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9926\n",
      "Epoch 00051: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0377 - acc: 0.9926 - val_loss: 0.4357 - val_acc: 0.8991\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9962\n",
      "Epoch 00052: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0275 - acc: 0.9962 - val_loss: 0.4104 - val_acc: 0.9001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9917\n",
      "Epoch 00053: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0396 - acc: 0.9917 - val_loss: 0.4045 - val_acc: 0.9080\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9961\n",
      "Epoch 00054: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0261 - acc: 0.9961 - val_loss: 0.4166 - val_acc: 0.9036\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9945\n",
      "Epoch 00055: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0305 - acc: 0.9944 - val_loss: 0.4980 - val_acc: 0.8817\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9915\n",
      "Epoch 00056: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0402 - acc: 0.9915 - val_loss: 0.4531 - val_acc: 0.8882\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9926\n",
      "Epoch 00057: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0366 - acc: 0.9926 - val_loss: 0.4094 - val_acc: 0.9061\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9962\n",
      "Epoch 00058: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0255 - acc: 0.9962 - val_loss: 0.4306 - val_acc: 0.9005\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9925\n",
      "Epoch 00059: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0371 - acc: 0.9924 - val_loss: 0.4259 - val_acc: 0.8982\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9935\n",
      "Epoch 00060: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0327 - acc: 0.9935 - val_loss: 0.4289 - val_acc: 0.9022\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0275 - acc: 0.9946 - val_loss: 0.4157 - val_acc: 0.9033\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9975\n",
      "Epoch 00062: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0205 - acc: 0.9975 - val_loss: 0.4417 - val_acc: 0.9038\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9970\n",
      "Epoch 00063: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0219 - acc: 0.9969 - val_loss: 0.4731 - val_acc: 0.8926\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9947\n",
      "Epoch 00064: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0267 - acc: 0.9947 - val_loss: 0.4064 - val_acc: 0.9087\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9971\n",
      "Epoch 00065: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0195 - acc: 0.9971 - val_loss: 0.4296 - val_acc: 0.9029\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9957\n",
      "Epoch 00066: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0238 - acc: 0.9957 - val_loss: 0.4271 - val_acc: 0.9015\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9957\n",
      "Epoch 00067: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0236 - acc: 0.9957 - val_loss: 0.4251 - val_acc: 0.9008\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9943\n",
      "Epoch 00068: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0282 - acc: 0.9943 - val_loss: 0.4173 - val_acc: 0.9059\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9946\n",
      "Epoch 00069: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0267 - acc: 0.9946 - val_loss: 0.4145 - val_acc: 0.9075\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9975\n",
      "Epoch 00070: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0180 - acc: 0.9975 - val_loss: 0.4182 - val_acc: 0.9057\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9975\n",
      "Epoch 00071: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0181 - acc: 0.9975 - val_loss: 0.4089 - val_acc: 0.9099\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9968\n",
      "Epoch 00072: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0189 - acc: 0.9968 - val_loss: 0.4810 - val_acc: 0.8977\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0277 - acc: 0.9935 - val_loss: 0.4497 - val_acc: 0.8984\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9935\n",
      "Epoch 00074: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0296 - acc: 0.9934 - val_loss: 0.4277 - val_acc: 0.9050\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9921\n",
      "Epoch 00075: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0331 - acc: 0.9921 - val_loss: 0.4222 - val_acc: 0.9085\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9962\n",
      "Epoch 00076: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0211 - acc: 0.9962 - val_loss: 0.4183 - val_acc: 0.9092\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9983\n",
      "Epoch 00077: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0148 - acc: 0.9983 - val_loss: 0.4311 - val_acc: 0.9080\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9982\n",
      "Epoch 00078: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0144 - acc: 0.9982 - val_loss: 0.4652 - val_acc: 0.9003\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9965\n",
      "Epoch 00079: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0196 - acc: 0.9964 - val_loss: 0.4301 - val_acc: 0.9036\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9958\n",
      "Epoch 00080: val_loss did not improve from 0.36410\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0221 - acc: 0.9958 - val_loss: 0.4337 - val_acc: 0.9087\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 476us/sample - loss: 0.4221 - acc: 0.8810\n",
      "Loss: 0.42213802426774927 Accuracy: 0.8809969\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9842 - acc: 0.3887\n",
      "Epoch 00001: val_loss improved from inf to 1.87350, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/001-1.8735.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.9840 - acc: 0.3887 - val_loss: 1.8735 - val_acc: 0.3485\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1259 - acc: 0.6606\n",
      "Epoch 00002: val_loss improved from 1.87350 to 0.91888, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/002-0.9189.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 1.1259 - acc: 0.6606 - val_loss: 0.9189 - val_acc: 0.7305\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.7569\n",
      "Epoch 00003: val_loss improved from 0.91888 to 0.69499, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/003-0.6950.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.8378 - acc: 0.7569 - val_loss: 0.6950 - val_acc: 0.8083\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6629 - acc: 0.8091\n",
      "Epoch 00004: val_loss improved from 0.69499 to 0.59568, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/004-0.5957.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.6629 - acc: 0.8091 - val_loss: 0.5957 - val_acc: 0.8339\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8421\n",
      "Epoch 00005: val_loss improved from 0.59568 to 0.57187, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/005-0.5719.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.5506 - acc: 0.8421 - val_loss: 0.5719 - val_acc: 0.8307\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8663\n",
      "Epoch 00006: val_loss improved from 0.57187 to 0.44055, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/006-0.4405.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.4692 - acc: 0.8663 - val_loss: 0.4405 - val_acc: 0.8742\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8842\n",
      "Epoch 00007: val_loss improved from 0.44055 to 0.39630, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/007-0.3963.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.4076 - acc: 0.8842 - val_loss: 0.3963 - val_acc: 0.8877\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8961\n",
      "Epoch 00008: val_loss improved from 0.39630 to 0.37631, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/008-0.3763.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.3613 - acc: 0.8960 - val_loss: 0.3763 - val_acc: 0.8928\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9093\n",
      "Epoch 00009: val_loss improved from 0.37631 to 0.36417, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/009-0.3642.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.3230 - acc: 0.9093 - val_loss: 0.3642 - val_acc: 0.8945\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9174\n",
      "Epoch 00010: val_loss did not improve from 0.36417\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2903 - acc: 0.9174 - val_loss: 0.3715 - val_acc: 0.8887\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9239\n",
      "Epoch 00011: val_loss improved from 0.36417 to 0.30645, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/011-0.3065.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2710 - acc: 0.9238 - val_loss: 0.3065 - val_acc: 0.9152\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9290\n",
      "Epoch 00012: val_loss improved from 0.30645 to 0.28552, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/012-0.2855.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2477 - acc: 0.9289 - val_loss: 0.2855 - val_acc: 0.9164\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9338\n",
      "Epoch 00013: val_loss improved from 0.28552 to 0.28011, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/013-0.2801.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.2279 - acc: 0.9337 - val_loss: 0.2801 - val_acc: 0.9143\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9390\n",
      "Epoch 00014: val_loss improved from 0.28011 to 0.27371, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/014-0.2737.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2110 - acc: 0.9390 - val_loss: 0.2737 - val_acc: 0.9194\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9442\n",
      "Epoch 00015: val_loss improved from 0.27371 to 0.27342, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/015-0.2734.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1948 - acc: 0.9442 - val_loss: 0.2734 - val_acc: 0.9194\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9467\n",
      "Epoch 00016: val_loss improved from 0.27342 to 0.25790, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/016-0.2579.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1820 - acc: 0.9467 - val_loss: 0.2579 - val_acc: 0.9290\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9531\n",
      "Epoch 00017: val_loss did not improve from 0.25790\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1661 - acc: 0.9530 - val_loss: 0.2918 - val_acc: 0.9152\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9535\n",
      "Epoch 00018: val_loss did not improve from 0.25790\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1603 - acc: 0.9535 - val_loss: 0.2736 - val_acc: 0.9231\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9585\n",
      "Epoch 00019: val_loss improved from 0.25790 to 0.24336, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/019-0.2434.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1484 - acc: 0.9584 - val_loss: 0.2434 - val_acc: 0.9264\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9596\n",
      "Epoch 00020: val_loss did not improve from 0.24336\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.1411 - acc: 0.9595 - val_loss: 0.2553 - val_acc: 0.9257\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9639\n",
      "Epoch 00021: val_loss did not improve from 0.24336\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1298 - acc: 0.9638 - val_loss: 0.2578 - val_acc: 0.9248\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9646\n",
      "Epoch 00022: val_loss improved from 0.24336 to 0.23911, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/022-0.2391.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1264 - acc: 0.9646 - val_loss: 0.2391 - val_acc: 0.9311\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9707\n",
      "Epoch 00023: val_loss did not improve from 0.23911\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1104 - acc: 0.9707 - val_loss: 0.2892 - val_acc: 0.9159\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9722\n",
      "Epoch 00024: val_loss improved from 0.23911 to 0.23097, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv_checkpoint/024-0.2310.hdf5\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.1051 - acc: 0.9722 - val_loss: 0.2310 - val_acc: 0.9345\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9721\n",
      "Epoch 00025: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1002 - acc: 0.9721 - val_loss: 0.2532 - val_acc: 0.9315\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9746\n",
      "Epoch 00026: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0943 - acc: 0.9747 - val_loss: 0.2517 - val_acc: 0.9304\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9760\n",
      "Epoch 00027: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0902 - acc: 0.9760 - val_loss: 0.2416 - val_acc: 0.9304\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9773\n",
      "Epoch 00028: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0856 - acc: 0.9773 - val_loss: 0.2691 - val_acc: 0.9234\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9787\n",
      "Epoch 00029: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0808 - acc: 0.9786 - val_loss: 0.2571 - val_acc: 0.9236\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9814\n",
      "Epoch 00030: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0735 - acc: 0.9814 - val_loss: 0.2563 - val_acc: 0.9294\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9814\n",
      "Epoch 00031: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0712 - acc: 0.9813 - val_loss: 0.2710 - val_acc: 0.9248\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9835\n",
      "Epoch 00032: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0671 - acc: 0.9835 - val_loss: 0.2416 - val_acc: 0.9334\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9841\n",
      "Epoch 00033: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0623 - acc: 0.9841 - val_loss: 0.3123 - val_acc: 0.9159\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9861\n",
      "Epoch 00034: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0586 - acc: 0.9861 - val_loss: 0.2398 - val_acc: 0.9308\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9856\n",
      "Epoch 00035: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0564 - acc: 0.9856 - val_loss: 0.2577 - val_acc: 0.9273\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9883\n",
      "Epoch 00036: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0500 - acc: 0.9883 - val_loss: 0.2684 - val_acc: 0.9229\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9866\n",
      "Epoch 00037: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0533 - acc: 0.9866 - val_loss: 0.2485 - val_acc: 0.9283\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9880\n",
      "Epoch 00038: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0503 - acc: 0.9880 - val_loss: 0.2358 - val_acc: 0.9327\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9902\n",
      "Epoch 00039: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0429 - acc: 0.9901 - val_loss: 0.2501 - val_acc: 0.9297\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9897\n",
      "Epoch 00040: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 910us/sample - loss: 0.0440 - acc: 0.9897 - val_loss: 0.2628 - val_acc: 0.9283\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0389 - acc: 0.9919 - val_loss: 0.2807 - val_acc: 0.9220\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9914\n",
      "Epoch 00042: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0393 - acc: 0.9914 - val_loss: 0.3030 - val_acc: 0.9194\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9915\n",
      "Epoch 00043: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0371 - acc: 0.9914 - val_loss: 0.2501 - val_acc: 0.9336\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9898\n",
      "Epoch 00044: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0423 - acc: 0.9898 - val_loss: 0.2686 - val_acc: 0.9324\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9923\n",
      "Epoch 00045: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0362 - acc: 0.9923 - val_loss: 0.2527 - val_acc: 0.9341\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9943\n",
      "Epoch 00046: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0302 - acc: 0.9943 - val_loss: 0.2407 - val_acc: 0.9355\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9939\n",
      "Epoch 00047: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0292 - acc: 0.9939 - val_loss: 0.2741 - val_acc: 0.9266\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9936\n",
      "Epoch 00048: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0305 - acc: 0.9936 - val_loss: 0.2640 - val_acc: 0.9287\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0311 - acc: 0.9930 - val_loss: 0.3059 - val_acc: 0.9206\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9902\n",
      "Epoch 00050: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0409 - acc: 0.9901 - val_loss: 0.2492 - val_acc: 0.9357\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9952\n",
      "Epoch 00051: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0257 - acc: 0.9952 - val_loss: 0.2454 - val_acc: 0.9352\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9957\n",
      "Epoch 00052: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0224 - acc: 0.9957 - val_loss: 0.2907 - val_acc: 0.9259\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9952\n",
      "Epoch 00053: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0242 - acc: 0.9951 - val_loss: 0.2519 - val_acc: 0.9334\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9919\n",
      "Epoch 00054: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0333 - acc: 0.9919 - val_loss: 0.2390 - val_acc: 0.9364\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9967\n",
      "Epoch 00055: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0189 - acc: 0.9967 - val_loss: 0.2763 - val_acc: 0.9290\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9960\n",
      "Epoch 00056: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0206 - acc: 0.9960 - val_loss: 0.3397 - val_acc: 0.9096\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9933\n",
      "Epoch 00057: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0282 - acc: 0.9932 - val_loss: 0.2802 - val_acc: 0.9306\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9930\n",
      "Epoch 00058: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0280 - acc: 0.9930 - val_loss: 0.2553 - val_acc: 0.9352\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9946\n",
      "Epoch 00059: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0230 - acc: 0.9946 - val_loss: 0.2632 - val_acc: 0.9320\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9948\n",
      "Epoch 00060: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0224 - acc: 0.9948 - val_loss: 0.2595 - val_acc: 0.9329\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0232 - acc: 0.9951 - val_loss: 0.2616 - val_acc: 0.9355\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9976\n",
      "Epoch 00062: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0151 - acc: 0.9976 - val_loss: 0.2578 - val_acc: 0.9341\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9970\n",
      "Epoch 00063: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0162 - acc: 0.9970 - val_loss: 0.2903 - val_acc: 0.9278\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9952\n",
      "Epoch 00064: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0223 - acc: 0.9952 - val_loss: 0.2602 - val_acc: 0.9331\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9976\n",
      "Epoch 00065: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0151 - acc: 0.9975 - val_loss: 0.2635 - val_acc: 0.9376\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9940\n",
      "Epoch 00066: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0250 - acc: 0.9940 - val_loss: 0.2630 - val_acc: 0.9327\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9980\n",
      "Epoch 00067: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0130 - acc: 0.9980 - val_loss: 0.2743 - val_acc: 0.9362\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9937\n",
      "Epoch 00068: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0248 - acc: 0.9937 - val_loss: 0.2571 - val_acc: 0.9357\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9972\n",
      "Epoch 00069: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0146 - acc: 0.9971 - val_loss: 0.2786 - val_acc: 0.9355\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9935\n",
      "Epoch 00070: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0244 - acc: 0.9935 - val_loss: 0.2716 - val_acc: 0.9320\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9977\n",
      "Epoch 00071: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0142 - acc: 0.9976 - val_loss: 0.2836 - val_acc: 0.9331\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9939\n",
      "Epoch 00072: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0236 - acc: 0.9939 - val_loss: 0.2678 - val_acc: 0.9355\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0108 - acc: 0.9984 - val_loss: 0.2859 - val_acc: 0.9306\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9944\n",
      "Epoch 00074: val_loss did not improve from 0.23097\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0228 - acc: 0.9943 - val_loss: 0.2671 - val_acc: 0.9348\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 492us/sample - loss: 0.2937 - acc: 0.9144\n",
      "Loss: 0.2936535029860672 Accuracy: 0.9144341\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9187 - acc: 0.4046\n",
      "Epoch 00001: val_loss improved from inf to 1.56539, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/001-1.5654.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.9186 - acc: 0.4046 - val_loss: 1.5654 - val_acc: 0.5071\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9813 - acc: 0.7078\n",
      "Epoch 00002: val_loss improved from 1.56539 to 0.72882, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/002-0.7288.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.9813 - acc: 0.7078 - val_loss: 0.7288 - val_acc: 0.7897\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.8052\n",
      "Epoch 00003: val_loss improved from 0.72882 to 0.61194, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/003-0.6119.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.6688 - acc: 0.8052 - val_loss: 0.6119 - val_acc: 0.8255\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8487\n",
      "Epoch 00004: val_loss improved from 0.61194 to 0.43881, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/004-0.4388.hdf5\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.5156 - acc: 0.8487 - val_loss: 0.4388 - val_acc: 0.8672\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8764\n",
      "Epoch 00005: val_loss improved from 0.43881 to 0.35284, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/005-0.3528.hdf5\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.4213 - acc: 0.8765 - val_loss: 0.3528 - val_acc: 0.8977\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8981\n",
      "Epoch 00006: val_loss improved from 0.35284 to 0.29365, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/006-0.2937.hdf5\n",
      "36805/36805 [==============================] - 35s 939us/sample - loss: 0.3543 - acc: 0.8981 - val_loss: 0.2937 - val_acc: 0.9099\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.9078\n",
      "Epoch 00007: val_loss improved from 0.29365 to 0.28611, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/007-0.2861.hdf5\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.3120 - acc: 0.9078 - val_loss: 0.2861 - val_acc: 0.9164\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9208\n",
      "Epoch 00008: val_loss improved from 0.28611 to 0.27050, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/008-0.2705.hdf5\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.2745 - acc: 0.9208 - val_loss: 0.2705 - val_acc: 0.9140\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9269\n",
      "Epoch 00009: val_loss improved from 0.27050 to 0.23148, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/009-0.2315.hdf5\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.2482 - acc: 0.9269 - val_loss: 0.2315 - val_acc: 0.9324\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9328\n",
      "Epoch 00010: val_loss improved from 0.23148 to 0.21139, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/010-0.2114.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.2245 - acc: 0.9328 - val_loss: 0.2114 - val_acc: 0.9364\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9398\n",
      "Epoch 00011: val_loss did not improve from 0.21139\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.2041 - acc: 0.9397 - val_loss: 0.2205 - val_acc: 0.9341\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9450\n",
      "Epoch 00012: val_loss improved from 0.21139 to 0.19313, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/012-0.1931.hdf5\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.1867 - acc: 0.9450 - val_loss: 0.1931 - val_acc: 0.9453\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9491\n",
      "Epoch 00013: val_loss did not improve from 0.19313\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1711 - acc: 0.9491 - val_loss: 0.2036 - val_acc: 0.9411\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9541\n",
      "Epoch 00014: val_loss did not improve from 0.19313\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.1573 - acc: 0.9541 - val_loss: 0.2060 - val_acc: 0.9418\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9570\n",
      "Epoch 00015: val_loss did not improve from 0.19313\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1479 - acc: 0.9569 - val_loss: 0.2183 - val_acc: 0.9345\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9623\n",
      "Epoch 00016: val_loss improved from 0.19313 to 0.18503, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/016-0.1850.hdf5\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.1336 - acc: 0.9623 - val_loss: 0.1850 - val_acc: 0.9450\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9646\n",
      "Epoch 00017: val_loss did not improve from 0.18503\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1242 - acc: 0.9646 - val_loss: 0.2025 - val_acc: 0.9404\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9668\n",
      "Epoch 00018: val_loss did not improve from 0.18503\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.1155 - acc: 0.9668 - val_loss: 0.2283 - val_acc: 0.9317\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9698\n",
      "Epoch 00019: val_loss did not improve from 0.18503\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1088 - acc: 0.9698 - val_loss: 0.2416 - val_acc: 0.9257\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9725\n",
      "Epoch 00020: val_loss improved from 0.18503 to 0.17246, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/020-0.1725.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0984 - acc: 0.9725 - val_loss: 0.1725 - val_acc: 0.9474\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9730\n",
      "Epoch 00021: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0949 - acc: 0.9730 - val_loss: 0.1780 - val_acc: 0.9462\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9754\n",
      "Epoch 00022: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0841 - acc: 0.9754 - val_loss: 0.1969 - val_acc: 0.9408\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9780\n",
      "Epoch 00023: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0804 - acc: 0.9780 - val_loss: 0.2207 - val_acc: 0.9331\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9799\n",
      "Epoch 00024: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0749 - acc: 0.9798 - val_loss: 0.2230 - val_acc: 0.9322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9794\n",
      "Epoch 00025: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0733 - acc: 0.9794 - val_loss: 0.1837 - val_acc: 0.9434\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9824\n",
      "Epoch 00026: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0673 - acc: 0.9824 - val_loss: 0.1779 - val_acc: 0.9474\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9837\n",
      "Epoch 00027: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0618 - acc: 0.9837 - val_loss: 0.1806 - val_acc: 0.9476\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9839\n",
      "Epoch 00028: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0609 - acc: 0.9839 - val_loss: 0.2105 - val_acc: 0.9376\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9823\n",
      "Epoch 00029: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0631 - acc: 0.9822 - val_loss: 0.2926 - val_acc: 0.9080\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9854\n",
      "Epoch 00030: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0551 - acc: 0.9854 - val_loss: 0.1754 - val_acc: 0.9464\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9897\n",
      "Epoch 00031: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0427 - acc: 0.9897 - val_loss: 0.1985 - val_acc: 0.9443\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9887\n",
      "Epoch 00032: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0448 - acc: 0.9887 - val_loss: 0.1816 - val_acc: 0.9446\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9911\n",
      "Epoch 00033: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0387 - acc: 0.9911 - val_loss: 0.2542 - val_acc: 0.9294\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9902\n",
      "Epoch 00034: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0409 - acc: 0.9901 - val_loss: 0.1824 - val_acc: 0.9483\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9887\n",
      "Epoch 00035: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0437 - acc: 0.9886 - val_loss: 0.1773 - val_acc: 0.9520\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9896\n",
      "Epoch 00036: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0402 - acc: 0.9895 - val_loss: 0.1791 - val_acc: 0.9522\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9887\n",
      "Epoch 00037: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0430 - acc: 0.9888 - val_loss: 0.1874 - val_acc: 0.9474\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9938\n",
      "Epoch 00038: val_loss did not improve from 0.17246\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0299 - acc: 0.9938 - val_loss: 0.1919 - val_acc: 0.9467\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9928\n",
      "Epoch 00039: val_loss improved from 0.17246 to 0.16836, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv_checkpoint/039-0.1684.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0309 - acc: 0.9927 - val_loss: 0.1684 - val_acc: 0.9522\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9932\n",
      "Epoch 00040: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0292 - acc: 0.9931 - val_loss: 0.1913 - val_acc: 0.9439\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9907\n",
      "Epoch 00041: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 928us/sample - loss: 0.0365 - acc: 0.9906 - val_loss: 0.1904 - val_acc: 0.9488\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9931\n",
      "Epoch 00042: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0286 - acc: 0.9930 - val_loss: 0.2465 - val_acc: 0.9280\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9927\n",
      "Epoch 00043: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0284 - acc: 0.9927 - val_loss: 0.1791 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9957\n",
      "Epoch 00044: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0218 - acc: 0.9957 - val_loss: 0.1876 - val_acc: 0.9471\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9938\n",
      "Epoch 00045: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0257 - acc: 0.9938 - val_loss: 0.2141 - val_acc: 0.9418\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9925\n",
      "Epoch 00046: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0299 - acc: 0.9925 - val_loss: 0.1985 - val_acc: 0.9469\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9965\n",
      "Epoch 00047: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0184 - acc: 0.9965 - val_loss: 0.2057 - val_acc: 0.9448\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9928\n",
      "Epoch 00048: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0279 - acc: 0.9928 - val_loss: 0.2106 - val_acc: 0.9427\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9919\n",
      "Epoch 00049: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0297 - acc: 0.9919 - val_loss: 0.2017 - val_acc: 0.9476\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9975\n",
      "Epoch 00050: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0146 - acc: 0.9975 - val_loss: 0.2100 - val_acc: 0.9434\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9970\n",
      "Epoch 00051: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0148 - acc: 0.9970 - val_loss: 0.2172 - val_acc: 0.9415\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9965\n",
      "Epoch 00052: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 931us/sample - loss: 0.0167 - acc: 0.9965 - val_loss: 0.2475 - val_acc: 0.9331\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9965\n",
      "Epoch 00053: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0159 - acc: 0.9965 - val_loss: 0.2322 - val_acc: 0.9457\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9949\n",
      "Epoch 00054: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0196 - acc: 0.9948 - val_loss: 0.2012 - val_acc: 0.9499\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9945\n",
      "Epoch 00055: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0218 - acc: 0.9945 - val_loss: 0.2121 - val_acc: 0.9446\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9949\n",
      "Epoch 00056: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0205 - acc: 0.9949 - val_loss: 0.2070 - val_acc: 0.9509\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9981\n",
      "Epoch 00057: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0113 - acc: 0.9981 - val_loss: 0.2103 - val_acc: 0.9467\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9972\n",
      "Epoch 00058: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0137 - acc: 0.9972 - val_loss: 0.2467 - val_acc: 0.9373\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0158 - acc: 0.9961 - val_loss: 0.2245 - val_acc: 0.9455\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9948\n",
      "Epoch 00060: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0205 - acc: 0.9948 - val_loss: 0.1943 - val_acc: 0.9511\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 00061: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.0096 - acc: 0.9984 - val_loss: 0.2068 - val_acc: 0.9490\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9981\n",
      "Epoch 00062: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0102 - acc: 0.9981 - val_loss: 0.2168 - val_acc: 0.9471\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9966\n",
      "Epoch 00063: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0140 - acc: 0.9966 - val_loss: 0.2481 - val_acc: 0.9364\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9969\n",
      "Epoch 00064: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0130 - acc: 0.9969 - val_loss: 0.2573 - val_acc: 0.9401\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9957\n",
      "Epoch 00065: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.2148 - val_acc: 0.9478\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9981\n",
      "Epoch 00066: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0096 - acc: 0.9981 - val_loss: 0.2118 - val_acc: 0.9481\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9916\n",
      "Epoch 00067: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0290 - acc: 0.9916 - val_loss: 0.2067 - val_acc: 0.9478\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9980\n",
      "Epoch 00068: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0095 - acc: 0.9980 - val_loss: 0.2135 - val_acc: 0.9469\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 00069: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0078 - acc: 0.9987 - val_loss: 0.2216 - val_acc: 0.9483\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9954\n",
      "Epoch 00070: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0196 - acc: 0.9954 - val_loss: 0.1949 - val_acc: 0.9515\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9973\n",
      "Epoch 00071: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0115 - acc: 0.9973 - val_loss: 0.2135 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9984\n",
      "Epoch 00072: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0079 - acc: 0.9984 - val_loss: 0.2054 - val_acc: 0.9481\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9970\n",
      "Epoch 00073: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.0119 - acc: 0.9970 - val_loss: 0.2281 - val_acc: 0.9446\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9980\n",
      "Epoch 00074: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0101 - acc: 0.9979 - val_loss: 0.2102 - val_acc: 0.9515\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9974\n",
      "Epoch 00075: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0113 - acc: 0.9974 - val_loss: 0.2163 - val_acc: 0.9495\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9972\n",
      "Epoch 00076: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0118 - acc: 0.9972 - val_loss: 0.2008 - val_acc: 0.9541\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9987\n",
      "Epoch 00077: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.2267 - val_acc: 0.9462\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9987\n",
      "Epoch 00078: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0072 - acc: 0.9987 - val_loss: 0.2156 - val_acc: 0.9467\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9968\n",
      "Epoch 00079: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0126 - acc: 0.9968 - val_loss: 0.2678 - val_acc: 0.9399\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9955\n",
      "Epoch 00080: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0161 - acc: 0.9955 - val_loss: 0.2235 - val_acc: 0.9499\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9960\n",
      "Epoch 00081: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0150 - acc: 0.9960 - val_loss: 0.2228 - val_acc: 0.9481\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9990\n",
      "Epoch 00082: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0063 - acc: 0.9990 - val_loss: 0.2166 - val_acc: 0.9471\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9986\n",
      "Epoch 00083: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.2405 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9919\n",
      "Epoch 00084: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 35s 940us/sample - loss: 0.0277 - acc: 0.9919 - val_loss: 0.2158 - val_acc: 0.9515\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9988\n",
      "Epoch 00085: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0066 - acc: 0.9988 - val_loss: 0.2017 - val_acc: 0.9529\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9993\n",
      "Epoch 00086: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.0046 - acc: 0.9993 - val_loss: 0.2336 - val_acc: 0.9418\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9991\n",
      "Epoch 00087: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0054 - acc: 0.9991 - val_loss: 0.2346 - val_acc: 0.9485\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9982\n",
      "Epoch 00088: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 931us/sample - loss: 0.0084 - acc: 0.9981 - val_loss: 0.2221 - val_acc: 0.9502\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9945\n",
      "Epoch 00089: val_loss did not improve from 0.16836\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0198 - acc: 0.9945 - val_loss: 0.2329 - val_acc: 0.9492\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 547us/sample - loss: 0.2769 - acc: 0.9265\n",
      "Loss: 0.2769464774542756 Accuracy: 0.92647976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 75808)        303232      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,527,056\n",
      "Trainable params: 1,375,248\n",
      "Non-trainable params: 151,808\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 536us/sample - loss: 1.7964 - acc: 0.5445\n",
      "Loss: 1.7964190778207432 Accuracy: 0.5445483\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 25248)        100992      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 521,136\n",
      "Trainable params: 470,384\n",
      "Non-trainable params: 50,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 529us/sample - loss: 1.2226 - acc: 0.6407\n",
      "Loss: 1.2226354261053685 Accuracy: 0.6407061\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 10464)        41856       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 236,016\n",
      "Trainable params: 214,704\n",
      "Non-trainable params: 21,312\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 586us/sample - loss: 0.8492 - acc: 0.7556\n",
      "Loss: 0.8491813025989513 Accuracy: 0.75555557\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 5504)         22016       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 157,616\n",
      "Trainable params: 146,096\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 591us/sample - loss: 0.4221 - acc: 0.8810\n",
      "Loss: 0.42213802426774927 Accuracy: 0.8809969\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 1792)         7168        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 104,176\n",
      "Trainable params: 99,952\n",
      "Non-trainable params: 4,224\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 608us/sample - loss: 0.2937 - acc: 0.9144\n",
      "Loss: 0.2936535029860672 Accuracy: 0.9144341\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 576)          2304        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 100,656\n",
      "Trainable params: 98,736\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 605us/sample - loss: 0.2769 - acc: 0.9265\n",
      "Loss: 0.2769464774542756 Accuracy: 0.92647976\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 75808)        303232      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,527,056\n",
      "Trainable params: 1,375,248\n",
      "Non-trainable params: 151,808\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 602us/sample - loss: 3.6560 - acc: 0.5475\n",
      "Loss: 3.655951731177753 Accuracy: 0.54745585\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 25248)        100992      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 521,136\n",
      "Trainable params: 470,384\n",
      "Non-trainable params: 50,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 663us/sample - loss: 2.1882 - acc: 0.6274\n",
      "Loss: 2.1882454512151477 Accuracy: 0.62741435\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 10464)        41856       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 236,016\n",
      "Trainable params: 214,704\n",
      "Non-trainable params: 21,312\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 652us/sample - loss: 1.0913 - acc: 0.7755\n",
      "Loss: 1.0913434131865931 Accuracy: 0.77549326\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 5504)         22016       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 157,616\n",
      "Trainable params: 146,096\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 723us/sample - loss: 0.5103 - acc: 0.8852\n",
      "Loss: 0.510264226555948 Accuracy: 0.88515055\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 1792)         7168        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 104,176\n",
      "Trainable params: 99,952\n",
      "Non-trainable params: 4,224\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.3372 - acc: 0.9148\n",
      "Loss: 0.3372321854684954 Accuracy: 0.9148494\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 576)          2304        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 100,656\n",
      "Trainable params: 98,736\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 778us/sample - loss: 0.3422 - acc: 0.9252\n",
      "Loss: 0.3422230884525635 Accuracy: 0.92523366\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
