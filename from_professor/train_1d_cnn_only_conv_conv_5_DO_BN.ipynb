{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_40 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 2.7927 - acc: 0.2032\n",
      "Epoch 00001: val_loss improved from inf to 2.08284, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/001-2.0828.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 2.7896 - acc: 0.2038 - val_loss: 2.0828 - val_acc: 0.3329\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.0085 - acc: 0.3725\n",
      "Epoch 00002: val_loss improved from 2.08284 to 1.78809, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/002-1.7881.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 2.0094 - acc: 0.3722 - val_loss: 1.7881 - val_acc: 0.4396\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7271 - acc: 0.4502\n",
      "Epoch 00003: val_loss improved from 1.78809 to 1.69556, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/003-1.6956.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.7270 - acc: 0.4502 - val_loss: 1.6956 - val_acc: 0.4631\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5461 - acc: 0.5022\n",
      "Epoch 00004: val_loss improved from 1.69556 to 1.68302, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/004-1.6830.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.5468 - acc: 0.5023 - val_loss: 1.6830 - val_acc: 0.4740\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4236 - acc: 0.5401\n",
      "Epoch 00005: val_loss improved from 1.68302 to 1.63619, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/005-1.6362.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.4235 - acc: 0.5401 - val_loss: 1.6362 - val_acc: 0.4955\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3344 - acc: 0.5699\n",
      "Epoch 00006: val_loss improved from 1.63619 to 1.60176, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/006-1.6018.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.3343 - acc: 0.5699 - val_loss: 1.6018 - val_acc: 0.5092\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.5961\n",
      "Epoch 00007: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.2512 - acc: 0.5961 - val_loss: 1.6087 - val_acc: 0.4992\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1844 - acc: 0.6170\n",
      "Epoch 00008: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.1845 - acc: 0.6170 - val_loss: 1.6135 - val_acc: 0.5043\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1377 - acc: 0.6310\n",
      "Epoch 00009: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1380 - acc: 0.6311 - val_loss: 1.6092 - val_acc: 0.5050\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0887 - acc: 0.6461\n",
      "Epoch 00010: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.0888 - acc: 0.6462 - val_loss: 1.6637 - val_acc: 0.5003\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0481 - acc: 0.6579\n",
      "Epoch 00011: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.0487 - acc: 0.6577 - val_loss: 1.6473 - val_acc: 0.5059\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0165 - acc: 0.6660\n",
      "Epoch 00012: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.0169 - acc: 0.6660 - val_loss: 1.6610 - val_acc: 0.5024\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9799 - acc: 0.6762\n",
      "Epoch 00013: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.9802 - acc: 0.6762 - val_loss: 1.6839 - val_acc: 0.4997\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9575 - acc: 0.6905\n",
      "Epoch 00014: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9574 - acc: 0.6903 - val_loss: 1.6746 - val_acc: 0.5104\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9357 - acc: 0.6939\n",
      "Epoch 00015: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9356 - acc: 0.6939 - val_loss: 1.6802 - val_acc: 0.5125\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9084 - acc: 0.7042\n",
      "Epoch 00016: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9088 - acc: 0.7038 - val_loss: 1.6723 - val_acc: 0.5164\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.7121\n",
      "Epoch 00017: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8798 - acc: 0.7118 - val_loss: 1.7551 - val_acc: 0.4936\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7136\n",
      "Epoch 00018: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8682 - acc: 0.7134 - val_loss: 1.7312 - val_acc: 0.5071\n",
      "Epoch 19/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8544 - acc: 0.7203\n",
      "Epoch 00019: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8538 - acc: 0.7204 - val_loss: 1.7296 - val_acc: 0.5111\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8318 - acc: 0.7251\n",
      "Epoch 00020: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8319 - acc: 0.7253 - val_loss: 1.7276 - val_acc: 0.5094\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8220 - acc: 0.7274\n",
      "Epoch 00021: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8217 - acc: 0.7274 - val_loss: 1.7378 - val_acc: 0.5125\n",
      "Epoch 22/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8104 - acc: 0.7336\n",
      "Epoch 00022: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8103 - acc: 0.7335 - val_loss: 1.8083 - val_acc: 0.4950\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7348\n",
      "Epoch 00023: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8011 - acc: 0.7346 - val_loss: 1.7948 - val_acc: 0.5108\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7411\n",
      "Epoch 00024: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7839 - acc: 0.7413 - val_loss: 1.7679 - val_acc: 0.5183\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7805 - acc: 0.7411\n",
      "Epoch 00025: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.7806 - acc: 0.7410 - val_loss: 1.7936 - val_acc: 0.5085\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7454\n",
      "Epoch 00026: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7713 - acc: 0.7454 - val_loss: 1.7770 - val_acc: 0.5146\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7605 - acc: 0.7478\n",
      "Epoch 00027: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7610 - acc: 0.7473 - val_loss: 1.7921 - val_acc: 0.5064\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7504\n",
      "Epoch 00028: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7431 - acc: 0.7506 - val_loss: 1.8066 - val_acc: 0.5078\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7547\n",
      "Epoch 00029: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7381 - acc: 0.7547 - val_loss: 1.8372 - val_acc: 0.5160\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7280 - acc: 0.7594\n",
      "Epoch 00030: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7279 - acc: 0.7594 - val_loss: 1.8473 - val_acc: 0.5092\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7610\n",
      "Epoch 00031: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7129 - acc: 0.7610 - val_loss: 1.8646 - val_acc: 0.5101\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7131 - acc: 0.7627\n",
      "Epoch 00032: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7130 - acc: 0.7627 - val_loss: 1.8484 - val_acc: 0.5062\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7137 - acc: 0.7616\n",
      "Epoch 00033: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7139 - acc: 0.7615 - val_loss: 1.8677 - val_acc: 0.5178\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7659\n",
      "Epoch 00034: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6968 - acc: 0.7659 - val_loss: 1.8430 - val_acc: 0.5206\n",
      "Epoch 35/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.7687\n",
      "Epoch 00035: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6925 - acc: 0.7689 - val_loss: 1.8607 - val_acc: 0.5190\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.7712\n",
      "Epoch 00036: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6900 - acc: 0.7714 - val_loss: 1.8819 - val_acc: 0.5069\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.7728\n",
      "Epoch 00037: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.6803 - acc: 0.7728 - val_loss: 1.8786 - val_acc: 0.5113\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7721\n",
      "Epoch 00038: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6824 - acc: 0.7722 - val_loss: 1.8767 - val_acc: 0.5262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7743\n",
      "Epoch 00039: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6726 - acc: 0.7744 - val_loss: 1.8997 - val_acc: 0.5071\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6681 - acc: 0.7780\n",
      "Epoch 00040: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6681 - acc: 0.7779 - val_loss: 1.9124 - val_acc: 0.5178\n",
      "Epoch 41/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7769\n",
      "Epoch 00041: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6728 - acc: 0.7768 - val_loss: 1.8655 - val_acc: 0.5276\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6546 - acc: 0.7804\n",
      "Epoch 00042: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6548 - acc: 0.7804 - val_loss: 1.8886 - val_acc: 0.5201\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6555 - acc: 0.7795\n",
      "Epoch 00043: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6562 - acc: 0.7794 - val_loss: 1.9243 - val_acc: 0.5134\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7841\n",
      "Epoch 00044: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6469 - acc: 0.7841 - val_loss: 1.9406 - val_acc: 0.5122\n",
      "Epoch 45/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6475 - acc: 0.7825\n",
      "Epoch 00045: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6474 - acc: 0.7827 - val_loss: 1.9426 - val_acc: 0.5178\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.7817\n",
      "Epoch 00046: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6473 - acc: 0.7819 - val_loss: 1.9557 - val_acc: 0.5167\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7842\n",
      "Epoch 00047: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6430 - acc: 0.7841 - val_loss: 1.9344 - val_acc: 0.5208\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.7867\n",
      "Epoch 00048: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6363 - acc: 0.7868 - val_loss: 1.9484 - val_acc: 0.5141\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.7908\n",
      "Epoch 00049: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6233 - acc: 0.7908 - val_loss: 1.9352 - val_acc: 0.5255\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6233 - acc: 0.7902\n",
      "Epoch 00050: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6231 - acc: 0.7903 - val_loss: 1.9765 - val_acc: 0.5222\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.7918\n",
      "Epoch 00051: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6222 - acc: 0.7919 - val_loss: 1.9919 - val_acc: 0.5222\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6158 - acc: 0.7939\n",
      "Epoch 00052: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6158 - acc: 0.7938 - val_loss: 1.9463 - val_acc: 0.5220\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.7917\n",
      "Epoch 00053: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6194 - acc: 0.7916 - val_loss: 1.9587 - val_acc: 0.5269\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.7913\n",
      "Epoch 00054: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6219 - acc: 0.7910 - val_loss: 1.9969 - val_acc: 0.5213\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.7957\n",
      "Epoch 00055: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6074 - acc: 0.7957 - val_loss: 1.9853 - val_acc: 0.5304\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6140 - acc: 0.7951\n",
      "Epoch 00056: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6136 - acc: 0.7953 - val_loss: 1.9805 - val_acc: 0.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6099 - acc: 0.7932\n",
      "Epoch 00057: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6092 - acc: 0.7934 - val_loss: 1.9583 - val_acc: 0.5225\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.7985\n",
      "Epoch 00058: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6054 - acc: 0.7986 - val_loss: 2.0763 - val_acc: 0.5029\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.7976\n",
      "Epoch 00059: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6033 - acc: 0.7976 - val_loss: 1.9606 - val_acc: 0.5232\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.7974\n",
      "Epoch 00060: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5958 - acc: 0.7975 - val_loss: 2.0049 - val_acc: 0.5218\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8018\n",
      "Epoch 00061: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5920 - acc: 0.8017 - val_loss: 1.9928 - val_acc: 0.5250\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.8014\n",
      "Epoch 00062: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5938 - acc: 0.8015 - val_loss: 2.0197 - val_acc: 0.5229\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.7976\n",
      "Epoch 00063: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5964 - acc: 0.7974 - val_loss: 2.0389 - val_acc: 0.5155\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.7987\n",
      "Epoch 00064: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5981 - acc: 0.7986 - val_loss: 2.0142 - val_acc: 0.5234\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.8013\n",
      "Epoch 00065: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5893 - acc: 0.8013 - val_loss: 1.9952 - val_acc: 0.5316\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.8029\n",
      "Epoch 00066: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5856 - acc: 0.8028 - val_loss: 1.9908 - val_acc: 0.5292\n",
      "Epoch 67/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5831 - acc: 0.8048\n",
      "Epoch 00067: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5833 - acc: 0.8047 - val_loss: 2.0416 - val_acc: 0.5195\n",
      "Epoch 68/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8074\n",
      "Epoch 00068: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5749 - acc: 0.8074 - val_loss: 2.0510 - val_acc: 0.5185\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5808 - acc: 0.8056\n",
      "Epoch 00069: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5808 - acc: 0.8056 - val_loss: 2.0263 - val_acc: 0.5215\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8060\n",
      "Epoch 00070: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5739 - acc: 0.8060 - val_loss: 2.0717 - val_acc: 0.5246\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.8078\n",
      "Epoch 00071: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5782 - acc: 0.8076 - val_loss: 2.0679 - val_acc: 0.5197\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.8090\n",
      "Epoch 00072: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5719 - acc: 0.8090 - val_loss: 2.0530 - val_acc: 0.5243\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.8082\n",
      "Epoch 00073: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5725 - acc: 0.8083 - val_loss: 2.0309 - val_acc: 0.5281\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.8120\n",
      "Epoch 00074: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5672 - acc: 0.8120 - val_loss: 2.0363 - val_acc: 0.5262\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.8084\n",
      "Epoch 00075: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5687 - acc: 0.8084 - val_loss: 2.0191 - val_acc: 0.5283\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.8148\n",
      "Epoch 00076: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5582 - acc: 0.8147 - val_loss: 2.0458 - val_acc: 0.5299\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8100\n",
      "Epoch 00077: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5677 - acc: 0.8097 - val_loss: 2.0284 - val_acc: 0.5229\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.8132\n",
      "Epoch 00078: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5603 - acc: 0.8132 - val_loss: 2.0280 - val_acc: 0.5292\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.8106\n",
      "Epoch 00079: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5661 - acc: 0.8104 - val_loss: 2.1736 - val_acc: 0.5066\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8114\n",
      "Epoch 00080: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5591 - acc: 0.8114 - val_loss: 2.0574 - val_acc: 0.5283\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8119\n",
      "Epoch 00081: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5592 - acc: 0.8118 - val_loss: 2.0376 - val_acc: 0.5269\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.8112\n",
      "Epoch 00082: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5615 - acc: 0.8112 - val_loss: 2.0923 - val_acc: 0.5260\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8138\n",
      "Epoch 00083: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5563 - acc: 0.8138 - val_loss: 2.0767 - val_acc: 0.5320\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8115\n",
      "Epoch 00084: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5585 - acc: 0.8114 - val_loss: 2.0869 - val_acc: 0.5320\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8136\n",
      "Epoch 00085: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5550 - acc: 0.8137 - val_loss: 2.0740 - val_acc: 0.5330\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8122\n",
      "Epoch 00086: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5558 - acc: 0.8123 - val_loss: 2.0451 - val_acc: 0.5337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8154\n",
      "Epoch 00087: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5490 - acc: 0.8155 - val_loss: 2.1286 - val_acc: 0.5181\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8160\n",
      "Epoch 00088: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5508 - acc: 0.8158 - val_loss: 2.1133 - val_acc: 0.5199\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8187\n",
      "Epoch 00089: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5409 - acc: 0.8187 - val_loss: 2.0984 - val_acc: 0.5218\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8173\n",
      "Epoch 00090: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5449 - acc: 0.8171 - val_loss: 2.0874 - val_acc: 0.5267\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8188\n",
      "Epoch 00091: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5366 - acc: 0.8185 - val_loss: 2.1254 - val_acc: 0.5313\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.8213\n",
      "Epoch 00092: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5402 - acc: 0.8210 - val_loss: 2.0521 - val_acc: 0.5271\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8204\n",
      "Epoch 00093: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5381 - acc: 0.8201 - val_loss: 2.1195 - val_acc: 0.5330\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.8141\n",
      "Epoch 00094: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5501 - acc: 0.8140 - val_loss: 2.1450 - val_acc: 0.5215\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8174\n",
      "Epoch 00095: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5436 - acc: 0.8174 - val_loss: 2.1019 - val_acc: 0.5292\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8173\n",
      "Epoch 00096: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5401 - acc: 0.8176 - val_loss: 2.1043 - val_acc: 0.5299\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.8197\n",
      "Epoch 00097: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5403 - acc: 0.8197 - val_loss: 2.0673 - val_acc: 0.5339\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8216\n",
      "Epoch 00098: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5268 - acc: 0.8216 - val_loss: 2.0922 - val_acc: 0.5360\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8190\n",
      "Epoch 00099: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5372 - acc: 0.8190 - val_loss: 2.1095 - val_acc: 0.5316\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8223\n",
      "Epoch 00100: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.5272 - acc: 0.8223 - val_loss: 2.1570 - val_acc: 0.5262\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8219\n",
      "Epoch 00101: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5294 - acc: 0.8218 - val_loss: 2.1011 - val_acc: 0.5353\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8206\n",
      "Epoch 00102: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5334 - acc: 0.8206 - val_loss: 2.1530 - val_acc: 0.5188\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8248\n",
      "Epoch 00103: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5239 - acc: 0.8248 - val_loss: 2.2135 - val_acc: 0.5104\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8196\n",
      "Epoch 00104: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5340 - acc: 0.8196 - val_loss: 2.1224 - val_acc: 0.5269\n",
      "Epoch 105/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.8223\n",
      "Epoch 00105: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5311 - acc: 0.8226 - val_loss: 2.1140 - val_acc: 0.5302\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8237\n",
      "Epoch 00106: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5322 - acc: 0.8237 - val_loss: 2.1210 - val_acc: 0.5267\n",
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 284us/sample - loss: 1.8043 - acc: 0.4490\n",
      "Loss: 1.8042701047281362 Accuracy: 0.4490135\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7814 - acc: 0.2001\n",
      "Epoch 00001: val_loss improved from inf to 1.90158, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/001-1.9016.hdf5\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 2.7803 - acc: 0.2003 - val_loss: 1.9016 - val_acc: 0.3862\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9343 - acc: 0.3847\n",
      "Epoch 00002: val_loss improved from 1.90158 to 1.52449, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/002-1.5245.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.9342 - acc: 0.3847 - val_loss: 1.5245 - val_acc: 0.5141\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6377 - acc: 0.4786\n",
      "Epoch 00003: val_loss improved from 1.52449 to 1.37190, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/003-1.3719.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.6379 - acc: 0.4785 - val_loss: 1.3719 - val_acc: 0.5623\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4440 - acc: 0.5425\n",
      "Epoch 00004: val_loss improved from 1.37190 to 1.27708, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/004-1.2771.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.4441 - acc: 0.5426 - val_loss: 1.2771 - val_acc: 0.6147\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3319 - acc: 0.5753\n",
      "Epoch 00005: val_loss improved from 1.27708 to 1.18747, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/005-1.1875.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.3319 - acc: 0.5753 - val_loss: 1.1875 - val_acc: 0.6359\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2503 - acc: 0.6082\n",
      "Epoch 00006: val_loss improved from 1.18747 to 1.14039, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/006-1.1404.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.2505 - acc: 0.6082 - val_loss: 1.1404 - val_acc: 0.6550\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1765 - acc: 0.6299\n",
      "Epoch 00007: val_loss did not improve from 1.14039\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 1.1768 - acc: 0.6299 - val_loss: 1.1506 - val_acc: 0.6567\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1330 - acc: 0.6448\n",
      "Epoch 00008: val_loss improved from 1.14039 to 1.13208, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/008-1.1321.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.1328 - acc: 0.6449 - val_loss: 1.1321 - val_acc: 0.6674\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.6627\n",
      "Epoch 00009: val_loss improved from 1.13208 to 1.08831, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/009-1.0883.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 1.0856 - acc: 0.6627 - val_loss: 1.0883 - val_acc: 0.6709\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0509 - acc: 0.6711\n",
      "Epoch 00010: val_loss did not improve from 1.08831\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 1.0514 - acc: 0.6709 - val_loss: 1.0885 - val_acc: 0.6674\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0191 - acc: 0.6810\n",
      "Epoch 00011: val_loss improved from 1.08831 to 1.05621, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/011-1.0562.hdf5\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 1.0195 - acc: 0.6810 - val_loss: 1.0562 - val_acc: 0.6893\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9896 - acc: 0.6927\n",
      "Epoch 00012: val_loss improved from 1.05621 to 1.03487, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/012-1.0349.hdf5\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.9895 - acc: 0.6927 - val_loss: 1.0349 - val_acc: 0.6981\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9557 - acc: 0.7021\n",
      "Epoch 00013: val_loss improved from 1.03487 to 1.03072, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/013-1.0307.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.9563 - acc: 0.7019 - val_loss: 1.0307 - val_acc: 0.6883\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.7098\n",
      "Epoch 00014: val_loss did not improve from 1.03072\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.9339 - acc: 0.7098 - val_loss: 1.0889 - val_acc: 0.6709\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.7167\n",
      "Epoch 00015: val_loss improved from 1.03072 to 1.03067, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/015-1.0307.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.9122 - acc: 0.7165 - val_loss: 1.0307 - val_acc: 0.7018\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8960 - acc: 0.7196\n",
      "Epoch 00016: val_loss did not improve from 1.03067\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.8959 - acc: 0.7197 - val_loss: 1.0799 - val_acc: 0.6662\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8754 - acc: 0.7282\n",
      "Epoch 00017: val_loss did not improve from 1.03067\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.8757 - acc: 0.7280 - val_loss: 1.0374 - val_acc: 0.6879\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8526 - acc: 0.7371\n",
      "Epoch 00018: val_loss improved from 1.03067 to 0.98998, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/018-0.9900.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.8526 - acc: 0.7371 - val_loss: 0.9900 - val_acc: 0.7060\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8376 - acc: 0.7390\n",
      "Epoch 00019: val_loss did not improve from 0.98998\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.8377 - acc: 0.7391 - val_loss: 0.9965 - val_acc: 0.7002\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8223 - acc: 0.7421\n",
      "Epoch 00020: val_loss improved from 0.98998 to 0.97001, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/020-0.9700.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.8221 - acc: 0.7422 - val_loss: 0.9700 - val_acc: 0.7163\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8049 - acc: 0.7485\n",
      "Epoch 00021: val_loss did not improve from 0.97001\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.8048 - acc: 0.7485 - val_loss: 0.9761 - val_acc: 0.7179\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7481\n",
      "Epoch 00022: val_loss improved from 0.97001 to 0.96161, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/022-0.9616.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.7937 - acc: 0.7482 - val_loss: 0.9616 - val_acc: 0.7191\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7530\n",
      "Epoch 00023: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.7859 - acc: 0.7530 - val_loss: 0.9776 - val_acc: 0.7142\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7591\n",
      "Epoch 00024: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7653 - acc: 0.7592 - val_loss: 0.9657 - val_acc: 0.7193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7550 - acc: 0.7614\n",
      "Epoch 00025: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.7551 - acc: 0.7614 - val_loss: 0.9723 - val_acc: 0.7109\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.7647\n",
      "Epoch 00026: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.7472 - acc: 0.7648 - val_loss: 1.0369 - val_acc: 0.6949\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7669\n",
      "Epoch 00027: val_loss improved from 0.96161 to 0.95603, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/027-0.9560.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7320 - acc: 0.7667 - val_loss: 0.9560 - val_acc: 0.7235\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7220 - acc: 0.7688\n",
      "Epoch 00028: val_loss did not improve from 0.95603\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7219 - acc: 0.7688 - val_loss: 0.9649 - val_acc: 0.7237\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7749\n",
      "Epoch 00029: val_loss did not improve from 0.95603\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.7095 - acc: 0.7748 - val_loss: 0.9645 - val_acc: 0.7261\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7081 - acc: 0.7715\n",
      "Epoch 00030: val_loss improved from 0.95603 to 0.94118, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/030-0.9412.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.7081 - acc: 0.7715 - val_loss: 0.9412 - val_acc: 0.7338\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6848 - acc: 0.7821\n",
      "Epoch 00031: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.6849 - acc: 0.7821 - val_loss: 0.9820 - val_acc: 0.7160\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.7825\n",
      "Epoch 00032: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6844 - acc: 0.7825 - val_loss: 0.9761 - val_acc: 0.7149\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6743 - acc: 0.7844\n",
      "Epoch 00033: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6743 - acc: 0.7844 - val_loss: 0.9633 - val_acc: 0.7149\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6678 - acc: 0.7856\n",
      "Epoch 00034: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6678 - acc: 0.7857 - val_loss: 0.9432 - val_acc: 0.7282\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6637 - acc: 0.7888\n",
      "Epoch 00035: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6636 - acc: 0.7889 - val_loss: 0.9551 - val_acc: 0.7265\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.7899\n",
      "Epoch 00036: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.6477 - acc: 0.7899 - val_loss: 0.9610 - val_acc: 0.7263\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7944\n",
      "Epoch 00037: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.6422 - acc: 0.7945 - val_loss: 0.9722 - val_acc: 0.7181\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.7948\n",
      "Epoch 00038: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6357 - acc: 0.7947 - val_loss: 0.9635 - val_acc: 0.7179\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6255 - acc: 0.7977\n",
      "Epoch 00039: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6256 - acc: 0.7977 - val_loss: 0.9553 - val_acc: 0.7282\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.8000\n",
      "Epoch 00040: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6230 - acc: 0.8000 - val_loss: 0.9831 - val_acc: 0.7167\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.8037\n",
      "Epoch 00041: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.6092 - acc: 0.8036 - val_loss: 1.0015 - val_acc: 0.7119\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.8079\n",
      "Epoch 00042: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6040 - acc: 0.8079 - val_loss: 0.9610 - val_acc: 0.7282\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.8025\n",
      "Epoch 00043: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.6064 - acc: 0.8025 - val_loss: 1.0327 - val_acc: 0.6993\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5946 - acc: 0.8081\n",
      "Epoch 00044: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5944 - acc: 0.8082 - val_loss: 0.9444 - val_acc: 0.7349\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8101\n",
      "Epoch 00045: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5851 - acc: 0.8098 - val_loss: 0.9614 - val_acc: 0.7282\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5806 - acc: 0.8126\n",
      "Epoch 00046: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5810 - acc: 0.8124 - val_loss: 0.9645 - val_acc: 0.7270\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8153\n",
      "Epoch 00047: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5753 - acc: 0.8153 - val_loss: 0.9577 - val_acc: 0.7279\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8145\n",
      "Epoch 00048: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5736 - acc: 0.8143 - val_loss: 0.9655 - val_acc: 0.7254\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.8139\n",
      "Epoch 00049: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5700 - acc: 0.8138 - val_loss: 0.9543 - val_acc: 0.7244\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.8167\n",
      "Epoch 00050: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5639 - acc: 0.8166 - val_loss: 0.9524 - val_acc: 0.7314\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8188\n",
      "Epoch 00051: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5527 - acc: 0.8186 - val_loss: 1.0078 - val_acc: 0.7126\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8214\n",
      "Epoch 00052: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5472 - acc: 0.8214 - val_loss: 0.9736 - val_acc: 0.7226\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8209\n",
      "Epoch 00053: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5478 - acc: 0.8207 - val_loss: 1.0764 - val_acc: 0.6890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8249\n",
      "Epoch 00054: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5381 - acc: 0.8248 - val_loss: 0.9554 - val_acc: 0.7317\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8264\n",
      "Epoch 00055: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5286 - acc: 0.8263 - val_loss: 1.0714 - val_acc: 0.6923\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8293\n",
      "Epoch 00056: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.5282 - acc: 0.8292 - val_loss: 0.9776 - val_acc: 0.7314\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8268\n",
      "Epoch 00057: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5307 - acc: 0.8266 - val_loss: 0.9881 - val_acc: 0.7198\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8302\n",
      "Epoch 00058: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5195 - acc: 0.8302 - val_loss: 0.9526 - val_acc: 0.7349\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8306\n",
      "Epoch 00059: val_loss improved from 0.94118 to 0.93558, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/059-0.9356.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5188 - acc: 0.8305 - val_loss: 0.9356 - val_acc: 0.7354\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8310\n",
      "Epoch 00060: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5150 - acc: 0.8311 - val_loss: 0.9570 - val_acc: 0.7312\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8353\n",
      "Epoch 00061: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5036 - acc: 0.8353 - val_loss: 0.9825 - val_acc: 0.7181\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8356\n",
      "Epoch 00062: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5019 - acc: 0.8356 - val_loss: 0.9583 - val_acc: 0.7300\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8365\n",
      "Epoch 00063: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4986 - acc: 0.8364 - val_loss: 0.9654 - val_acc: 0.7284\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8412\n",
      "Epoch 00064: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4891 - acc: 0.8412 - val_loss: 0.9747 - val_acc: 0.7277\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8396\n",
      "Epoch 00065: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4887 - acc: 0.8395 - val_loss: 1.0115 - val_acc: 0.7172\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8407\n",
      "Epoch 00066: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4871 - acc: 0.8406 - val_loss: 1.0500 - val_acc: 0.7081\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8397\n",
      "Epoch 00067: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4884 - acc: 0.8397 - val_loss: 0.9529 - val_acc: 0.7312\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8421\n",
      "Epoch 00068: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4802 - acc: 0.8421 - val_loss: 0.9452 - val_acc: 0.7358\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8440\n",
      "Epoch 00069: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4717 - acc: 0.8441 - val_loss: 0.9930 - val_acc: 0.7198\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8439\n",
      "Epoch 00070: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4767 - acc: 0.8440 - val_loss: 0.9858 - val_acc: 0.7247\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8454\n",
      "Epoch 00071: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4647 - acc: 0.8454 - val_loss: 0.9816 - val_acc: 0.7279\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.8473\n",
      "Epoch 00072: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4669 - acc: 0.8473 - val_loss: 1.0065 - val_acc: 0.7233\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8482\n",
      "Epoch 00073: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.4596 - acc: 0.8482 - val_loss: 0.9452 - val_acc: 0.7407\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8522\n",
      "Epoch 00074: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4550 - acc: 0.8520 - val_loss: 1.0261 - val_acc: 0.7181\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8490\n",
      "Epoch 00075: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4585 - acc: 0.8489 - val_loss: 0.9773 - val_acc: 0.7244\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8507\n",
      "Epoch 00076: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4544 - acc: 0.8507 - val_loss: 1.0295 - val_acc: 0.7130\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8525\n",
      "Epoch 00077: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4477 - acc: 0.8524 - val_loss: 0.9946 - val_acc: 0.7223\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8530\n",
      "Epoch 00078: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4461 - acc: 0.8531 - val_loss: 0.9511 - val_acc: 0.7393\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8562\n",
      "Epoch 00079: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4404 - acc: 0.8562 - val_loss: 0.9849 - val_acc: 0.7328\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.8534\n",
      "Epoch 00080: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4451 - acc: 0.8533 - val_loss: 0.9625 - val_acc: 0.7328\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.8568\n",
      "Epoch 00081: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4387 - acc: 0.8568 - val_loss: 0.9644 - val_acc: 0.7382\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.8605\n",
      "Epoch 00082: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4279 - acc: 0.8604 - val_loss: 1.0134 - val_acc: 0.7158\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8581\n",
      "Epoch 00083: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4286 - acc: 0.8580 - val_loss: 0.9536 - val_acc: 0.7372\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8577\n",
      "Epoch 00084: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4315 - acc: 0.8575 - val_loss: 0.9442 - val_acc: 0.7410\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8581\n",
      "Epoch 00085: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4303 - acc: 0.8580 - val_loss: 1.0011 - val_acc: 0.7326\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8611\n",
      "Epoch 00086: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4223 - acc: 0.8612 - val_loss: 0.9800 - val_acc: 0.7300\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8625\n",
      "Epoch 00087: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4143 - acc: 0.8626 - val_loss: 1.0105 - val_acc: 0.7237\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8618\n",
      "Epoch 00088: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4199 - acc: 0.8617 - val_loss: 0.9907 - val_acc: 0.7221\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8658\n",
      "Epoch 00089: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4123 - acc: 0.8658 - val_loss: 1.0166 - val_acc: 0.7209\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8657\n",
      "Epoch 00090: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4120 - acc: 0.8657 - val_loss: 1.1225 - val_acc: 0.6895\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8657\n",
      "Epoch 00091: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4094 - acc: 0.8657 - val_loss: 1.0057 - val_acc: 0.7247\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8653\n",
      "Epoch 00092: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4049 - acc: 0.8654 - val_loss: 1.0387 - val_acc: 0.7149\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8692\n",
      "Epoch 00093: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4032 - acc: 0.8691 - val_loss: 1.0506 - val_acc: 0.7174\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8698\n",
      "Epoch 00094: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4002 - acc: 0.8698 - val_loss: 0.9941 - val_acc: 0.7307\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8684\n",
      "Epoch 00095: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4008 - acc: 0.8684 - val_loss: 1.0841 - val_acc: 0.6986\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8706\n",
      "Epoch 00096: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3945 - acc: 0.8706 - val_loss: 0.9818 - val_acc: 0.7324\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8695\n",
      "Epoch 00097: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3981 - acc: 0.8696 - val_loss: 0.9601 - val_acc: 0.7389\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8696\n",
      "Epoch 00098: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3933 - acc: 0.8696 - val_loss: 0.9498 - val_acc: 0.7384\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8733\n",
      "Epoch 00099: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3892 - acc: 0.8732 - val_loss: 0.9926 - val_acc: 0.7282\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8724\n",
      "Epoch 00100: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3878 - acc: 0.8724 - val_loss: 0.9841 - val_acc: 0.7317\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8726\n",
      "Epoch 00101: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3808 - acc: 0.8726 - val_loss: 1.0271 - val_acc: 0.7247\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8734\n",
      "Epoch 00102: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3835 - acc: 0.8732 - val_loss: 1.0209 - val_acc: 0.7247\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8765\n",
      "Epoch 00103: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3779 - acc: 0.8764 - val_loss: 0.9997 - val_acc: 0.7324\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8770\n",
      "Epoch 00104: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3767 - acc: 0.8768 - val_loss: 1.0508 - val_acc: 0.7202\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8762\n",
      "Epoch 00105: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3743 - acc: 0.8762 - val_loss: 1.0053 - val_acc: 0.7333\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8792\n",
      "Epoch 00106: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3696 - acc: 0.8793 - val_loss: 0.9776 - val_acc: 0.7345\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8777\n",
      "Epoch 00107: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3738 - acc: 0.8776 - val_loss: 1.0110 - val_acc: 0.7338\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8784\n",
      "Epoch 00108: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3699 - acc: 0.8782 - val_loss: 1.0159 - val_acc: 0.7275\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8786\n",
      "Epoch 00109: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3643 - acc: 0.8787 - val_loss: 0.9955 - val_acc: 0.7340\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8784\n",
      "Epoch 00110: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3668 - acc: 0.8785 - val_loss: 0.9869 - val_acc: 0.7365\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8825\n",
      "Epoch 00111: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3578 - acc: 0.8824 - val_loss: 0.9714 - val_acc: 0.7398\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8827\n",
      "Epoch 00112: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3622 - acc: 0.8827 - val_loss: 1.0117 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8828\n",
      "Epoch 00113: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3519 - acc: 0.8830 - val_loss: 1.0918 - val_acc: 0.7137\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8825\n",
      "Epoch 00114: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3598 - acc: 0.8825 - val_loss: 0.9892 - val_acc: 0.7424\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8832\n",
      "Epoch 00115: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3543 - acc: 0.8834 - val_loss: 1.0353 - val_acc: 0.7205\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8836\n",
      "Epoch 00116: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3547 - acc: 0.8836 - val_loss: 1.0145 - val_acc: 0.7356\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8856\n",
      "Epoch 00117: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3502 - acc: 0.8856 - val_loss: 0.9799 - val_acc: 0.7370\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8833\n",
      "Epoch 00118: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3487 - acc: 0.8833 - val_loss: 1.0020 - val_acc: 0.7331\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8832\n",
      "Epoch 00119: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3463 - acc: 0.8831 - val_loss: 0.9727 - val_acc: 0.7407\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8882\n",
      "Epoch 00120: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3406 - acc: 0.8881 - val_loss: 1.0344 - val_acc: 0.7286\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8898\n",
      "Epoch 00121: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3404 - acc: 0.8897 - val_loss: 1.0088 - val_acc: 0.7326\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8879\n",
      "Epoch 00122: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3445 - acc: 0.8879 - val_loss: 0.9785 - val_acc: 0.7424\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8887\n",
      "Epoch 00123: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3405 - acc: 0.8886 - val_loss: 0.9968 - val_acc: 0.7356\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8887\n",
      "Epoch 00124: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3370 - acc: 0.8887 - val_loss: 1.0683 - val_acc: 0.7158\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8902\n",
      "Epoch 00125: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3343 - acc: 0.8902 - val_loss: 1.0123 - val_acc: 0.7321\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8923\n",
      "Epoch 00126: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3298 - acc: 0.8923 - val_loss: 0.9660 - val_acc: 0.7454\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8935\n",
      "Epoch 00127: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3300 - acc: 0.8934 - val_loss: 0.9985 - val_acc: 0.7349\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8898\n",
      "Epoch 00128: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3337 - acc: 0.8896 - val_loss: 1.0194 - val_acc: 0.7358\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8933\n",
      "Epoch 00129: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3309 - acc: 0.8932 - val_loss: 1.0203 - val_acc: 0.7358\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8910\n",
      "Epoch 00130: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.3311 - acc: 0.8912 - val_loss: 0.9784 - val_acc: 0.7384\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8939\n",
      "Epoch 00131: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3230 - acc: 0.8938 - val_loss: 1.0232 - val_acc: 0.7300\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8954\n",
      "Epoch 00132: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3199 - acc: 0.8955 - val_loss: 0.9911 - val_acc: 0.7403\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8944\n",
      "Epoch 00133: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3207 - acc: 0.8943 - val_loss: 1.0839 - val_acc: 0.7195\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8957\n",
      "Epoch 00134: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3213 - acc: 0.8957 - val_loss: 1.0768 - val_acc: 0.7228\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8970\n",
      "Epoch 00135: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3163 - acc: 0.8969 - val_loss: 1.1603 - val_acc: 0.7042\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8973\n",
      "Epoch 00136: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3133 - acc: 0.8973 - val_loss: 0.9846 - val_acc: 0.7361\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8957\n",
      "Epoch 00137: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3164 - acc: 0.8957 - val_loss: 0.9945 - val_acc: 0.7461\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8957\n",
      "Epoch 00138: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3135 - acc: 0.8957 - val_loss: 1.0148 - val_acc: 0.7354\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.8976\n",
      "Epoch 00139: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3166 - acc: 0.8976 - val_loss: 0.9915 - val_acc: 0.7398\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8985\n",
      "Epoch 00140: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3107 - acc: 0.8985 - val_loss: 0.9923 - val_acc: 0.7424\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8948\n",
      "Epoch 00141: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.3177 - acc: 0.8948 - val_loss: 1.0029 - val_acc: 0.7428\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.8995\n",
      "Epoch 00142: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3100 - acc: 0.8994 - val_loss: 1.0056 - val_acc: 0.7375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.9002\n",
      "Epoch 00143: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3039 - acc: 0.9001 - val_loss: 0.9829 - val_acc: 0.7482\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.8990\n",
      "Epoch 00144: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.3035 - acc: 0.8990 - val_loss: 1.0427 - val_acc: 0.7331\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8970\n",
      "Epoch 00145: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3110 - acc: 0.8971 - val_loss: 0.9736 - val_acc: 0.7503\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9004\n",
      "Epoch 00146: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3009 - acc: 0.9004 - val_loss: 1.0045 - val_acc: 0.7386\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9009\n",
      "Epoch 00147: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3027 - acc: 0.9008 - val_loss: 1.0243 - val_acc: 0.7379\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.8999\n",
      "Epoch 00148: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3075 - acc: 0.8999 - val_loss: 0.9929 - val_acc: 0.7489\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9027\n",
      "Epoch 00149: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2977 - acc: 0.9027 - val_loss: 1.0005 - val_acc: 0.7377\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9007\n",
      "Epoch 00150: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2992 - acc: 0.9007 - val_loss: 1.0058 - val_acc: 0.7417\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9052\n",
      "Epoch 00151: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2947 - acc: 0.9053 - val_loss: 1.0222 - val_acc: 0.7438\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9036\n",
      "Epoch 00152: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.2959 - acc: 0.9036 - val_loss: 1.0311 - val_acc: 0.7370\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9013\n",
      "Epoch 00153: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3010 - acc: 0.9012 - val_loss: 1.0420 - val_acc: 0.7291\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9034\n",
      "Epoch 00154: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2925 - acc: 0.9034 - val_loss: 1.0092 - val_acc: 0.7461\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9032\n",
      "Epoch 00155: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2945 - acc: 0.9033 - val_loss: 1.1165 - val_acc: 0.7133\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9034\n",
      "Epoch 00156: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2949 - acc: 0.9034 - val_loss: 1.0731 - val_acc: 0.7184\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9053\n",
      "Epoch 00157: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2917 - acc: 0.9053 - val_loss: 1.0128 - val_acc: 0.7466\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9058\n",
      "Epoch 00158: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2880 - acc: 0.9058 - val_loss: 1.0358 - val_acc: 0.7375\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9048\n",
      "Epoch 00159: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2886 - acc: 0.9047 - val_loss: 0.9812 - val_acc: 0.7498\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 316us/sample - loss: 1.0973 - acc: 0.6885\n",
      "Loss: 1.097267709664714 Accuracy: 0.6884735\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.6178 - acc: 0.2159\n",
      "Epoch 00001: val_loss improved from inf to 1.95258, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/001-1.9526.hdf5\n",
      "36805/36805 [==============================] - 22s 611us/sample - loss: 2.6169 - acc: 0.2162 - val_loss: 1.9526 - val_acc: 0.3201\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8290 - acc: 0.4174\n",
      "Epoch 00002: val_loss improved from 1.95258 to 1.38704, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/002-1.3870.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 1.8289 - acc: 0.4174 - val_loss: 1.3870 - val_acc: 0.5623\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5293 - acc: 0.5113\n",
      "Epoch 00003: val_loss improved from 1.38704 to 1.22565, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/003-1.2256.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.5290 - acc: 0.5114 - val_loss: 1.2256 - val_acc: 0.6177\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3344 - acc: 0.5779\n",
      "Epoch 00004: val_loss improved from 1.22565 to 1.09019, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/004-1.0902.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.3342 - acc: 0.5780 - val_loss: 1.0902 - val_acc: 0.6678\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2044 - acc: 0.6225\n",
      "Epoch 00005: val_loss improved from 1.09019 to 1.04910, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/005-1.0491.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.2047 - acc: 0.6224 - val_loss: 1.0491 - val_acc: 0.6739\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1158 - acc: 0.6515\n",
      "Epoch 00006: val_loss improved from 1.04910 to 0.93213, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/006-0.9321.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.1152 - acc: 0.6517 - val_loss: 0.9321 - val_acc: 0.7209\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0415 - acc: 0.6779\n",
      "Epoch 00007: val_loss improved from 0.93213 to 0.91204, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/007-0.9120.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 1.0409 - acc: 0.6779 - val_loss: 0.9120 - val_acc: 0.7293\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9822 - acc: 0.6986\n",
      "Epoch 00008: val_loss improved from 0.91204 to 0.87234, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/008-0.8723.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.9826 - acc: 0.6985 - val_loss: 0.8723 - val_acc: 0.7391\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9322 - acc: 0.7148\n",
      "Epoch 00009: val_loss did not improve from 0.87234\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.9332 - acc: 0.7146 - val_loss: 0.9922 - val_acc: 0.6976\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8878 - acc: 0.7297\n",
      "Epoch 00010: val_loss improved from 0.87234 to 0.80592, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/010-0.8059.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.8883 - acc: 0.7297 - val_loss: 0.8059 - val_acc: 0.7605\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.7450\n",
      "Epoch 00011: val_loss improved from 0.80592 to 0.77635, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/011-0.7764.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.8458 - acc: 0.7451 - val_loss: 0.7764 - val_acc: 0.7717\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8101 - acc: 0.7544\n",
      "Epoch 00012: val_loss did not improve from 0.77635\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.8104 - acc: 0.7546 - val_loss: 0.8117 - val_acc: 0.7536\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7668\n",
      "Epoch 00013: val_loss did not improve from 0.77635\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7777 - acc: 0.7666 - val_loss: 0.8059 - val_acc: 0.7678\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7745\n",
      "Epoch 00014: val_loss improved from 0.77635 to 0.69212, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/014-0.6921.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.7464 - acc: 0.7743 - val_loss: 0.6921 - val_acc: 0.8032\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7829\n",
      "Epoch 00015: val_loss did not improve from 0.69212\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.7202 - acc: 0.7830 - val_loss: 0.7953 - val_acc: 0.7561\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.7901\n",
      "Epoch 00016: val_loss improved from 0.69212 to 0.66358, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/016-0.6636.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.6976 - acc: 0.7903 - val_loss: 0.6636 - val_acc: 0.8183\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7994\n",
      "Epoch 00017: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.6724 - acc: 0.7994 - val_loss: 0.7812 - val_acc: 0.7689\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8056\n",
      "Epoch 00018: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.6485 - acc: 0.8055 - val_loss: 0.7487 - val_acc: 0.7862\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.8109\n",
      "Epoch 00019: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.6325 - acc: 0.8109 - val_loss: 0.7366 - val_acc: 0.7906\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6200 - acc: 0.8137\n",
      "Epoch 00020: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.6203 - acc: 0.8137 - val_loss: 0.6668 - val_acc: 0.8104\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8209\n",
      "Epoch 00021: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.6019 - acc: 0.8210 - val_loss: 0.7371 - val_acc: 0.7808\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8292\n",
      "Epoch 00022: val_loss improved from 0.66358 to 0.65024, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/022-0.6502.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.5769 - acc: 0.8292 - val_loss: 0.6502 - val_acc: 0.8143\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.8306\n",
      "Epoch 00023: val_loss improved from 0.65024 to 0.59473, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/023-0.5947.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5685 - acc: 0.8306 - val_loss: 0.5947 - val_acc: 0.8337\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8343\n",
      "Epoch 00024: val_loss did not improve from 0.59473\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.5538 - acc: 0.8343 - val_loss: 0.6027 - val_acc: 0.8314\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8383\n",
      "Epoch 00025: val_loss improved from 0.59473 to 0.58547, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/025-0.5855.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5429 - acc: 0.8381 - val_loss: 0.5855 - val_acc: 0.8383\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8420\n",
      "Epoch 00026: val_loss did not improve from 0.58547\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5295 - acc: 0.8418 - val_loss: 0.6228 - val_acc: 0.8267\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8447\n",
      "Epoch 00027: val_loss did not improve from 0.58547\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.5193 - acc: 0.8446 - val_loss: 0.6004 - val_acc: 0.8323\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8485\n",
      "Epoch 00028: val_loss improved from 0.58547 to 0.57381, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/028-0.5738.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5117 - acc: 0.8486 - val_loss: 0.5738 - val_acc: 0.8439\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8483\n",
      "Epoch 00029: val_loss did not improve from 0.57381\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5011 - acc: 0.8483 - val_loss: 0.5909 - val_acc: 0.8288\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8539\n",
      "Epoch 00030: val_loss did not improve from 0.57381\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4887 - acc: 0.8539 - val_loss: 0.6099 - val_acc: 0.8209\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8540\n",
      "Epoch 00031: val_loss improved from 0.57381 to 0.56241, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/031-0.5624.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4871 - acc: 0.8540 - val_loss: 0.5624 - val_acc: 0.8458\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8571\n",
      "Epoch 00032: val_loss did not improve from 0.56241\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4765 - acc: 0.8570 - val_loss: 0.5636 - val_acc: 0.8383\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8579\n",
      "Epoch 00033: val_loss did not improve from 0.56241\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4709 - acc: 0.8579 - val_loss: 0.5984 - val_acc: 0.8297\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8616\n",
      "Epoch 00034: val_loss improved from 0.56241 to 0.55570, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/034-0.5557.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4557 - acc: 0.8615 - val_loss: 0.5557 - val_acc: 0.8449\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8636\n",
      "Epoch 00035: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4542 - acc: 0.8637 - val_loss: 0.5794 - val_acc: 0.8393\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8658\n",
      "Epoch 00036: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4466 - acc: 0.8657 - val_loss: 0.5775 - val_acc: 0.8355\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8639\n",
      "Epoch 00037: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4425 - acc: 0.8640 - val_loss: 0.5621 - val_acc: 0.8439\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8680\n",
      "Epoch 00038: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4348 - acc: 0.8678 - val_loss: 0.5661 - val_acc: 0.8435\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8681\n",
      "Epoch 00039: val_loss improved from 0.55570 to 0.51766, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/039-0.5177.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4363 - acc: 0.8682 - val_loss: 0.5177 - val_acc: 0.8586\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8741\n",
      "Epoch 00040: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4188 - acc: 0.8740 - val_loss: 0.5384 - val_acc: 0.8477\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8741\n",
      "Epoch 00041: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4144 - acc: 0.8741 - val_loss: 0.5943 - val_acc: 0.8355\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8755\n",
      "Epoch 00042: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.4112 - acc: 0.8756 - val_loss: 0.5445 - val_acc: 0.8472\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8783\n",
      "Epoch 00043: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3988 - acc: 0.8782 - val_loss: 0.5627 - val_acc: 0.8425\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8779\n",
      "Epoch 00044: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3959 - acc: 0.8779 - val_loss: 0.5826 - val_acc: 0.8402\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8807\n",
      "Epoch 00045: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3922 - acc: 0.8807 - val_loss: 0.5530 - val_acc: 0.8456\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8822\n",
      "Epoch 00046: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3870 - acc: 0.8821 - val_loss: 0.5345 - val_acc: 0.8537\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8825\n",
      "Epoch 00047: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3846 - acc: 0.8825 - val_loss: 0.5459 - val_acc: 0.8425\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8832\n",
      "Epoch 00048: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3782 - acc: 0.8831 - val_loss: 0.6523 - val_acc: 0.8109\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8872\n",
      "Epoch 00049: val_loss improved from 0.51766 to 0.50354, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/049-0.5035.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3708 - acc: 0.8871 - val_loss: 0.5035 - val_acc: 0.8628\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8862\n",
      "Epoch 00050: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3711 - acc: 0.8864 - val_loss: 0.5421 - val_acc: 0.8514\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8857\n",
      "Epoch 00051: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3748 - acc: 0.8857 - val_loss: 0.5346 - val_acc: 0.8567\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8885\n",
      "Epoch 00052: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3630 - acc: 0.8886 - val_loss: 0.5992 - val_acc: 0.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8889\n",
      "Epoch 00053: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3638 - acc: 0.8889 - val_loss: 0.5341 - val_acc: 0.8493\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8928\n",
      "Epoch 00054: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3522 - acc: 0.8927 - val_loss: 0.5087 - val_acc: 0.8609\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8940\n",
      "Epoch 00055: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3477 - acc: 0.8940 - val_loss: 0.5459 - val_acc: 0.8528\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8918\n",
      "Epoch 00056: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3528 - acc: 0.8919 - val_loss: 0.5917 - val_acc: 0.8304\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8919\n",
      "Epoch 00057: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3465 - acc: 0.8919 - val_loss: 0.5415 - val_acc: 0.8519\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8941\n",
      "Epoch 00058: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3424 - acc: 0.8941 - val_loss: 0.6046 - val_acc: 0.8346\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8963\n",
      "Epoch 00059: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3387 - acc: 0.8962 - val_loss: 0.5277 - val_acc: 0.8505\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8995\n",
      "Epoch 00060: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3344 - acc: 0.8995 - val_loss: 0.5442 - val_acc: 0.8409\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8996\n",
      "Epoch 00061: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3307 - acc: 0.8995 - val_loss: 0.5094 - val_acc: 0.8637\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8982\n",
      "Epoch 00062: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3240 - acc: 0.8982 - val_loss: 0.5426 - val_acc: 0.8472\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8996\n",
      "Epoch 00063: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3247 - acc: 0.8995 - val_loss: 0.5367 - val_acc: 0.8558\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.9002\n",
      "Epoch 00064: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3236 - acc: 0.9001 - val_loss: 0.5143 - val_acc: 0.8528\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9019\n",
      "Epoch 00065: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3175 - acc: 0.9019 - val_loss: 0.5253 - val_acc: 0.8556\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9009\n",
      "Epoch 00066: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3204 - acc: 0.9008 - val_loss: 0.5426 - val_acc: 0.8523\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9033\n",
      "Epoch 00067: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3075 - acc: 0.9032 - val_loss: 0.5273 - val_acc: 0.8586\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9064\n",
      "Epoch 00068: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3026 - acc: 0.9064 - val_loss: 0.5381 - val_acc: 0.8491\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9070\n",
      "Epoch 00069: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3009 - acc: 0.9071 - val_loss: 0.6499 - val_acc: 0.8230\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9034\n",
      "Epoch 00070: val_loss improved from 0.50354 to 0.49490, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/070-0.4949.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.3088 - acc: 0.9035 - val_loss: 0.4949 - val_acc: 0.8595\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9064\n",
      "Epoch 00071: val_loss improved from 0.49490 to 0.49024, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/071-0.4902.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.3001 - acc: 0.9063 - val_loss: 0.4902 - val_acc: 0.8595\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9067\n",
      "Epoch 00072: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3003 - acc: 0.9068 - val_loss: 0.5081 - val_acc: 0.8630\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9072\n",
      "Epoch 00073: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2975 - acc: 0.9072 - val_loss: 0.6161 - val_acc: 0.8279\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9094\n",
      "Epoch 00074: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2932 - acc: 0.9093 - val_loss: 0.6169 - val_acc: 0.8337\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9082\n",
      "Epoch 00075: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2939 - acc: 0.9081 - val_loss: 0.5145 - val_acc: 0.8616\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9098\n",
      "Epoch 00076: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2870 - acc: 0.9099 - val_loss: 0.5127 - val_acc: 0.8609\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9124\n",
      "Epoch 00077: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2848 - acc: 0.9124 - val_loss: 0.5567 - val_acc: 0.8484\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9116\n",
      "Epoch 00078: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2799 - acc: 0.9116 - val_loss: 0.5203 - val_acc: 0.8560\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9115\n",
      "Epoch 00079: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2846 - acc: 0.9114 - val_loss: 0.5017 - val_acc: 0.8656\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9143\n",
      "Epoch 00080: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2736 - acc: 0.9143 - val_loss: 0.5596 - val_acc: 0.8512\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9124\n",
      "Epoch 00081: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2806 - acc: 0.9123 - val_loss: 0.4978 - val_acc: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9146\n",
      "Epoch 00082: val_loss improved from 0.49024 to 0.47455, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/082-0.4746.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.2693 - acc: 0.9147 - val_loss: 0.4746 - val_acc: 0.8749\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9173\n",
      "Epoch 00083: val_loss improved from 0.47455 to 0.47264, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/083-0.4726.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2683 - acc: 0.9173 - val_loss: 0.4726 - val_acc: 0.8649\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9153\n",
      "Epoch 00084: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2704 - acc: 0.9153 - val_loss: 0.5026 - val_acc: 0.8679\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9184\n",
      "Epoch 00085: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2642 - acc: 0.9185 - val_loss: 0.4937 - val_acc: 0.8635\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9185\n",
      "Epoch 00086: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2609 - acc: 0.9184 - val_loss: 0.5457 - val_acc: 0.8519\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9183\n",
      "Epoch 00087: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2603 - acc: 0.9184 - val_loss: 0.5191 - val_acc: 0.8668\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9192\n",
      "Epoch 00088: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2593 - acc: 0.9192 - val_loss: 0.5175 - val_acc: 0.8649\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9184\n",
      "Epoch 00089: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2579 - acc: 0.9183 - val_loss: 0.4999 - val_acc: 0.8670\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9193\n",
      "Epoch 00090: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2567 - acc: 0.9191 - val_loss: 0.5062 - val_acc: 0.8689\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9205\n",
      "Epoch 00091: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2563 - acc: 0.9205 - val_loss: 0.4863 - val_acc: 0.8742\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9192\n",
      "Epoch 00092: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2563 - acc: 0.9193 - val_loss: 0.5221 - val_acc: 0.8591\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9224\n",
      "Epoch 00093: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2470 - acc: 0.9225 - val_loss: 0.5137 - val_acc: 0.8630\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9243\n",
      "Epoch 00094: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2423 - acc: 0.9244 - val_loss: 0.4911 - val_acc: 0.8693\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9241\n",
      "Epoch 00095: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2446 - acc: 0.9241 - val_loss: 0.5119 - val_acc: 0.8640\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9229\n",
      "Epoch 00096: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2436 - acc: 0.9230 - val_loss: 0.5332 - val_acc: 0.8563\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9253\n",
      "Epoch 00097: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2430 - acc: 0.9252 - val_loss: 0.4854 - val_acc: 0.8700\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9241\n",
      "Epoch 00098: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2373 - acc: 0.9241 - val_loss: 0.5267 - val_acc: 0.8661\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9238\n",
      "Epoch 00099: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2355 - acc: 0.9238 - val_loss: 0.5566 - val_acc: 0.8577\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9248\n",
      "Epoch 00100: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2374 - acc: 0.9247 - val_loss: 0.5087 - val_acc: 0.8693\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9248\n",
      "Epoch 00101: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2353 - acc: 0.9247 - val_loss: 0.4738 - val_acc: 0.8749\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9265\n",
      "Epoch 00102: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2306 - acc: 0.9265 - val_loss: 0.5399 - val_acc: 0.8584\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9272\n",
      "Epoch 00103: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2306 - acc: 0.9273 - val_loss: 0.5138 - val_acc: 0.8700\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9253\n",
      "Epoch 00104: val_loss improved from 0.47264 to 0.46607, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/104-0.4661.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2312 - acc: 0.9251 - val_loss: 0.4661 - val_acc: 0.8812\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9268\n",
      "Epoch 00105: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2321 - acc: 0.9268 - val_loss: 0.4775 - val_acc: 0.8793\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9270\n",
      "Epoch 00106: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2276 - acc: 0.9271 - val_loss: 0.5061 - val_acc: 0.8679\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9276\n",
      "Epoch 00107: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2274 - acc: 0.9277 - val_loss: 0.4996 - val_acc: 0.8726\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9302\n",
      "Epoch 00108: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2248 - acc: 0.9302 - val_loss: 0.4944 - val_acc: 0.8651\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9281\n",
      "Epoch 00109: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2261 - acc: 0.9280 - val_loss: 0.4694 - val_acc: 0.8786\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9293\n",
      "Epoch 00110: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2222 - acc: 0.9292 - val_loss: 0.5016 - val_acc: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9316\n",
      "Epoch 00111: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2145 - acc: 0.9316 - val_loss: 0.4810 - val_acc: 0.8782\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9304\n",
      "Epoch 00112: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2192 - acc: 0.9304 - val_loss: 0.4851 - val_acc: 0.8714\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9300\n",
      "Epoch 00113: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2182 - acc: 0.9300 - val_loss: 0.4701 - val_acc: 0.8793\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9312\n",
      "Epoch 00114: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2146 - acc: 0.9313 - val_loss: 0.5690 - val_acc: 0.8519\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9316\n",
      "Epoch 00115: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2156 - acc: 0.9316 - val_loss: 0.4665 - val_acc: 0.8782\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9319\n",
      "Epoch 00116: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2148 - acc: 0.9319 - val_loss: 0.5176 - val_acc: 0.8612\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9318\n",
      "Epoch 00117: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2122 - acc: 0.9319 - val_loss: 0.4752 - val_acc: 0.8737\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9323\n",
      "Epoch 00118: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2125 - acc: 0.9322 - val_loss: 0.5875 - val_acc: 0.8574\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9342\n",
      "Epoch 00119: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2088 - acc: 0.9342 - val_loss: 0.4959 - val_acc: 0.8761\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9361\n",
      "Epoch 00120: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2038 - acc: 0.9362 - val_loss: 0.4784 - val_acc: 0.8786\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9338\n",
      "Epoch 00121: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2113 - acc: 0.9338 - val_loss: 0.4717 - val_acc: 0.8800\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9333\n",
      "Epoch 00122: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2089 - acc: 0.9332 - val_loss: 0.5370 - val_acc: 0.8684\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9361\n",
      "Epoch 00123: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2040 - acc: 0.9361 - val_loss: 0.5305 - val_acc: 0.8656\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9359\n",
      "Epoch 00124: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2040 - acc: 0.9359 - val_loss: 0.5076 - val_acc: 0.8677\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9376\n",
      "Epoch 00125: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2028 - acc: 0.9376 - val_loss: 0.5499 - val_acc: 0.8591\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9375\n",
      "Epoch 00126: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1976 - acc: 0.9375 - val_loss: 0.5465 - val_acc: 0.8588\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9397\n",
      "Epoch 00127: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1935 - acc: 0.9396 - val_loss: 0.5156 - val_acc: 0.8721\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9376\n",
      "Epoch 00128: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1984 - acc: 0.9376 - val_loss: 0.5051 - val_acc: 0.8710\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9364\n",
      "Epoch 00129: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2006 - acc: 0.9363 - val_loss: 0.5132 - val_acc: 0.8717\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9372\n",
      "Epoch 00130: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1982 - acc: 0.9372 - val_loss: 0.4813 - val_acc: 0.8763\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9397\n",
      "Epoch 00131: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1912 - acc: 0.9398 - val_loss: 0.4793 - val_acc: 0.8740\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9388\n",
      "Epoch 00132: val_loss improved from 0.46607 to 0.45023, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/132-0.4502.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1931 - acc: 0.9388 - val_loss: 0.4502 - val_acc: 0.8873\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9416\n",
      "Epoch 00133: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1865 - acc: 0.9416 - val_loss: 0.4950 - val_acc: 0.8758\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9395\n",
      "Epoch 00134: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1931 - acc: 0.9395 - val_loss: 0.5113 - val_acc: 0.8710\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9389\n",
      "Epoch 00135: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1863 - acc: 0.9389 - val_loss: 0.5148 - val_acc: 0.8721\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9415\n",
      "Epoch 00136: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1872 - acc: 0.9415 - val_loss: 0.5370 - val_acc: 0.8614\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9399\n",
      "Epoch 00137: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1901 - acc: 0.9398 - val_loss: 0.5514 - val_acc: 0.8614\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9383\n",
      "Epoch 00138: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1919 - acc: 0.9384 - val_loss: 0.5736 - val_acc: 0.8521\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9428\n",
      "Epoch 00139: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1840 - acc: 0.9428 - val_loss: 0.5280 - val_acc: 0.8626\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9422\n",
      "Epoch 00140: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1807 - acc: 0.9421 - val_loss: 0.4960 - val_acc: 0.8812\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9414\n",
      "Epoch 00141: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1841 - acc: 0.9414 - val_loss: 0.5034 - val_acc: 0.8724\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9421\n",
      "Epoch 00142: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1857 - acc: 0.9420 - val_loss: 0.5083 - val_acc: 0.8717\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9404\n",
      "Epoch 00143: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1839 - acc: 0.9403 - val_loss: 0.4835 - val_acc: 0.8782\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9433\n",
      "Epoch 00144: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1777 - acc: 0.9432 - val_loss: 0.4777 - val_acc: 0.8838\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9423\n",
      "Epoch 00145: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1807 - acc: 0.9423 - val_loss: 0.6053 - val_acc: 0.8521\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9435\n",
      "Epoch 00146: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1782 - acc: 0.9436 - val_loss: 0.4805 - val_acc: 0.8779\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9437\n",
      "Epoch 00147: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1751 - acc: 0.9436 - val_loss: 0.4696 - val_acc: 0.8800\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9431\n",
      "Epoch 00148: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1807 - acc: 0.9430 - val_loss: 0.4718 - val_acc: 0.8840\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9441\n",
      "Epoch 00149: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1746 - acc: 0.9440 - val_loss: 0.5119 - val_acc: 0.8765\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9448\n",
      "Epoch 00150: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1777 - acc: 0.9447 - val_loss: 0.5317 - val_acc: 0.8633\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9443\n",
      "Epoch 00151: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1730 - acc: 0.9443 - val_loss: 0.5112 - val_acc: 0.8733\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9434\n",
      "Epoch 00152: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1780 - acc: 0.9435 - val_loss: 0.4558 - val_acc: 0.8845\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9458\n",
      "Epoch 00153: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1704 - acc: 0.9458 - val_loss: 0.4571 - val_acc: 0.8852\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9450\n",
      "Epoch 00154: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1721 - acc: 0.9450 - val_loss: 0.4636 - val_acc: 0.8889\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9450\n",
      "Epoch 00155: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1713 - acc: 0.9450 - val_loss: 0.5024 - val_acc: 0.8754\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9474\n",
      "Epoch 00156: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1680 - acc: 0.9474 - val_loss: 0.5200 - val_acc: 0.8717\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9453\n",
      "Epoch 00157: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1724 - acc: 0.9454 - val_loss: 0.5041 - val_acc: 0.8749\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9477\n",
      "Epoch 00158: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1675 - acc: 0.9477 - val_loss: 0.4965 - val_acc: 0.8737\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9482\n",
      "Epoch 00159: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1646 - acc: 0.9482 - val_loss: 0.4895 - val_acc: 0.8751\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9485\n",
      "Epoch 00160: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1633 - acc: 0.9486 - val_loss: 0.4768 - val_acc: 0.8835\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9484\n",
      "Epoch 00161: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1650 - acc: 0.9484 - val_loss: 0.5077 - val_acc: 0.8689\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9474\n",
      "Epoch 00162: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1648 - acc: 0.9473 - val_loss: 0.4784 - val_acc: 0.8782\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9481\n",
      "Epoch 00163: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1663 - acc: 0.9481 - val_loss: 0.5312 - val_acc: 0.8696\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9462\n",
      "Epoch 00164: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1648 - acc: 0.9462 - val_loss: 0.5379 - val_acc: 0.8730\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9486\n",
      "Epoch 00165: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1629 - acc: 0.9485 - val_loss: 0.5329 - val_acc: 0.8698\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9492\n",
      "Epoch 00166: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1628 - acc: 0.9491 - val_loss: 0.5538 - val_acc: 0.8686\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9471\n",
      "Epoch 00167: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1671 - acc: 0.9471 - val_loss: 0.4735 - val_acc: 0.8845\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9506\n",
      "Epoch 00168: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1587 - acc: 0.9506 - val_loss: 0.4682 - val_acc: 0.8845\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9513\n",
      "Epoch 00169: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1551 - acc: 0.9513 - val_loss: 0.5339 - val_acc: 0.8691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9511\n",
      "Epoch 00170: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1569 - acc: 0.9511 - val_loss: 0.5189 - val_acc: 0.8775\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9488\n",
      "Epoch 00171: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1613 - acc: 0.9488 - val_loss: 0.5417 - val_acc: 0.8649\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9491\n",
      "Epoch 00172: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1585 - acc: 0.9491 - val_loss: 0.5058 - val_acc: 0.8728\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9504\n",
      "Epoch 00173: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.1556 - acc: 0.9505 - val_loss: 0.4546 - val_acc: 0.8873\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9504\n",
      "Epoch 00174: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1603 - acc: 0.9504 - val_loss: 0.5623 - val_acc: 0.8661\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9497\n",
      "Epoch 00175: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1565 - acc: 0.9496 - val_loss: 0.4872 - val_acc: 0.8754\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9488\n",
      "Epoch 00176: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1593 - acc: 0.9488 - val_loss: 0.4988 - val_acc: 0.8765\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9509\n",
      "Epoch 00177: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1529 - acc: 0.9508 - val_loss: 0.5138 - val_acc: 0.8717\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9503\n",
      "Epoch 00178: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1538 - acc: 0.9503 - val_loss: 0.5960 - val_acc: 0.8586\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9470\n",
      "Epoch 00179: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1754 - acc: 0.9470 - val_loss: 0.4906 - val_acc: 0.8770\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9514\n",
      "Epoch 00180: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1488 - acc: 0.9514 - val_loss: 0.4916 - val_acc: 0.8807\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9515\n",
      "Epoch 00181: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1505 - acc: 0.9514 - val_loss: 0.4904 - val_acc: 0.8812\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9533\n",
      "Epoch 00182: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1485 - acc: 0.9533 - val_loss: 0.4785 - val_acc: 0.8856\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9533\n",
      "Epoch 00183: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1482 - acc: 0.9531 - val_loss: 0.4647 - val_acc: 0.8866\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9499\n",
      "Epoch 00184: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1550 - acc: 0.9499 - val_loss: 0.5723 - val_acc: 0.8616\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9539\n",
      "Epoch 00185: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1509 - acc: 0.9539 - val_loss: 0.5047 - val_acc: 0.8784\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9527\n",
      "Epoch 00186: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1486 - acc: 0.9527 - val_loss: 0.5157 - val_acc: 0.8803\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9533\n",
      "Epoch 00187: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1432 - acc: 0.9533 - val_loss: 0.5148 - val_acc: 0.8698\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9525\n",
      "Epoch 00188: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1464 - acc: 0.9525 - val_loss: 0.5345 - val_acc: 0.8714\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9529\n",
      "Epoch 00189: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1473 - acc: 0.9529 - val_loss: 0.4927 - val_acc: 0.8803\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9552\n",
      "Epoch 00190: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1446 - acc: 0.9551 - val_loss: 0.5414 - val_acc: 0.8717\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9536\n",
      "Epoch 00191: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1454 - acc: 0.9536 - val_loss: 0.4868 - val_acc: 0.8807\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9544\n",
      "Epoch 00192: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1462 - acc: 0.9544 - val_loss: 0.5315 - val_acc: 0.8707\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9556\n",
      "Epoch 00193: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1412 - acc: 0.9556 - val_loss: 0.5268 - val_acc: 0.8700\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9541\n",
      "Epoch 00194: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1427 - acc: 0.9542 - val_loss: 0.5141 - val_acc: 0.8793\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9550\n",
      "Epoch 00195: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1454 - acc: 0.9550 - val_loss: 0.4946 - val_acc: 0.8775\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9551\n",
      "Epoch 00196: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1411 - acc: 0.9551 - val_loss: 0.4964 - val_acc: 0.8819\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9521\n",
      "Epoch 00197: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1503 - acc: 0.9520 - val_loss: 0.4875 - val_acc: 0.8828\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9550\n",
      "Epoch 00198: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1407 - acc: 0.9549 - val_loss: 0.5374 - val_acc: 0.8696\n",
      "Epoch 199/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9541\n",
      "Epoch 00199: val_loss improved from 0.45023 to 0.44975, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/199-0.4498.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1429 - acc: 0.9541 - val_loss: 0.4498 - val_acc: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9558\n",
      "Epoch 00200: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1391 - acc: 0.9558 - val_loss: 0.4927 - val_acc: 0.8819\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9547\n",
      "Epoch 00201: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1394 - acc: 0.9547 - val_loss: 0.5022 - val_acc: 0.8775\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9552\n",
      "Epoch 00202: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1421 - acc: 0.9553 - val_loss: 0.5126 - val_acc: 0.8763\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9554\n",
      "Epoch 00203: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1389 - acc: 0.9555 - val_loss: 0.5100 - val_acc: 0.8733\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9558\n",
      "Epoch 00204: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1367 - acc: 0.9557 - val_loss: 0.4922 - val_acc: 0.8805\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9579\n",
      "Epoch 00205: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1341 - acc: 0.9579 - val_loss: 0.5133 - val_acc: 0.8786\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9570\n",
      "Epoch 00206: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1328 - acc: 0.9570 - val_loss: 0.5714 - val_acc: 0.8661\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9558\n",
      "Epoch 00207: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1400 - acc: 0.9558 - val_loss: 0.4636 - val_acc: 0.8880\n",
      "Epoch 208/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9575\n",
      "Epoch 00208: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1347 - acc: 0.9575 - val_loss: 0.5012 - val_acc: 0.8693\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9589\n",
      "Epoch 00209: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1317 - acc: 0.9589 - val_loss: 0.4788 - val_acc: 0.8840\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9588\n",
      "Epoch 00210: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1332 - acc: 0.9588 - val_loss: 0.4861 - val_acc: 0.8824\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9557\n",
      "Epoch 00211: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1398 - acc: 0.9557 - val_loss: 0.4646 - val_acc: 0.8891\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9584\n",
      "Epoch 00212: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1356 - acc: 0.9583 - val_loss: 0.5116 - val_acc: 0.8756\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9561\n",
      "Epoch 00213: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1354 - acc: 0.9561 - val_loss: 0.5104 - val_acc: 0.8814\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9572\n",
      "Epoch 00214: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1327 - acc: 0.9572 - val_loss: 0.4988 - val_acc: 0.8819\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9582\n",
      "Epoch 00215: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1346 - acc: 0.9581 - val_loss: 0.4866 - val_acc: 0.8817\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9593\n",
      "Epoch 00216: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1328 - acc: 0.9593 - val_loss: 0.4922 - val_acc: 0.8866\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9575\n",
      "Epoch 00217: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1360 - acc: 0.9575 - val_loss: 0.4592 - val_acc: 0.8866\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9596\n",
      "Epoch 00218: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1281 - acc: 0.9596 - val_loss: 0.6224 - val_acc: 0.8505\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9585\n",
      "Epoch 00219: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1296 - acc: 0.9585 - val_loss: 0.4904 - val_acc: 0.8859\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9598\n",
      "Epoch 00220: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1296 - acc: 0.9599 - val_loss: 0.4893 - val_acc: 0.8833\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9585\n",
      "Epoch 00221: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1305 - acc: 0.9586 - val_loss: 0.4813 - val_acc: 0.8838\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9594\n",
      "Epoch 00222: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1278 - acc: 0.9594 - val_loss: 0.4649 - val_acc: 0.8933\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9615\n",
      "Epoch 00223: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1269 - acc: 0.9614 - val_loss: 0.4807 - val_acc: 0.8873\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9606\n",
      "Epoch 00224: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1274 - acc: 0.9606 - val_loss: 0.5029 - val_acc: 0.8819\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9574\n",
      "Epoch 00225: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1304 - acc: 0.9574 - val_loss: 0.4945 - val_acc: 0.8831\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9596\n",
      "Epoch 00226: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1282 - acc: 0.9596 - val_loss: 0.5227 - val_acc: 0.8742\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9592\n",
      "Epoch 00227: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1270 - acc: 0.9592 - val_loss: 0.5781 - val_acc: 0.8668\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9597\n",
      "Epoch 00228: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1296 - acc: 0.9598 - val_loss: 0.4732 - val_acc: 0.8880\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9596\n",
      "Epoch 00229: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1272 - acc: 0.9596 - val_loss: 0.5242 - val_acc: 0.8793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9599\n",
      "Epoch 00230: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1245 - acc: 0.9599 - val_loss: 0.4805 - val_acc: 0.8842\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9602\n",
      "Epoch 00231: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1278 - acc: 0.9602 - val_loss: 0.4896 - val_acc: 0.8833\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9609\n",
      "Epoch 00232: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1253 - acc: 0.9610 - val_loss: 0.5524 - val_acc: 0.8705\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9589\n",
      "Epoch 00233: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1305 - acc: 0.9589 - val_loss: 0.4581 - val_acc: 0.8905\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9594\n",
      "Epoch 00234: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1237 - acc: 0.9595 - val_loss: 0.4920 - val_acc: 0.8845\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9617\n",
      "Epoch 00235: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1225 - acc: 0.9617 - val_loss: 0.4882 - val_acc: 0.8810\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9606\n",
      "Epoch 00236: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1237 - acc: 0.9606 - val_loss: 0.5227 - val_acc: 0.8812\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9610\n",
      "Epoch 00237: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1237 - acc: 0.9610 - val_loss: 0.5272 - val_acc: 0.8754\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9614\n",
      "Epoch 00238: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1244 - acc: 0.9615 - val_loss: 0.4613 - val_acc: 0.8924\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9628\n",
      "Epoch 00239: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1180 - acc: 0.9629 - val_loss: 0.5356 - val_acc: 0.8779\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9605\n",
      "Epoch 00240: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1223 - acc: 0.9605 - val_loss: 0.5313 - val_acc: 0.8724\n",
      "Epoch 241/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9627\n",
      "Epoch 00241: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1202 - acc: 0.9628 - val_loss: 0.5155 - val_acc: 0.8761\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9609\n",
      "Epoch 00242: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1223 - acc: 0.9608 - val_loss: 0.4732 - val_acc: 0.8889\n",
      "Epoch 243/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9626\n",
      "Epoch 00243: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1203 - acc: 0.9627 - val_loss: 0.5402 - val_acc: 0.8724\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9631\n",
      "Epoch 00244: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1209 - acc: 0.9632 - val_loss: 0.4952 - val_acc: 0.8856\n",
      "Epoch 245/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9628\n",
      "Epoch 00245: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1169 - acc: 0.9628 - val_loss: 0.5326 - val_acc: 0.8761\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9622\n",
      "Epoch 00246: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1167 - acc: 0.9622 - val_loss: 0.5107 - val_acc: 0.8789\n",
      "Epoch 247/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9621\n",
      "Epoch 00247: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1200 - acc: 0.9620 - val_loss: 0.4777 - val_acc: 0.8835\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9633\n",
      "Epoch 00248: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1178 - acc: 0.9633 - val_loss: 0.4686 - val_acc: 0.8917\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9649\n",
      "Epoch 00249: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1128 - acc: 0.9649 - val_loss: 0.4692 - val_acc: 0.8901\n",
      "Epoch 250/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9616\n",
      "Epoch 00250: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1215 - acc: 0.9616 - val_loss: 0.4791 - val_acc: 0.8831\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9642\n",
      "Epoch 00251: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1165 - acc: 0.9643 - val_loss: 0.4624 - val_acc: 0.8903\n",
      "Epoch 252/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9628\n",
      "Epoch 00252: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1160 - acc: 0.9629 - val_loss: 0.4770 - val_acc: 0.8903\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9631\n",
      "Epoch 00253: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1143 - acc: 0.9631 - val_loss: 0.4815 - val_acc: 0.8877\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9620\n",
      "Epoch 00254: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1186 - acc: 0.9620 - val_loss: 0.4939 - val_acc: 0.8852\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9622\n",
      "Epoch 00255: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1187 - acc: 0.9622 - val_loss: 0.4782 - val_acc: 0.8847\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9634\n",
      "Epoch 00256: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1154 - acc: 0.9634 - val_loss: 0.4779 - val_acc: 0.8863\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9642\n",
      "Epoch 00257: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1136 - acc: 0.9642 - val_loss: 0.4886 - val_acc: 0.8828\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9639\n",
      "Epoch 00258: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1161 - acc: 0.9639 - val_loss: 0.5168 - val_acc: 0.8842\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9637\n",
      "Epoch 00259: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1144 - acc: 0.9636 - val_loss: 0.4823 - val_acc: 0.8854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9630\n",
      "Epoch 00260: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1190 - acc: 0.9630 - val_loss: 0.5334 - val_acc: 0.8812\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9647\n",
      "Epoch 00261: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1122 - acc: 0.9647 - val_loss: 0.5126 - val_acc: 0.8784\n",
      "Epoch 262/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9625\n",
      "Epoch 00262: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1171 - acc: 0.9625 - val_loss: 0.5021 - val_acc: 0.8835\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9646\n",
      "Epoch 00263: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1126 - acc: 0.9646 - val_loss: 0.5173 - val_acc: 0.8821\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9639\n",
      "Epoch 00264: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1123 - acc: 0.9639 - val_loss: 0.5168 - val_acc: 0.8782\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9662\n",
      "Epoch 00265: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1103 - acc: 0.9662 - val_loss: 0.5515 - val_acc: 0.8765\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9643\n",
      "Epoch 00266: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1122 - acc: 0.9644 - val_loss: 0.4997 - val_acc: 0.8800\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9642\n",
      "Epoch 00267: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1110 - acc: 0.9642 - val_loss: 0.4923 - val_acc: 0.8845\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9647\n",
      "Epoch 00268: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1100 - acc: 0.9647 - val_loss: 0.5187 - val_acc: 0.8826\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9671\n",
      "Epoch 00269: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1074 - acc: 0.9672 - val_loss: 0.5206 - val_acc: 0.8847\n",
      "Epoch 270/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9629\n",
      "Epoch 00270: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1143 - acc: 0.9628 - val_loss: 0.4835 - val_acc: 0.8912\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9656\n",
      "Epoch 00271: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1093 - acc: 0.9655 - val_loss: 0.5638 - val_acc: 0.8658\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9620\n",
      "Epoch 00272: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1177 - acc: 0.9620 - val_loss: 0.5317 - val_acc: 0.8712\n",
      "Epoch 273/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9655\n",
      "Epoch 00273: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1138 - acc: 0.9654 - val_loss: 0.5146 - val_acc: 0.8826\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9666\n",
      "Epoch 00274: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1049 - acc: 0.9666 - val_loss: 0.4989 - val_acc: 0.8828\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9665\n",
      "Epoch 00275: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1058 - acc: 0.9665 - val_loss: 0.4726 - val_acc: 0.8915\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9659\n",
      "Epoch 00276: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1082 - acc: 0.9659 - val_loss: 0.4776 - val_acc: 0.8880\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9661\n",
      "Epoch 00277: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1083 - acc: 0.9661 - val_loss: 0.5031 - val_acc: 0.8842\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9667\n",
      "Epoch 00278: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1079 - acc: 0.9667 - val_loss: 0.4934 - val_acc: 0.8896\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9645\n",
      "Epoch 00279: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1112 - acc: 0.9645 - val_loss: 0.5003 - val_acc: 0.8866\n",
      "Epoch 280/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9656\n",
      "Epoch 00280: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1087 - acc: 0.9656 - val_loss: 0.5038 - val_acc: 0.8863\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9657\n",
      "Epoch 00281: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1065 - acc: 0.9656 - val_loss: 0.4991 - val_acc: 0.8859\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9650\n",
      "Epoch 00282: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1076 - acc: 0.9650 - val_loss: 0.4948 - val_acc: 0.8896\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9672\n",
      "Epoch 00283: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1030 - acc: 0.9672 - val_loss: 0.5248 - val_acc: 0.8796\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9675\n",
      "Epoch 00284: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1051 - acc: 0.9675 - val_loss: 0.6143 - val_acc: 0.8612\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9660\n",
      "Epoch 00285: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1067 - acc: 0.9660 - val_loss: 0.5444 - val_acc: 0.8793\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9662\n",
      "Epoch 00286: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1079 - acc: 0.9661 - val_loss: 0.5080 - val_acc: 0.8849\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9674\n",
      "Epoch 00287: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1035 - acc: 0.9675 - val_loss: 0.5380 - val_acc: 0.8772\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9678\n",
      "Epoch 00288: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1061 - acc: 0.9678 - val_loss: 0.5070 - val_acc: 0.8828\n",
      "Epoch 289/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9675\n",
      "Epoch 00289: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1069 - acc: 0.9675 - val_loss: 0.5441 - val_acc: 0.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9674\n",
      "Epoch 00290: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1071 - acc: 0.9674 - val_loss: 0.4617 - val_acc: 0.8931\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9666\n",
      "Epoch 00291: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1026 - acc: 0.9665 - val_loss: 0.5005 - val_acc: 0.8933\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9660\n",
      "Epoch 00292: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1046 - acc: 0.9660 - val_loss: 0.5394 - val_acc: 0.8796\n",
      "Epoch 293/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9679\n",
      "Epoch 00293: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1023 - acc: 0.9679 - val_loss: 0.5237 - val_acc: 0.8875\n",
      "Epoch 294/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9677\n",
      "Epoch 00294: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1014 - acc: 0.9677 - val_loss: 0.4805 - val_acc: 0.8919\n",
      "Epoch 295/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9647\n",
      "Epoch 00295: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1104 - acc: 0.9648 - val_loss: 0.4804 - val_acc: 0.8894\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9684\n",
      "Epoch 00296: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1025 - acc: 0.9683 - val_loss: 0.5238 - val_acc: 0.8887\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9679\n",
      "Epoch 00297: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1013 - acc: 0.9679 - val_loss: 0.4759 - val_acc: 0.8940\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9692\n",
      "Epoch 00298: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1021 - acc: 0.9692 - val_loss: 0.4989 - val_acc: 0.8919\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9679\n",
      "Epoch 00299: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1018 - acc: 0.9678 - val_loss: 0.5029 - val_acc: 0.8910\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 364us/sample - loss: 0.5562 - acc: 0.8538\n",
      "Loss: 0.5561911058079293 Accuracy: 0.8537902\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5863 - acc: 0.2200\n",
      "Epoch 00001: val_loss improved from inf to 1.79158, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/001-1.7916.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 2.5862 - acc: 0.2200 - val_loss: 1.7916 - val_acc: 0.4018\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6182 - acc: 0.4804\n",
      "Epoch 00002: val_loss improved from 1.79158 to 1.13488, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/002-1.1349.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 1.6169 - acc: 0.4807 - val_loss: 1.1349 - val_acc: 0.6560\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2519 - acc: 0.6040\n",
      "Epoch 00003: val_loss improved from 1.13488 to 0.91621, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/003-0.9162.hdf5\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 1.2517 - acc: 0.6041 - val_loss: 0.9162 - val_acc: 0.7251\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0557 - acc: 0.6717\n",
      "Epoch 00004: val_loss improved from 0.91621 to 0.78597, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/004-0.7860.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 1.0556 - acc: 0.6716 - val_loss: 0.7860 - val_acc: 0.7794\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7221\n",
      "Epoch 00005: val_loss improved from 0.78597 to 0.68841, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/005-0.6884.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.9045 - acc: 0.7221 - val_loss: 0.6884 - val_acc: 0.8069\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7575\n",
      "Epoch 00006: val_loss improved from 0.68841 to 0.62102, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/006-0.6210.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.7980 - acc: 0.7576 - val_loss: 0.6210 - val_acc: 0.8272\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.7816\n",
      "Epoch 00007: val_loss improved from 0.62102 to 0.59469, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/007-0.5947.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.7256 - acc: 0.7816 - val_loss: 0.5947 - val_acc: 0.8304\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.8012\n",
      "Epoch 00008: val_loss improved from 0.59469 to 0.57503, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/008-0.5750.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.6545 - acc: 0.8012 - val_loss: 0.5750 - val_acc: 0.8388\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.8176\n",
      "Epoch 00009: val_loss improved from 0.57503 to 0.51476, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/009-0.5148.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.6052 - acc: 0.8177 - val_loss: 0.5148 - val_acc: 0.8505\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8303\n",
      "Epoch 00010: val_loss improved from 0.51476 to 0.46321, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/010-0.4632.hdf5\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.5643 - acc: 0.8303 - val_loss: 0.4632 - val_acc: 0.8758\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8429\n",
      "Epoch 00011: val_loss improved from 0.46321 to 0.46015, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/011-0.4601.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.5260 - acc: 0.8428 - val_loss: 0.4601 - val_acc: 0.8670\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8518\n",
      "Epoch 00012: val_loss improved from 0.46015 to 0.44779, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/012-0.4478.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.4913 - acc: 0.8519 - val_loss: 0.4478 - val_acc: 0.8761\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.8584\n",
      "Epoch 00013: val_loss improved from 0.44779 to 0.39837, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/013-0.3984.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.4692 - acc: 0.8585 - val_loss: 0.3984 - val_acc: 0.8861\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8660\n",
      "Epoch 00014: val_loss improved from 0.39837 to 0.38309, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/014-0.3831.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.4454 - acc: 0.8659 - val_loss: 0.3831 - val_acc: 0.8898\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8739\n",
      "Epoch 00015: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.4164 - acc: 0.8739 - val_loss: 0.3940 - val_acc: 0.8896\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8798\n",
      "Epoch 00016: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.4013 - acc: 0.8798 - val_loss: 0.4072 - val_acc: 0.8873\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8832\n",
      "Epoch 00017: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.3867 - acc: 0.8831 - val_loss: 0.4218 - val_acc: 0.8747\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8886\n",
      "Epoch 00018: val_loss improved from 0.38309 to 0.33867, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/018-0.3387.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3666 - acc: 0.8884 - val_loss: 0.3387 - val_acc: 0.9012\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8919\n",
      "Epoch 00019: val_loss improved from 0.33867 to 0.31896, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/019-0.3190.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3581 - acc: 0.8918 - val_loss: 0.3190 - val_acc: 0.9173\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8943\n",
      "Epoch 00020: val_loss improved from 0.31896 to 0.30342, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/020-0.3034.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3483 - acc: 0.8943 - val_loss: 0.3034 - val_acc: 0.9192\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8985\n",
      "Epoch 00021: val_loss did not improve from 0.30342\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.3353 - acc: 0.8985 - val_loss: 0.3980 - val_acc: 0.8838\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9008\n",
      "Epoch 00022: val_loss improved from 0.30342 to 0.29567, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/022-0.2957.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3207 - acc: 0.9008 - val_loss: 0.2957 - val_acc: 0.9208\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9054\n",
      "Epoch 00023: val_loss did not improve from 0.29567\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3094 - acc: 0.9053 - val_loss: 0.3553 - val_acc: 0.8956\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9080\n",
      "Epoch 00024: val_loss improved from 0.29567 to 0.25957, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/024-0.2596.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3039 - acc: 0.9080 - val_loss: 0.2596 - val_acc: 0.9320\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9101\n",
      "Epoch 00025: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2945 - acc: 0.9101 - val_loss: 0.2924 - val_acc: 0.9175\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9146\n",
      "Epoch 00026: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2876 - acc: 0.9147 - val_loss: 0.2906 - val_acc: 0.9122\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9137\n",
      "Epoch 00027: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2801 - acc: 0.9137 - val_loss: 0.2939 - val_acc: 0.9150\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9170\n",
      "Epoch 00028: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.2757 - acc: 0.9169 - val_loss: 0.3345 - val_acc: 0.8996\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9156\n",
      "Epoch 00029: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2713 - acc: 0.9155 - val_loss: 0.2992 - val_acc: 0.9217\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9212\n",
      "Epoch 00030: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2576 - acc: 0.9211 - val_loss: 0.2675 - val_acc: 0.9224\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9204\n",
      "Epoch 00031: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2562 - acc: 0.9205 - val_loss: 0.2668 - val_acc: 0.9238\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9234\n",
      "Epoch 00032: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2491 - acc: 0.9234 - val_loss: 0.2906 - val_acc: 0.9145\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9252\n",
      "Epoch 00033: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2429 - acc: 0.9250 - val_loss: 0.2688 - val_acc: 0.9171\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9263\n",
      "Epoch 00034: val_loss improved from 0.25957 to 0.25347, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/034-0.2535.hdf5\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2393 - acc: 0.9263 - val_loss: 0.2535 - val_acc: 0.9278\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9274\n",
      "Epoch 00035: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2338 - acc: 0.9274 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9284\n",
      "Epoch 00036: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2284 - acc: 0.9284 - val_loss: 0.3014 - val_acc: 0.9182\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9301\n",
      "Epoch 00037: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2250 - acc: 0.9301 - val_loss: 0.2855 - val_acc: 0.9178\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9325\n",
      "Epoch 00038: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2194 - acc: 0.9325 - val_loss: 0.2663 - val_acc: 0.9248\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9326\n",
      "Epoch 00039: val_loss improved from 0.25347 to 0.24531, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/039-0.2453.hdf5\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2156 - acc: 0.9325 - val_loss: 0.2453 - val_acc: 0.9292\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9341\n",
      "Epoch 00040: val_loss did not improve from 0.24531\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2118 - acc: 0.9341 - val_loss: 0.2496 - val_acc: 0.9334\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9362\n",
      "Epoch 00041: val_loss did not improve from 0.24531\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2054 - acc: 0.9363 - val_loss: 0.2685 - val_acc: 0.9248\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9370\n",
      "Epoch 00042: val_loss improved from 0.24531 to 0.22884, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/042-0.2288.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.2026 - acc: 0.9370 - val_loss: 0.2288 - val_acc: 0.9357\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9369\n",
      "Epoch 00043: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2044 - acc: 0.9369 - val_loss: 0.2564 - val_acc: 0.9269\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9393\n",
      "Epoch 00044: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1977 - acc: 0.9392 - val_loss: 0.2469 - val_acc: 0.9317\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9378\n",
      "Epoch 00045: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1975 - acc: 0.9378 - val_loss: 0.2465 - val_acc: 0.9306\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9407\n",
      "Epoch 00046: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1885 - acc: 0.9409 - val_loss: 0.2413 - val_acc: 0.9315\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9419\n",
      "Epoch 00047: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1847 - acc: 0.9419 - val_loss: 0.2432 - val_acc: 0.9297\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9410\n",
      "Epoch 00048: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1881 - acc: 0.9409 - val_loss: 0.2898 - val_acc: 0.9126\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9435\n",
      "Epoch 00049: val_loss improved from 0.22884 to 0.22737, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/049-0.2274.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1805 - acc: 0.9435 - val_loss: 0.2274 - val_acc: 0.9355\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9431\n",
      "Epoch 00050: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1803 - acc: 0.9431 - val_loss: 0.2333 - val_acc: 0.9352\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9441\n",
      "Epoch 00051: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1748 - acc: 0.9441 - val_loss: 0.2578 - val_acc: 0.9269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9458\n",
      "Epoch 00052: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1721 - acc: 0.9458 - val_loss: 0.2914 - val_acc: 0.9185\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9456\n",
      "Epoch 00053: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1699 - acc: 0.9456 - val_loss: 0.2567 - val_acc: 0.9276\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9477\n",
      "Epoch 00054: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1687 - acc: 0.9477 - val_loss: 0.3040 - val_acc: 0.9087\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9473\n",
      "Epoch 00055: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1656 - acc: 0.9472 - val_loss: 0.2520 - val_acc: 0.9336\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9485\n",
      "Epoch 00056: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1635 - acc: 0.9485 - val_loss: 0.2368 - val_acc: 0.9366\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9490\n",
      "Epoch 00057: val_loss improved from 0.22737 to 0.22066, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/057-0.2207.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1614 - acc: 0.9490 - val_loss: 0.2207 - val_acc: 0.9345\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9497\n",
      "Epoch 00058: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1594 - acc: 0.9498 - val_loss: 0.2301 - val_acc: 0.9355\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9483\n",
      "Epoch 00059: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1617 - acc: 0.9484 - val_loss: 0.2373 - val_acc: 0.9266\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9513\n",
      "Epoch 00060: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1543 - acc: 0.9513 - val_loss: 0.2278 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9495\n",
      "Epoch 00061: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1587 - acc: 0.9495 - val_loss: 0.2391 - val_acc: 0.9311\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9510\n",
      "Epoch 00062: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1541 - acc: 0.9508 - val_loss: 0.2210 - val_acc: 0.9383\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9508\n",
      "Epoch 00063: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1540 - acc: 0.9508 - val_loss: 0.2267 - val_acc: 0.9376\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9529\n",
      "Epoch 00064: val_loss improved from 0.22066 to 0.21734, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/064-0.2173.hdf5\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1474 - acc: 0.9529 - val_loss: 0.2173 - val_acc: 0.9415\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9554\n",
      "Epoch 00065: val_loss did not improve from 0.21734\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1407 - acc: 0.9554 - val_loss: 0.2465 - val_acc: 0.9299\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9537\n",
      "Epoch 00066: val_loss improved from 0.21734 to 0.21076, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/066-0.2108.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1447 - acc: 0.9537 - val_loss: 0.2108 - val_acc: 0.9385\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9539\n",
      "Epoch 00067: val_loss did not improve from 0.21076\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1468 - acc: 0.9539 - val_loss: 0.2600 - val_acc: 0.9285\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9549\n",
      "Epoch 00068: val_loss improved from 0.21076 to 0.20823, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/068-0.2082.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1418 - acc: 0.9549 - val_loss: 0.2082 - val_acc: 0.9420\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9548\n",
      "Epoch 00069: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1409 - acc: 0.9549 - val_loss: 0.2138 - val_acc: 0.9413\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9574\n",
      "Epoch 00070: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1382 - acc: 0.9574 - val_loss: 0.2271 - val_acc: 0.9376\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9577\n",
      "Epoch 00071: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1323 - acc: 0.9577 - val_loss: 0.2271 - val_acc: 0.9317\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9587\n",
      "Epoch 00072: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1300 - acc: 0.9587 - val_loss: 0.2315 - val_acc: 0.9364\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9580\n",
      "Epoch 00073: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1323 - acc: 0.9580 - val_loss: 0.2651 - val_acc: 0.9283\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9564\n",
      "Epoch 00074: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1356 - acc: 0.9564 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9584\n",
      "Epoch 00075: val_loss improved from 0.20823 to 0.19534, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/075-0.1953.hdf5\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1282 - acc: 0.9583 - val_loss: 0.1953 - val_acc: 0.9481\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9575\n",
      "Epoch 00076: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1277 - acc: 0.9576 - val_loss: 0.2370 - val_acc: 0.9324\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9582\n",
      "Epoch 00077: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1283 - acc: 0.9582 - val_loss: 0.2389 - val_acc: 0.9329\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9597\n",
      "Epoch 00078: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1275 - acc: 0.9596 - val_loss: 0.2417 - val_acc: 0.9287\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9606\n",
      "Epoch 00079: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1231 - acc: 0.9606 - val_loss: 0.2260 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9612\n",
      "Epoch 00080: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1209 - acc: 0.9612 - val_loss: 0.2759 - val_acc: 0.9236\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9593\n",
      "Epoch 00081: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1260 - acc: 0.9594 - val_loss: 0.2080 - val_acc: 0.9385\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9626\n",
      "Epoch 00082: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1152 - acc: 0.9626 - val_loss: 0.2743 - val_acc: 0.9278\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9614\n",
      "Epoch 00083: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.1191 - acc: 0.9614 - val_loss: 0.2389 - val_acc: 0.9329\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9625\n",
      "Epoch 00084: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1190 - acc: 0.9625 - val_loss: 0.2359 - val_acc: 0.9392\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9629\n",
      "Epoch 00085: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1138 - acc: 0.9630 - val_loss: 0.2048 - val_acc: 0.9411\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9641\n",
      "Epoch 00086: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1135 - acc: 0.9641 - val_loss: 0.2205 - val_acc: 0.9378\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9641\n",
      "Epoch 00087: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1127 - acc: 0.9642 - val_loss: 0.2300 - val_acc: 0.9373\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9648\n",
      "Epoch 00088: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1096 - acc: 0.9648 - val_loss: 0.2554 - val_acc: 0.9327\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9634\n",
      "Epoch 00089: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1121 - acc: 0.9634 - val_loss: 0.2199 - val_acc: 0.9411\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9636\n",
      "Epoch 00090: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.1121 - acc: 0.9636 - val_loss: 0.2439 - val_acc: 0.9294\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9653\n",
      "Epoch 00091: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1098 - acc: 0.9653 - val_loss: 0.2354 - val_acc: 0.9371\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9653\n",
      "Epoch 00092: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1062 - acc: 0.9654 - val_loss: 0.3351 - val_acc: 0.9140\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9662\n",
      "Epoch 00093: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1044 - acc: 0.9661 - val_loss: 0.2232 - val_acc: 0.9390\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9667\n",
      "Epoch 00094: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.1049 - acc: 0.9667 - val_loss: 0.2085 - val_acc: 0.9434\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9669\n",
      "Epoch 00095: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1036 - acc: 0.9670 - val_loss: 0.2330 - val_acc: 0.9343\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9665\n",
      "Epoch 00096: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1061 - acc: 0.9665 - val_loss: 0.2231 - val_acc: 0.9429\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9673\n",
      "Epoch 00097: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1006 - acc: 0.9673 - val_loss: 0.2236 - val_acc: 0.9408\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9671\n",
      "Epoch 00098: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0991 - acc: 0.9671 - val_loss: 0.2363 - val_acc: 0.9378\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9666\n",
      "Epoch 00099: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1013 - acc: 0.9666 - val_loss: 0.2674 - val_acc: 0.9222\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9676\n",
      "Epoch 00100: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0991 - acc: 0.9677 - val_loss: 0.2373 - val_acc: 0.9369\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9688\n",
      "Epoch 00101: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0988 - acc: 0.9688 - val_loss: 0.2295 - val_acc: 0.9399\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9661\n",
      "Epoch 00102: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1011 - acc: 0.9661 - val_loss: 0.2334 - val_acc: 0.9355\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9698\n",
      "Epoch 00103: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0940 - acc: 0.9697 - val_loss: 0.2361 - val_acc: 0.9364\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9689\n",
      "Epoch 00104: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0984 - acc: 0.9689 - val_loss: 0.2167 - val_acc: 0.9404\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9658\n",
      "Epoch 00105: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1019 - acc: 0.9658 - val_loss: 0.2202 - val_acc: 0.9392\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9694\n",
      "Epoch 00106: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0940 - acc: 0.9694 - val_loss: 0.2279 - val_acc: 0.9317\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9702\n",
      "Epoch 00107: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0918 - acc: 0.9701 - val_loss: 0.2817 - val_acc: 0.9245\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9707\n",
      "Epoch 00108: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0925 - acc: 0.9706 - val_loss: 0.2283 - val_acc: 0.9387\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9688\n",
      "Epoch 00109: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0939 - acc: 0.9688 - val_loss: 0.2738 - val_acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9690\n",
      "Epoch 00110: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0924 - acc: 0.9691 - val_loss: 0.2368 - val_acc: 0.9362\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9720\n",
      "Epoch 00111: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0889 - acc: 0.9720 - val_loss: 0.2305 - val_acc: 0.9439\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9704\n",
      "Epoch 00112: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0906 - acc: 0.9703 - val_loss: 0.2018 - val_acc: 0.9448\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9707\n",
      "Epoch 00113: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0883 - acc: 0.9706 - val_loss: 0.2361 - val_acc: 0.9371\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9719\n",
      "Epoch 00114: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0886 - acc: 0.9719 - val_loss: 0.2334 - val_acc: 0.9366\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9731\n",
      "Epoch 00115: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0855 - acc: 0.9731 - val_loss: 0.2062 - val_acc: 0.9446\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9716\n",
      "Epoch 00116: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0884 - acc: 0.9716 - val_loss: 0.2879 - val_acc: 0.9252\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9726\n",
      "Epoch 00117: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0851 - acc: 0.9726 - val_loss: 0.2173 - val_acc: 0.9408\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9729\n",
      "Epoch 00118: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0845 - acc: 0.9729 - val_loss: 0.2283 - val_acc: 0.9397\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9734\n",
      "Epoch 00119: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0835 - acc: 0.9733 - val_loss: 0.2237 - val_acc: 0.9392\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9750\n",
      "Epoch 00120: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0804 - acc: 0.9749 - val_loss: 0.3242 - val_acc: 0.9201\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9715\n",
      "Epoch 00121: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0869 - acc: 0.9715 - val_loss: 0.2725 - val_acc: 0.9257\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9741\n",
      "Epoch 00122: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0825 - acc: 0.9739 - val_loss: 0.2279 - val_acc: 0.9434\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9713\n",
      "Epoch 00123: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0870 - acc: 0.9714 - val_loss: 0.2119 - val_acc: 0.9453\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9743\n",
      "Epoch 00124: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0776 - acc: 0.9742 - val_loss: 0.2132 - val_acc: 0.9446\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9753\n",
      "Epoch 00125: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0785 - acc: 0.9753 - val_loss: 0.2479 - val_acc: 0.9322\n",
      "Epoch 126/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9741\n",
      "Epoch 00126: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0805 - acc: 0.9741 - val_loss: 0.2724 - val_acc: 0.9348\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9758\n",
      "Epoch 00127: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0792 - acc: 0.9758 - val_loss: 0.2326 - val_acc: 0.9399\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9750\n",
      "Epoch 00128: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0769 - acc: 0.9749 - val_loss: 0.2245 - val_acc: 0.9413\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9757\n",
      "Epoch 00129: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0763 - acc: 0.9757 - val_loss: 0.2396 - val_acc: 0.9385\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9753\n",
      "Epoch 00130: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0769 - acc: 0.9753 - val_loss: 0.2176 - val_acc: 0.9436\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9757\n",
      "Epoch 00131: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0760 - acc: 0.9757 - val_loss: 0.2245 - val_acc: 0.9427\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9770\n",
      "Epoch 00132: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0715 - acc: 0.9770 - val_loss: 0.2276 - val_acc: 0.9434\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9741\n",
      "Epoch 00133: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0773 - acc: 0.9741 - val_loss: 0.2445 - val_acc: 0.9385\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9767\n",
      "Epoch 00134: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0731 - acc: 0.9767 - val_loss: 0.2386 - val_acc: 0.9380\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9756\n",
      "Epoch 00135: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0754 - acc: 0.9756 - val_loss: 0.2258 - val_acc: 0.9399\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9769\n",
      "Epoch 00136: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0707 - acc: 0.9769 - val_loss: 0.2170 - val_acc: 0.9427\n",
      "Epoch 137/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9761\n",
      "Epoch 00137: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0743 - acc: 0.9761 - val_loss: 0.2484 - val_acc: 0.9376\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9765\n",
      "Epoch 00138: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0706 - acc: 0.9765 - val_loss: 0.2476 - val_acc: 0.9390\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9748\n",
      "Epoch 00139: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0759 - acc: 0.9748 - val_loss: 0.2262 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9787\n",
      "Epoch 00140: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0678 - acc: 0.9786 - val_loss: 0.2229 - val_acc: 0.9394\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9773\n",
      "Epoch 00141: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0729 - acc: 0.9773 - val_loss: 0.2063 - val_acc: 0.9462\n",
      "Epoch 142/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9788\n",
      "Epoch 00142: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0664 - acc: 0.9788 - val_loss: 0.4502 - val_acc: 0.8910\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9775\n",
      "Epoch 00143: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0713 - acc: 0.9774 - val_loss: 0.2536 - val_acc: 0.9397\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9759\n",
      "Epoch 00144: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.0784 - acc: 0.9758 - val_loss: 0.2116 - val_acc: 0.9443\n",
      "Epoch 145/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9777\n",
      "Epoch 00145: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0669 - acc: 0.9777 - val_loss: 0.2195 - val_acc: 0.9436\n",
      "Epoch 146/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9792\n",
      "Epoch 00146: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0647 - acc: 0.9791 - val_loss: 0.2549 - val_acc: 0.9343\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9775\n",
      "Epoch 00147: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0678 - acc: 0.9775 - val_loss: 0.2496 - val_acc: 0.9373\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9785\n",
      "Epoch 00148: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0649 - acc: 0.9784 - val_loss: 0.2467 - val_acc: 0.9345\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9791\n",
      "Epoch 00149: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0666 - acc: 0.9792 - val_loss: 0.2234 - val_acc: 0.9406\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9785\n",
      "Epoch 00150: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0644 - acc: 0.9785 - val_loss: 0.2296 - val_acc: 0.9397\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9774\n",
      "Epoch 00151: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0706 - acc: 0.9774 - val_loss: 0.2576 - val_acc: 0.9394\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9801\n",
      "Epoch 00152: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0617 - acc: 0.9801 - val_loss: 0.2563 - val_acc: 0.9366\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9793\n",
      "Epoch 00153: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0633 - acc: 0.9793 - val_loss: 0.2392 - val_acc: 0.9408\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9787\n",
      "Epoch 00154: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0661 - acc: 0.9787 - val_loss: 0.2262 - val_acc: 0.9436\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9822\n",
      "Epoch 00155: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0580 - acc: 0.9823 - val_loss: 0.2940 - val_acc: 0.9250\n",
      "Epoch 156/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9800\n",
      "Epoch 00156: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0607 - acc: 0.9800 - val_loss: 0.2480 - val_acc: 0.9422\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9786\n",
      "Epoch 00157: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0664 - acc: 0.9785 - val_loss: 0.3569 - val_acc: 0.9201\n",
      "Epoch 158/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9788\n",
      "Epoch 00158: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0666 - acc: 0.9789 - val_loss: 0.5028 - val_acc: 0.8749\n",
      "Epoch 159/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9809\n",
      "Epoch 00159: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0595 - acc: 0.9809 - val_loss: 0.2311 - val_acc: 0.9404\n",
      "Epoch 160/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9814\n",
      "Epoch 00160: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0602 - acc: 0.9814 - val_loss: 0.2297 - val_acc: 0.9397\n",
      "Epoch 161/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9801\n",
      "Epoch 00161: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0599 - acc: 0.9802 - val_loss: 0.2218 - val_acc: 0.9411\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9809\n",
      "Epoch 00162: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0598 - acc: 0.9809 - val_loss: 0.2581 - val_acc: 0.9366\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9800\n",
      "Epoch 00163: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0611 - acc: 0.9800 - val_loss: 0.2868 - val_acc: 0.9276\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9794\n",
      "Epoch 00164: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0631 - acc: 0.9794 - val_loss: 0.3102 - val_acc: 0.9224\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9812\n",
      "Epoch 00165: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0573 - acc: 0.9811 - val_loss: 0.2406 - val_acc: 0.9415\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9800\n",
      "Epoch 00166: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0601 - acc: 0.9800 - val_loss: 0.2319 - val_acc: 0.9401\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9816\n",
      "Epoch 00167: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0593 - acc: 0.9816 - val_loss: 0.2696 - val_acc: 0.9338\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9804\n",
      "Epoch 00168: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0610 - acc: 0.9803 - val_loss: 0.2772 - val_acc: 0.9334\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9806\n",
      "Epoch 00169: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0601 - acc: 0.9806 - val_loss: 0.2357 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9816\n",
      "Epoch 00170: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0579 - acc: 0.9816 - val_loss: 0.2316 - val_acc: 0.9383\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9808\n",
      "Epoch 00171: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0568 - acc: 0.9808 - val_loss: 0.2190 - val_acc: 0.9439\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9820\n",
      "Epoch 00172: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0567 - acc: 0.9819 - val_loss: 0.2512 - val_acc: 0.9441\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9815\n",
      "Epoch 00173: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0556 - acc: 0.9815 - val_loss: 0.2284 - val_acc: 0.9411\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9821\n",
      "Epoch 00174: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0558 - acc: 0.9821 - val_loss: 0.3552 - val_acc: 0.9133\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9821\n",
      "Epoch 00175: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0561 - acc: 0.9821 - val_loss: 0.2391 - val_acc: 0.9406\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VFX++PH3mclMZtILECAEAkgnEHoUxV6woKKILlhX/blf1rLuoq6uu+o2e0FdXXR1sWJBRbHgqiCoiBSp0nuA9N6mnt8fJ5MCSQiQSSDzeT3PPDNz55Zz79x7Pvecc++5SmuNEEIIAWBp6wQIIYQ4dkhQEEIIUUOCghBCiBoSFIQQQtSQoCCEEKKGBAUhhBA1JCgIIYSoIUFBCCFEDQkKQgghaoS1dQIOV4cOHXRqampbJ0MIIY4rK1asyNNadzzUeMddUEhNTWX58uVtnQwhhDiuKKV2NWc8qT4SQghRQ4KCEEKIGhIUhBBC1Dju2hQa4vF4yMzMpKqqqq2TctxyOBx069YNm83W1kkRQrShdhEUMjMziY6OJjU1FaVUWyfnuKO1Jj8/n8zMTHr27NnWyRFCtKF2UX1UVVVFYmKiBIQjpJQiMTFRSlpCiPYRFAAJCEdJtp8QAtpRUDgUn68Sl2svfr+nrZMihBDHrJAJCn5/JW73frRu+aBQVFTEv/71ryOa9vzzz6eoqKjZ4z/wwAM8/vjjR7QsIYQ4lJAJCrWrqlt8zk0FBa/X2+S0n332GXFxcS2eJiGEOBIhExQCdeZat3xQuOeee9i2bRvp6elMnz6dhQsXcsoppzBhwgQGDhwIwCWXXMKIESMYNGgQM2fOrJk2NTWVvLw8du7cyYABA7jpppsYNGgQ55xzDpWVlU0ud9WqVWRkZDBkyBAuvfRSCgsLAZgxYwYDBw5kyJAhXHnllQB8++23pKenk56ezrBhwygtLW3x7SCEOP4F7ZJUpVQK8BqQhDk9n6m1fuaAcU4D5gI7qgd9oLV+6GiWu2XLHZSVrTpouNY+/P4KLJYIlLIe1jyjotLp0+fpRn9/+OGHWbduHatWmeUuXLiQlStXsm7duppLPF955RUSEhKorKxk1KhRXHbZZSQmJh6Q9i28/fbbvPTSS1xxxRXMmTOHqVOnNrrca665hmeffZZTTz2VP//5zzz44IM8/fTTPPzww+zYsYPw8PCaqqnHH3+c559/nrFjx1JWVobD4TisbSCECA3BLCl4gd9rrQcCGcA0pdTABsZbrLVOr34dVUA4lowePbreNf8zZsxg6NChZGRksGfPHrZs2XLQND179iQ9PR2AESNGsHPnzkbnX1xcTFFREaeeeioA1157LYsWLQJgyJAhTJkyhTfeeIOwMBP3x44dy5133smMGTMoKiqqGS6EEHUFLWfQWu8H9ld/LlVKbQCSgV+CtUyg0TN6n6+ciooNOJ19CAuLDWYSAIiMjKz5vHDhQr766iuWLFlCREQEp512WoP3BISHh9d8tlqth6w+asynn37KokWL+OSTT/j73//O2rVrueeee7jgggv47LPPGDt2LPPnz6d///5HNH8hRPvVKm0KSqlUYBiwtIGfT1RKrVZKfa6UGhTEVACgtb/F5xwdHd1kHX1xcTHx8fFERESwceNGfvzxx6NeZmxsLPHx8SxevBiA119/nVNPPRW/38+ePXs4/fTTeeSRRyguLqasrIxt27aRlpbG3XffzahRo9i4ceNRp0EI0f4EvQ5BKRUFzAHu0FqXHPDzSqCH1rpMKXU+8BHQp4F53AzcDNC9e/cjTUn1e8s3NCcmJjJ27FgGDx7M+PHjueCCC+r9ft555/Hiiy8yYMAA+vXrR0ZGRossd9asWdxyyy1UVFTQq1cvXn31VXw+H1OnTqW4uBitNbfddhtxcXHcf//9LFiwAIvFwqBBgxg/fnyLpEEI0b6oYFyNUzNzpWzAPGC+1vrJZoy/Exiptc5rbJyRI0fqAx+ys2HDBgYMGNDkvH2+Kioq1uFw9MRmS2xy3FDVnO0ohDg+KaVWaK1HHmq8oFUfKXMN6H+ADY0FBKVU5+rxUEqNrk5PfpDSAwTnklQhhGgvgll9NBa4GlirlApcI3ov0B1Aa/0icDnwG6WUF6gErtRBy7UD1Uct36YghBDtRTCvPvqO2py4sXGeA54LVhrqC94dzUII0V7IHc1CCCFqhExQCObVR0II0V5IUBBCCFEjZIKCqT5SHCtBISoq6rCGCyFEawiZoGCooNzRLIQQ7UXIBYVglBTuuecenn/++ZrvgQfhlJWVceaZZzJ8+HDS0tKYO3dus+eptWb69OkMHjyYtLQ03nnnHQD279/PuHHjSE9PZ/DgwSxevBifz8d1111XM+5TTz3V4usohAgN7a+rzDvugFUHd50NEOErQ6kwsBxmt9Hp6fB0411nT548mTvuuINp06YB8O677zJ//nwcDgcffvghMTEx5OXlkZGRwYQJE5r1POQPPviAVatWsXr1avLy8hg1ahTjxo3jrbfe4txzz+W+++7D5/NRUVHBqlWr2Lt3L+vWrQM4rCe5CSFEXe0vKDRJBaVFYdiwYeTk5LBv3z5yc3OJj48nJSUFj8fDvffey6JFi7BYLOzdu5fs7Gw6d+58yHl+9913XHXVVVitVpKSkjj11FNZtmwZo0aN4oYbbsDj8XDJJZeQnp5Or1692L59O7feeisXXHAB55xzThDWUggRCtpfUGjijL6ybC1WayROZ68WX+ykSZN4//33ycrKYvLkyQC8+eab5ObmsmLFCmw2G6mpqQ12mX04xo0bx6JFi/j000+57rrruPPOO7nmmmtYvXo18+fP58UXX+Tdd9/llVdeaYnVEkKEGGlTaCGTJ09m9uzZvP/++0yaNAkwXWZ36tQJm83GggUL2LVrV7Pnd8opp/DOO+/g8/nIzc1l0aJFjB49ml27dpGUlMRNN93EjTfeyMqVK8nLy8Pv93PZZZfxt7/9jZUrVwZlHYUQ7V/7Kyk0wdTlBycoDBo0iNLSUpKTk+nSpQsAU6ZM4aKLLiItLY2RI0ce1kNtLr30UpYsWcLQoUNRSvHoo4/SuXNnZs2axWOPPYbNZiMqKorXXnuNvXv3cv311+P3myur/vnPfwZlHYUQ7V9Qu84OhiPtOhugvPwXlLIREXHQIxsE0nW2EO1Zm3edfWw6dm5eE0KIY1FIBQWlLEhQEEKIxoVUUJA7moUQomkhFxSkpCCEEI0LqaAQzKuPhBCiPQipoGCqjyQoCCFEY0IsKASnobmoqIh//etfRzTt+eefL30VCSGOGSEWFBTQ8g3NTQUFr9fb5LSfffYZcXFxLZ4mIYQ4EiEVFILVpnDPPfewbds20tPTmT59OgsXLuSUU05hwoQJDBw4EIBLLrmEESNGMGjQIGbOnFkzbWpqKnl5eezcuZMBAwZw0003MWjQIM455xwqKysPWtYnn3zCmDFjGDZsGGeddRbZ2dkAlJWVcf3115OWlsaQIUOYM2cOAF988QXDhw9n6NChnHnmmS2+7kKI9qXddXPRRM/Z+P1JaJ2A1Xp48zxEz9k8/PDDrFu3jlXVC164cCErV65k3bp19OzZE4BXXnmFhIQEKisrGTVqFJdddhmJiYn15rNlyxbefvttXnrpJa644grmzJnD1KlT641z8skn8+OPP6KU4uWXX+bRRx/liSee4K9//SuxsbGsXbsWgMLCQnJzc7nppptYtGgRPXv2pKCg4PBWXAgRctpdUDhWjB49uiYgAMyYMYMPP/wQgD179rBly5aDgkLPnj1JT08HYMSIEezcufOg+WZmZjJ58mT279+P2+2uWcZXX33F7Nmza8aLj4/nk08+Ydy4cTXjJCQktOg6CiHan3YXFJo6o3e58nG79xMdfcjuP45aZGRkzeeFCxfy1VdfsWTJEiIiIjjttNMa7EI7PDy85rPVam2w+ujWW2/lzjvvZMKECSxcuJAHHnggKOkXQoSmkGpTMA3NtPhlqdHR0ZSWljb6e3FxMfHx8URERLBx40Z+/PHHI15WcXExycnJAMyaNatm+Nlnn13vkaCFhYVkZGSwaNEiduzYASDVR0KIQwrJoNDSjc2JiYmMHTuWwYMHM3369IN+P++88/B6vQwYMIB77rmHjIyMI17WAw88wKRJkxgxYgQdOnSoGf6nP/2JwsJCBg8ezNChQ1mwYAEdO3Zk5syZTJw4kaFDh9Y8/EcIIRoTUl1nu91ZuFyZREUNQ6nDbG0OAdJ1thDtl3Sd3aDgVB8JIUR7EWJBIbC60lOqEEI0JMSCQnDaFIQQor0IWlBQSqUopRYopX5RSq1XSt3ewDhKKTVDKbVVKbVGKTU8WOmpXh4g1UdCCNGYYN6n4AV+r7VeqZSKBlYopf6ntf6lzjjjgT7VrzHAC9XvQSIlBSGEaErQSgpa6/1a65XVn0uBDUDyAaNdDLymjR+BOKVUl2ClSYKCEEI0rVXaFJRSqcAwYOkBPyUDe+p8z+TgwNGC6Th2GpqjoqLaOglCCHGQoAcFpVQUMAe4Q2tdcoTzuFkptVwptTw3N/doUgNIm4IQQjQmqEFBKWXDBIQ3tdYfNDDKXiClzvdu1cPq0VrP1FqP1FqP7Nix49GkKDDHo5jHwe655556XUw88MADPP7445SVlXHmmWcyfPhw0tLSmDt37iHn1VgX2w11gd1Yd9lCCHGkgtbQrMylPv8BNmitn2xktI+B3yqlZmMamIu11vuPZrl3fHEHq7Ia7jtbax9+fwUWixOlmr/q6Z3Tefq8xnvamzx5MnfccQfTpk0D4N1332X+/Pk4HA4+/PBDYmJiyMvLIyMjgwkTJtRcBdWQhrrY9vv9DXaB3VB32UIIcTSCefXRWOBqYK1SKpBL3wt0B9Bavwh8BpwPbAUqgOuDmJ6gGTZsGDk5Oezbt4/c3Fzi4+NJSUnB4/Fw7733smjRIiwWC3v37iU7O5vOnTs3Oq+GutjOzc1tsAvshrrLFkKIoxG0oKC1/o7a+prGxtHAtJZcblNn9D5fJRUV63E4emGzteyzBSZNmsT7779PVlZWTcdzb775Jrm5uaxYsQKbzUZqamqDXWYHNLeLbSGECBa5o7mFTJ48mdmzZ/P+++8zadIkwHRz3alTJ2w2GwsWLGDXrl1NzqOxLrYb6wK7oe6yhRDiaIRUUAjmHc2DBg2itLSU5ORkunQxt1pMmTKF5cuXk5aWxmuvvUb//v2bnEdjXWw31gV2Q91lCyHE0QiprrP9fjfl5WsID++B3X40VzG1T9J1thDtl3Sd3SC5o1kIIZoSUkGh9lLQtr+jWQghjkXtJig0rxrMchjjhhbZJkIIaCdBweFwkJ+f34yMTaqPGqK1Jj8/H4fD0dZJEUK0sWDevNZqunXrRmZmJs3pF6mqKo+wMA9hYcWtkLLjh8PhoFu3bm2dDCFEG2sXQcFms9Xc7XsoixYNJzn5Nnr3fiTIqRJCiONPu6g+OhxK2dHa3dbJEEKIY1K7KCk0y+bN8Omn2PuH4fdLUBBCiIaETklhzRq4807C861SUhBCiEaETlBwOgGwuqWkIIQQjQnBoCAlBSGEaEzoBIXqa/CtbquUFIQQohGhExQCJQWPRUoKQgjRiJALChaXlBSEEKIxoRMUaqqPlJQUhBCiEaETFAIlBbdFSgpCCNGIkAsKpqTgaePECCHEsSnkgoLFhZQUhBCiEaETFMLCwGKRNgUhhGhC6AQFpcDpxOKWkoIQQjQmdIICmKDg0lJSEEKIRoRWUHA4sLg0fr+rrVMihBDHpNAKCk4nFhf4fOVtnRIhhDgmhV5Q8Cj8/nL8fm9bp0YIIY45IRcUrNU1Rz6fPKNZCCEOFFpBobpNAcDrLWrjxAghxLEntIKC04ly+QHweqWkIIQQBwpaUFBKvaKUylFKrWvk99OUUsVKqVXVrz8HKy01nE4sLh8gJQUhhGhIWBDn/V/gOeC1JsZZrLW+MIhpqM/hQFWZfo8kKAghxMGCVlLQWi8CCoI1/yPidNYJClJ9JIQQB2rrNoUTlVKrlVKfK6UGBX1pTidUmcuPpKQghBAHC2b10aGsBHporcuUUucDHwF9GhpRKXUzcDNA9+7dj3yJNUFBSVAQQogGtFlJQWtdorUuq/78GWBTSnVoZNyZWuuRWuuRHTt2PPKFOhyoykqsligJCkII0YA2CwpKqc5KKVX9eXR1WvKDulCnE7TGpuOkTUEIIRoQtOojpdTbwGlAB6VUJvAXwAagtX4RuBz4jVLKC1QCV2qtdbDSA9Q8aMfui5GSghBCNCBoQUFrfdUhfn8Oc8lq63E4ALD5pPpICCEa0tZXH7Wu6pKCzRslfR8JIUQDQjQoREhJQQghGtCsoKCUul0pFaOM/yilViqlzgl24lpcTVBwSlAQQogGNLekcIPWugQ4B4gHrgYeDlqqgqW6TSHM68TrLUFrfxsnSAghji3NDQqq+v184HWt9fo6w44f1SWFME844MfnK2vb9AghxDGmuUFhhVLqS0xQmK+UigaOv9Ps6qBgddsB6epCCCEO1NxLUn8NpAPbtdYVSqkE4PrgJStIAtVHHhsgneIJIcSBmltSOBHYpLUuUkpNBf4EHH85aqCkUBMUpKQghBB1NTcovABUKKWGAr8HttH0cxKOTTXVR2a1JSgIIUR9zQ0K3uouKC4GntNaPw9EBy9ZQSJBQQghmtTcNoVSpdQfMZeinqKUslDdj9FxpbpNweI2F05Jm4IQQtTX3JLCZMCFuV8hC+gGPBa0VAVLdVCwus1XKSkIIUR9zQoK1YHgTSBWKXUhUKW1Pv7aFJSqfk6zG4tF7moWQogDNbebiyuAn4BJwBXAUqXU5cFMWNA4nVBZSVhYnHSKJ4QQB2hum8J9wCitdQ6AUqoj8BXwfrASFjQOR3VQiMfjCe4zfYQQ4njT3DYFSyAgVMs/jGmPLU4nVFURHp6My5XZ1qkRQohjSnNLCl8opeYDb1d/nwx8FpwkBVl19VF4eArl5WvbOjVCCHFMaVZQ0FpPV0pdBoytHjRTa/1h8JIVRNXVR+Hhabjd2fj9biwWe1unSgghjgnNfhyn1noOMCeIaWkd1SUFh6M7oHG59uJ09mzrVAkhxDGhyaCglCoFdEM/AVprHROUVAWT0wmlpYSHpwDgcu2RoCCEENWaDApa6+OvK4tDcTohJ6cmKFRV7W7jBAkhxLHj+LyC6GhUtyk4HLUlBSGEEEboBYXqNgWrNZKwsAQJCkIIUUfoBYWICCgvByA8PEWCghBC1BF6QaFTJygoAI8HhyNF2hSEEKKO0AsKycnmPStLSgpCCHGA0AsKXbua9717CQ/vjtdbiM9X3rZpEkKIY0ToBoV9+2quQKqqktKCEEJAiAeF2hvYpF1BCCEgFINCx44QFlYdFLoDcgObEEIEBC0oKKVeUUrlKKXWNfK7UkrNUEptVUqtUUoND1Za6rFYoEuX6qDQDaXCqKra1iqLFkKIY10wSwr/Bc5r4vfxQJ/q183AC0FMS31du8LevVgsYTgcvamo2NxqixZCiGNZ0IKC1noRUNDEKBcDr2njRyBOKdUlWOmpp2tX2LcPgIiIvlRWSlAQQgho2zaFZKDuZT+Z1cOCr05QcDr7Ulm5Fa39rbJoIYQ4ljX7eQptSSl1M6aKie7dux/9DLt2haIiqKggIqIvfn8VLldm9TMWhBChqKoKrFZzHYpS5vuWLWZYx46QkGA+A/h8kJ9veszxeiEy0nSr5nKB1qbfTacTbDaT1ZSXm3G0hsJCMzwqynz3+838Gnod+FtSEqSkBHc7tGVQ2AvUXb1u1cMOorWeCcwEGDlyZEPPdzg8gbua9+/HmdAXgIqKzRIURKvR2mQmPl9txhB4Bb5rbX4PZDzJyWbY5s3gdptMpnt3k7ns3An795vhLpcZPy4OYmNrM6IdO8x1FnFx5veqKqisNPMvKYHwcDO+xwMVFeZVWVn72eMxGVKXLpCVZdKWnAxlZbB7t5mHywXR0WaZ+flmXZ1Ok8kG1s/trp2v223ml5AAe/eaYdHRZnhpqck8HQ7zCg83rz17TGYdGFZUZOYbH29eMTFm2vx806NN4Deovy4REebldJpxCwvNOBaLmXdVlZk2wGIxafP7zTbzt0Hlwt13w8MPB3cZbRkUPgZ+q5SaDYwBirXW+1tlyXXuVYjoZoKCaVc4q1UWLw6P3w/FxeZgjo83B7HPZzI2j8e8V1RAZqY5sANnV4F3i8VMZ7FATo7JcKA2oyovr32VlZnMICbGZAw7dphlB87qAplKebl5d7kOXl5j7x6PWXbgpY/+9AYwGafH0zLzOpDdXptxVl/JXbNNLRaz7aE2Q7bZzDZUymT0SpntpFTtNDZb7Tyjosw2XrHCBJjISJNB2+2QmFgbvEpKzHtVlQkiF11U+1tcnJlvYaF5lZSYtPTubeahlBlusZhlRkSYdamqqv0/4+Nrg25gOZGR0L+/Wb/cXPMqKjLTRkWZs/aoKFN6CMzH4TDjB+bhdpv0RUbWbpf4eLMNS0vNd6vVvCyW2s91X3WHn3BCcP7nuoIWFJRSbwOnAR2UUpnAXwAbgNb6ReAz4HxgK1ABXB+stBykTlcXdvvJWCyRcgXSIfj9tQe3w2Ey16wsc2AGznhLSmrPzHw+M05ZmTnAAxl33ReYM9OiIpOhR0aazLi4uPYALyyEomINWjWcsLBKsPjA4wRtPcRaaBj6OhSnwM7TGxwjcGZaVmYyiI4dzUFcVmYOysDZZUSEOWvs0KH+gdvUu91evS3sudjtFmJtiYSFQbnKItwSidMSXZNx1s1EIyPN+549ZtsOGGCWX1oKu3aZTLRPH3MWHzibDgsz27WoyEwbGwuWxO3sr9hNUYmH9I5j6BgTg9MJTqdmv2cTHcO74auMwm6nenhtdUmA223+44jYCtbkrGbrvlzCHZqkuGhO6X4KNqvt4H1H+7Eo03yZXZZNVlkWEbYI/NqPT/voHd+b8LDwetO4vC4KKgtIikqqmdbn97GlYAux4bF0jurMqqxV5FXkcXbvs2umK64qZl3OOpRSJEUm0Tuh9yH2iYP5tZ/dxbtJcCYQE24eLun2ubFbj/xZ7l6/lzDLwdltlbeKXUW76JPYB4BNeZsorDJFlm4x3egW061m/fMr8klwJmAefBk8QQsKWuurDvG7BqYFa/lNqlNSUEodM1cgrctZR+/43jhtzhaZn99vMrPiYvMqKjLveYUuNhasY3vJJuLLT0QXpbDW/x4lOpNuxVdSURDLHvca7HaFXTnZm1NJYYGC8o5Q2MtkvrG7YdS/wOo2mey6K6Hs4IvHlALdfRH0+h/WhN2E6WjCffFYHVVYqzrBV1NxJO2CtLexlHbHX9AdV6/P8MZsxm4NR4dnYrHsYJBtAmOjrmZ56SeUe0oZaf01uy0L+c73BD48WLDSNbwP/ePSGdnhdLzaxa6KDQxNOJHTOl9KVUkkM7ffw7v7HgXgqhP+jxEdTsZqsTKk82Cy3dt4bcOLDOo0gIdOf4g3V89m7saPuWbYrxibMpZFuxaxYOcCvt/zPTtK91HhqaBnXE+SY5KxW+2kxqbSJ7EPn235jHU567hl5C1kdMtg7sa5nNXrLC4beBmfbv6UaZ9NY1fxLsIsYVzY90L2l+5n6d6lAMQ54kiJTGFcj3H8/Yy/k1Oew0OLHmJ9znoKqwpJ65VGt5hubPWUsy1rG5vyNxFmDaNzr87Yel3AyIGXMzRpKP/b/j8e/f5Rbh5xM5efdTmv/vwqf1v2HKu+X1Xzv4Rbwzk19VRiwmNYnbWaLQVb6B3fmzlXzOE/y//Df37+D2GWMJxhTiLtkVwz5BruP/V+7HYLWaxi4syJ7CjaUe+/7pPQhzsy7qDCU8HanLUs2bOEfaX7KPeUkxSZhCPMwa7iXQftI3arnQEdBuC0Oan0VLK3dC95FXkAxDviGdRpEOXucrYWbKXUXQpAhC2CCo85s1h9y2oGdRzERW9fxOdbP68377EpY8nolsHOop1E2iNJdCayJHMJ63PWE2WPIs4RR6wjltjwWJw2JzsKd7ApfxNV3ioSnYnMuWIOi3cv5qFvH+IfZ/6DP5z0B3NsaT8nv3Iym/I3YbPYKHOXodEMSRrC6amnc9Pwm+gZ3xOPz8Ntn9/Gv1f8m/CwcE5IOIFzep1DalwqBZUFvLjiRbLKsohzxBFmCatZ74CUmBQWXb+ISFsko14axdVDruavZ/y1+ZnAEVC6pcqwrWTkyJF6+fLlRzcTrc3p1//9Hzz+OOvXT6a0dAUZGVtbJpENKHWVsjl/M8WuYk7ufjJ2q529JXvZVbyLk1JO4uvtX3PW62fRJ6EPL174Iqenno5Sqia5OTlQ6stj1tqXWLe9gNyiMjyWMjr40kjJu4HcfZFsqfqeHckPUx73E7ij0XtHwOqroagnOAqh7zxI/RY6rgertyZtqqITOiKnettUn4WohveLZDWcs/xPMM96A0X+PdgtDir9ZViwMKbDuZzf81KW5nzD2oJlvDD+ZTyWEi5951IAukZ3pdxdTmFVIY4wB1XeKhQKjcZuteP2mXqd2PBYhncZjsfvISkyiaTIJF5f8zql7lKi7FE4whw1B8/VQ65mSNIQCioL2JC3gaWZS9lfZmohAxmHRVmICY+hqKqIW0bcgt1qZ8ZPMw5at85RnckqyyI2PJZiV3HNe0BMeAyndD+FXvG9CLeGs71oOznlObi8rpr/tmdcT/p16McXW78wmxGFUop7T76Xx5c8Tp+EPlw79FqyyrJ4bc1rdIrsxJS0KSgUe0r2sLNoJ59v/ZwuUV3Ir8zHbrUzNmUsMeExrMleQ055DpH2SHrE9mBAhwFoNFsKtrBo1yL82k/X6K7sK92HM8xJpbeSHrE92FW8i+FdhjM1bSrpndPx+r3M2zyPxbsX4/K56BLVhXN6n8NjPzxWs12nDplKojORSk8lO4t38uW2L7m438XEO+OZvW42ic5Enjr3KVLjUrFarGwr2MZfFv6FDXkbAOgU2YmTUk6id3xvImwRZJVlUeIqYVTXUaTGpVLhqcBqsaK1ZnX2ajbkbcDldREeFk5ydDLJ0cnEOeJYk72GzQWbiQmPISV/q9PJAAAgAElEQVQmhdHJoymuKmZT/iaGJg3l9i9u59fDfs1Zvc5i4rsTmTZqGuNPGI/NamNt9lpeXPEie4r31CwzqyyL4V2GM6rrKCq9lRS7iimuKqbYVUyFp4IesT3o36E/veN788zSZ9iUvwmAXvG92F64nYfPfJi7T76bxbsWM+6/47i438V0iuxEtD0an/axcv9KftjzA37tZ1iXYViUheX7lnN9+vUkOhNZlb2KRbsW1ezrZ/c6m4kDJrJi3wq82su47uNIjknG5/exs2gn935zLz1iexATHsOyfctYfP1iRnYdeThZT+1xrtQKrfUhJw7NoACmcm7kSJg9mx077mfXrn8wblwlFsuRFxEb89HGj/j1x7+moNLctnHb6Nt44twnGDFzBGuz1zLj1Ld45Ke/UOFyoX1WCtV2rK4OUJyCjs5EZZ6E7+sHYOIU6PQLuCPAEwleB8TuAb8FLKbVK9zdhR6Vl2INryAz/EtK1b6adNhUOMMSxjG00whGJg9jUNfefLP7c1bsX8516dcxJGkIb6x5A601o5JHYVVWKr2VNUX9bQXbuH/B/RRWFRITHsNXV3/FqORRbM7fzKxVs3htzWtklmQS54gjzhFHZkkmYZYw0jql8fU1XxMdbh75rbVGKcXm/M28seYNkiKTuGboNZS5y9hZtJMRXUccVFTPr8hn6d6lnNrjVKwWKx9t/IiecT0Z021MvfG01mwt2IrT5iQ5Opnv93zPl9u+JL8in/4d+vPb0b9FKcWe4j2Ue8qp8laxJnsNEbYILu53Md/u+pY/ffMnpg6Zyi0jb2HuxrnsLNrJqamnMqzzMKyWhquo/NrP/tL9dInugkVZWJ21msySTDK6ZXDJO5fw3e7v6JPQh+9v+J6OkR2b3F9+zPyRGz++kb6JfXnu/OfoGt31kPtYbnkuczfNZd7meQzrPIzfn/R7HvnuEd795V3+dIpZn8BJRmMC/+8Nw27grF617Wtaa55Y8gR3/e8u4p3xnHfCeTx5zpMkRSXVm97r97KtYBtJUUnEhscecnkt4dqPruXDDR/SN7EvBZUFbL5180HVNIH97cDPh1JUVcTv5/+eMd3GcMOwG7j6w6uZvW42K29eycsrX+bVVa+SMz2HKHtUvekySzKZtWoW3+z8xgTLU//C9cNqa8ddXhclrhIsykJiRGKTafhi6xdc8NYF+LWftya+xVVpTVbANEmCwqFccIGpyF69mqys19m48RpGjVpPZOTAo593HQ8ufJAHvn2AIR1GMKnzvXy2fS5Lyl+ne+F17E54FYp7QGx1kfr1+bD7ZGLHvkPUoG+xxmRjc3diV9R7eFUldiK4zv4pvzrpNAYONI2Le92/8PH2t3GEOeiT2IcJ/SbgCDOtXT6/jyWZSyisLMRmtXFy95MP2oEP186inTz47YP8ZuRvGJ08ut5vPr+PNdlr6NehHx6fh6kfTmVn0U6+ueabQ2aE7Vmpq5QnljzBdenXkRqX2tbJOWKlrlIi7ZE1ddzHgh/2/MDYV8YC8Nz455g2Ong10sVVxaQ+k8op3U+pOUF5d9K7QVtewNtr36bcU86Nw288qvlIUDiUu++Gp5+GsjLK3ZtZtmww/fvPonPna5o1eaWnkrfWvsU7699hXc46ssqysCgLcY44Em3dGGO/nrz9kXxuu4mwddfi/XAm+OwQXgLTBkLMXjoWn8fl1tf5OO5MhiSO4sXzX6ZrV9NIWNemvE08tOghbhlxC6f0OOXo170VHc6ZmRCHS2vN0BeHsq90H7t/t5sIW0RQlxc4yQOYc8UcJg6YGNTltSQJCofy+utwzTXwyy/o/n357rs4kpKupW/f5xoc/YGFDzBv8zymDpnK/tL9vPzzyxRUFtC/Q39Gd8mgKrcb69Zptu0rwBW3Grr/AEBM3hlM4QsG9rPRpw/07Qvr3Z9z99d/YO6Vczkh4QT82l9T9yyEODyb8zdT4akgvXN60JdVVFVE6tOp+LSPnD/ktNhFIa2huUHhuLijOSgGDzbv69ahBgwgOnokpaXLGhz1uZ+e48FvHyQlJoXfzf8dFmXhwt6Xku6+lU1fjuOjzxQlJeaa6KsugjNO05R3+YKV5R/zyDn/IN5Z/zK9noznwn7ja74fS8VxIY43fRP7ttqy4hxxvDzhZcrcZcdVQDgcoRsU+vc3F3CvWweTJhEdPZrMzKfx+11YLOaa6byKPB7+7mGeXPIkF/e7mDlXzOGjxVt4e1YU8/7RjY9d5jr2SZPg0kvhnHPMde7mOuLx1S8hRHty+cDL2zoJQRW6QcHpNFcgrTOPe4iJGY3P72bFrrlkVtn4aNNHvP/L+1R5q7gu/TruGvwc11xt5a23+hMVBddfD1OmwIknHnyDjxBCHK9CNyiAqUKqDgraPoDpa2DlosmAuSZ9StoU/t/Q3/H+CwNIn2xuxLr3XtNGHRPTlgkXQojgkKDw0Ufk5u/m3PensLYY/jBkBJNG/4u0Tmls3ehk8nmwYQNMnQp//7vpgEwIIdqrkA8KbuXnsrcuYWPJRp49cQzDYwoZnTyaDz4w1UNxcfDVV3DmmW2dWCGECL7Qvuxl8GDuOA8WF/zMKxNe4cL+V1BZuZknn8zj8sshPR1WrZKAIIQIHSFdUlgRXcoLo+AP5elclXYVlZXbeOaZDTzxRAcuvhjeesv0RimEEKEipEsKL6ycSYTPwp8W+AD48svePPnkvxk79kfefVcCghAi9IRsUCisLOSttW8xxZJO7Mr15O8s5cYbYdCg/fzpT2cBeYechxBCtDchGxRmrZ5FpbeS/xt6I/j9TL+lhKIimDmzBIejnPz8eW2dRCGEaHUhGxReWvkSGd0ySD9zCt+rk3l1fjJ/+ANkZPQnPDyFvLyP2jqJQgjR6kIyKGzI3cAvub8wNW0qxMTwSPRf6WAr4v77QSlFhw4XU1j4JT5fRVsnVQghWlVIBoU5G+YAcOmAS9m8GeaVjOM36t9EhJsG5w4dLsHvr6Sw8H9tmUwhhGh1IRsUTko5ia7RXXnmGbCFaf7P/RRUd8kdGzuOsLA4qUISQoSckAsK2wu3syprFRP7T6SkBP77X/jVJC+dbQUwx5QgLBYbiYkXkpf3CX6/t+kZCiFEOxJyQeGDDR8AMHHARD7+GCoq4KbfhsNZZ8F770H1Q4c6dLgErzef4uLv2jK5QgjRqkIuKKzNWUtKTAo943vy3nuQnAwZGZiHIuzcCStWABAffy5WazT7989s0/QKIURrCrmgkF2WTeeozpSUwPz5cPnl5lk7XHyxeTjyu+ZB3GFhUXTt+v/IyXmXqqpdbZtoIYRoJSEXFHLKc0iKSuKTT8DlMgUEABIS4Oyz4Y03IM/czZycfBtKKTIzn267BAshRCsKuaCQXZ5Np4hONVVHJ55Y58cHHoCCAvNsTZcLhyOFTp2uZN++l/B48tsqyUII0WpCKihorckpz6FTZBKLF8P48dVVRwGjR8OsWfDdd3DVVVBVRUrK3fj9lezc+dc2S7cQQrSWkAoKhVWFeP1eHL5OFBSY5yUcZPJkeOYZ+PBDOOssorzd6NLlRvbte56Kik2tnmYhhGhNIRUUcspzACjPSQJg6NBGRrztNnN56pIl8Oij9Oz5EBaLk23bprdSSoUQom2EVFDILssGoHCPCQppaU2MfPnlcO658Prr2K0d6NHjPvLzP6Gw8OtWSKkQQrSN0AoK5SYoZG7qRGoqxMYeYoJrr4XMTFiwgOTk23E4Utm69U609gU9rUII0RaCGhSUUucppTYppbYqpe5p4PfrlFK5SqlV1a8bg5meQPXR9rVJDBnSjAkuvthEjlmzsFod9Or1KOXla9i//5VgJlMIIdpM0IKCUsoKPA+MBwYCVymlBjYw6jta6/Tq18vBSg+Y6iOLsrB1bULj7Ql1ORym4XnOHCgpoWPHy4mNPYXt2+/G7c4OZlKFEKJNBLOkMBrYqrXerrV2A7OBi4O4vEPKKc8h3t4Rv9favJICwK9/DZWVMGsWSin69p2Jz1fOli23BTWtQohj1GuvwXPPtXUqgiaYQSEZ2FPne2b1sANdppRao5R6XymV0tCMlFI3K6WWK6WW5+bmHnGCssuzcfo7AU1ceXSg0aNhzBh49lnw+4mM7E9q6p/JzX2XrKzXjjgtQojj1IwZ8PzzbZ2KoGnrhuZPgFSt9RDgf8CshkbSWs/UWo/UWo/s2LHjES8suzwbS2USTif06nUYE95+O2zZAp9/DkBKyl3Exp7Kxo03kJPz3hGnRwhxnNEaNm2Cozg5PdYFMyjsBeqe+XerHlZDa52vtXZVf30ZGBHE9JBTnoMqTyIlBazWw5jw8suha1eYPh0eeADL8p9JS5tHTEwGGzb8iry8uUFLsxDiGLJ/P5SVme5wfO3zKsRgBoVlQB+lVE+llB24Evi47ghKqS51vk4ANgQxPWSXZeMv7USnToc5oc0GTz0F5eXw0EMwZgxhV13PkOQ3iIoawfr1k8jP/zQoaRZCHML8+TB7dussa1N1rwZaQ3777A8taEFBa+0FfgvMx2T272qt1yulHlJKTage7Tal1Hql1GrgNuC6YKWn3F1Ouaccd2HS4QcFgCuugF27oLjYdJw3dy5hDz3GkCFfEBk5hPXrJ1FWtq6lky2EOJTHHoM//al1lrVxY+3ndlqFFNQ2Ba31Z1rrvlrr3lrrv1cP+7PW+uPqz3/UWg/SWg/VWp+utd7Y9ByPXOAehYrcIygp1BUdDX/5C1x9Nbz6KrYSH2lp8wgLi2X9+svwekuqF5gDH31U8yQ3IUSQZGebm0xb41jbVKf/s5yc4C+vDbR1Q3OrCQSFsqwjLCkc6M47zaWqL7xAeHhnBg58h8rKbaxZdQ6u5/8G/fubLrh/+KEFFiaEaFRWlnk4SmucuW/aBE6n+SwlheNboIsLXXaUJYWAQYPg/PPNpar79xMXN440/TC9b1hO+G/vx3VCHNpigS+/PHjaggJT5G2nDVVCtBqPp7Zuf/fu4C9v48bq5/ciQeF4lxSZxEXdr4bilJYJCgD33w8lJTB4MFx0EYlnTCdmXwx7/prOkkd2UDEgAv+XDTRAv/oq3HWXeW6DEOLI5ebWVhvt2dP0uEerstK0K44dW7vsdihkgsKYbmO4o8drUN5C1UdgzhhWrYK+fWHxYrjvPtSmLXS7byUDBr5B/nAXatkKyvctqz/d4sXmfenSFkqIECEqK6v2c7CDwtatJgANGmQe39tOg0JYWyegNQXahY7i/reD9etnnrugNSgFgAKSkqZQcVU56vX/x45XTsIycTLJyb8hJioDFSgh/PhjbcKcTtOILYRovuw6fZAFOygEGpn79TOZSGsFheJiU9WckNAqiwuZkgLUBoUWKynUVR0Q6oo48zp0ZATdNgwgP/8Tfv75ZNbNGQD5+ejISBNMfD7TjcZvfhOERAnRzgVKCuHhwQ8K69eb47xv39YNCtdeay5aaSUhFxQsllYLuGC3o049jbgFeZyY/CN9+84k6ucyAPImxJsd+tVXYedO+Prrxi+p83jg0UdNfaY4/rjdpg2pblWHaBmBkkJ6evCDwrJlMGAAREa2blD48UdT1ezxtMriQi4odOxoAkOruf9+qKgg7OQz6Zo1gtTMM/B1iGbvGeZ+Bu9dt5rxsrJg+/aG5/HSS3D33TB1Kvj9rZRw0WK+/dZcbfbWW22dkiNXXAwXXVT/5q1jQVaWqXbt1y+4QUFrWL4cRo0y31srKOTkmMDncsGGoHb4UCPkgkJQqo6akpFhrjIKC4OMDNTcj7GOO5tBv9qMP9xKWGEVxemmI6ayL57H73fVn76kxNxB3amTmc9LLx3e8tvy5rmyMjk7BlNNCLBiRdum42h8/TXMmwf/+Edbp6S+7GxISoKUFNi3D7ze4CwnM9Msa+RI871jR3MpbFMnabt2wS231G/3OFxr19Z+/vnnI5/PYZCg0BoGD4aVK2HCBCgthTPPxBaRhGWkud45d3oGnigo+fwplnzXjf2vX0XFF6+gv/4abr3VnJHMmwdnnGGqITIzD16xhjL/rVuhSxfTXlFR0XLrU1bW+G+7d9emZepUGD4cqqpabtnHo8ANjMdzUAgEttmzj61An5UFnTuboODzmQ7rWtL8+WafXlZ9BWHdkoLPB4WFDU+3fz+ceSb8+9/w8lE8OywQFGw2k4e0Bq31cfUaMWKEPlK9e2t91VVHPPnR8/u1XrlSa4/HfH/xRa0nTtTa79f+88/Tnj4pOndKL61NtlrzqrzuAu1252m9davWkZFan3GG1j6fmcfbb2ttsZgVC8w3sKzzztM6PFxrpbTu31/rjRuPfh02btTa4dD6rbcO/u2LL0ya77tP69Wra9dh1qyjX+7xyufTOjZW67Aw8z8UFwd3eR9/fOT/8969WvfsqfWSJQf/Nnas1j16mHX485+PKoktasAArS+7TOtPPzX72vfft9y8c3O1tlq1PvNMrf/4R/MfVlaa39580yzvl18Onq6kROvBg82x2quX1sOHHzyO3691Xt6h03DDDVp37Kj1iSdqfcopR7U6wHLdjDy2zTP5w30dTVCIjtb69tuPePLg+sc/ajJR7w1TdO7s3+mt/x6pl7xj1Qu+QS9YgP7hhx5670OjzHi//a3WTz5pdtSePc2wyy7TuqzMzG/OHDPsqae0/vprs2PFx2u9YEH95ebmav3OO7VB5lCuvdbM9/TT6w8vKtK6WzeTaVitZieOitK6Tx9zUPj9R7uFjozPp/X69eYAbE4atm0z22n+fPP90UdN4G7u9jnQunVme11xhXlfuLB5061cqXV29uEtKzPTbPtx48z3BQu0PvdcrfPzmzf9iy+aNN58c/3hLpc5ubjzTq0vuEDrTp1qM8eGPPOM1n/96+Gl/UjFx2s9bZrWa9eatM+e3XLznjmz9sSmW7f6mfv//meGf/tt/Wn8fq0nTTInal9+afYf0HrHjtpxnn/eBDMwx2lTRo0yJ4HTppkM7Ej3Qy1B4SAVFWZt//73I5o8+BYtMgns21fr8vKawR5PkS4o+Frv2vWoXr/+Sr14UazOPl3V7KzuoT21O2+HCRBgMuFp07SOiNA6La229LBtm9kR7XatP/zQDFu3rjagBM7mP/jA7Kh1Sx0BO3aYTCcx0WT+u3eb4X6/1tdcYw6Ezz83mQZoPX261i+8YD5/913Lbq/bb9f69dcPPd6f/1x7YJ922qHHnzbNjDt5stZer9adO5vvzz/f9HRer9lugaAc8NJLZvrFi837E08cOg3LlplgP3BgvX3hkP7yl9p1Xb1a66FDzedbbmne9BMnmvGTksz6BPz4oxn+3ntaf/ON+TxjRsPzmDGjNg0//ND8tB+JqiqznIceMicloPVjj9UfZ+NGrT/6qPEA6/ebE6dp07R+8MH6451zjtapqWZ/PzBYrlplhr3/fv35Pf20Gf7ww+b71q31//dAMBkzxlRdnHCC1m73wemqrDT/gdNp9vWXXzbTbd58eNuoDgkKB9i1y6ztSy8d0eTB53Jp/etfmzPEJkfL0Rt+uV4vey9RL50Vphf+D71ggUWvXHmy3v/WDdrbraP2W61aX321Wem6Cgq0zsgwGfuIESZAJCVpPWiQ1snJpkoocEAnJ2t98slajx+v9f33a/2f/2h9/vla22zmbBe0fuQRM9+HHjLf77/ffP/kEzP/fftMJhkfb4LPoTIJn0/rrKyGf1uxQuubbjJF86VLzfK6dDHbrTErVph1vfji2hJOoGrF49F6zx4zrw8+0Pqnn8wZdUSEmSYysrZKolMnc5a2ZUvDpQ2/35TcwPyHWpsgP2OGyWgTE8043bpp/atfNb0NSkpMRtGhgwm8N9zQ9PgBbrfZHiedZKr3Bg0y6Rk82Mxn2bKmp/d4TDVXly4HB/GnnjLDMjPNeowbZ8arqKg/j48/Nsu66CKtu3Y1GV9gewW2d0N+/tnsO3PnmvWva84ck3n26WO2bVVV7W+7d5t0zZxpvnfoYJYdUFKidUpK7T6dlGTS/vPPteM89pj5LTbWpL1rVxPA8/LMfnDPPVr/7W8HZx5795phL7xQO2zbNrPtL7yw/n4ydKg57qqqTDVu794m0583r+ETjk8/Ncfm//2f+f3ll02+AKZUf4QkKBxg2TKztnPnHtHkxyS/36eLi5fq7dvv18uWDdMLFqC/nY/+7iNT1fTLL1frvXtf0sXFS7XHU2QmKi3VeupUc9b8+9+bA+u772oPnFGjzIE4caKpIhoyxJQAAr/fd5+ZT0aGybxuvNEMv+aaxou2339v6qMtFnPwu1zmjOnaa02GmZZmDrwhQ8yB+Y9/mINq1y5zYOfm1h7cd9xhiudWq/n+5pu1yykqMuuyeLEp+QwYYDKvggKTIYGp1ti2rbY0U/c1bJh5D5S6evY0QWLNGnOwg8l4AlVLWpuDO3CG3qePef/nP2vHB5NJaG2CU79+5rPXq/W995p1DVTvrFlj6o2VMoH3vvvM9OPGmbPYm282/1v37iYI1T3DfOMNM+4nn2h9/fXmc69eZt6dO5t0X3+9qUrT2pz1d+1qqkQefND8R2CCv91u9o1t20w6Jkww2z9gwQIz7tNP1w4rKDDLSU832+SVV8w4d91lxh8+3Pz/gdKdx2Pm//TTZnmBbZWQoPWzz5p127bNBOOBA7W+9FLz+7nnmv1B64MP6sDJSaBN5Pbbzbb8739N5n/jjWZ/6NDBnAy8+qpJ0+WXm/1t1SqzT1ssZv8GrZcvN8fMPfeYdQxwuczvZ59tgv+mTeYEKirq4OD3yCNm3Ph48z5vnhnu95v/OzLS7CPPPWeCTVJS/W3y009meTabSccRkqBwgMBJX0NtaO2F212gi4q+13v2zNDr1l2uv/uuo16wgOqX0j/9NFRv2vQbnZn5L11YuEi73XXqmq++2pzJZmYePOOSEq23bzcHR0Cg/jkiwpzNNlQErqu4uPZsPTravMfFaT1lSm1m3LevOTigtsgeFWUyUrvdHHQWi3nddZcZf/Rokwn9+9+10wReMTGm8Ttg7FgTgK66yqT7+efN2e3y5VrfdlvtQe7x1M5r8mQz7erVJlgMGGDaZ/bu1frxx00GA1pfeaXZPoHquP79TYC6//7ane6hh0wmNWdO7VkgmHULnKnGx5tMTGuTjn/+s7b+uWNHsw4XXGC+n3GG1rfeakplgaDk9ZqzSoultkpw6VLTphEdbUqAa9aYzLdvXzM/MBmvxWKCyPjxJqOqezJwxRX1/88zzjCZ1OOPmyqX664zgXrFCvO712uCYGD6Dh3Mf6WUSX9MTO1v48eb/e7rr02jLpgA07ev2S47d5p5/uc/ZvrAWf/Uqebz0qXm99JSE+xPOcUEFqVMtVBdW7bUlobAnMXX3a+Li82Jh9Vqzuibaoc66aSDTyyefPLg8bxec/Jy1lmmtHtgen71K7OPg2m7sdvNdrzsMrP/B6ok331X6w0bGk/PIUhQOMBnn5n9vm57T3vn9/t1efkmnZv7kd6x4yH9889n6kWLYuoECvRPP6XpLVt+pzN3P6+zdr6i9+yZoXNz52qv9xB12T6fObNqqvqmIe+9Z87MXn+9tirA7zdBx+Mxn5991mSyzzxjSiARESZDKCoymYXNZjLlZ5+tzVTBZAZz55oz+fXr69eLa11b31u3xFPXypVa5+SYz4ES0IENgevXm1JAILCdd57JzAKZx6JFJtNrqKpk715zJh1Iw/TppoF0+nQTlP72t4avSPH7G26rsNtNpnHiieZMuG7VW0N16CtXmrTb7eb1yy/mfwychY8ZY8Z7/32TGf/+9+as9p//rC1hBOTlaX3JJfUzxLvuOniZW7aY/y4727SPXHKJKaHceKOpFlm8uH4J0+83B+tFF5k0HlhdsnSpCUTjxtUuNxA0tK7dJ8AEy4au9tqyxdT5f/114/vv1q1mnzwUr9eUaB9/XOvf/a7htrjm8PvNmeuYMeaES2uzXQKlohbQ3KCgzLjHj5EjR+rly5e3dTKOW1prXK5MysvXUVa2isLCrygu/g6t3fXGU8qG1RpNWFgsnTpNplOnXxER0R+tfbhce3A4emKxtFJ/ilrX9i21erW5c/XCC839EjffDF27wtlnwznnNNgHVY3MTHM9e2IibNsGsbGNj7t+PTzyCMycCQ5H/d9eeQX++EfT9cg11zS9zAN5PPD00ybtDzxweNMeyOUCu/3w5jFrFlx3nbkJ7Y9/NMPKyuCyy2DSJLjxxubPS2vzdMG9e829ApdcYm7SbCl+f+PdD/h85v/5/nuTBpvNDPd4zDqOGGG6vjia7dvOKKVWaK1HHnI8CQrCZPT78fnKsNkSKC9fS2HhV3i9pVRV7aSg4HPAj1I2zKO3NTZbBxISzsNqjcZm60hc3DgiI4dgs3VAHcsH4v33w9ChcPnlRzefuoHqeLNrF3TvfvymXxwRCQqixVRVZVJUtICKil+wWBzY7ckUFX1DUdG3aO3B48kHzO3+SoUTHt4Vu70rYWGx2GyJhId3x+FIITy8O3Z7Z+z2zthsHVHKgseTR1hYHBaLvW1XUoh2rrlBIaSepyCOjMPRjc6dr643rGvX2moGr7eE4uIfqKzcgsuVicuVidu9H7c7m/Lytbhc+4ADHz2qML2s+AgLiyMh4Tzc7lxcrj1ERg7G6TwBpaxERPQjIWE8dntb9E8iROiRoCCOWlhYDImJ5wHnNfi73+/F7c7C5dqN252F252N252N1h7s9iRKS1dSWDgfuz2ZyMhBlJWtJj9/HuCvrq5SxMSMITp6DFVV26is3IbbnYXdnkR09Bjs9iTCwmIID08hImIA0dHDUcp6QBpMR4MWS3hwN4YQxzkJCiLoLJYwHI5uOBzdDms6rTVlZT+Tnz+P/Px57Nv3Ak5nHyIi+hMXdxou1x4KC+fj9Rbh99d2ume1xhIZORi7vSNudy5VVTtxu/dhsZU36KcAAAynSURBVDhJTDyfiIhBKGWpDhzm3WZLJCpqGE5nH6zWSCoqNuF2ZxEXN+6gACNEeyZtCuK4obVutBHb56vC5dpDWdlKCgu/obJyK253NnZ7RxyOVByOVNzubPLyPsTtbrqXT9Ogbh5o4nSeQHz8WVRUbMLrLcFisREZmUZ09Ais1iiUsmOx2LHbOxMe3gOvtwiPJwfQWCwRhIenYLd3QqmQ6pBYHIOkoVmIRph93o/WfrT2AX5crn2Ulf1MVdUOPJ5cIiIGYLE4ycx8koqKTURGDiIsLAG/v4rS0hX4fMXNXp5SdsLDu1UHiCS09qGUBas1Bp+vBLc7h8jIQcTEZGC3JwEatzsLpcKx2ztjsYSjtQ+3ex9WaxRxcWdgsdiorNxOeflavN5CEhLOJzy8S7A2mWgHJCgIESRa+3G59uD3u/D73WjtwuXaR1XVLmy2BGy2TihlxecrxeXaQ1XVblyuPbhce3C7s6tLIj58vlKs1ihstgTKytbi95c3a/kWSySg8fvrPiPDQlTUEOz2ZCwWB4GgF3gPC4vG6eyHUla83qLqdHaomZ/T2QufrwKXK5Pw8K44HD2xWiOxWCKwWp01bTFudy6VlZuJiBiIUjbKy9fidPaqDmbiWCZXHwkRJEpZcDh61BsWHT3iqObp93uorNyK11uA1hq7vTNau3C7s/H73ShlwW7vgsu1l4KCT1EqjMjINCIj07BYnOTmvkdp6TLc7n0145v2EvNeUfELOTnvYKq1nPj9lYeVvrCweOz2zlRUbAQCJ5IK0Chlo0OHi7HZktDaXR0ozXtYWCx2excsFgdKhaGUtfo9DIejBxER/Sgt/ZnKyq1ERg4kLCwet3s/dntnIiOH4Hbvx+PJwensi9ZuSkqW4nD0IiZmDH5/BW53Lg5HD5RS+HzlFBTMp6RkCR06TCQ29sSj+k9ClZQUhAgR5gosCxaLDZ+vEo8nH6UseL0lVFVtr24DScbt3kdV1W78/gp8vkr8/orqks5eoqNHEhU1jPLy9Wj9/9u79xi5yjKO49/fzF67LNt2u6VcSi9QCSgKhSCRS0ggSglSVK4ioJIQEkgkxggERcJfolGiCeGiNBasQkDQxqByUUr4o0CpLS2X0qXWQCnt9rLb7u50LzOPf5x3h9npTrsue/ac6T6fZLKn756d/uadyzPn9r79NDWdTFfXy2zf/jiFwgCZTF3xOItUw+BgVziGM75zi9fVzaK/vwPI09Awl5qaVnp61oaz1SItLecwZcoJmOXJ5TaRzTZRXz+bfL6bQiEX7uNjOjtforFxAbNmXQdk6O//iL6+j6itbWXmzKuprW0ll9tIZ+fL9PSswyxPff3RzJixmIaGuSVbZHkKhT7M+igU+hgY2MngYBfNzQtpavocfX1byOe7Q1HuYXCwM4waMJ3a2ulks82xXvjpu4+cc6kQjamTDx/Y+fDh2U8u9x69ve/S1HQyU6acSG/vO+Tze6mrm0Vf3wf09Kynvv4Yamvb6O3dwNCpyd3da9m16+80Nh5PXd0sdu9+nnx+L4cffibTpl1Ac/NpbNnyAB0dT9Lf/xEgGhrmUyj0sG/fB9TUNJPJNNDfv41stpmpU89j795V9Pa+HRJnqKubycDAjmFFBjI0Ni4gk6knl2sv2303PjKZhnBrDLe6kmNfeY466iaOPfa2Md23FwXnnBslMyOXaw/HeNrIZGro79/Bjh1/Bgo0NMyhufmL1NZOBSCfz9HZ+RKDg7sp3U2XydQXbzU108hmm9izZyW9vRtpaJhNNttCoZAjm22ipqaFfL6bgYFdDAzsLG7BDN3y+Rxm/cNOnW5tvZiZM68Y02NMxTEFSRcCvwKywG/N7Kdlv68HHgVOA3YCV5rZ5jgzOedcOUlMmbJgWFtd3YxhV+6XymYbaW1dNKr7bmw87lPnm0ixnTytqLzdDywCTgKulnRS2Wo3ALvN7HjgPuDeuPI455w7uDivqDkDaDezTRaNy/w4sLhsncXA0rD8FHC+Uj3EpnPOHdriLApHAx+U/PvD0DbiOhYd0ekCWsvvSNKNklZJWtXR0RFTXOecc1Vx7b2ZPWxmp5vZ6W1tbUnHcc65Q1acRWELMLvk38eEthHXkVQDtBAdcHbOOZeAOIvC68ACSfMk1QFXAcvL1lkOXB+WLwP+adV2jqxzzh1CYjsl1cwGJd0C/IPolNQlZvaWpHuIJpBeDjwCPCapHdhFVDicc84lJNbrFMzsWeDZsra7Spb3AZfHmcE559zoVd0VzZI6gP+O8c9nADvGMU7cPG+8qilvNWUFzxu3seSdY2YHPVOn6orCpyFp1Wgu804LzxuvaspbTVnB88YtzrxVcUqqc865ieFFwTnnXNFkKwoPJx3g/+R541VNeaspK3jeuMWWd1IdU3DOOXdgk21LwTnn3AFMmqIg6UJJGyS1S7o96TzlJM2W9C9Jb0t6S9L3QvvdkrZIWhNuFyWdFUDSZknrQqZVoW26pOclbQw/pyWdE0DSCSX9t0bSHkm3pqlvJS2RtF3S+pK2EftTkV+H1/KbkhamJO/PJb0bMj0jaWponyspV9LPD6Yga8XnXtIdoW83SPrKRGY9QN4nSrJulrQmtI9/30ZT5R3aN6Irqt8H5gN1wFrgpKRzlWU8ElgYlpuB94jmobgb+EHS+UbIuxmYUdb2M+D2sHw7cG/SOSu8Fj4G5qSpb4FzgYXA+oP1J3AR8DdAwJnAqynJ+2WgJizfW5J3bul6Kck64nMf3nNrgXpgXvjcyCadt+z3vwDuiqtvJ8uWwmjmdkiUmW01s9VheS/wDvsPNZ52pfNjLAUuTTBLJecD75vZWC+AjIWZvUw01EupSv25GHjUIiuBqZKOnJikkZHymtlz9smkxiuJBsFMXIW+rWQx8LiZ9ZnZf4B2os+PCXOgvGG+mSuAP8b1/0+WojCauR1SQ9Jc4FTg1dB0S9gkX5KWXTKAAc9JekPSjaHtCDPbGpY/Bo5IJtoBXcXwN1Qa+3ZIpf6shtfzd4m2ZobMk/RvSSsknZNUqDIjPfdp79tzgG1mtrGkbVz7drIUhaoh6TDgT8CtZrYHeAA4DjgF2Eq06ZgGZ5vZQqLpVm+WdG7pLy3atk3VqW2KRuu9BHgyNKW1b/eTxv6sRNKdwCCwLDRtBY41s1OB7wN/kHR4UvmCqnnuy1zN8C814963k6UojGZuh8RJqiUqCMvM7GkAM9tmZnkzKwC/YYI3ZSsxsy3h53bgGaJc24Z2Y4Sf25NLOKJFwGoz2wbp7dsSlfozta9nSd8GLgauCYWMsCtmZ1h+g2g//WcSC8kBn/s0920N8HXgiaG2OPp2shSF0cztkKiwr/AR4B0z+2VJe+m+4q8B68v/dqJJapLUPLRMdIBxPcPnx7ge+EsyCSsa9i0rjX1bplJ/LgeuC2chnQl0lexmSoykC4EfApeYWW9Je5ukbFieDywANiWTspip0nO/HLhKUr2keURZX5vofBVcALxrZh8ONcTStxN5VD3JG9EZG+8RVdI7k84zQr6ziXYPvAmsCbeLgMeAdaF9OXBkCrLOJzpDYy3w1lB/Es2v/SKwEXgBmJ501pLMTUSz+rWUtKWmb4mK1VZggGg/9g2V+pPorKP7w2t5HXB6SvK2E+2PH3r9PhjW/UZ4nawBVgNfTUHWis89cGfo2w3AojT0bWj/HXBT2brj3rd+RbNzzrmiybL7yDnn3Ch4UXDOOVfkRcE551yRFwXnnHNFXhScc84VeVFwbgJJOk/SX5PO4VwlXhScc84VeVFwbgSSviXptTBG/UOSspK6Jd2naL6LFyW1hXVPkbSyZB6BoXkPjpf0gqS1klZLOi7c/WGSngpzDywLV7M7lwpeFJwrI+lE4ErgLDM7BcgD1xBdFb3KzD4LrAB+Ev7kUeA2M/s80VWyQ+3LgPvN7AvAl4iuUoVoBNxbicbunw+cFfuDcm6UapIO4FwKnQ+cBrwevsQ3Eg1GV+CTwch+DzwtqQWYamYrQvtS4MkwNtTRZvYMgJntAwj395qF8WvCDFpzgVfif1jOHZwXBef2J2Cpmd0xrFH6cdl6Yx0jpq9kOY+/D12K+O4j5/b3InCZpJlQnCt5DtH75bKwzjeBV8ysC9hdMrnJtcAKi2bP+1DSpeE+6iVNmdBH4dwY+DcU58qY2duSfkQ0s1yGaLTKm4Ee4Izwu+1Exx0gGtb6wfChvwn4Tmi/FnhI0j3hPi6fwIfh3Jj4KKnOjZKkbjM7LOkczsXJdx8555wr8i0F55xzRb6l4JxzrsiLgnPOuSIvCs4554q8KDjnnCvyouCcc67Ii4Jzzrmi/wGe6Meh+BGGEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 390us/sample - loss: 0.2656 - acc: 0.9238\n",
      "Loss: 0.26559395780196937 Accuracy: 0.92377985\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3234 - acc: 0.2957\n",
      "Epoch 00001: val_loss improved from inf to 1.55178, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/001-1.5518.hdf5\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 2.3233 - acc: 0.2957 - val_loss: 1.5518 - val_acc: 0.4992\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4253 - acc: 0.5412\n",
      "Epoch 00002: val_loss improved from 1.55178 to 0.99858, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/002-0.9986.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.4256 - acc: 0.5412 - val_loss: 0.9986 - val_acc: 0.6781\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6581\n",
      "Epoch 00003: val_loss improved from 0.99858 to 0.75564, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/003-0.7556.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0694 - acc: 0.6582 - val_loss: 0.7556 - val_acc: 0.7617\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7361\n",
      "Epoch 00004: val_loss improved from 0.75564 to 0.55334, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/004-0.5533.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.8372 - acc: 0.7361 - val_loss: 0.5533 - val_acc: 0.8318\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.7836\n",
      "Epoch 00005: val_loss improved from 0.55334 to 0.49779, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/005-0.4978.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.6859 - acc: 0.7835 - val_loss: 0.4978 - val_acc: 0.8544\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8190\n",
      "Epoch 00006: val_loss improved from 0.49779 to 0.41374, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/006-0.4137.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5797 - acc: 0.8190 - val_loss: 0.4137 - val_acc: 0.8751\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8421\n",
      "Epoch 00007: val_loss improved from 0.41374 to 0.36017, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/007-0.3602.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.5063 - acc: 0.8422 - val_loss: 0.3602 - val_acc: 0.8910\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8586\n",
      "Epoch 00008: val_loss improved from 0.36017 to 0.35755, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/008-0.3575.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.4536 - acc: 0.8586 - val_loss: 0.3575 - val_acc: 0.8952\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8728\n",
      "Epoch 00009: val_loss improved from 0.35755 to 0.31313, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/009-0.3131.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.4101 - acc: 0.8727 - val_loss: 0.3131 - val_acc: 0.9138\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8815\n",
      "Epoch 00010: val_loss did not improve from 0.31313\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.3807 - acc: 0.8815 - val_loss: 0.3200 - val_acc: 0.9038\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8906\n",
      "Epoch 00011: val_loss improved from 0.31313 to 0.29475, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/011-0.2947.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.3528 - acc: 0.8905 - val_loss: 0.2947 - val_acc: 0.9175\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8977\n",
      "Epoch 00012: val_loss improved from 0.29475 to 0.27143, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/012-0.2714.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.3288 - acc: 0.8977 - val_loss: 0.2714 - val_acc: 0.9173\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9032\n",
      "Epoch 00013: val_loss improved from 0.27143 to 0.26826, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/013-0.2683.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.3108 - acc: 0.9032 - val_loss: 0.2683 - val_acc: 0.9196\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9088\n",
      "Epoch 00014: val_loss improved from 0.26826 to 0.25094, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/014-0.2509.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.2911 - acc: 0.9088 - val_loss: 0.2509 - val_acc: 0.9241\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9138\n",
      "Epoch 00015: val_loss did not improve from 0.25094\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.2740 - acc: 0.9138 - val_loss: 0.2839 - val_acc: 0.9136\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9194\n",
      "Epoch 00016: val_loss did not improve from 0.25094\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2587 - acc: 0.9194 - val_loss: 0.3009 - val_acc: 0.9136\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9223\n",
      "Epoch 00017: val_loss improved from 0.25094 to 0.24071, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/017-0.2407.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.2467 - acc: 0.9222 - val_loss: 0.2407 - val_acc: 0.9273\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9262\n",
      "Epoch 00018: val_loss did not improve from 0.24071\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.2341 - acc: 0.9262 - val_loss: 0.2423 - val_acc: 0.9283\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9297\n",
      "Epoch 00019: val_loss improved from 0.24071 to 0.22239, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/019-0.2224.hdf5\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.2233 - acc: 0.9296 - val_loss: 0.2224 - val_acc: 0.9334\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9337\n",
      "Epoch 00020: val_loss did not improve from 0.22239\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.2113 - acc: 0.9337 - val_loss: 0.2253 - val_acc: 0.9345\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9364\n",
      "Epoch 00021: val_loss did not improve from 0.22239\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2037 - acc: 0.9364 - val_loss: 0.2605 - val_acc: 0.9285\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9378\n",
      "Epoch 00022: val_loss improved from 0.22239 to 0.22137, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/022-0.2214.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1988 - acc: 0.9377 - val_loss: 0.2214 - val_acc: 0.9322\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9439\n",
      "Epoch 00023: val_loss improved from 0.22137 to 0.21105, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/023-0.2110.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1821 - acc: 0.9439 - val_loss: 0.2110 - val_acc: 0.9357\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9420\n",
      "Epoch 00024: val_loss improved from 0.21105 to 0.21099, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/024-0.2110.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1843 - acc: 0.9420 - val_loss: 0.2110 - val_acc: 0.9373\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9451\n",
      "Epoch 00025: val_loss improved from 0.21099 to 0.19841, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/025-0.1984.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.1766 - acc: 0.9451 - val_loss: 0.1984 - val_acc: 0.9429\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9458\n",
      "Epoch 00026: val_loss did not improve from 0.19841\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1698 - acc: 0.9458 - val_loss: 0.2030 - val_acc: 0.9399\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9489\n",
      "Epoch 00027: val_loss improved from 0.19841 to 0.18234, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/027-0.1823.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1612 - acc: 0.9489 - val_loss: 0.1823 - val_acc: 0.9450\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9494\n",
      "Epoch 00028: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1614 - acc: 0.9495 - val_loss: 0.1872 - val_acc: 0.9474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9518\n",
      "Epoch 00029: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1521 - acc: 0.9519 - val_loss: 0.2107 - val_acc: 0.9359\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9535\n",
      "Epoch 00030: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1458 - acc: 0.9534 - val_loss: 0.1971 - val_acc: 0.9427\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9548\n",
      "Epoch 00031: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1466 - acc: 0.9547 - val_loss: 0.1987 - val_acc: 0.9443\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9573\n",
      "Epoch 00032: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1357 - acc: 0.9573 - val_loss: 0.2117 - val_acc: 0.9383\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9553\n",
      "Epoch 00033: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1396 - acc: 0.9553 - val_loss: 0.1958 - val_acc: 0.9441\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9592\n",
      "Epoch 00034: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1290 - acc: 0.9591 - val_loss: 0.1871 - val_acc: 0.9455\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9602\n",
      "Epoch 00035: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1293 - acc: 0.9602 - val_loss: 0.1945 - val_acc: 0.9453\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9620\n",
      "Epoch 00036: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1190 - acc: 0.9621 - val_loss: 0.1911 - val_acc: 0.9415\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9619\n",
      "Epoch 00037: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1205 - acc: 0.9619 - val_loss: 0.2773 - val_acc: 0.9189\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9630\n",
      "Epoch 00038: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1127 - acc: 0.9630 - val_loss: 0.1934 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9628\n",
      "Epoch 00039: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1142 - acc: 0.9628 - val_loss: 0.1890 - val_acc: 0.9429\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9650\n",
      "Epoch 00040: val_loss improved from 0.18234 to 0.18037, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/040-0.1804.hdf5\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.1082 - acc: 0.9650 - val_loss: 0.1804 - val_acc: 0.9471\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9669\n",
      "Epoch 00041: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.1045 - acc: 0.9669 - val_loss: 0.2207 - val_acc: 0.9446\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9672\n",
      "Epoch 00042: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.1028 - acc: 0.9672 - val_loss: 0.1979 - val_acc: 0.9427\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9668\n",
      "Epoch 00043: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1064 - acc: 0.9668 - val_loss: 0.1809 - val_acc: 0.9492\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9697\n",
      "Epoch 00044: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0975 - acc: 0.9696 - val_loss: 0.1825 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9683\n",
      "Epoch 00045: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0960 - acc: 0.9683 - val_loss: 0.2059 - val_acc: 0.9481\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9702\n",
      "Epoch 00046: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0936 - acc: 0.9702 - val_loss: 0.2059 - val_acc: 0.9420\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9706\n",
      "Epoch 00047: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0916 - acc: 0.9706 - val_loss: 0.1934 - val_acc: 0.9476\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9713\n",
      "Epoch 00048: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0896 - acc: 0.9713 - val_loss: 0.1847 - val_acc: 0.9464\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9706\n",
      "Epoch 00049: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0919 - acc: 0.9706 - val_loss: 0.2219 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9722\n",
      "Epoch 00050: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0864 - acc: 0.9722 - val_loss: 0.1869 - val_acc: 0.9471\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9740\n",
      "Epoch 00051: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0824 - acc: 0.9740 - val_loss: 0.2145 - val_acc: 0.9425\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9758\n",
      "Epoch 00052: val_loss improved from 0.18037 to 0.17716, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/052-0.1772.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0765 - acc: 0.9758 - val_loss: 0.1772 - val_acc: 0.9485\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9749\n",
      "Epoch 00053: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0798 - acc: 0.9749 - val_loss: 0.1947 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9751\n",
      "Epoch 00054: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0775 - acc: 0.9751 - val_loss: 0.2322 - val_acc: 0.9345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9747\n",
      "Epoch 00055: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0792 - acc: 0.9747 - val_loss: 0.1989 - val_acc: 0.9464\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9775\n",
      "Epoch 00056: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0721 - acc: 0.9775 - val_loss: 0.2690 - val_acc: 0.9297\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9748\n",
      "Epoch 00057: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0786 - acc: 0.9748 - val_loss: 0.1966 - val_acc: 0.9471\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9777\n",
      "Epoch 00058: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0711 - acc: 0.9777 - val_loss: 0.2035 - val_acc: 0.9478\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9788\n",
      "Epoch 00059: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0665 - acc: 0.9788 - val_loss: 0.2020 - val_acc: 0.9476\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9766\n",
      "Epoch 00060: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0731 - acc: 0.9766 - val_loss: 0.2264 - val_acc: 0.9425\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9797\n",
      "Epoch 00061: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0650 - acc: 0.9798 - val_loss: 0.1945 - val_acc: 0.9474\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9793\n",
      "Epoch 00062: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0662 - acc: 0.9794 - val_loss: 0.1863 - val_acc: 0.9478\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9789\n",
      "Epoch 00063: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0663 - acc: 0.9789 - val_loss: 0.2022 - val_acc: 0.9425\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9809\n",
      "Epoch 00064: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0608 - acc: 0.9809 - val_loss: 0.2578 - val_acc: 0.9383\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9808\n",
      "Epoch 00065: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0612 - acc: 0.9808 - val_loss: 0.2425 - val_acc: 0.9369\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9812\n",
      "Epoch 00066: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0592 - acc: 0.9812 - val_loss: 0.1826 - val_acc: 0.9485\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00067: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0698 - acc: 0.9777 - val_loss: 0.1963 - val_acc: 0.9488\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9816\n",
      "Epoch 00068: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0590 - acc: 0.9816 - val_loss: 0.2073 - val_acc: 0.9504\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9814\n",
      "Epoch 00069: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0583 - acc: 0.9814 - val_loss: 0.2067 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9828\n",
      "Epoch 00070: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0548 - acc: 0.9828 - val_loss: 0.2569 - val_acc: 0.9359\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9787\n",
      "Epoch 00071: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0668 - acc: 0.9787 - val_loss: 0.1957 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9831\n",
      "Epoch 00072: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0532 - acc: 0.9830 - val_loss: 0.2168 - val_acc: 0.9471\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9843\n",
      "Epoch 00073: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0499 - acc: 0.9842 - val_loss: 0.1991 - val_acc: 0.9506\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9808\n",
      "Epoch 00074: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0629 - acc: 0.9808 - val_loss: 0.2115 - val_acc: 0.9485\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9819\n",
      "Epoch 00075: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0563 - acc: 0.9819 - val_loss: 0.2176 - val_acc: 0.9455\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9858\n",
      "Epoch 00076: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0469 - acc: 0.9858 - val_loss: 0.2142 - val_acc: 0.9441\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9854\n",
      "Epoch 00077: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0477 - acc: 0.9854 - val_loss: 0.2744 - val_acc: 0.9352\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9805\n",
      "Epoch 00078: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0609 - acc: 0.9805 - val_loss: 0.2422 - val_acc: 0.9376\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9852\n",
      "Epoch 00079: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0480 - acc: 0.9852 - val_loss: 0.2236 - val_acc: 0.9457\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9830\n",
      "Epoch 00080: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0550 - acc: 0.9830 - val_loss: 0.1964 - val_acc: 0.9481\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9866\n",
      "Epoch 00081: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0433 - acc: 0.9866 - val_loss: 0.2081 - val_acc: 0.9439\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9853\n",
      "Epoch 00082: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0500 - acc: 0.9853 - val_loss: 0.2175 - val_acc: 0.9457\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9848\n",
      "Epoch 00083: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0514 - acc: 0.9848 - val_loss: 0.1992 - val_acc: 0.9502\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9869\n",
      "Epoch 00084: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0431 - acc: 0.9867 - val_loss: 0.2306 - val_acc: 0.9425\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9834\n",
      "Epoch 00085: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0526 - acc: 0.9834 - val_loss: 0.2332 - val_acc: 0.9418\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9860\n",
      "Epoch 00086: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0459 - acc: 0.9860 - val_loss: 0.1997 - val_acc: 0.9522\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9869\n",
      "Epoch 00087: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0421 - acc: 0.9869 - val_loss: 0.3272 - val_acc: 0.9206\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9846\n",
      "Epoch 00088: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0493 - acc: 0.9846 - val_loss: 0.1875 - val_acc: 0.9567\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9878\n",
      "Epoch 00089: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0400 - acc: 0.9877 - val_loss: 0.2205 - val_acc: 0.9495\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9847\n",
      "Epoch 00090: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0482 - acc: 0.9847 - val_loss: 0.2416 - val_acc: 0.9460\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9885\n",
      "Epoch 00091: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0386 - acc: 0.9885 - val_loss: 0.1970 - val_acc: 0.9532\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9871\n",
      "Epoch 00092: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0420 - acc: 0.9871 - val_loss: 0.2126 - val_acc: 0.9497\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9878\n",
      "Epoch 00093: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0389 - acc: 0.9878 - val_loss: 0.2442 - val_acc: 0.9455\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9878\n",
      "Epoch 00094: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0385 - acc: 0.9878 - val_loss: 0.2150 - val_acc: 0.9464\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9886\n",
      "Epoch 00095: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0378 - acc: 0.9887 - val_loss: 0.2282 - val_acc: 0.9485\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9876\n",
      "Epoch 00096: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0404 - acc: 0.9876 - val_loss: 0.2201 - val_acc: 0.9464\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9889\n",
      "Epoch 00097: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0352 - acc: 0.9889 - val_loss: 0.2642 - val_acc: 0.9315\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9890\n",
      "Epoch 00098: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0360 - acc: 0.9889 - val_loss: 0.2421 - val_acc: 0.9457\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9872\n",
      "Epoch 00099: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0447 - acc: 0.9871 - val_loss: 0.2372 - val_acc: 0.9425\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9887\n",
      "Epoch 00100: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0383 - acc: 0.9887 - val_loss: 0.2194 - val_acc: 0.9513\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9908\n",
      "Epoch 00101: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0306 - acc: 0.9908 - val_loss: 0.2318 - val_acc: 0.9467\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 00102: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0367 - acc: 0.9887 - val_loss: 0.2480 - val_acc: 0.9441\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9891\n",
      "Epoch 00103: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0365 - acc: 0.9891 - val_loss: 0.2151 - val_acc: 0.9522\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9892\n",
      "Epoch 00104: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0367 - acc: 0.9892 - val_loss: 0.2184 - val_acc: 0.9488\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9884\n",
      "Epoch 00105: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0383 - acc: 0.9884 - val_loss: 0.2271 - val_acc: 0.9464\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9903\n",
      "Epoch 00106: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0321 - acc: 0.9902 - val_loss: 0.2385 - val_acc: 0.9443\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9869\n",
      "Epoch 00107: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0426 - acc: 0.9869 - val_loss: 0.2087 - val_acc: 0.9527\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9910\n",
      "Epoch 00108: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0293 - acc: 0.9910 - val_loss: 0.2069 - val_acc: 0.9513\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 00109: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0325 - acc: 0.9900 - val_loss: 0.2146 - val_acc: 0.9525\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9906\n",
      "Epoch 00110: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0306 - acc: 0.9906 - val_loss: 0.2335 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9884\n",
      "Epoch 00111: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0366 - acc: 0.9885 - val_loss: 0.2469 - val_acc: 0.9448\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9921\n",
      "Epoch 00112: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0286 - acc: 0.9920 - val_loss: 0.2260 - val_acc: 0.9495\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9877\n",
      "Epoch 00113: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0392 - acc: 0.9877 - val_loss: 0.2094 - val_acc: 0.9529\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9895\n",
      "Epoch 00114: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0347 - acc: 0.9895 - val_loss: 0.2273 - val_acc: 0.9474\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9898\n",
      "Epoch 00115: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0334 - acc: 0.9898 - val_loss: 0.1880 - val_acc: 0.9567\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9897\n",
      "Epoch 00116: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0335 - acc: 0.9897 - val_loss: 0.2107 - val_acc: 0.9520\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9900\n",
      "Epoch 00117: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0349 - acc: 0.9899 - val_loss: 0.2215 - val_acc: 0.9509\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9928\n",
      "Epoch 00118: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0255 - acc: 0.9928 - val_loss: 0.2291 - val_acc: 0.9469\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9916\n",
      "Epoch 00119: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0293 - acc: 0.9916 - val_loss: 0.2227 - val_acc: 0.9492\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9922\n",
      "Epoch 00120: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0272 - acc: 0.9923 - val_loss: 0.2445 - val_acc: 0.9467\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9919\n",
      "Epoch 00121: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0290 - acc: 0.9918 - val_loss: 0.3074 - val_acc: 0.9399\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9911\n",
      "Epoch 00122: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0299 - acc: 0.9910 - val_loss: 0.2765 - val_acc: 0.9394\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9881\n",
      "Epoch 00123: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0405 - acc: 0.9881 - val_loss: 0.2056 - val_acc: 0.9520\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9923\n",
      "Epoch 00124: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0265 - acc: 0.9923 - val_loss: 0.2044 - val_acc: 0.9550\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9922\n",
      "Epoch 00125: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0269 - acc: 0.9922 - val_loss: 0.2224 - val_acc: 0.9522\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9921\n",
      "Epoch 00126: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0265 - acc: 0.9921 - val_loss: 0.2254 - val_acc: 0.9520\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9917\n",
      "Epoch 00127: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0283 - acc: 0.9917 - val_loss: 0.2549 - val_acc: 0.9522\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9920\n",
      "Epoch 00128: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0279 - acc: 0.9920 - val_loss: 0.2527 - val_acc: 0.9464\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9910\n",
      "Epoch 00129: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0296 - acc: 0.9910 - val_loss: 0.2648 - val_acc: 0.9492\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9932\n",
      "Epoch 00130: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.2233 - val_acc: 0.9560\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9939\n",
      "Epoch 00131: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0231 - acc: 0.9939 - val_loss: 0.2283 - val_acc: 0.9534\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9927\n",
      "Epoch 00132: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0255 - acc: 0.9927 - val_loss: 0.2718 - val_acc: 0.9408\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9902\n",
      "Epoch 00133: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0340 - acc: 0.9902 - val_loss: 0.2278 - val_acc: 0.9525\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9929\n",
      "Epoch 00134: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.2826 - val_acc: 0.9420\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9894\n",
      "Epoch 00135: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0355 - acc: 0.9895 - val_loss: 0.2243 - val_acc: 0.9543\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9940\n",
      "Epoch 00136: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0218 - acc: 0.9940 - val_loss: 0.2508 - val_acc: 0.9506\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9945\n",
      "Epoch 00137: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0199 - acc: 0.9945 - val_loss: 0.2710 - val_acc: 0.9464\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9888\n",
      "Epoch 00138: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0368 - acc: 0.9888 - val_loss: 0.2438 - val_acc: 0.9474\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9938\n",
      "Epoch 00139: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.2811 - val_acc: 0.9453\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938\n",
      "Epoch 00140: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0216 - acc: 0.9937 - val_loss: 0.2452 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9906\n",
      "Epoch 00141: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0310 - acc: 0.9906 - val_loss: 0.2500 - val_acc: 0.9467\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9934\n",
      "Epoch 00142: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0232 - acc: 0.9934 - val_loss: 0.2361 - val_acc: 0.9529\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9926\n",
      "Epoch 00143: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0252 - acc: 0.9926 - val_loss: 0.2291 - val_acc: 0.9497\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9937\n",
      "Epoch 00144: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0223 - acc: 0.9937 - val_loss: 0.2992 - val_acc: 0.9429\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9941\n",
      "Epoch 00145: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0212 - acc: 0.9941 - val_loss: 0.2369 - val_acc: 0.9488\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9926\n",
      "Epoch 00146: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0249 - acc: 0.9926 - val_loss: 0.2512 - val_acc: 0.9488\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9931\n",
      "Epoch 00147: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.2561 - val_acc: 0.9502\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9901\n",
      "Epoch 00148: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0339 - acc: 0.9901 - val_loss: 0.2310 - val_acc: 0.9506\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9941\n",
      "Epoch 00149: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0218 - acc: 0.9941 - val_loss: 0.2715 - val_acc: 0.9443\n",
      "Epoch 150/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9939\n",
      "Epoch 00150: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0206 - acc: 0.9939 - val_loss: 0.2691 - val_acc: 0.9432\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00151: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0251 - acc: 0.9923 - val_loss: 0.2616 - val_acc: 0.9455\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9906\n",
      "Epoch 00152: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0305 - acc: 0.9906 - val_loss: 0.2249 - val_acc: 0.9506\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 435us/sample - loss: 0.2204 - acc: 0.9375\n",
      "Loss: 0.22042345849648196 Accuracy: 0.937487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 364us/sample - loss: 1.8043 - acc: 0.4490\n",
      "Loss: 1.8042701047281362 Accuracy: 0.4490135\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 423us/sample - loss: 1.0973 - acc: 0.6885\n",
      "Loss: 1.097267709664714 Accuracy: 0.6884735\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 435us/sample - loss: 0.5562 - acc: 0.8538\n",
      "Loss: 0.5561911058079293 Accuracy: 0.8537902\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 466us/sample - loss: 0.2656 - acc: 0.9238\n",
      "Loss: 0.26559395780196937 Accuracy: 0.92377985\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 498us/sample - loss: 0.2204 - acc: 0.9375\n",
      "Loss: 0.22042345849648196 Accuracy: 0.937487\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base)) as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, accuracy, loss])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
