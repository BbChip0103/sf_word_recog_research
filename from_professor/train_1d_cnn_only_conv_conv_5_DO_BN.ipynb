{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,592\n",
      "Trainable params: 682,576\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,472\n",
      "Trainable params: 455,424\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 306,128\n",
      "Trainable params: 306,016\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,800\n",
      "Trainable params: 214,560\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 186,768\n",
      "Trainable params: 186,272\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 302,736\n",
      "Trainable params: 301,728\n",
      "Non-trainable params: 1,008\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 927,888\n",
      "Trainable params: 925,856\n",
      "Non-trainable params: 2,032\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 2, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,521,680\n",
      "Trainable params: 3,517,600\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.8694 - acc: 0.1739\n",
      "Epoch 00001: val_loss improved from inf to 2.46351, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/001-2.4635.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 2.8686 - acc: 0.1742 - val_loss: 2.4635 - val_acc: 0.2001\n",
      "Epoch 2/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 2.2827 - acc: 0.2908\n",
      "Epoch 00002: val_loss improved from 2.46351 to 2.26946, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/002-2.2695.hdf5\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 2.2822 - acc: 0.2910 - val_loss: 2.2695 - val_acc: 0.3003\n",
      "Epoch 3/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 2.0109 - acc: 0.3685\n",
      "Epoch 00003: val_loss improved from 2.26946 to 2.22027, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/003-2.2203.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 2.0110 - acc: 0.3685 - val_loss: 2.2203 - val_acc: 0.3198\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8485 - acc: 0.4166\n",
      "Epoch 00004: val_loss improved from 2.22027 to 2.16924, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/004-2.1692.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.8496 - acc: 0.4163 - val_loss: 2.1692 - val_acc: 0.3352\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7259 - acc: 0.4559\n",
      "Epoch 00005: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.7261 - acc: 0.4559 - val_loss: 2.1719 - val_acc: 0.3429\n",
      "Epoch 6/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.6332 - acc: 0.4875\n",
      "Epoch 00006: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.6348 - acc: 0.4873 - val_loss: 2.2729 - val_acc: 0.3110\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5558 - acc: 0.5115\n",
      "Epoch 00007: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.5564 - acc: 0.5113 - val_loss: 2.2378 - val_acc: 0.3343\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4883 - acc: 0.5317\n",
      "Epoch 00008: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 1.4891 - acc: 0.5313 - val_loss: 2.2729 - val_acc: 0.3364\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4358 - acc: 0.5484\n",
      "Epoch 00009: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.4357 - acc: 0.5485 - val_loss: 2.2379 - val_acc: 0.3420\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3755 - acc: 0.5652\n",
      "Epoch 00010: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.3755 - acc: 0.5652 - val_loss: 2.2329 - val_acc: 0.3501\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3421 - acc: 0.5737\n",
      "Epoch 00011: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 1.3422 - acc: 0.5736 - val_loss: 2.3184 - val_acc: 0.3368\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3039 - acc: 0.5855\n",
      "Epoch 00012: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.3038 - acc: 0.5856 - val_loss: 2.3631 - val_acc: 0.3275\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.5989\n",
      "Epoch 00013: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.2642 - acc: 0.5988 - val_loss: 2.4830 - val_acc: 0.3033\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2323 - acc: 0.6067\n",
      "Epoch 00014: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.2323 - acc: 0.6067 - val_loss: 2.3373 - val_acc: 0.3443\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2063 - acc: 0.6164\n",
      "Epoch 00015: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 1.2062 - acc: 0.6163 - val_loss: 2.3458 - val_acc: 0.3378\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1806 - acc: 0.6204\n",
      "Epoch 00016: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1807 - acc: 0.6205 - val_loss: 2.4853 - val_acc: 0.3135\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6277\n",
      "Epoch 00017: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 1.1609 - acc: 0.6277 - val_loss: 2.4025 - val_acc: 0.3450\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1324 - acc: 0.6368\n",
      "Epoch 00018: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1323 - acc: 0.6368 - val_loss: 2.4211 - val_acc: 0.3443\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1116 - acc: 0.6429\n",
      "Epoch 00019: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.1115 - acc: 0.6429 - val_loss: 2.6875 - val_acc: 0.3061\n",
      "Epoch 20/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6461\n",
      "Epoch 00020: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 1.1028 - acc: 0.6458 - val_loss: 2.5653 - val_acc: 0.3324\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.6563\n",
      "Epoch 00021: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.0711 - acc: 0.6563 - val_loss: 2.6649 - val_acc: 0.3126\n",
      "Epoch 22/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.6561\n",
      "Epoch 00022: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.0665 - acc: 0.6561 - val_loss: 2.4639 - val_acc: 0.3499\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0468 - acc: 0.6653\n",
      "Epoch 00023: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.0468 - acc: 0.6653 - val_loss: 2.4837 - val_acc: 0.3475\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0352 - acc: 0.6664\n",
      "Epoch 00024: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 1.0353 - acc: 0.6664 - val_loss: 2.4230 - val_acc: 0.3604\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0222 - acc: 0.6702\n",
      "Epoch 00025: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 1.0223 - acc: 0.6701 - val_loss: 2.7353 - val_acc: 0.3191\n",
      "Epoch 26/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0045 - acc: 0.6740\n",
      "Epoch 00026: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.0052 - acc: 0.6737 - val_loss: 2.5025 - val_acc: 0.3459\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.6786\n",
      "Epoch 00027: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.9985 - acc: 0.6785 - val_loss: 2.5312 - val_acc: 0.3552\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9840 - acc: 0.6840\n",
      "Epoch 00028: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.9842 - acc: 0.6839 - val_loss: 2.7383 - val_acc: 0.3270\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9800 - acc: 0.6833\n",
      "Epoch 00029: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.9800 - acc: 0.6832 - val_loss: 2.7014 - val_acc: 0.3277\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9627 - acc: 0.6887\n",
      "Epoch 00030: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.9632 - acc: 0.6885 - val_loss: 2.9535 - val_acc: 0.2991\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.6890\n",
      "Epoch 00031: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.9537 - acc: 0.6890 - val_loss: 2.5285 - val_acc: 0.3613\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.6893\n",
      "Epoch 00032: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.9568 - acc: 0.6890 - val_loss: 2.7039 - val_acc: 0.3413\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9446 - acc: 0.6889\n",
      "Epoch 00033: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.9447 - acc: 0.6889 - val_loss: 2.9056 - val_acc: 0.3100\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9366 - acc: 0.6966\n",
      "Epoch 00034: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9365 - acc: 0.6966 - val_loss: 2.6900 - val_acc: 0.3433\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9244 - acc: 0.6951\n",
      "Epoch 00035: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.9243 - acc: 0.6951 - val_loss: 2.5783 - val_acc: 0.3594\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.7027\n",
      "Epoch 00036: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.9166 - acc: 0.7029 - val_loss: 2.7174 - val_acc: 0.3359\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7016\n",
      "Epoch 00037: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9163 - acc: 0.7016 - val_loss: 2.6036 - val_acc: 0.3601\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7041\n",
      "Epoch 00038: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9052 - acc: 0.7040 - val_loss: 2.6150 - val_acc: 0.3559\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7073\n",
      "Epoch 00039: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9081 - acc: 0.7072 - val_loss: 2.8040 - val_acc: 0.3324\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7075\n",
      "Epoch 00040: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8965 - acc: 0.7074 - val_loss: 2.9480 - val_acc: 0.3238\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8896 - acc: 0.7085\n",
      "Epoch 00041: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8899 - acc: 0.7085 - val_loss: 2.7245 - val_acc: 0.3420\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7130\n",
      "Epoch 00042: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.8846 - acc: 0.7130 - val_loss: 2.8631 - val_acc: 0.3303\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8841 - acc: 0.7111\n",
      "Epoch 00043: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8841 - acc: 0.7111 - val_loss: 2.8380 - val_acc: 0.3415\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7102\n",
      "Epoch 00044: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8854 - acc: 0.7104 - val_loss: 2.9905 - val_acc: 0.3235\n",
      "Epoch 45/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8650 - acc: 0.7164\n",
      "Epoch 00045: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.8646 - acc: 0.7163 - val_loss: 2.7673 - val_acc: 0.3427\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8563 - acc: 0.7199\n",
      "Epoch 00046: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8563 - acc: 0.7198 - val_loss: 3.3523 - val_acc: 0.2821\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8636 - acc: 0.7180\n",
      "Epoch 00047: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.8638 - acc: 0.7179 - val_loss: 2.7418 - val_acc: 0.3482\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8495 - acc: 0.7239\n",
      "Epoch 00048: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.8495 - acc: 0.7239 - val_loss: 2.7013 - val_acc: 0.3597\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.7220\n",
      "Epoch 00049: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.8481 - acc: 0.7219 - val_loss: 2.8544 - val_acc: 0.3329\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8501 - acc: 0.7221\n",
      "Epoch 00050: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.8500 - acc: 0.7221 - val_loss: 2.7938 - val_acc: 0.3454\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7253\n",
      "Epoch 00051: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.8347 - acc: 0.7253 - val_loss: 2.8553 - val_acc: 0.3457\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7279\n",
      "Epoch 00052: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.8326 - acc: 0.7277 - val_loss: 2.8682 - val_acc: 0.3359\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8319 - acc: 0.7257\n",
      "Epoch 00053: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8320 - acc: 0.7256 - val_loss: 2.7371 - val_acc: 0.3648\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8282 - acc: 0.7275\n",
      "Epoch 00054: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.8282 - acc: 0.7275 - val_loss: 2.7755 - val_acc: 0.3571\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7286\n",
      "Epoch 00055: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8228 - acc: 0.7286 - val_loss: 2.7538 - val_acc: 0.3536\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8179 - acc: 0.7348\n",
      "Epoch 00056: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.8183 - acc: 0.7348 - val_loss: 2.7898 - val_acc: 0.3564\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8116 - acc: 0.7352\n",
      "Epoch 00057: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8116 - acc: 0.7351 - val_loss: 2.8430 - val_acc: 0.3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8140 - acc: 0.7336\n",
      "Epoch 00058: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8142 - acc: 0.7335 - val_loss: 3.0302 - val_acc: 0.3317\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8120 - acc: 0.7348\n",
      "Epoch 00059: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.8122 - acc: 0.7348 - val_loss: 2.8443 - val_acc: 0.3552\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8098 - acc: 0.7352\n",
      "Epoch 00060: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.8099 - acc: 0.7352 - val_loss: 3.0418 - val_acc: 0.3224\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8050 - acc: 0.7345\n",
      "Epoch 00061: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8048 - acc: 0.7345 - val_loss: 2.9563 - val_acc: 0.3308\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.7372\n",
      "Epoch 00062: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7977 - acc: 0.7367 - val_loss: 2.8925 - val_acc: 0.3478\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7916 - acc: 0.7420\n",
      "Epoch 00063: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.7914 - acc: 0.7421 - val_loss: 2.8638 - val_acc: 0.3562\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.7388\n",
      "Epoch 00064: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7976 - acc: 0.7387 - val_loss: 2.9021 - val_acc: 0.3496\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7868 - acc: 0.7423\n",
      "Epoch 00065: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7867 - acc: 0.7425 - val_loss: 3.1268 - val_acc: 0.3056\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7434\n",
      "Epoch 00066: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7938 - acc: 0.7433 - val_loss: 2.8591 - val_acc: 0.3508\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7843 - acc: 0.7426\n",
      "Epoch 00067: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.7842 - acc: 0.7425 - val_loss: 3.3113 - val_acc: 0.2944\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7816 - acc: 0.7452\n",
      "Epoch 00068: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.7817 - acc: 0.7450 - val_loss: 2.9061 - val_acc: 0.3529\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7818 - acc: 0.7451\n",
      "Epoch 00069: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.7815 - acc: 0.7452 - val_loss: 2.9674 - val_acc: 0.3513\n",
      "Epoch 70/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7809 - acc: 0.7449\n",
      "Epoch 00070: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7819 - acc: 0.7447 - val_loss: 2.8308 - val_acc: 0.3625\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7442\n",
      "Epoch 00071: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.7723 - acc: 0.7443 - val_loss: 2.9169 - val_acc: 0.3573\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7441\n",
      "Epoch 00072: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7722 - acc: 0.7442 - val_loss: 2.8961 - val_acc: 0.3478\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7603 - acc: 0.7551\n",
      "Epoch 00073: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7615 - acc: 0.7548 - val_loss: 2.8831 - val_acc: 0.3534\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7611 - acc: 0.7504\n",
      "Epoch 00074: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7610 - acc: 0.7504 - val_loss: 3.0391 - val_acc: 0.3459\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7655 - acc: 0.7496\n",
      "Epoch 00075: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7656 - acc: 0.7496 - val_loss: 3.3456 - val_acc: 0.2940\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7555 - acc: 0.7516\n",
      "Epoch 00076: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7552 - acc: 0.7517 - val_loss: 3.0273 - val_acc: 0.3450\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7635 - acc: 0.7477\n",
      "Epoch 00077: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.7637 - acc: 0.7476 - val_loss: 3.0877 - val_acc: 0.3378\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7540\n",
      "Epoch 00078: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.7619 - acc: 0.7541 - val_loss: 3.1636 - val_acc: 0.3347\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7580 - acc: 0.7490\n",
      "Epoch 00079: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.7580 - acc: 0.7491 - val_loss: 2.9162 - val_acc: 0.3566\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7502 - acc: 0.7538\n",
      "Epoch 00080: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7502 - acc: 0.7538 - val_loss: 2.9468 - val_acc: 0.3545\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7548\n",
      "Epoch 00081: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7501 - acc: 0.7548 - val_loss: 2.8755 - val_acc: 0.3699\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7519 - acc: 0.7538\n",
      "Epoch 00082: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.7519 - acc: 0.7538 - val_loss: 3.4866 - val_acc: 0.2926\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.7557\n",
      "Epoch 00083: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.7489 - acc: 0.7557 - val_loss: 3.2225 - val_acc: 0.3175\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7568\n",
      "Epoch 00084: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.7404 - acc: 0.7568 - val_loss: 3.0688 - val_acc: 0.3403\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7465 - acc: 0.7565\n",
      "Epoch 00085: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.7466 - acc: 0.7565 - val_loss: 3.1735 - val_acc: 0.3191\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7551\n",
      "Epoch 00086: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7427 - acc: 0.7551 - val_loss: 2.9739 - val_acc: 0.3473\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7345 - acc: 0.7601\n",
      "Epoch 00087: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7348 - acc: 0.7601 - val_loss: 2.8954 - val_acc: 0.3550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7364 - acc: 0.7586\n",
      "Epoch 00088: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.7364 - acc: 0.7586 - val_loss: 2.9461 - val_acc: 0.3510\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7314 - acc: 0.7619\n",
      "Epoch 00089: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.7318 - acc: 0.7616 - val_loss: 3.1086 - val_acc: 0.3366\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.7614\n",
      "Epoch 00090: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.7345 - acc: 0.7613 - val_loss: 3.1454 - val_acc: 0.3457\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7221 - acc: 0.7632\n",
      "Epoch 00091: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7223 - acc: 0.7632 - val_loss: 2.9630 - val_acc: 0.3599\n",
      "Epoch 92/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7240 - acc: 0.7617\n",
      "Epoch 00092: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7245 - acc: 0.7615 - val_loss: 3.0757 - val_acc: 0.3375\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.7642\n",
      "Epoch 00093: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7197 - acc: 0.7642 - val_loss: 2.9876 - val_acc: 0.3489\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7287 - acc: 0.7586\n",
      "Epoch 00094: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7292 - acc: 0.7585 - val_loss: 3.1043 - val_acc: 0.3487\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7210 - acc: 0.7642\n",
      "Epoch 00095: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7207 - acc: 0.7642 - val_loss: 3.1449 - val_acc: 0.3310\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7191 - acc: 0.7641\n",
      "Epoch 00096: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7193 - acc: 0.7641 - val_loss: 2.9542 - val_acc: 0.3683\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7239 - acc: 0.7658\n",
      "Epoch 00097: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7242 - acc: 0.7658 - val_loss: 3.0446 - val_acc: 0.3571\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7143 - acc: 0.7658\n",
      "Epoch 00098: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7142 - acc: 0.7658 - val_loss: 2.9390 - val_acc: 0.3601\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7148 - acc: 0.7662\n",
      "Epoch 00099: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7149 - acc: 0.7661 - val_loss: 3.0067 - val_acc: 0.3471\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7090 - acc: 0.7707\n",
      "Epoch 00100: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.7087 - acc: 0.7708 - val_loss: 2.9801 - val_acc: 0.3655\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7166 - acc: 0.7648\n",
      "Epoch 00101: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7165 - acc: 0.7647 - val_loss: 3.2456 - val_acc: 0.3226\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7019 - acc: 0.7693\n",
      "Epoch 00102: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7020 - acc: 0.7692 - val_loss: 3.0827 - val_acc: 0.3531\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7120 - acc: 0.7681\n",
      "Epoch 00103: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7120 - acc: 0.7680 - val_loss: 2.9483 - val_acc: 0.3648\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7089 - acc: 0.7708\n",
      "Epoch 00104: val_loss did not improve from 2.16924\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.7086 - acc: 0.7708 - val_loss: 3.1925 - val_acc: 0.3475\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 174us/sample - loss: 2.2044 - acc: 0.3171\n",
      "Loss: 2.20443851611077 Accuracy: 0.31713396\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7976 - acc: 0.2080\n",
      "Epoch 00001: val_loss improved from inf to 2.32704, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/001-2.3270.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 2.7957 - acc: 0.2084 - val_loss: 2.3270 - val_acc: 0.2544\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0270 - acc: 0.3609\n",
      "Epoch 00002: val_loss improved from 2.32704 to 1.80522, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/002-1.8052.hdf5\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 2.0269 - acc: 0.3609 - val_loss: 1.8052 - val_acc: 0.4368\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7798 - acc: 0.4329\n",
      "Epoch 00003: val_loss improved from 1.80522 to 1.70006, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/003-1.7001.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 1.7799 - acc: 0.4330 - val_loss: 1.7001 - val_acc: 0.4773\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6315 - acc: 0.4804\n",
      "Epoch 00004: val_loss improved from 1.70006 to 1.65607, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/004-1.6561.hdf5\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 1.6314 - acc: 0.4804 - val_loss: 1.6561 - val_acc: 0.4812\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5002 - acc: 0.5212\n",
      "Epoch 00005: val_loss did not improve from 1.65607\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 1.5001 - acc: 0.5211 - val_loss: 1.7086 - val_acc: 0.4670\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4019 - acc: 0.5502\n",
      "Epoch 00006: val_loss improved from 1.65607 to 1.53020, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/006-1.5302.hdf5\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 1.4020 - acc: 0.5502 - val_loss: 1.5302 - val_acc: 0.5313\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3069 - acc: 0.5819\n",
      "Epoch 00007: val_loss did not improve from 1.53020\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 1.3066 - acc: 0.5820 - val_loss: 1.6024 - val_acc: 0.5038\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2396 - acc: 0.5996\n",
      "Epoch 00008: val_loss did not improve from 1.53020\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 1.2402 - acc: 0.5994 - val_loss: 1.6681 - val_acc: 0.4736\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1717 - acc: 0.6228\n",
      "Epoch 00009: val_loss did not improve from 1.53020\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 1.1716 - acc: 0.6227 - val_loss: 1.5814 - val_acc: 0.5118\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1277 - acc: 0.6363\n",
      "Epoch 00010: val_loss did not improve from 1.53020\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 1.1281 - acc: 0.6363 - val_loss: 1.5349 - val_acc: 0.5141\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0850 - acc: 0.6512\n",
      "Epoch 00011: val_loss improved from 1.53020 to 1.50410, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/011-1.5041.hdf5\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 1.0848 - acc: 0.6511 - val_loss: 1.5041 - val_acc: 0.5311\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0382 - acc: 0.6608\n",
      "Epoch 00012: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 1.0376 - acc: 0.6609 - val_loss: 1.5281 - val_acc: 0.5427\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9979 - acc: 0.6745\n",
      "Epoch 00013: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.9976 - acc: 0.6747 - val_loss: 1.5669 - val_acc: 0.5190\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9603 - acc: 0.6877\n",
      "Epoch 00014: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.9607 - acc: 0.6877 - val_loss: 1.6445 - val_acc: 0.5085\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9300 - acc: 0.6950\n",
      "Epoch 00015: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.9304 - acc: 0.6948 - val_loss: 1.6441 - val_acc: 0.5052\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7060\n",
      "Epoch 00016: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.9010 - acc: 0.7060 - val_loss: 1.5114 - val_acc: 0.5425\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8716 - acc: 0.7153\n",
      "Epoch 00017: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.8716 - acc: 0.7153 - val_loss: 1.5390 - val_acc: 0.5469\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7239\n",
      "Epoch 00018: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.8436 - acc: 0.7237 - val_loss: 1.5132 - val_acc: 0.5542\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7285\n",
      "Epoch 00019: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.8257 - acc: 0.7284 - val_loss: 1.5863 - val_acc: 0.5311\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.7352\n",
      "Epoch 00020: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.8034 - acc: 0.7352 - val_loss: 1.5939 - val_acc: 0.5360\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7461\n",
      "Epoch 00021: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.7748 - acc: 0.7460 - val_loss: 1.5645 - val_acc: 0.5448\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7449\n",
      "Epoch 00022: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.7677 - acc: 0.7448 - val_loss: 1.5847 - val_acc: 0.5518\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7522\n",
      "Epoch 00023: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.7492 - acc: 0.7521 - val_loss: 1.5423 - val_acc: 0.5609\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7351 - acc: 0.7569\n",
      "Epoch 00024: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.7351 - acc: 0.7567 - val_loss: 1.5349 - val_acc: 0.5639\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7077 - acc: 0.7664\n",
      "Epoch 00025: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.7079 - acc: 0.7664 - val_loss: 1.5917 - val_acc: 0.5525\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7017 - acc: 0.7682\n",
      "Epoch 00026: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.7018 - acc: 0.7681 - val_loss: 1.5404 - val_acc: 0.5646\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.7735\n",
      "Epoch 00027: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.6857 - acc: 0.7735 - val_loss: 1.5815 - val_acc: 0.5497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6722 - acc: 0.7764\n",
      "Epoch 00028: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.6722 - acc: 0.7764 - val_loss: 1.5465 - val_acc: 0.5609\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.7818\n",
      "Epoch 00029: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.6592 - acc: 0.7818 - val_loss: 1.5904 - val_acc: 0.5546\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.7847\n",
      "Epoch 00030: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.6491 - acc: 0.7848 - val_loss: 1.5511 - val_acc: 0.5663\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.7882\n",
      "Epoch 00031: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.6370 - acc: 0.7882 - val_loss: 1.6216 - val_acc: 0.5565\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.7876\n",
      "Epoch 00032: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.6340 - acc: 0.7876 - val_loss: 1.6410 - val_acc: 0.5551\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6186 - acc: 0.7927\n",
      "Epoch 00033: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.6186 - acc: 0.7927 - val_loss: 1.5580 - val_acc: 0.5784\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6080 - acc: 0.7966\n",
      "Epoch 00034: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.6080 - acc: 0.7966 - val_loss: 1.5868 - val_acc: 0.5695\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.8017\n",
      "Epoch 00035: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.5930 - acc: 0.8017 - val_loss: 1.6299 - val_acc: 0.5504\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5844 - acc: 0.8053\n",
      "Epoch 00036: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5854 - acc: 0.8049 - val_loss: 1.5754 - val_acc: 0.5658\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8094\n",
      "Epoch 00037: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.5732 - acc: 0.8094 - val_loss: 1.6104 - val_acc: 0.5702\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.8080\n",
      "Epoch 00038: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.5734 - acc: 0.8079 - val_loss: 1.6443 - val_acc: 0.5521\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.8120\n",
      "Epoch 00039: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.5598 - acc: 0.8120 - val_loss: 1.6124 - val_acc: 0.5653\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8191\n",
      "Epoch 00040: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5492 - acc: 0.8190 - val_loss: 1.5919 - val_acc: 0.5709\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.8186\n",
      "Epoch 00041: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5354 - acc: 0.8186 - val_loss: 1.5898 - val_acc: 0.5749\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8197\n",
      "Epoch 00042: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.5388 - acc: 0.8198 - val_loss: 1.6209 - val_acc: 0.5719\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8205\n",
      "Epoch 00043: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5319 - acc: 0.8205 - val_loss: 1.6686 - val_acc: 0.5600\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8261\n",
      "Epoch 00044: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5223 - acc: 0.8261 - val_loss: 1.6421 - val_acc: 0.5609\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.8250\n",
      "Epoch 00045: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.5196 - acc: 0.8248 - val_loss: 1.7122 - val_acc: 0.5388\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8304\n",
      "Epoch 00046: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.5076 - acc: 0.8305 - val_loss: 1.6135 - val_acc: 0.5723\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8311\n",
      "Epoch 00047: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.5034 - acc: 0.8311 - val_loss: 1.5853 - val_acc: 0.5772\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8283\n",
      "Epoch 00048: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.5008 - acc: 0.8284 - val_loss: 1.5934 - val_acc: 0.5809\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4901 - acc: 0.8353\n",
      "Epoch 00049: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.4902 - acc: 0.8352 - val_loss: 1.7280 - val_acc: 0.5448\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4844 - acc: 0.8375\n",
      "Epoch 00050: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.4842 - acc: 0.8376 - val_loss: 1.5853 - val_acc: 0.5858\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8423\n",
      "Epoch 00051: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4772 - acc: 0.8424 - val_loss: 1.6026 - val_acc: 0.5795\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8460\n",
      "Epoch 00052: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4658 - acc: 0.8458 - val_loss: 1.6607 - val_acc: 0.5681\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8444\n",
      "Epoch 00053: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4708 - acc: 0.8442 - val_loss: 1.6215 - val_acc: 0.5793\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8456\n",
      "Epoch 00054: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.4627 - acc: 0.8456 - val_loss: 1.6070 - val_acc: 0.5879\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8473\n",
      "Epoch 00055: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4609 - acc: 0.8471 - val_loss: 1.6207 - val_acc: 0.5768\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8520\n",
      "Epoch 00056: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.4464 - acc: 0.8520 - val_loss: 1.7346 - val_acc: 0.5514\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8491\n",
      "Epoch 00057: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.4509 - acc: 0.8491 - val_loss: 1.6012 - val_acc: 0.5884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8518\n",
      "Epoch 00058: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.4428 - acc: 0.8516 - val_loss: 1.6390 - val_acc: 0.5754\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8549\n",
      "Epoch 00059: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.4363 - acc: 0.8549 - val_loss: 1.7306 - val_acc: 0.5574\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8541\n",
      "Epoch 00060: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4373 - acc: 0.8541 - val_loss: 1.9023 - val_acc: 0.5313\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8555\n",
      "Epoch 00061: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.4338 - acc: 0.8555 - val_loss: 1.8305 - val_acc: 0.5420\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8548\n",
      "Epoch 00062: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.4305 - acc: 0.8548 - val_loss: 1.6390 - val_acc: 0.5842\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8585\n",
      "Epoch 00063: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4237 - acc: 0.8585 - val_loss: 1.6242 - val_acc: 0.5886\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8636\n",
      "Epoch 00064: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.4151 - acc: 0.8636 - val_loss: 1.6627 - val_acc: 0.5782\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8614\n",
      "Epoch 00065: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.4134 - acc: 0.8615 - val_loss: 1.5991 - val_acc: 0.5956\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8613\n",
      "Epoch 00066: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.4162 - acc: 0.8613 - val_loss: 1.6225 - val_acc: 0.5938\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8652\n",
      "Epoch 00067: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4011 - acc: 0.8651 - val_loss: 1.8729 - val_acc: 0.5423\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8680\n",
      "Epoch 00068: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4009 - acc: 0.8680 - val_loss: 1.6761 - val_acc: 0.5788\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8637\n",
      "Epoch 00069: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4020 - acc: 0.8637 - val_loss: 1.6321 - val_acc: 0.5872\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8704\n",
      "Epoch 00070: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3935 - acc: 0.8706 - val_loss: 1.6158 - val_acc: 0.6019\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8723\n",
      "Epoch 00071: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3906 - acc: 0.8723 - val_loss: 1.6652 - val_acc: 0.5826\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8726\n",
      "Epoch 00072: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.3838 - acc: 0.8726 - val_loss: 1.7872 - val_acc: 0.5679\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8709\n",
      "Epoch 00073: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3880 - acc: 0.8709 - val_loss: 1.6611 - val_acc: 0.5912\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8739\n",
      "Epoch 00074: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3800 - acc: 0.8739 - val_loss: 1.6383 - val_acc: 0.5945\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8759\n",
      "Epoch 00075: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3763 - acc: 0.8759 - val_loss: 1.6685 - val_acc: 0.5861\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8781\n",
      "Epoch 00076: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3691 - acc: 0.8781 - val_loss: 1.6332 - val_acc: 0.5984\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8786\n",
      "Epoch 00077: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3651 - acc: 0.8786 - val_loss: 1.6181 - val_acc: 0.6010\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8788\n",
      "Epoch 00078: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3712 - acc: 0.8788 - val_loss: 1.6577 - val_acc: 0.5977\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8809\n",
      "Epoch 00079: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3643 - acc: 0.8809 - val_loss: 1.7020 - val_acc: 0.5849\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8779\n",
      "Epoch 00080: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.3639 - acc: 0.8779 - val_loss: 1.6435 - val_acc: 0.5982\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8799\n",
      "Epoch 00081: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3598 - acc: 0.8799 - val_loss: 1.6237 - val_acc: 0.6031\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8792\n",
      "Epoch 00082: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3626 - acc: 0.8793 - val_loss: 1.6091 - val_acc: 0.6040\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8836\n",
      "Epoch 00083: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3542 - acc: 0.8836 - val_loss: 1.6920 - val_acc: 0.5835\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8828\n",
      "Epoch 00084: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3563 - acc: 0.8827 - val_loss: 1.6337 - val_acc: 0.5970\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3510 - acc: 0.8840\n",
      "Epoch 00085: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3510 - acc: 0.8840 - val_loss: 1.7408 - val_acc: 0.5795\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8845\n",
      "Epoch 00086: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.3503 - acc: 0.8846 - val_loss: 1.6728 - val_acc: 0.5980\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8849\n",
      "Epoch 00087: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3474 - acc: 0.8849 - val_loss: 1.6229 - val_acc: 0.5977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8885\n",
      "Epoch 00088: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3404 - acc: 0.8883 - val_loss: 1.7220 - val_acc: 0.5921\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8864\n",
      "Epoch 00089: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3423 - acc: 0.8866 - val_loss: 1.6527 - val_acc: 0.6033\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8892\n",
      "Epoch 00090: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3367 - acc: 0.8892 - val_loss: 1.7069 - val_acc: 0.5975\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8868\n",
      "Epoch 00091: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3409 - acc: 0.8869 - val_loss: 1.7140 - val_acc: 0.5791\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8888\n",
      "Epoch 00092: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3371 - acc: 0.8888 - val_loss: 1.7031 - val_acc: 0.5919\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8894\n",
      "Epoch 00093: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3362 - acc: 0.8895 - val_loss: 1.6888 - val_acc: 0.5935\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8916\n",
      "Epoch 00094: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3327 - acc: 0.8916 - val_loss: 1.6958 - val_acc: 0.6026\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8910\n",
      "Epoch 00095: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3295 - acc: 0.8909 - val_loss: 1.6564 - val_acc: 0.6021\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8954\n",
      "Epoch 00096: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3217 - acc: 0.8954 - val_loss: 1.6317 - val_acc: 0.6110\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8925\n",
      "Epoch 00097: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3281 - acc: 0.8925 - val_loss: 1.6690 - val_acc: 0.6038\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8951\n",
      "Epoch 00098: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.3193 - acc: 0.8950 - val_loss: 1.7098 - val_acc: 0.5917\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8928\n",
      "Epoch 00099: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.3223 - acc: 0.8928 - val_loss: 1.7260 - val_acc: 0.5905\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.8928\n",
      "Epoch 00100: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.3196 - acc: 0.8928 - val_loss: 1.6661 - val_acc: 0.6047\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8984\n",
      "Epoch 00101: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3106 - acc: 0.8984 - val_loss: 1.6781 - val_acc: 0.6045\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.8965\n",
      "Epoch 00102: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.3142 - acc: 0.8966 - val_loss: 1.6925 - val_acc: 0.6052\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8968\n",
      "Epoch 00103: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3149 - acc: 0.8966 - val_loss: 1.8090 - val_acc: 0.5751\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.8985\n",
      "Epoch 00104: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3079 - acc: 0.8985 - val_loss: 2.0214 - val_acc: 0.5446\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8985\n",
      "Epoch 00105: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3089 - acc: 0.8984 - val_loss: 1.6434 - val_acc: 0.6140\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9014\n",
      "Epoch 00106: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3056 - acc: 0.9014 - val_loss: 1.6561 - val_acc: 0.6059\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8977\n",
      "Epoch 00107: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3122 - acc: 0.8976 - val_loss: 1.9001 - val_acc: 0.5607\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9038\n",
      "Epoch 00108: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2988 - acc: 0.9038 - val_loss: 1.7433 - val_acc: 0.5903\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.8999\n",
      "Epoch 00109: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3042 - acc: 0.8996 - val_loss: 1.6776 - val_acc: 0.6049\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.8987\n",
      "Epoch 00110: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.3044 - acc: 0.8987 - val_loss: 1.7236 - val_acc: 0.5910\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9039\n",
      "Epoch 00111: val_loss did not improve from 1.50410\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.2973 - acc: 0.9039 - val_loss: 1.8192 - val_acc: 0.5698\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 198us/sample - loss: 1.6162 - acc: 0.4974\n",
      "Loss: 1.6161952246510476 Accuracy: 0.49740395\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7263 - acc: 0.2147\n",
      "Epoch 00001: val_loss improved from inf to 1.92165, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/001-1.9216.hdf5\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 2.7246 - acc: 0.2151 - val_loss: 1.9216 - val_acc: 0.3871\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9143 - acc: 0.3943\n",
      "Epoch 00002: val_loss improved from 1.92165 to 1.58588, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/002-1.5859.hdf5\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 1.9139 - acc: 0.3943 - val_loss: 1.5859 - val_acc: 0.4845\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6646 - acc: 0.4714\n",
      "Epoch 00003: val_loss improved from 1.58588 to 1.44252, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/003-1.4425.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.6643 - acc: 0.4715 - val_loss: 1.4425 - val_acc: 0.5502\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5111 - acc: 0.5218\n",
      "Epoch 00004: val_loss improved from 1.44252 to 1.33873, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/004-1.3387.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 1.5116 - acc: 0.5217 - val_loss: 1.3387 - val_acc: 0.5800\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3884 - acc: 0.5608\n",
      "Epoch 00005: val_loss improved from 1.33873 to 1.28291, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/005-1.2829.hdf5\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 1.3882 - acc: 0.5609 - val_loss: 1.2829 - val_acc: 0.5982\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2985 - acc: 0.5902\n",
      "Epoch 00006: val_loss did not improve from 1.28291\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.2987 - acc: 0.5902 - val_loss: 1.3534 - val_acc: 0.5653\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2210 - acc: 0.6171\n",
      "Epoch 00007: val_loss improved from 1.28291 to 1.19678, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/007-1.1968.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 1.2209 - acc: 0.6171 - val_loss: 1.1968 - val_acc: 0.6375\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6366\n",
      "Epoch 00008: val_loss improved from 1.19678 to 1.16514, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/008-1.1651.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 1.1558 - acc: 0.6367 - val_loss: 1.1651 - val_acc: 0.6324\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0990 - acc: 0.6536\n",
      "Epoch 00009: val_loss improved from 1.16514 to 1.14091, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/009-1.1409.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 1.0987 - acc: 0.6536 - val_loss: 1.1409 - val_acc: 0.6476\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0482 - acc: 0.6723\n",
      "Epoch 00010: val_loss did not improve from 1.14091\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 1.0484 - acc: 0.6723 - val_loss: 1.1516 - val_acc: 0.6415\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0008 - acc: 0.6868\n",
      "Epoch 00011: val_loss improved from 1.14091 to 1.11851, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/011-1.1185.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 1.0012 - acc: 0.6866 - val_loss: 1.1185 - val_acc: 0.6646\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9661 - acc: 0.6982\n",
      "Epoch 00012: val_loss improved from 1.11851 to 1.10663, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/012-1.1066.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.9665 - acc: 0.6982 - val_loss: 1.1066 - val_acc: 0.6709\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9258 - acc: 0.7098\n",
      "Epoch 00013: val_loss did not improve from 1.10663\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.9258 - acc: 0.7098 - val_loss: 1.2436 - val_acc: 0.6152\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9041 - acc: 0.7192\n",
      "Epoch 00014: val_loss improved from 1.10663 to 1.09775, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/014-1.0978.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.9045 - acc: 0.7191 - val_loss: 1.0978 - val_acc: 0.6706\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7299\n",
      "Epoch 00015: val_loss improved from 1.09775 to 1.06349, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/015-1.0635.hdf5\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.8663 - acc: 0.7299 - val_loss: 1.0635 - val_acc: 0.6860\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7380\n",
      "Epoch 00016: val_loss did not improve from 1.06349\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.8391 - acc: 0.7380 - val_loss: 1.0788 - val_acc: 0.6781\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8127 - acc: 0.7460\n",
      "Epoch 00017: val_loss did not improve from 1.06349\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.8123 - acc: 0.7462 - val_loss: 1.0766 - val_acc: 0.6858\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7885 - acc: 0.7514\n",
      "Epoch 00018: val_loss did not improve from 1.06349\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.7886 - acc: 0.7513 - val_loss: 1.1377 - val_acc: 0.6632\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.7564\n",
      "Epoch 00019: val_loss did not improve from 1.06349\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.7692 - acc: 0.7564 - val_loss: 1.1187 - val_acc: 0.6737\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7638\n",
      "Epoch 00020: val_loss did not improve from 1.06349\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.7424 - acc: 0.7637 - val_loss: 1.0730 - val_acc: 0.6832\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7703\n",
      "Epoch 00021: val_loss improved from 1.06349 to 1.04705, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/021-1.0470.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.7232 - acc: 0.7703 - val_loss: 1.0470 - val_acc: 0.6951\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7752\n",
      "Epoch 00022: val_loss improved from 1.04705 to 1.03184, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/022-1.0318.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.7118 - acc: 0.7752 - val_loss: 1.0318 - val_acc: 0.7007\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.7812\n",
      "Epoch 00023: val_loss did not improve from 1.03184\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.6922 - acc: 0.7813 - val_loss: 1.0595 - val_acc: 0.6930\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.7891\n",
      "Epoch 00024: val_loss improved from 1.03184 to 1.02738, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/024-1.0274.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.6682 - acc: 0.7891 - val_loss: 1.0274 - val_acc: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.7911\n",
      "Epoch 00025: val_loss did not improve from 1.02738\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.6586 - acc: 0.7911 - val_loss: 1.1028 - val_acc: 0.6816\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6371 - acc: 0.7971\n",
      "Epoch 00026: val_loss did not improve from 1.02738\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.6373 - acc: 0.7971 - val_loss: 1.0505 - val_acc: 0.7007\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.8017\n",
      "Epoch 00027: val_loss improved from 1.02738 to 1.02227, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/027-1.0223.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.6267 - acc: 0.8017 - val_loss: 1.0223 - val_acc: 0.7042\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.8041\n",
      "Epoch 00028: val_loss did not improve from 1.02227\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.6124 - acc: 0.8040 - val_loss: 1.0421 - val_acc: 0.6967\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8059\n",
      "Epoch 00029: val_loss did not improve from 1.02227\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5970 - acc: 0.8059 - val_loss: 1.0436 - val_acc: 0.6965\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8108\n",
      "Epoch 00030: val_loss did not improve from 1.02227\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.5887 - acc: 0.8107 - val_loss: 1.3539 - val_acc: 0.6226\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.8114\n",
      "Epoch 00031: val_loss did not improve from 1.02227\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.5781 - acc: 0.8115 - val_loss: 1.0715 - val_acc: 0.6976\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8189\n",
      "Epoch 00032: val_loss improved from 1.02227 to 1.00592, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/032-1.0059.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.5626 - acc: 0.8189 - val_loss: 1.0059 - val_acc: 0.7128\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8218\n",
      "Epoch 00033: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5566 - acc: 0.8218 - val_loss: 1.1107 - val_acc: 0.6816\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8234\n",
      "Epoch 00034: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5489 - acc: 0.8234 - val_loss: 1.0405 - val_acc: 0.7074\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8262\n",
      "Epoch 00035: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.5374 - acc: 0.8262 - val_loss: 1.0198 - val_acc: 0.7170\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8295\n",
      "Epoch 00036: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5297 - acc: 0.8294 - val_loss: 1.0432 - val_acc: 0.7095\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8332\n",
      "Epoch 00037: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5184 - acc: 0.8333 - val_loss: 1.0107 - val_acc: 0.7186\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8354\n",
      "Epoch 00038: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.5114 - acc: 0.8353 - val_loss: 1.0672 - val_acc: 0.7021\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8387\n",
      "Epoch 00039: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5039 - acc: 0.8388 - val_loss: 1.0325 - val_acc: 0.7151\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8365\n",
      "Epoch 00040: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.4957 - acc: 0.8364 - val_loss: 1.0353 - val_acc: 0.7137\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8429\n",
      "Epoch 00041: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.4857 - acc: 0.8429 - val_loss: 1.0491 - val_acc: 0.7086\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.8444\n",
      "Epoch 00042: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4784 - acc: 0.8443 - val_loss: 1.0610 - val_acc: 0.7091\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.8473\n",
      "Epoch 00043: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4720 - acc: 0.8474 - val_loss: 1.1316 - val_acc: 0.6823\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8498\n",
      "Epoch 00044: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4672 - acc: 0.8497 - val_loss: 1.0676 - val_acc: 0.7042\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8515\n",
      "Epoch 00045: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.4618 - acc: 0.8514 - val_loss: 1.0415 - val_acc: 0.7219\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8531\n",
      "Epoch 00046: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4520 - acc: 0.8531 - val_loss: 1.0446 - val_acc: 0.7177\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8552\n",
      "Epoch 00047: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4452 - acc: 0.8550 - val_loss: 1.0257 - val_acc: 0.7223\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8573\n",
      "Epoch 00048: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4396 - acc: 0.8572 - val_loss: 1.0220 - val_acc: 0.7233\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8606\n",
      "Epoch 00049: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4348 - acc: 0.8606 - val_loss: 1.0276 - val_acc: 0.7198\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8605\n",
      "Epoch 00050: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4272 - acc: 0.8606 - val_loss: 1.0531 - val_acc: 0.7200\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8631\n",
      "Epoch 00051: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4191 - acc: 0.8631 - val_loss: 1.0488 - val_acc: 0.7188\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8637\n",
      "Epoch 00052: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4179 - acc: 0.8637 - val_loss: 1.0410 - val_acc: 0.7191\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8643\n",
      "Epoch 00053: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4115 - acc: 0.8643 - val_loss: 1.0391 - val_acc: 0.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8683\n",
      "Epoch 00054: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4047 - acc: 0.8683 - val_loss: 1.0514 - val_acc: 0.7149\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8690\n",
      "Epoch 00055: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4020 - acc: 0.8688 - val_loss: 1.0734 - val_acc: 0.7095\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8715\n",
      "Epoch 00056: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3964 - acc: 0.8713 - val_loss: 1.0554 - val_acc: 0.7172\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8685\n",
      "Epoch 00057: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3964 - acc: 0.8685 - val_loss: 1.0624 - val_acc: 0.7200\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8726\n",
      "Epoch 00058: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3900 - acc: 0.8727 - val_loss: 1.2802 - val_acc: 0.6632\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8721\n",
      "Epoch 00059: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3884 - acc: 0.8722 - val_loss: 1.0744 - val_acc: 0.7144\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8778\n",
      "Epoch 00060: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3778 - acc: 0.8777 - val_loss: 1.0412 - val_acc: 0.7207\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8769\n",
      "Epoch 00061: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3792 - acc: 0.8768 - val_loss: 1.0896 - val_acc: 0.7098\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8811\n",
      "Epoch 00062: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3687 - acc: 0.8811 - val_loss: 1.1053 - val_acc: 0.7081\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8804\n",
      "Epoch 00063: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3652 - acc: 0.8803 - val_loss: 1.3794 - val_acc: 0.6567\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8802\n",
      "Epoch 00064: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3676 - acc: 0.8802 - val_loss: 1.0733 - val_acc: 0.7123\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8843\n",
      "Epoch 00065: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3559 - acc: 0.8844 - val_loss: 1.1711 - val_acc: 0.6956\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8839\n",
      "Epoch 00066: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3592 - acc: 0.8837 - val_loss: 1.0816 - val_acc: 0.7058\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8864\n",
      "Epoch 00067: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3544 - acc: 0.8865 - val_loss: 1.1278 - val_acc: 0.7060\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8857\n",
      "Epoch 00068: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3542 - acc: 0.8858 - val_loss: 1.0580 - val_acc: 0.7249\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8862\n",
      "Epoch 00069: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3487 - acc: 0.8863 - val_loss: 1.1143 - val_acc: 0.7084\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8884\n",
      "Epoch 00070: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3403 - acc: 0.8883 - val_loss: 1.0473 - val_acc: 0.7286\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8872\n",
      "Epoch 00071: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3416 - acc: 0.8873 - val_loss: 1.0478 - val_acc: 0.7293\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.8906\n",
      "Epoch 00072: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3368 - acc: 0.8906 - val_loss: 1.0644 - val_acc: 0.7202\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8915\n",
      "Epoch 00073: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3356 - acc: 0.8915 - val_loss: 1.0876 - val_acc: 0.7172\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8916\n",
      "Epoch 00074: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3348 - acc: 0.8916 - val_loss: 1.0324 - val_acc: 0.7358\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8942\n",
      "Epoch 00075: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.3260 - acc: 0.8941 - val_loss: 1.0564 - val_acc: 0.7270\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8974\n",
      "Epoch 00076: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3226 - acc: 0.8975 - val_loss: 1.0914 - val_acc: 0.7154\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8962\n",
      "Epoch 00077: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3183 - acc: 0.8961 - val_loss: 1.2613 - val_acc: 0.6806\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8915\n",
      "Epoch 00078: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.3281 - acc: 0.8915 - val_loss: 1.0317 - val_acc: 0.7314\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8962\n",
      "Epoch 00079: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3218 - acc: 0.8960 - val_loss: 1.0412 - val_acc: 0.7284\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9000\n",
      "Epoch 00080: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3107 - acc: 0.8999 - val_loss: 1.0851 - val_acc: 0.7205\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8994\n",
      "Epoch 00081: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3126 - acc: 0.8994 - val_loss: 1.0545 - val_acc: 0.7303\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8976\n",
      "Epoch 00082: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3165 - acc: 0.8977 - val_loss: 1.1357 - val_acc: 0.7093\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9005\n",
      "Epoch 00083: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3096 - acc: 0.9004 - val_loss: 1.1842 - val_acc: 0.6890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9016\n",
      "Epoch 00084: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3071 - acc: 0.9015 - val_loss: 1.1294 - val_acc: 0.7151\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9005\n",
      "Epoch 00085: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3017 - acc: 0.9003 - val_loss: 1.2162 - val_acc: 0.6925\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9034\n",
      "Epoch 00086: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2955 - acc: 0.9032 - val_loss: 1.0474 - val_acc: 0.7317\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9052\n",
      "Epoch 00087: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.2938 - acc: 0.9053 - val_loss: 1.0864 - val_acc: 0.7144\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9023\n",
      "Epoch 00088: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2976 - acc: 0.9023 - val_loss: 1.1132 - val_acc: 0.7209\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9052\n",
      "Epoch 00089: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2919 - acc: 0.9053 - val_loss: 1.0977 - val_acc: 0.7167\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9050\n",
      "Epoch 00090: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2877 - acc: 0.9050 - val_loss: 1.0490 - val_acc: 0.7314\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9058\n",
      "Epoch 00091: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2857 - acc: 0.9058 - val_loss: 1.0533 - val_acc: 0.7261\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9075\n",
      "Epoch 00092: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2867 - acc: 0.9075 - val_loss: 1.0689 - val_acc: 0.7275\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9089\n",
      "Epoch 00093: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2823 - acc: 0.9089 - val_loss: 1.0690 - val_acc: 0.7265\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9090\n",
      "Epoch 00094: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2818 - acc: 0.9091 - val_loss: 1.0577 - val_acc: 0.7317\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9086\n",
      "Epoch 00095: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2805 - acc: 0.9086 - val_loss: 1.0181 - val_acc: 0.7426\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9079\n",
      "Epoch 00096: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2818 - acc: 0.9079 - val_loss: 1.0131 - val_acc: 0.7491\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9090\n",
      "Epoch 00097: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2811 - acc: 0.9091 - val_loss: 1.0482 - val_acc: 0.7326\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9113\n",
      "Epoch 00098: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2765 - acc: 0.9113 - val_loss: 1.0474 - val_acc: 0.7305\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9122\n",
      "Epoch 00099: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2696 - acc: 0.9122 - val_loss: 1.0323 - val_acc: 0.7470\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9140\n",
      "Epoch 00100: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2711 - acc: 0.9140 - val_loss: 1.0464 - val_acc: 0.7365\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9109\n",
      "Epoch 00101: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2714 - acc: 0.9109 - val_loss: 1.1315 - val_acc: 0.7219\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9129\n",
      "Epoch 00102: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2687 - acc: 0.9129 - val_loss: 1.0239 - val_acc: 0.7396\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9138\n",
      "Epoch 00103: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2686 - acc: 0.9138 - val_loss: 1.0817 - val_acc: 0.7286\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9152\n",
      "Epoch 00104: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2616 - acc: 0.9152 - val_loss: 1.0339 - val_acc: 0.7421\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9137\n",
      "Epoch 00105: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2654 - acc: 0.9137 - val_loss: 1.0809 - val_acc: 0.7305\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9135\n",
      "Epoch 00106: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2658 - acc: 0.9134 - val_loss: 1.0473 - val_acc: 0.7365\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9165\n",
      "Epoch 00107: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2567 - acc: 0.9165 - val_loss: 1.0785 - val_acc: 0.7305\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9168\n",
      "Epoch 00108: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2577 - acc: 0.9168 - val_loss: 1.0866 - val_acc: 0.7368\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9163\n",
      "Epoch 00109: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2586 - acc: 0.9163 - val_loss: 1.0626 - val_acc: 0.7375\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9171\n",
      "Epoch 00110: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2577 - acc: 0.9170 - val_loss: 1.0328 - val_acc: 0.7456\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9194\n",
      "Epoch 00111: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2524 - acc: 0.9194 - val_loss: 1.0604 - val_acc: 0.7412\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9189\n",
      "Epoch 00112: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2529 - acc: 0.9188 - val_loss: 1.0284 - val_acc: 0.7424\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9186\n",
      "Epoch 00113: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2489 - acc: 0.9185 - val_loss: 1.0853 - val_acc: 0.7296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9207\n",
      "Epoch 00114: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2459 - acc: 0.9207 - val_loss: 1.0372 - val_acc: 0.7496\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9190\n",
      "Epoch 00115: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2543 - acc: 0.9190 - val_loss: 1.0368 - val_acc: 0.7454\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9222\n",
      "Epoch 00116: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2424 - acc: 0.9222 - val_loss: 1.0591 - val_acc: 0.7428\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9226\n",
      "Epoch 00117: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2426 - acc: 0.9226 - val_loss: 1.0391 - val_acc: 0.7391\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9235\n",
      "Epoch 00118: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2396 - acc: 0.9234 - val_loss: 1.1025 - val_acc: 0.7286\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9224\n",
      "Epoch 00119: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2397 - acc: 0.9225 - val_loss: 1.0567 - val_acc: 0.7452\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9209\n",
      "Epoch 00120: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2409 - acc: 0.9210 - val_loss: 1.0350 - val_acc: 0.7501\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9228\n",
      "Epoch 00121: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2390 - acc: 0.9228 - val_loss: 1.0627 - val_acc: 0.7403\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9232\n",
      "Epoch 00122: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2382 - acc: 0.9232 - val_loss: 1.0843 - val_acc: 0.7379\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9256\n",
      "Epoch 00123: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2344 - acc: 0.9254 - val_loss: 1.0902 - val_acc: 0.7382\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9250\n",
      "Epoch 00124: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2332 - acc: 0.9250 - val_loss: 1.0616 - val_acc: 0.7375\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9281\n",
      "Epoch 00125: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2264 - acc: 0.9281 - val_loss: 1.0809 - val_acc: 0.7405\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9241\n",
      "Epoch 00126: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2342 - acc: 0.9242 - val_loss: 1.0553 - val_acc: 0.7428\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9263\n",
      "Epoch 00127: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2288 - acc: 0.9263 - val_loss: 1.1062 - val_acc: 0.7340\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9271\n",
      "Epoch 00128: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2292 - acc: 0.9271 - val_loss: 1.0660 - val_acc: 0.7384\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9293\n",
      "Epoch 00129: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2248 - acc: 0.9292 - val_loss: 1.0527 - val_acc: 0.7454\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9265\n",
      "Epoch 00130: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2304 - acc: 0.9265 - val_loss: 1.1273 - val_acc: 0.7263\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9285\n",
      "Epoch 00131: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2220 - acc: 0.9285 - val_loss: 1.0400 - val_acc: 0.7461\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9244\n",
      "Epoch 00132: val_loss did not improve from 1.00592\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2305 - acc: 0.9243 - val_loss: 1.0691 - val_acc: 0.7412\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 240us/sample - loss: 1.1109 - acc: 0.6725\n",
      "Loss: 1.1108930741143745 Accuracy: 0.67248183\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6571 - acc: 0.2215\n",
      "Epoch 00001: val_loss improved from inf to 1.89579, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/001-1.8958.hdf5\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 2.6571 - acc: 0.2216 - val_loss: 1.8958 - val_acc: 0.3692\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8111 - acc: 0.4259\n",
      "Epoch 00002: val_loss improved from 1.89579 to 1.34169, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/002-1.3417.hdf5\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 1.8111 - acc: 0.4259 - val_loss: 1.3417 - val_acc: 0.5700\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5279 - acc: 0.5145\n",
      "Epoch 00003: val_loss improved from 1.34169 to 1.20394, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/003-1.2039.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 1.5279 - acc: 0.5145 - val_loss: 1.2039 - val_acc: 0.6240\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3693 - acc: 0.5682\n",
      "Epoch 00004: val_loss improved from 1.20394 to 1.12122, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/004-1.1212.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 1.3689 - acc: 0.5682 - val_loss: 1.1212 - val_acc: 0.6504\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2522 - acc: 0.6075\n",
      "Epoch 00005: val_loss improved from 1.12122 to 1.08410, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/005-1.0841.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 1.2521 - acc: 0.6074 - val_loss: 1.0841 - val_acc: 0.6646\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1706 - acc: 0.6347\n",
      "Epoch 00006: val_loss did not improve from 1.08410\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 1.1705 - acc: 0.6347 - val_loss: 1.1380 - val_acc: 0.6445\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.6545\n",
      "Epoch 00007: val_loss improved from 1.08410 to 0.97902, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/007-0.9790.hdf5\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 1.1041 - acc: 0.6546 - val_loss: 0.9790 - val_acc: 0.7102\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0538 - acc: 0.6733\n",
      "Epoch 00008: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 1.0541 - acc: 0.6731 - val_loss: 0.9839 - val_acc: 0.7002\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.6897\n",
      "Epoch 00009: val_loss improved from 0.97902 to 0.94265, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/009-0.9427.hdf5\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.9999 - acc: 0.6898 - val_loss: 0.9427 - val_acc: 0.7205\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9701 - acc: 0.6994\n",
      "Epoch 00010: val_loss improved from 0.94265 to 0.91761, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/010-0.9176.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.9704 - acc: 0.6993 - val_loss: 0.9176 - val_acc: 0.7256\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.7124\n",
      "Epoch 00011: val_loss improved from 0.91761 to 0.89079, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/011-0.8908.hdf5\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.9296 - acc: 0.7125 - val_loss: 0.8908 - val_acc: 0.7482\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8939 - acc: 0.7211\n",
      "Epoch 00012: val_loss improved from 0.89079 to 0.87226, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/012-0.8723.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.8940 - acc: 0.7212 - val_loss: 0.8723 - val_acc: 0.7414\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8676 - acc: 0.7313\n",
      "Epoch 00013: val_loss improved from 0.87226 to 0.86846, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/013-0.8685.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.8679 - acc: 0.7312 - val_loss: 0.8685 - val_acc: 0.7484\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7424\n",
      "Epoch 00014: val_loss did not improve from 0.86846\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.8385 - acc: 0.7423 - val_loss: 0.8759 - val_acc: 0.7403\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7479\n",
      "Epoch 00015: val_loss improved from 0.86846 to 0.85071, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/015-0.8507.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.8147 - acc: 0.7479 - val_loss: 0.8507 - val_acc: 0.7549\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7952 - acc: 0.7525\n",
      "Epoch 00016: val_loss did not improve from 0.85071\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.7951 - acc: 0.7525 - val_loss: 0.8655 - val_acc: 0.7410\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7714 - acc: 0.7603\n",
      "Epoch 00017: val_loss improved from 0.85071 to 0.82999, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/017-0.8300.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.7714 - acc: 0.7604 - val_loss: 0.8300 - val_acc: 0.7603\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7662\n",
      "Epoch 00018: val_loss did not improve from 0.82999\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.7508 - acc: 0.7661 - val_loss: 0.8860 - val_acc: 0.7396\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7288 - acc: 0.7744\n",
      "Epoch 00019: val_loss did not improve from 0.82999\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.7293 - acc: 0.7743 - val_loss: 0.8479 - val_acc: 0.7519\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7202 - acc: 0.7765\n",
      "Epoch 00020: val_loss did not improve from 0.82999\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.7198 - acc: 0.7767 - val_loss: 0.8303 - val_acc: 0.7638\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6960 - acc: 0.7838\n",
      "Epoch 00021: val_loss did not improve from 0.82999\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.6961 - acc: 0.7838 - val_loss: 0.8366 - val_acc: 0.7587\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.7904\n",
      "Epoch 00022: val_loss improved from 0.82999 to 0.80869, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/022-0.8087.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.6792 - acc: 0.7903 - val_loss: 0.8087 - val_acc: 0.7685\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6681 - acc: 0.7904\n",
      "Epoch 00023: val_loss did not improve from 0.80869\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.6680 - acc: 0.7904 - val_loss: 0.8669 - val_acc: 0.7470\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.7965\n",
      "Epoch 00024: val_loss did not improve from 0.80869\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.6518 - acc: 0.7965 - val_loss: 0.8464 - val_acc: 0.7547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6387 - acc: 0.8008\n",
      "Epoch 00025: val_loss did not improve from 0.80869\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.6387 - acc: 0.8008 - val_loss: 0.8426 - val_acc: 0.7573\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6272 - acc: 0.8043\n",
      "Epoch 00026: val_loss did not improve from 0.80869\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.6275 - acc: 0.8042 - val_loss: 0.8335 - val_acc: 0.7652\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8115\n",
      "Epoch 00027: val_loss improved from 0.80869 to 0.76895, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/027-0.7689.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.6107 - acc: 0.8114 - val_loss: 0.7689 - val_acc: 0.7899\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8136\n",
      "Epoch 00028: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.5969 - acc: 0.8134 - val_loss: 0.7841 - val_acc: 0.7773\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5924 - acc: 0.8134\n",
      "Epoch 00029: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.5926 - acc: 0.8133 - val_loss: 0.8275 - val_acc: 0.7652\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5775 - acc: 0.8191\n",
      "Epoch 00030: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.5773 - acc: 0.8192 - val_loss: 0.9245 - val_acc: 0.7342\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.8241\n",
      "Epoch 00031: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.5632 - acc: 0.8242 - val_loss: 0.8116 - val_acc: 0.7685\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.8270\n",
      "Epoch 00032: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.5534 - acc: 0.8270 - val_loss: 0.7930 - val_acc: 0.7859\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8318\n",
      "Epoch 00033: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.5449 - acc: 0.8317 - val_loss: 0.7838 - val_acc: 0.7761\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.8320\n",
      "Epoch 00034: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.5352 - acc: 0.8319 - val_loss: 0.7753 - val_acc: 0.7775\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8343\n",
      "Epoch 00035: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.5277 - acc: 0.8343 - val_loss: 0.8139 - val_acc: 0.7717\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8390\n",
      "Epoch 00036: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.5169 - acc: 0.8390 - val_loss: 0.8427 - val_acc: 0.7580\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8390\n",
      "Epoch 00037: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.5155 - acc: 0.8391 - val_loss: 0.7866 - val_acc: 0.7785\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8386\n",
      "Epoch 00038: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.5061 - acc: 0.8386 - val_loss: 0.8568 - val_acc: 0.7566\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4951 - acc: 0.8428\n",
      "Epoch 00039: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.4953 - acc: 0.8429 - val_loss: 0.7854 - val_acc: 0.7789\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8455\n",
      "Epoch 00040: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.4881 - acc: 0.8455 - val_loss: 0.7953 - val_acc: 0.7754\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8484\n",
      "Epoch 00041: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.4802 - acc: 0.8484 - val_loss: 0.8110 - val_acc: 0.7724\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.8505\n",
      "Epoch 00042: val_loss did not improve from 0.76895\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.4755 - acc: 0.8505 - val_loss: 0.8009 - val_acc: 0.7775\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8552\n",
      "Epoch 00043: val_loss improved from 0.76895 to 0.75016, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/043-0.7502.hdf5\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.4599 - acc: 0.8551 - val_loss: 0.7502 - val_acc: 0.7934\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8566\n",
      "Epoch 00044: val_loss did not improve from 0.75016\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.4563 - acc: 0.8566 - val_loss: 0.7596 - val_acc: 0.7890\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8599\n",
      "Epoch 00045: val_loss did not improve from 0.75016\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.4455 - acc: 0.8599 - val_loss: 0.7631 - val_acc: 0.7904\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4440 - acc: 0.8571\n",
      "Epoch 00046: val_loss did not improve from 0.75016\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.4440 - acc: 0.8571 - val_loss: 0.7774 - val_acc: 0.7848\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8621\n",
      "Epoch 00047: val_loss improved from 0.75016 to 0.74259, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/047-0.7426.hdf5\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.4399 - acc: 0.8621 - val_loss: 0.7426 - val_acc: 0.7929\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8617\n",
      "Epoch 00048: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.4347 - acc: 0.8616 - val_loss: 0.7626 - val_acc: 0.7843\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8625\n",
      "Epoch 00049: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.4254 - acc: 0.8624 - val_loss: 0.8506 - val_acc: 0.7605\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8647\n",
      "Epoch 00050: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.4233 - acc: 0.8648 - val_loss: 0.7849 - val_acc: 0.7773\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8695\n",
      "Epoch 00051: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.4113 - acc: 0.8693 - val_loss: 0.8785 - val_acc: 0.7556\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8696\n",
      "Epoch 00052: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.4129 - acc: 0.8696 - val_loss: 0.8361 - val_acc: 0.7699\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8762\n",
      "Epoch 00053: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3973 - acc: 0.8762 - val_loss: 0.7562 - val_acc: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8721\n",
      "Epoch 00054: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.3995 - acc: 0.8721 - val_loss: 0.8056 - val_acc: 0.7817\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8763\n",
      "Epoch 00055: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3864 - acc: 0.8763 - val_loss: 0.8366 - val_acc: 0.7708\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8744\n",
      "Epoch 00056: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.3883 - acc: 0.8743 - val_loss: 0.8197 - val_acc: 0.7761\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8785\n",
      "Epoch 00057: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.3789 - acc: 0.8786 - val_loss: 0.7904 - val_acc: 0.7885\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8789\n",
      "Epoch 00058: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3752 - acc: 0.8789 - val_loss: 0.7816 - val_acc: 0.7829\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8781\n",
      "Epoch 00059: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3747 - acc: 0.8781 - val_loss: 0.8350 - val_acc: 0.7754\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8807\n",
      "Epoch 00060: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.3729 - acc: 0.8806 - val_loss: 0.7542 - val_acc: 0.7964\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8823\n",
      "Epoch 00061: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.3668 - acc: 0.8823 - val_loss: 1.7985 - val_acc: 0.6084\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8860\n",
      "Epoch 00062: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3553 - acc: 0.8859 - val_loss: 0.8122 - val_acc: 0.7778\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8858\n",
      "Epoch 00063: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.3563 - acc: 0.8855 - val_loss: 0.7678 - val_acc: 0.7929\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8874\n",
      "Epoch 00064: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3516 - acc: 0.8874 - val_loss: 0.7676 - val_acc: 0.7897\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8886\n",
      "Epoch 00065: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.3517 - acc: 0.8885 - val_loss: 0.7775 - val_acc: 0.7885\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8884\n",
      "Epoch 00066: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.3438 - acc: 0.8884 - val_loss: 0.8625 - val_acc: 0.7675\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8914\n",
      "Epoch 00067: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.3409 - acc: 0.8914 - val_loss: 0.7580 - val_acc: 0.7929\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8920\n",
      "Epoch 00068: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3353 - acc: 0.8918 - val_loss: 0.7985 - val_acc: 0.7873\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8898\n",
      "Epoch 00069: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3426 - acc: 0.8899 - val_loss: 0.8780 - val_acc: 0.7694\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.8967\n",
      "Epoch 00070: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3241 - acc: 0.8967 - val_loss: 0.7576 - val_acc: 0.7929\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.8954\n",
      "Epoch 00071: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3274 - acc: 0.8954 - val_loss: 0.7856 - val_acc: 0.7869\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8960\n",
      "Epoch 00072: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.3255 - acc: 0.8960 - val_loss: 0.8347 - val_acc: 0.7754\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.8982\n",
      "Epoch 00073: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.3152 - acc: 0.8982 - val_loss: 0.7441 - val_acc: 0.8001\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8961\n",
      "Epoch 00074: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.3215 - acc: 0.8962 - val_loss: 0.7623 - val_acc: 0.8001\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8985\n",
      "Epoch 00075: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.3159 - acc: 0.8984 - val_loss: 0.7696 - val_acc: 0.7997\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9020\n",
      "Epoch 00076: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.3071 - acc: 0.9020 - val_loss: 0.7711 - val_acc: 0.7971\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9001\n",
      "Epoch 00077: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.3105 - acc: 0.9001 - val_loss: 0.7667 - val_acc: 0.7973\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9050\n",
      "Epoch 00078: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2977 - acc: 0.9050 - val_loss: 0.8176 - val_acc: 0.7864\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9028\n",
      "Epoch 00079: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.3015 - acc: 0.9028 - val_loss: 0.8320 - val_acc: 0.7757\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9020\n",
      "Epoch 00080: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.3033 - acc: 0.9021 - val_loss: 0.9063 - val_acc: 0.7657\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9071\n",
      "Epoch 00081: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2905 - acc: 0.9071 - val_loss: 0.7995 - val_acc: 0.7897\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9044\n",
      "Epoch 00082: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2989 - acc: 0.9044 - val_loss: 0.7886 - val_acc: 0.7866\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9060\n",
      "Epoch 00083: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.2927 - acc: 0.9060 - val_loss: 0.7985 - val_acc: 0.7904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9062\n",
      "Epoch 00084: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2869 - acc: 0.9062 - val_loss: 0.8166 - val_acc: 0.7848\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9079\n",
      "Epoch 00085: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.2819 - acc: 0.9079 - val_loss: 0.9124 - val_acc: 0.7501\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9084\n",
      "Epoch 00086: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2823 - acc: 0.9081 - val_loss: 0.7641 - val_acc: 0.7992\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9104\n",
      "Epoch 00087: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2805 - acc: 0.9104 - val_loss: 0.7569 - val_acc: 0.8015\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9106\n",
      "Epoch 00088: val_loss did not improve from 0.74259\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2772 - acc: 0.9106 - val_loss: 0.7687 - val_acc: 0.7971\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9093\n",
      "Epoch 00089: val_loss improved from 0.74259 to 0.73897, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/089-0.7390.hdf5\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2808 - acc: 0.9093 - val_loss: 0.7390 - val_acc: 0.7983\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9110\n",
      "Epoch 00090: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2772 - acc: 0.9110 - val_loss: 0.7973 - val_acc: 0.7876\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9121\n",
      "Epoch 00091: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.2707 - acc: 0.9122 - val_loss: 0.7819 - val_acc: 0.7873\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9133\n",
      "Epoch 00092: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2705 - acc: 0.9133 - val_loss: 1.0892 - val_acc: 0.7342\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9125\n",
      "Epoch 00093: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2687 - acc: 0.9125 - val_loss: 0.7763 - val_acc: 0.7990\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9153\n",
      "Epoch 00094: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2594 - acc: 0.9153 - val_loss: 0.8254 - val_acc: 0.7829\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9154\n",
      "Epoch 00095: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.2665 - acc: 0.9154 - val_loss: 0.8378 - val_acc: 0.7796\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9159\n",
      "Epoch 00096: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2588 - acc: 0.9159 - val_loss: 0.7767 - val_acc: 0.7934\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9168\n",
      "Epoch 00097: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2591 - acc: 0.9168 - val_loss: 0.7873 - val_acc: 0.7952\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9184\n",
      "Epoch 00098: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2542 - acc: 0.9185 - val_loss: 0.7712 - val_acc: 0.7992\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9180\n",
      "Epoch 00099: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2540 - acc: 0.9181 - val_loss: 0.7615 - val_acc: 0.7997\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9175\n",
      "Epoch 00100: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2540 - acc: 0.9175 - val_loss: 0.8106 - val_acc: 0.7899\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9171\n",
      "Epoch 00101: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2537 - acc: 0.9172 - val_loss: 0.7775 - val_acc: 0.7978\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9196\n",
      "Epoch 00102: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 0.2472 - acc: 0.9194 - val_loss: 0.7899 - val_acc: 0.7913\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9204\n",
      "Epoch 00103: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2429 - acc: 0.9204 - val_loss: 0.8246 - val_acc: 0.7843\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9201\n",
      "Epoch 00104: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2427 - acc: 0.9201 - val_loss: 0.7978 - val_acc: 0.7929\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9216\n",
      "Epoch 00105: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2446 - acc: 0.9217 - val_loss: 0.8055 - val_acc: 0.7864\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9235\n",
      "Epoch 00106: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.2381 - acc: 0.9235 - val_loss: 0.8462 - val_acc: 0.7817\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9253\n",
      "Epoch 00107: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2387 - acc: 0.9253 - val_loss: 0.7545 - val_acc: 0.8020\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9256\n",
      "Epoch 00108: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2358 - acc: 0.9257 - val_loss: 0.7973 - val_acc: 0.7922\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9236\n",
      "Epoch 00109: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2349 - acc: 0.9236 - val_loss: 0.7769 - val_acc: 0.7962\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9239\n",
      "Epoch 00110: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2365 - acc: 0.9240 - val_loss: 0.8875 - val_acc: 0.7729\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9257\n",
      "Epoch 00111: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.2316 - acc: 0.9257 - val_loss: 0.7852 - val_acc: 0.8006\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9271\n",
      "Epoch 00112: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.7938 - val_acc: 0.8027\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9257\n",
      "Epoch 00113: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.2293 - acc: 0.9258 - val_loss: 0.8533 - val_acc: 0.7817\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9255\n",
      "Epoch 00114: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.2302 - acc: 0.9255 - val_loss: 0.8109 - val_acc: 0.7934\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9268\n",
      "Epoch 00115: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.2253 - acc: 0.9268 - val_loss: 0.7798 - val_acc: 0.8048\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9285\n",
      "Epoch 00116: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.2212 - acc: 0.9285 - val_loss: 0.8268 - val_acc: 0.7850\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9278\n",
      "Epoch 00117: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2206 - acc: 0.9279 - val_loss: 0.8048 - val_acc: 0.7941\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9283\n",
      "Epoch 00118: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 0.2226 - acc: 0.9283 - val_loss: 0.7826 - val_acc: 0.8027\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9282\n",
      "Epoch 00119: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2229 - acc: 0.9281 - val_loss: 0.7759 - val_acc: 0.8050\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9290\n",
      "Epoch 00120: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.2171 - acc: 0.9291 - val_loss: 0.8351 - val_acc: 0.7892\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9308\n",
      "Epoch 00121: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.2145 - acc: 0.9309 - val_loss: 0.7872 - val_acc: 0.8027\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9308\n",
      "Epoch 00122: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.2174 - acc: 0.9308 - val_loss: 0.8402 - val_acc: 0.7876\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9317\n",
      "Epoch 00123: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.2130 - acc: 0.9317 - val_loss: 0.7810 - val_acc: 0.8006\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9322\n",
      "Epoch 00124: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.2087 - acc: 0.9321 - val_loss: 0.7868 - val_acc: 0.8013\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9316\n",
      "Epoch 00125: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.2141 - acc: 0.9316 - val_loss: 0.9332 - val_acc: 0.7717\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9340\n",
      "Epoch 00126: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2082 - acc: 0.9340 - val_loss: 0.8266 - val_acc: 0.7948\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9321\n",
      "Epoch 00127: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2065 - acc: 0.9321 - val_loss: 0.8425 - val_acc: 0.7952\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9358\n",
      "Epoch 00128: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.2001 - acc: 0.9358 - val_loss: 0.7794 - val_acc: 0.8053\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9330\n",
      "Epoch 00129: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2052 - acc: 0.9331 - val_loss: 0.8348 - val_acc: 0.7887\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9348\n",
      "Epoch 00130: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.2051 - acc: 0.9348 - val_loss: 0.7718 - val_acc: 0.8074\n",
      "Epoch 131/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9349\n",
      "Epoch 00131: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2008 - acc: 0.9349 - val_loss: 0.7805 - val_acc: 0.8095\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9362\n",
      "Epoch 00132: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1979 - acc: 0.9361 - val_loss: 0.7795 - val_acc: 0.8106\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9365\n",
      "Epoch 00133: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2033 - acc: 0.9365 - val_loss: 0.7558 - val_acc: 0.8127\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9349\n",
      "Epoch 00134: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.2036 - acc: 0.9349 - val_loss: 0.7979 - val_acc: 0.8050\n",
      "Epoch 135/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9346\n",
      "Epoch 00135: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.2045 - acc: 0.9345 - val_loss: 0.7977 - val_acc: 0.7994\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9360\n",
      "Epoch 00136: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1971 - acc: 0.9360 - val_loss: 0.9589 - val_acc: 0.7696\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9370\n",
      "Epoch 00137: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1988 - acc: 0.9369 - val_loss: 0.9767 - val_acc: 0.7657\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9365\n",
      "Epoch 00138: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1977 - acc: 0.9365 - val_loss: 0.7717 - val_acc: 0.8097\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9370\n",
      "Epoch 00139: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1910 - acc: 0.9370 - val_loss: 0.8165 - val_acc: 0.7994\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9376\n",
      "Epoch 00140: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1912 - acc: 0.9375 - val_loss: 0.8194 - val_acc: 0.7934\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9389\n",
      "Epoch 00141: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1934 - acc: 0.9389 - val_loss: 0.7744 - val_acc: 0.8076\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9414\n",
      "Epoch 00142: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1871 - acc: 0.9413 - val_loss: 0.8496 - val_acc: 0.7897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9386\n",
      "Epoch 00143: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1907 - acc: 0.9386 - val_loss: 0.8263 - val_acc: 0.8001\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9398\n",
      "Epoch 00144: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1886 - acc: 0.9398 - val_loss: 0.7980 - val_acc: 0.8004\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9399\n",
      "Epoch 00145: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1863 - acc: 0.9399 - val_loss: 0.8095 - val_acc: 0.7962\n",
      "Epoch 146/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9422\n",
      "Epoch 00146: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1829 - acc: 0.9421 - val_loss: 0.8244 - val_acc: 0.7945\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9404\n",
      "Epoch 00147: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1860 - acc: 0.9404 - val_loss: 0.8256 - val_acc: 0.7978\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9393\n",
      "Epoch 00148: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1879 - acc: 0.9391 - val_loss: 0.7855 - val_acc: 0.8104\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9408\n",
      "Epoch 00149: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.1855 - acc: 0.9409 - val_loss: 0.7863 - val_acc: 0.8102\n",
      "Epoch 150/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9428\n",
      "Epoch 00150: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1833 - acc: 0.9428 - val_loss: 1.0645 - val_acc: 0.7559\n",
      "Epoch 151/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9420\n",
      "Epoch 00151: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1833 - acc: 0.9419 - val_loss: 0.7766 - val_acc: 0.8090\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9418\n",
      "Epoch 00152: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1787 - acc: 0.9418 - val_loss: 0.7910 - val_acc: 0.8055\n",
      "Epoch 153/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9407\n",
      "Epoch 00153: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1829 - acc: 0.9406 - val_loss: 0.9646 - val_acc: 0.7661\n",
      "Epoch 154/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9449\n",
      "Epoch 00154: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1768 - acc: 0.9449 - val_loss: 0.8625 - val_acc: 0.7894\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9425\n",
      "Epoch 00155: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1811 - acc: 0.9425 - val_loss: 0.8480 - val_acc: 0.8046\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9426\n",
      "Epoch 00156: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1794 - acc: 0.9426 - val_loss: 0.7835 - val_acc: 0.8092\n",
      "Epoch 157/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9449\n",
      "Epoch 00157: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1713 - acc: 0.9449 - val_loss: 0.9164 - val_acc: 0.7862\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9455\n",
      "Epoch 00158: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1721 - acc: 0.9454 - val_loss: 0.8050 - val_acc: 0.8076\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9444\n",
      "Epoch 00159: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.1721 - acc: 0.9444 - val_loss: 0.8276 - val_acc: 0.8004\n",
      "Epoch 160/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9445\n",
      "Epoch 00160: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1767 - acc: 0.9445 - val_loss: 0.8196 - val_acc: 0.7999\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9427\n",
      "Epoch 00161: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1752 - acc: 0.9427 - val_loss: 0.8477 - val_acc: 0.7932\n",
      "Epoch 162/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9451\n",
      "Epoch 00162: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.1718 - acc: 0.9451 - val_loss: 0.7984 - val_acc: 0.8104\n",
      "Epoch 163/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9452\n",
      "Epoch 00163: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1701 - acc: 0.9453 - val_loss: 0.8337 - val_acc: 0.7994\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9451\n",
      "Epoch 00164: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1718 - acc: 0.9450 - val_loss: 0.8302 - val_acc: 0.8011\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9449\n",
      "Epoch 00165: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1767 - acc: 0.9449 - val_loss: 0.8046 - val_acc: 0.8008\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9470\n",
      "Epoch 00166: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1673 - acc: 0.9470 - val_loss: 0.8606 - val_acc: 0.7918\n",
      "Epoch 167/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9467\n",
      "Epoch 00167: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1670 - acc: 0.9468 - val_loss: 0.8586 - val_acc: 0.7952\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9445\n",
      "Epoch 00168: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1723 - acc: 0.9445 - val_loss: 0.8129 - val_acc: 0.7978\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9477\n",
      "Epoch 00169: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1652 - acc: 0.9476 - val_loss: 0.7786 - val_acc: 0.8106\n",
      "Epoch 170/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9467\n",
      "Epoch 00170: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1672 - acc: 0.9467 - val_loss: 0.7912 - val_acc: 0.8090\n",
      "Epoch 171/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9496\n",
      "Epoch 00171: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1627 - acc: 0.9496 - val_loss: 0.7957 - val_acc: 0.8062\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9467\n",
      "Epoch 00172: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1646 - acc: 0.9467 - val_loss: 0.7827 - val_acc: 0.8109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9475\n",
      "Epoch 00173: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.1626 - acc: 0.9474 - val_loss: 0.8233 - val_acc: 0.8001\n",
      "Epoch 174/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9441\n",
      "Epoch 00174: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1735 - acc: 0.9441 - val_loss: 0.8282 - val_acc: 0.7950\n",
      "Epoch 175/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9490\n",
      "Epoch 00175: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1614 - acc: 0.9488 - val_loss: 0.7923 - val_acc: 0.8090\n",
      "Epoch 176/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9515\n",
      "Epoch 00176: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1558 - acc: 0.9514 - val_loss: 0.8685 - val_acc: 0.7913\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9469\n",
      "Epoch 00177: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1671 - acc: 0.9470 - val_loss: 0.8956 - val_acc: 0.7876\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9475\n",
      "Epoch 00178: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.1621 - acc: 0.9474 - val_loss: 0.8351 - val_acc: 0.7997\n",
      "Epoch 179/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9474\n",
      "Epoch 00179: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1622 - acc: 0.9473 - val_loss: 0.8105 - val_acc: 0.8011\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9511\n",
      "Epoch 00180: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.1555 - acc: 0.9511 - val_loss: 0.8129 - val_acc: 0.8109\n",
      "Epoch 181/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9486\n",
      "Epoch 00181: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.1577 - acc: 0.9487 - val_loss: 0.7731 - val_acc: 0.8097\n",
      "Epoch 182/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9497\n",
      "Epoch 00182: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.1582 - acc: 0.9498 - val_loss: 0.8410 - val_acc: 0.7980\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9533\n",
      "Epoch 00183: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.1508 - acc: 0.9533 - val_loss: 1.0285 - val_acc: 0.7692\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9510\n",
      "Epoch 00184: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1535 - acc: 0.9510 - val_loss: 0.9191 - val_acc: 0.7890\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9493\n",
      "Epoch 00185: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1588 - acc: 0.9493 - val_loss: 0.8479 - val_acc: 0.7987\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9526\n",
      "Epoch 00186: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.1519 - acc: 0.9526 - val_loss: 0.8340 - val_acc: 0.7964\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9512\n",
      "Epoch 00187: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.1556 - acc: 0.9512 - val_loss: 0.7960 - val_acc: 0.8083\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9511\n",
      "Epoch 00188: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1528 - acc: 0.9511 - val_loss: 0.8215 - val_acc: 0.8036\n",
      "Epoch 189/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9490\n",
      "Epoch 00189: val_loss did not improve from 0.73897\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1565 - acc: 0.9490 - val_loss: 0.8922 - val_acc: 0.7955\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 275us/sample - loss: 0.8090 - acc: 0.7807\n",
      "Loss: 0.8090206719880783 Accuracy: 0.78068537\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4695 - acc: 0.2629\n",
      "Epoch 00001: val_loss improved from inf to 1.69677, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/001-1.6968.hdf5\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 2.4693 - acc: 0.2629 - val_loss: 1.6968 - val_acc: 0.4559\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6346 - acc: 0.4826\n",
      "Epoch 00002: val_loss improved from 1.69677 to 1.21809, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/002-1.2181.hdf5\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 1.6346 - acc: 0.4826 - val_loss: 1.2181 - val_acc: 0.6382\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3654 - acc: 0.5696\n",
      "Epoch 00003: val_loss improved from 1.21809 to 1.07142, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/003-1.0714.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.3655 - acc: 0.5696 - val_loss: 1.0714 - val_acc: 0.6916\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1929 - acc: 0.6279\n",
      "Epoch 00004: val_loss improved from 1.07142 to 0.95601, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/004-0.9560.hdf5\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 1.1931 - acc: 0.6279 - val_loss: 0.9560 - val_acc: 0.7130\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0698 - acc: 0.6689\n",
      "Epoch 00005: val_loss improved from 0.95601 to 0.88669, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/005-0.8867.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 1.0698 - acc: 0.6689 - val_loss: 0.8867 - val_acc: 0.7372\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.7013\n",
      "Epoch 00006: val_loss improved from 0.88669 to 0.81904, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/006-0.8190.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.9674 - acc: 0.7013 - val_loss: 0.8190 - val_acc: 0.7650\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8964 - acc: 0.7260\n",
      "Epoch 00007: val_loss improved from 0.81904 to 0.78079, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/007-0.7808.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.8963 - acc: 0.7260 - val_loss: 0.7808 - val_acc: 0.7803\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7458\n",
      "Epoch 00008: val_loss improved from 0.78079 to 0.73942, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/008-0.7394.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.8340 - acc: 0.7458 - val_loss: 0.7394 - val_acc: 0.7906\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.7652\n",
      "Epoch 00009: val_loss did not improve from 0.73942\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.7787 - acc: 0.7651 - val_loss: 0.7926 - val_acc: 0.7675\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7765\n",
      "Epoch 00010: val_loss improved from 0.73942 to 0.72122, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/010-0.7212.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.7396 - acc: 0.7764 - val_loss: 0.7212 - val_acc: 0.8036\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.7888\n",
      "Epoch 00011: val_loss improved from 0.72122 to 0.64917, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/011-0.6492.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.6931 - acc: 0.7888 - val_loss: 0.6492 - val_acc: 0.8197\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6588 - acc: 0.8019\n",
      "Epoch 00012: val_loss improved from 0.64917 to 0.62511, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/012-0.6251.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.6588 - acc: 0.8019 - val_loss: 0.6251 - val_acc: 0.8232\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6294 - acc: 0.8087\n",
      "Epoch 00013: val_loss did not improve from 0.62511\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.6294 - acc: 0.8087 - val_loss: 0.6350 - val_acc: 0.8197\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.8193\n",
      "Epoch 00014: val_loss improved from 0.62511 to 0.59599, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/014-0.5960.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5990 - acc: 0.8193 - val_loss: 0.5960 - val_acc: 0.8281\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8249\n",
      "Epoch 00015: val_loss did not improve from 0.59599\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.5752 - acc: 0.8249 - val_loss: 0.6353 - val_acc: 0.8223\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8333\n",
      "Epoch 00016: val_loss improved from 0.59599 to 0.56613, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/016-0.5661.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5563 - acc: 0.8333 - val_loss: 0.5661 - val_acc: 0.8400\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.8403\n",
      "Epoch 00017: val_loss improved from 0.56613 to 0.55350, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/017-0.5535.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5307 - acc: 0.8403 - val_loss: 0.5535 - val_acc: 0.8488\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8462\n",
      "Epoch 00018: val_loss did not improve from 0.55350\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.5117 - acc: 0.8462 - val_loss: 0.5688 - val_acc: 0.8446\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8512\n",
      "Epoch 00019: val_loss did not improve from 0.55350\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4908 - acc: 0.8511 - val_loss: 0.5624 - val_acc: 0.8432\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8551\n",
      "Epoch 00020: val_loss improved from 0.55350 to 0.54408, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/020-0.5441.hdf5\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.4789 - acc: 0.8551 - val_loss: 0.5441 - val_acc: 0.8493\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8604\n",
      "Epoch 00021: val_loss improved from 0.54408 to 0.50773, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/021-0.5077.hdf5\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.4603 - acc: 0.8603 - val_loss: 0.5077 - val_acc: 0.8616\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8624\n",
      "Epoch 00022: val_loss did not improve from 0.50773\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.4493 - acc: 0.8624 - val_loss: 0.5466 - val_acc: 0.8532\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8681\n",
      "Epoch 00023: val_loss did not improve from 0.50773\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.4367 - acc: 0.8681 - val_loss: 0.5078 - val_acc: 0.8579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8714\n",
      "Epoch 00024: val_loss did not improve from 0.50773\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.4203 - acc: 0.8713 - val_loss: 0.5268 - val_acc: 0.8560\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8757\n",
      "Epoch 00025: val_loss improved from 0.50773 to 0.49891, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/025-0.4989.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4112 - acc: 0.8757 - val_loss: 0.4989 - val_acc: 0.8682\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8802\n",
      "Epoch 00026: val_loss did not improve from 0.49891\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4001 - acc: 0.8802 - val_loss: 0.5141 - val_acc: 0.8602\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8813\n",
      "Epoch 00027: val_loss improved from 0.49891 to 0.49783, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/027-0.4978.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3876 - acc: 0.8813 - val_loss: 0.4978 - val_acc: 0.8637\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8850\n",
      "Epoch 00028: val_loss improved from 0.49783 to 0.46862, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/028-0.4686.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3773 - acc: 0.8850 - val_loss: 0.4686 - val_acc: 0.8730\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8865\n",
      "Epoch 00029: val_loss did not improve from 0.46862\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3684 - acc: 0.8865 - val_loss: 0.4996 - val_acc: 0.8633\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8910\n",
      "Epoch 00030: val_loss did not improve from 0.46862\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3583 - acc: 0.8909 - val_loss: 0.4968 - val_acc: 0.8658\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8912\n",
      "Epoch 00031: val_loss did not improve from 0.46862\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3530 - acc: 0.8912 - val_loss: 0.4958 - val_acc: 0.8661\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.8980\n",
      "Epoch 00032: val_loss improved from 0.46862 to 0.46444, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/032-0.4644.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3375 - acc: 0.8981 - val_loss: 0.4644 - val_acc: 0.8754\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8993\n",
      "Epoch 00033: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.3315 - acc: 0.8992 - val_loss: 0.4724 - val_acc: 0.8684\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8979\n",
      "Epoch 00034: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.3295 - acc: 0.8977 - val_loss: 0.4852 - val_acc: 0.8710\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8995\n",
      "Epoch 00035: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.3232 - acc: 0.8995 - val_loss: 0.4737 - val_acc: 0.8696\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.9013\n",
      "Epoch 00036: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3166 - acc: 0.9013 - val_loss: 0.4762 - val_acc: 0.8754\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9051\n",
      "Epoch 00037: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3094 - acc: 0.9050 - val_loss: 0.4907 - val_acc: 0.8742\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9092\n",
      "Epoch 00038: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.2935 - acc: 0.9092 - val_loss: 0.4954 - val_acc: 0.8602\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9073\n",
      "Epoch 00039: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.2932 - acc: 0.9073 - val_loss: 0.4877 - val_acc: 0.8717\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9099\n",
      "Epoch 00040: val_loss did not improve from 0.46444\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2882 - acc: 0.9099 - val_loss: 0.4973 - val_acc: 0.8705\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9120\n",
      "Epoch 00041: val_loss improved from 0.46444 to 0.45600, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/041-0.4560.hdf5\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2849 - acc: 0.9119 - val_loss: 0.4560 - val_acc: 0.8772\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9151\n",
      "Epoch 00042: val_loss did not improve from 0.45600\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2768 - acc: 0.9151 - val_loss: 0.4689 - val_acc: 0.8733\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9151\n",
      "Epoch 00043: val_loss improved from 0.45600 to 0.45377, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/043-0.4538.hdf5\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.2708 - acc: 0.9150 - val_loss: 0.4538 - val_acc: 0.8821\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9150\n",
      "Epoch 00044: val_loss improved from 0.45377 to 0.44090, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/044-0.4409.hdf5\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.2688 - acc: 0.9151 - val_loss: 0.4409 - val_acc: 0.8838\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9184\n",
      "Epoch 00045: val_loss did not improve from 0.44090\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2610 - acc: 0.9184 - val_loss: 0.4474 - val_acc: 0.8768\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9196\n",
      "Epoch 00046: val_loss improved from 0.44090 to 0.43218, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/046-0.4322.hdf5\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.2551 - acc: 0.9196 - val_loss: 0.4322 - val_acc: 0.8875\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9218\n",
      "Epoch 00047: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.2529 - acc: 0.9218 - val_loss: 0.4581 - val_acc: 0.8817\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9233\n",
      "Epoch 00048: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.2486 - acc: 0.9233 - val_loss: 0.4679 - val_acc: 0.8747\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9230\n",
      "Epoch 00049: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.2444 - acc: 0.9229 - val_loss: 0.4476 - val_acc: 0.8821\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9257\n",
      "Epoch 00050: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.2369 - acc: 0.9257 - val_loss: 0.4920 - val_acc: 0.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9268\n",
      "Epoch 00051: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.2307 - acc: 0.9269 - val_loss: 0.5152 - val_acc: 0.8607\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9253\n",
      "Epoch 00052: val_loss did not improve from 0.43218\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.2315 - acc: 0.9253 - val_loss: 0.4789 - val_acc: 0.8744\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9276\n",
      "Epoch 00053: val_loss improved from 0.43218 to 0.42319, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/053-0.4232.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2282 - acc: 0.9276 - val_loss: 0.4232 - val_acc: 0.8894\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9285\n",
      "Epoch 00054: val_loss did not improve from 0.42319\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.2212 - acc: 0.9285 - val_loss: 0.5006 - val_acc: 0.8656\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9302\n",
      "Epoch 00055: val_loss did not improve from 0.42319\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.2225 - acc: 0.9302 - val_loss: 0.4460 - val_acc: 0.8826\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9307\n",
      "Epoch 00056: val_loss did not improve from 0.42319\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2203 - acc: 0.9307 - val_loss: 0.4398 - val_acc: 0.8828\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9321\n",
      "Epoch 00057: val_loss did not improve from 0.42319\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.2177 - acc: 0.9321 - val_loss: 0.4805 - val_acc: 0.8747\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9337\n",
      "Epoch 00058: val_loss improved from 0.42319 to 0.41727, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/058-0.4173.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2113 - acc: 0.9337 - val_loss: 0.4173 - val_acc: 0.8859\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9327\n",
      "Epoch 00059: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.2089 - acc: 0.9327 - val_loss: 0.4670 - val_acc: 0.8791\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9349\n",
      "Epoch 00060: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.2067 - acc: 0.9350 - val_loss: 0.4293 - val_acc: 0.8949\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9376\n",
      "Epoch 00061: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.1954 - acc: 0.9376 - val_loss: 0.4579 - val_acc: 0.8805\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9374\n",
      "Epoch 00062: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1991 - acc: 0.9375 - val_loss: 0.4316 - val_acc: 0.8821\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9370\n",
      "Epoch 00063: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1968 - acc: 0.9370 - val_loss: 0.4636 - val_acc: 0.8786\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9394\n",
      "Epoch 00064: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.1914 - acc: 0.9393 - val_loss: 0.4589 - val_acc: 0.8859\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9392\n",
      "Epoch 00065: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.1928 - acc: 0.9392 - val_loss: 0.4606 - val_acc: 0.8903\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9394\n",
      "Epoch 00066: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.1876 - acc: 0.9394 - val_loss: 0.4406 - val_acc: 0.8889\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9422\n",
      "Epoch 00067: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1819 - acc: 0.9422 - val_loss: 0.4259 - val_acc: 0.8877\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9434\n",
      "Epoch 00068: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1778 - acc: 0.9434 - val_loss: 0.4228 - val_acc: 0.8880\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9421\n",
      "Epoch 00069: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1798 - acc: 0.9421 - val_loss: 0.4840 - val_acc: 0.8821\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9438\n",
      "Epoch 00070: val_loss did not improve from 0.41727\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.1769 - acc: 0.9438 - val_loss: 0.4401 - val_acc: 0.8863\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9442\n",
      "Epoch 00071: val_loss improved from 0.41727 to 0.41328, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/071-0.4133.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1744 - acc: 0.9442 - val_loss: 0.4133 - val_acc: 0.8984\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9449\n",
      "Epoch 00072: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.1716 - acc: 0.9449 - val_loss: 0.4366 - val_acc: 0.8866\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9458\n",
      "Epoch 00073: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.1713 - acc: 0.9458 - val_loss: 0.4522 - val_acc: 0.8866\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9467\n",
      "Epoch 00074: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.1682 - acc: 0.9467 - val_loss: 0.4298 - val_acc: 0.8940\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9465\n",
      "Epoch 00075: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1674 - acc: 0.9465 - val_loss: 0.4321 - val_acc: 0.8973\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9486\n",
      "Epoch 00076: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.1602 - acc: 0.9486 - val_loss: 0.4349 - val_acc: 0.8945\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9490\n",
      "Epoch 00077: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.1582 - acc: 0.9490 - val_loss: 0.4568 - val_acc: 0.8938\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9468\n",
      "Epoch 00078: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1627 - acc: 0.9468 - val_loss: 0.5644 - val_acc: 0.8675\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9489\n",
      "Epoch 00079: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1579 - acc: 0.9489 - val_loss: 0.4218 - val_acc: 0.9008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9501\n",
      "Epoch 00080: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1538 - acc: 0.9501 - val_loss: 0.4516 - val_acc: 0.8898\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9512\n",
      "Epoch 00081: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.1535 - acc: 0.9512 - val_loss: 0.4463 - val_acc: 0.8917\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9524\n",
      "Epoch 00082: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.1499 - acc: 0.9524 - val_loss: 0.4617 - val_acc: 0.8849\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9518\n",
      "Epoch 00083: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.1500 - acc: 0.9517 - val_loss: 0.4467 - val_acc: 0.8852\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9522\n",
      "Epoch 00084: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1500 - acc: 0.9522 - val_loss: 0.4528 - val_acc: 0.8921\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9529\n",
      "Epoch 00085: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.1480 - acc: 0.9529 - val_loss: 0.5080 - val_acc: 0.8789\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9524\n",
      "Epoch 00086: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.1493 - acc: 0.9523 - val_loss: 0.4638 - val_acc: 0.8863\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9534\n",
      "Epoch 00087: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.1443 - acc: 0.9533 - val_loss: 0.4277 - val_acc: 0.8984\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9534\n",
      "Epoch 00088: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.1458 - acc: 0.9533 - val_loss: 0.4316 - val_acc: 0.8938\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9557\n",
      "Epoch 00089: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.1417 - acc: 0.9557 - val_loss: 0.4467 - val_acc: 0.8891\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9552\n",
      "Epoch 00090: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.1400 - acc: 0.9553 - val_loss: 0.4322 - val_acc: 0.8917\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9565\n",
      "Epoch 00091: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.1378 - acc: 0.9565 - val_loss: 0.4422 - val_acc: 0.8928\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9557\n",
      "Epoch 00092: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1395 - acc: 0.9557 - val_loss: 0.4757 - val_acc: 0.8863\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9562\n",
      "Epoch 00093: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.1355 - acc: 0.9561 - val_loss: 0.4264 - val_acc: 0.8921\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9557\n",
      "Epoch 00094: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1350 - acc: 0.9557 - val_loss: 0.4335 - val_acc: 0.8977\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9568\n",
      "Epoch 00095: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1350 - acc: 0.9568 - val_loss: 0.5613 - val_acc: 0.8710\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9601\n",
      "Epoch 00096: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.1259 - acc: 0.9600 - val_loss: 0.4471 - val_acc: 0.8963\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9597\n",
      "Epoch 00097: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.1283 - acc: 0.9597 - val_loss: 0.4323 - val_acc: 0.9038\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9604\n",
      "Epoch 00098: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1255 - acc: 0.9603 - val_loss: 0.4554 - val_acc: 0.8873\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9610\n",
      "Epoch 00099: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.1251 - acc: 0.9610 - val_loss: 0.4543 - val_acc: 0.8940\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9607\n",
      "Epoch 00100: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.1233 - acc: 0.9607 - val_loss: 0.4553 - val_acc: 0.8963\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9609\n",
      "Epoch 00101: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.1237 - acc: 0.9609 - val_loss: 0.4645 - val_acc: 0.8819\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9607\n",
      "Epoch 00102: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1253 - acc: 0.9607 - val_loss: 0.5540 - val_acc: 0.8721\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9602\n",
      "Epoch 00103: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1226 - acc: 0.9602 - val_loss: 0.4973 - val_acc: 0.8779\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9619\n",
      "Epoch 00104: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1192 - acc: 0.9620 - val_loss: 0.4234 - val_acc: 0.8984\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9625\n",
      "Epoch 00105: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.1194 - acc: 0.9625 - val_loss: 0.4788 - val_acc: 0.8866\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9604\n",
      "Epoch 00106: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1236 - acc: 0.9603 - val_loss: 0.4778 - val_acc: 0.8847\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9622\n",
      "Epoch 00107: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.1161 - acc: 0.9622 - val_loss: 0.4590 - val_acc: 0.8896\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9657\n",
      "Epoch 00108: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1115 - acc: 0.9655 - val_loss: 0.4921 - val_acc: 0.8863\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9639\n",
      "Epoch 00109: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.1148 - acc: 0.9639 - val_loss: 0.4629 - val_acc: 0.8977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9636\n",
      "Epoch 00110: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.1130 - acc: 0.9636 - val_loss: 0.4517 - val_acc: 0.8940\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9654\n",
      "Epoch 00111: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1100 - acc: 0.9654 - val_loss: 0.4478 - val_acc: 0.8917\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9643\n",
      "Epoch 00112: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.1105 - acc: 0.9643 - val_loss: 0.4389 - val_acc: 0.8975\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9665\n",
      "Epoch 00113: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1069 - acc: 0.9665 - val_loss: 0.4919 - val_acc: 0.8840\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9660\n",
      "Epoch 00114: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.1077 - acc: 0.9660 - val_loss: 0.4449 - val_acc: 0.8968\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9661\n",
      "Epoch 00115: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.1067 - acc: 0.9661 - val_loss: 0.4529 - val_acc: 0.8961\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9644\n",
      "Epoch 00116: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.1129 - acc: 0.9644 - val_loss: 0.4482 - val_acc: 0.8945\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9662\n",
      "Epoch 00117: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.1088 - acc: 0.9661 - val_loss: 0.4780 - val_acc: 0.8949\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9661\n",
      "Epoch 00118: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.1084 - acc: 0.9661 - val_loss: 0.4293 - val_acc: 0.9024\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9683\n",
      "Epoch 00119: val_loss improved from 0.41328 to 0.40968, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_5_conv_checkpoint/119-0.4097.hdf5\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.1021 - acc: 0.9683 - val_loss: 0.4097 - val_acc: 0.9031\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9671\n",
      "Epoch 00120: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.1046 - acc: 0.9671 - val_loss: 0.4528 - val_acc: 0.8947\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9685\n",
      "Epoch 00121: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.1011 - acc: 0.9685 - val_loss: 0.4734 - val_acc: 0.8970\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9656\n",
      "Epoch 00122: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1045 - acc: 0.9657 - val_loss: 0.4701 - val_acc: 0.8905\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9680\n",
      "Epoch 00123: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.1010 - acc: 0.9680 - val_loss: 0.4387 - val_acc: 0.8989\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9699\n",
      "Epoch 00124: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.0969 - acc: 0.9699 - val_loss: 0.4880 - val_acc: 0.8866\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9678\n",
      "Epoch 00125: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0996 - acc: 0.9678 - val_loss: 0.5144 - val_acc: 0.8833\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9684\n",
      "Epoch 00126: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0989 - acc: 0.9684 - val_loss: 0.4757 - val_acc: 0.8882\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9686\n",
      "Epoch 00127: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1001 - acc: 0.9686 - val_loss: 0.5359 - val_acc: 0.8805\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9695\n",
      "Epoch 00128: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.0977 - acc: 0.9694 - val_loss: 0.4448 - val_acc: 0.8977\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9690\n",
      "Epoch 00129: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.0978 - acc: 0.9690 - val_loss: 0.4372 - val_acc: 0.9026\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9704\n",
      "Epoch 00130: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.0942 - acc: 0.9704 - val_loss: 0.4435 - val_acc: 0.9008\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9718\n",
      "Epoch 00131: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.0894 - acc: 0.9719 - val_loss: 0.4272 - val_acc: 0.9026\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9711\n",
      "Epoch 00132: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.0941 - acc: 0.9711 - val_loss: 0.4169 - val_acc: 0.9012\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9704\n",
      "Epoch 00133: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.0929 - acc: 0.9704 - val_loss: 0.5786 - val_acc: 0.8742\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9724\n",
      "Epoch 00134: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.0903 - acc: 0.9724 - val_loss: 0.4380 - val_acc: 0.8998\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9704\n",
      "Epoch 00135: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0930 - acc: 0.9704 - val_loss: 0.4429 - val_acc: 0.9036\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9715\n",
      "Epoch 00136: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0897 - acc: 0.9715 - val_loss: 0.4443 - val_acc: 0.8991\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9730\n",
      "Epoch 00137: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.0869 - acc: 0.9730 - val_loss: 0.4293 - val_acc: 0.9024\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9712\n",
      "Epoch 00138: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.0919 - acc: 0.9713 - val_loss: 0.4238 - val_acc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9719\n",
      "Epoch 00139: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0885 - acc: 0.9720 - val_loss: 0.4499 - val_acc: 0.8994\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9724\n",
      "Epoch 00140: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0885 - acc: 0.9724 - val_loss: 0.4900 - val_acc: 0.8898\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9718\n",
      "Epoch 00141: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.0899 - acc: 0.9718 - val_loss: 0.4509 - val_acc: 0.9036\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9724\n",
      "Epoch 00142: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0877 - acc: 0.9724 - val_loss: 0.4287 - val_acc: 0.8998\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9726\n",
      "Epoch 00143: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0868 - acc: 0.9726 - val_loss: 0.4283 - val_acc: 0.9045\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9711\n",
      "Epoch 00144: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.0913 - acc: 0.9711 - val_loss: 0.4357 - val_acc: 0.9059\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9736\n",
      "Epoch 00145: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0837 - acc: 0.9736 - val_loss: 0.4441 - val_acc: 0.9026\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9731\n",
      "Epoch 00146: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.0847 - acc: 0.9731 - val_loss: 0.4591 - val_acc: 0.9012\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9740\n",
      "Epoch 00147: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0853 - acc: 0.9739 - val_loss: 0.4536 - val_acc: 0.9033\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9741\n",
      "Epoch 00148: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0859 - acc: 0.9740 - val_loss: 0.4501 - val_acc: 0.8977\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9766\n",
      "Epoch 00149: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0772 - acc: 0.9766 - val_loss: 0.4433 - val_acc: 0.8968\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9739\n",
      "Epoch 00150: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.0836 - acc: 0.9739 - val_loss: 0.4592 - val_acc: 0.9005\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9734\n",
      "Epoch 00151: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0840 - acc: 0.9733 - val_loss: 0.4339 - val_acc: 0.9029\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9768\n",
      "Epoch 00152: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0764 - acc: 0.9768 - val_loss: 0.4300 - val_acc: 0.9059\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9746\n",
      "Epoch 00153: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0802 - acc: 0.9746 - val_loss: 0.4708 - val_acc: 0.8982\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9739\n",
      "Epoch 00154: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0840 - acc: 0.9739 - val_loss: 0.4578 - val_acc: 0.9019\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9766\n",
      "Epoch 00155: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0784 - acc: 0.9766 - val_loss: 0.5835 - val_acc: 0.8789\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9726\n",
      "Epoch 00156: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0852 - acc: 0.9726 - val_loss: 0.4359 - val_acc: 0.9012\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9757\n",
      "Epoch 00157: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0772 - acc: 0.9757 - val_loss: 0.4496 - val_acc: 0.8991\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9746\n",
      "Epoch 00158: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0793 - acc: 0.9746 - val_loss: 0.4602 - val_acc: 0.8980\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9741\n",
      "Epoch 00159: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.0814 - acc: 0.9740 - val_loss: 0.4610 - val_acc: 0.8959\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9770\n",
      "Epoch 00160: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0749 - acc: 0.9770 - val_loss: 0.4432 - val_acc: 0.9064\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9773\n",
      "Epoch 00161: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0741 - acc: 0.9773 - val_loss: 0.4540 - val_acc: 0.9029\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9753\n",
      "Epoch 00162: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0769 - acc: 0.9753 - val_loss: 0.4705 - val_acc: 0.8940\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9768\n",
      "Epoch 00163: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0737 - acc: 0.9768 - val_loss: 0.4656 - val_acc: 0.8961\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9767\n",
      "Epoch 00164: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.0739 - acc: 0.9767 - val_loss: 0.4396 - val_acc: 0.9096\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9777\n",
      "Epoch 00165: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0725 - acc: 0.9777 - val_loss: 0.4527 - val_acc: 0.9052\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9765\n",
      "Epoch 00166: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.0780 - acc: 0.9765 - val_loss: 0.4337 - val_acc: 0.9068\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9742\n",
      "Epoch 00167: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.0808 - acc: 0.9741 - val_loss: 0.4314 - val_acc: 0.9103\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9785\n",
      "Epoch 00168: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.0715 - acc: 0.9785 - val_loss: 0.4349 - val_acc: 0.9008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9769\n",
      "Epoch 00169: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.0727 - acc: 0.9769 - val_loss: 0.4470 - val_acc: 0.8970\n",
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9771\n",
      "Epoch 00170: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0742 - acc: 0.9771 - val_loss: 0.4336 - val_acc: 0.9043\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9788\n",
      "Epoch 00171: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0699 - acc: 0.9788 - val_loss: 0.4491 - val_acc: 0.9061\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9793\n",
      "Epoch 00172: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.0699 - acc: 0.9793 - val_loss: 0.4609 - val_acc: 0.8991\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9789\n",
      "Epoch 00173: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.0683 - acc: 0.9789 - val_loss: 0.5328 - val_acc: 0.8835\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9785\n",
      "Epoch 00174: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.0690 - acc: 0.9785 - val_loss: 0.4776 - val_acc: 0.9003\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9796\n",
      "Epoch 00175: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0668 - acc: 0.9796 - val_loss: 0.4539 - val_acc: 0.9087\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9799\n",
      "Epoch 00176: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0667 - acc: 0.9799 - val_loss: 0.4295 - val_acc: 0.9061\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9787\n",
      "Epoch 00177: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0697 - acc: 0.9787 - val_loss: 0.4215 - val_acc: 0.9089\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9772\n",
      "Epoch 00178: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0721 - acc: 0.9772 - val_loss: 0.4644 - val_acc: 0.8970\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9795\n",
      "Epoch 00179: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.0670 - acc: 0.9795 - val_loss: 0.4840 - val_acc: 0.8977\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9768\n",
      "Epoch 00180: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0737 - acc: 0.9768 - val_loss: 0.4905 - val_acc: 0.8901\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9800\n",
      "Epoch 00181: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.0661 - acc: 0.9799 - val_loss: 0.4442 - val_acc: 0.9061\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9790\n",
      "Epoch 00182: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0691 - acc: 0.9790 - val_loss: 0.4328 - val_acc: 0.9078\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9797\n",
      "Epoch 00183: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0660 - acc: 0.9798 - val_loss: 0.5083 - val_acc: 0.8917\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9790\n",
      "Epoch 00184: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0662 - acc: 0.9789 - val_loss: 0.4587 - val_acc: 0.9012\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9768\n",
      "Epoch 00185: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0749 - acc: 0.9768 - val_loss: 0.4384 - val_acc: 0.9066\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9806\n",
      "Epoch 00186: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0630 - acc: 0.9807 - val_loss: 0.4399 - val_acc: 0.9087\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9797\n",
      "Epoch 00187: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0639 - acc: 0.9797 - val_loss: 0.4282 - val_acc: 0.9075\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9789\n",
      "Epoch 00188: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0667 - acc: 0.9789 - val_loss: 0.4222 - val_acc: 0.9082\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9788\n",
      "Epoch 00189: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0683 - acc: 0.9788 - val_loss: 0.4515 - val_acc: 0.9050\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9811\n",
      "Epoch 00190: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0627 - acc: 0.9810 - val_loss: 0.4522 - val_acc: 0.9038\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9806\n",
      "Epoch 00191: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0645 - acc: 0.9806 - val_loss: 0.4536 - val_acc: 0.9031\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9805\n",
      "Epoch 00192: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0614 - acc: 0.9805 - val_loss: 0.4604 - val_acc: 0.9001\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9800\n",
      "Epoch 00193: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0646 - acc: 0.9800 - val_loss: 0.4666 - val_acc: 0.8996\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9812\n",
      "Epoch 00194: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0628 - acc: 0.9812 - val_loss: 0.4493 - val_acc: 0.9061\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9824\n",
      "Epoch 00195: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0571 - acc: 0.9824 - val_loss: 0.4334 - val_acc: 0.9061\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9818\n",
      "Epoch 00196: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0593 - acc: 0.9818 - val_loss: 0.4539 - val_acc: 0.9066\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9827\n",
      "Epoch 00197: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0590 - acc: 0.9827 - val_loss: 0.4540 - val_acc: 0.8977\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9813\n",
      "Epoch 00198: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0599 - acc: 0.9813 - val_loss: 0.4553 - val_acc: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9805\n",
      "Epoch 00199: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0646 - acc: 0.9805 - val_loss: 0.4348 - val_acc: 0.9082\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9801\n",
      "Epoch 00200: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0646 - acc: 0.9801 - val_loss: 0.4726 - val_acc: 0.8987\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9807\n",
      "Epoch 00201: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0631 - acc: 0.9807 - val_loss: 0.4398 - val_acc: 0.9040\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9825\n",
      "Epoch 00202: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.0576 - acc: 0.9825 - val_loss: 0.4406 - val_acc: 0.9061\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9817\n",
      "Epoch 00203: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0596 - acc: 0.9817 - val_loss: 0.4883 - val_acc: 0.9024\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9792\n",
      "Epoch 00204: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0682 - acc: 0.9792 - val_loss: 0.4409 - val_acc: 0.9047\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9823\n",
      "Epoch 00205: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0595 - acc: 0.9823 - val_loss: 0.4423 - val_acc: 0.9024\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9825\n",
      "Epoch 00206: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0580 - acc: 0.9824 - val_loss: 0.4871 - val_acc: 0.9012\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9820\n",
      "Epoch 00207: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0568 - acc: 0.9820 - val_loss: 0.4425 - val_acc: 0.9061\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9828\n",
      "Epoch 00208: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0567 - acc: 0.9828 - val_loss: 0.4828 - val_acc: 0.8977\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9811\n",
      "Epoch 00209: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.0616 - acc: 0.9811 - val_loss: 0.4614 - val_acc: 0.9005\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9819\n",
      "Epoch 00210: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0573 - acc: 0.9819 - val_loss: 0.4518 - val_acc: 0.9064\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9831\n",
      "Epoch 00211: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0547 - acc: 0.9831 - val_loss: 0.4580 - val_acc: 0.9052\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9825\n",
      "Epoch 00212: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0588 - acc: 0.9824 - val_loss: 0.4313 - val_acc: 0.9078\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9821\n",
      "Epoch 00213: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0592 - acc: 0.9821 - val_loss: 0.4823 - val_acc: 0.8935\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9819\n",
      "Epoch 00214: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.0595 - acc: 0.9819 - val_loss: 0.4387 - val_acc: 0.9103\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9835\n",
      "Epoch 00215: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.0535 - acc: 0.9835 - val_loss: 0.4815 - val_acc: 0.9001\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9822\n",
      "Epoch 00216: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0575 - acc: 0.9822 - val_loss: 0.4445 - val_acc: 0.9075\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9829\n",
      "Epoch 00217: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0562 - acc: 0.9829 - val_loss: 0.4516 - val_acc: 0.9031\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9833\n",
      "Epoch 00218: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0541 - acc: 0.9833 - val_loss: 0.4714 - val_acc: 0.9045\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9834\n",
      "Epoch 00219: val_loss did not improve from 0.40968\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0530 - acc: 0.9833 - val_loss: 0.4514 - val_acc: 0.9050\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 305us/sample - loss: 0.4895 - acc: 0.8737\n",
      "Loss: 0.4894733119357536 Accuracy: 0.8737279\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3346 - acc: 0.2973\n",
      "Epoch 00001: val_loss improved from inf to 1.52820, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/001-1.5282.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 2.3347 - acc: 0.2973 - val_loss: 1.5282 - val_acc: 0.5176\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4914 - acc: 0.5225\n",
      "Epoch 00002: val_loss improved from 1.52820 to 1.10314, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/002-1.1031.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 1.4918 - acc: 0.5225 - val_loss: 1.1031 - val_acc: 0.6560\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1658 - acc: 0.6282\n",
      "Epoch 00003: val_loss improved from 1.10314 to 0.87231, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/003-0.8723.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 1.1660 - acc: 0.6282 - val_loss: 0.8723 - val_acc: 0.7475\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9622 - acc: 0.7012\n",
      "Epoch 00004: val_loss improved from 0.87231 to 0.71595, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/004-0.7160.hdf5\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.9623 - acc: 0.7011 - val_loss: 0.7160 - val_acc: 0.7890\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8197 - acc: 0.7489\n",
      "Epoch 00005: val_loss improved from 0.71595 to 0.64961, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/005-0.6496.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.8199 - acc: 0.7488 - val_loss: 0.6496 - val_acc: 0.8150\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.7845\n",
      "Epoch 00006: val_loss improved from 0.64961 to 0.55492, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/006-0.5549.hdf5\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.7044 - acc: 0.7845 - val_loss: 0.5549 - val_acc: 0.8418\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.8077\n",
      "Epoch 00007: val_loss improved from 0.55492 to 0.55325, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/007-0.5533.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.6295 - acc: 0.8077 - val_loss: 0.5533 - val_acc: 0.8509\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.8284\n",
      "Epoch 00008: val_loss improved from 0.55325 to 0.48654, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/008-0.4865.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.5659 - acc: 0.8283 - val_loss: 0.4865 - val_acc: 0.8644\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8435\n",
      "Epoch 00009: val_loss improved from 0.48654 to 0.48296, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/009-0.4830.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.5144 - acc: 0.8434 - val_loss: 0.4830 - val_acc: 0.8584\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8574\n",
      "Epoch 00010: val_loss improved from 0.48296 to 0.39150, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/010-0.3915.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.4691 - acc: 0.8575 - val_loss: 0.3915 - val_acc: 0.8868\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8682\n",
      "Epoch 00011: val_loss did not improve from 0.39150\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.4262 - acc: 0.8682 - val_loss: 0.3974 - val_acc: 0.8852\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8801\n",
      "Epoch 00012: val_loss improved from 0.39150 to 0.34755, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/012-0.3476.hdf5\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.3931 - acc: 0.8801 - val_loss: 0.3476 - val_acc: 0.8984\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8863\n",
      "Epoch 00013: val_loss improved from 0.34755 to 0.33548, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/013-0.3355.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.3719 - acc: 0.8863 - val_loss: 0.3355 - val_acc: 0.9061\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8939\n",
      "Epoch 00014: val_loss improved from 0.33548 to 0.33488, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/014-0.3349.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3465 - acc: 0.8939 - val_loss: 0.3349 - val_acc: 0.9047\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8992\n",
      "Epoch 00015: val_loss improved from 0.33488 to 0.32345, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/015-0.3234.hdf5\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.3268 - acc: 0.8992 - val_loss: 0.3234 - val_acc: 0.9071\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9067\n",
      "Epoch 00016: val_loss improved from 0.32345 to 0.30953, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/016-0.3095.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.3036 - acc: 0.9067 - val_loss: 0.3095 - val_acc: 0.9171\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9100\n",
      "Epoch 00017: val_loss did not improve from 0.30953\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2909 - acc: 0.9100 - val_loss: 0.3183 - val_acc: 0.9133\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9140\n",
      "Epoch 00018: val_loss did not improve from 0.30953\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2774 - acc: 0.9140 - val_loss: 0.3324 - val_acc: 0.9106\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9174\n",
      "Epoch 00019: val_loss did not improve from 0.30953\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2663 - acc: 0.9175 - val_loss: 0.3154 - val_acc: 0.9110\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9223\n",
      "Epoch 00020: val_loss did not improve from 0.30953\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2506 - acc: 0.9223 - val_loss: 0.3133 - val_acc: 0.9066\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9252\n",
      "Epoch 00021: val_loss improved from 0.30953 to 0.28089, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/021-0.2809.hdf5\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2424 - acc: 0.9251 - val_loss: 0.2809 - val_acc: 0.9171\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9267\n",
      "Epoch 00022: val_loss did not improve from 0.28089\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2309 - acc: 0.9267 - val_loss: 0.3368 - val_acc: 0.9045\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9308\n",
      "Epoch 00023: val_loss did not improve from 0.28089\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2201 - acc: 0.9308 - val_loss: 0.3128 - val_acc: 0.9103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9351\n",
      "Epoch 00024: val_loss improved from 0.28089 to 0.26447, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/024-0.2645.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2103 - acc: 0.9351 - val_loss: 0.2645 - val_acc: 0.9271\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9357\n",
      "Epoch 00025: val_loss improved from 0.26447 to 0.25197, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/025-0.2520.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2044 - acc: 0.9357 - val_loss: 0.2520 - val_acc: 0.9306\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9382\n",
      "Epoch 00026: val_loss did not improve from 0.25197\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1930 - acc: 0.9382 - val_loss: 0.2672 - val_acc: 0.9227\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9400\n",
      "Epoch 00027: val_loss did not improve from 0.25197\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1887 - acc: 0.9400 - val_loss: 0.2643 - val_acc: 0.9248\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9409\n",
      "Epoch 00028: val_loss did not improve from 0.25197\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1863 - acc: 0.9409 - val_loss: 0.2698 - val_acc: 0.9241\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9432\n",
      "Epoch 00029: val_loss did not improve from 0.25197\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.1791 - acc: 0.9431 - val_loss: 0.2701 - val_acc: 0.9287\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9472\n",
      "Epoch 00030: val_loss did not improve from 0.25197\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1700 - acc: 0.9472 - val_loss: 0.2721 - val_acc: 0.9220\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9483\n",
      "Epoch 00031: val_loss improved from 0.25197 to 0.24733, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/031-0.2473.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1648 - acc: 0.9483 - val_loss: 0.2473 - val_acc: 0.9324\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9504\n",
      "Epoch 00032: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1570 - acc: 0.9503 - val_loss: 0.2925 - val_acc: 0.9168\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9502\n",
      "Epoch 00033: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1594 - acc: 0.9501 - val_loss: 0.2991 - val_acc: 0.9178\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9526\n",
      "Epoch 00034: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.1486 - acc: 0.9526 - val_loss: 0.2895 - val_acc: 0.9206\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9541\n",
      "Epoch 00035: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1407 - acc: 0.9541 - val_loss: 0.2966 - val_acc: 0.9161\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9546\n",
      "Epoch 00036: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1379 - acc: 0.9545 - val_loss: 0.2554 - val_acc: 0.9308\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9561\n",
      "Epoch 00037: val_loss improved from 0.24733 to 0.23067, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/037-0.2307.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1364 - acc: 0.9561 - val_loss: 0.2307 - val_acc: 0.9336\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9594\n",
      "Epoch 00038: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1298 - acc: 0.9594 - val_loss: 0.2536 - val_acc: 0.9313\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9589\n",
      "Epoch 00039: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.1265 - acc: 0.9589 - val_loss: 0.2690 - val_acc: 0.9324\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9598\n",
      "Epoch 00040: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1239 - acc: 0.9598 - val_loss: 0.2639 - val_acc: 0.9287\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9618\n",
      "Epoch 00041: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1183 - acc: 0.9618 - val_loss: 0.2493 - val_acc: 0.9327\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9621\n",
      "Epoch 00042: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1155 - acc: 0.9622 - val_loss: 0.2476 - val_acc: 0.9350\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9648\n",
      "Epoch 00043: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.1101 - acc: 0.9647 - val_loss: 0.2410 - val_acc: 0.9327\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9645\n",
      "Epoch 00044: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1107 - acc: 0.9645 - val_loss: 0.2597 - val_acc: 0.9297\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9645\n",
      "Epoch 00045: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1099 - acc: 0.9645 - val_loss: 0.3144 - val_acc: 0.9217\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9687\n",
      "Epoch 00046: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0993 - acc: 0.9686 - val_loss: 0.2650 - val_acc: 0.9334\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9657\n",
      "Epoch 00047: val_loss did not improve from 0.23067\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1031 - acc: 0.9657 - val_loss: 0.2933 - val_acc: 0.9290\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9686\n",
      "Epoch 00048: val_loss improved from 0.23067 to 0.22327, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_6_conv_checkpoint/048-0.2233.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.0974 - acc: 0.9686 - val_loss: 0.2233 - val_acc: 0.9408\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9676\n",
      "Epoch 00049: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0980 - acc: 0.9676 - val_loss: 0.2585 - val_acc: 0.9287\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9701\n",
      "Epoch 00050: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.0955 - acc: 0.9701 - val_loss: 0.2544 - val_acc: 0.9324\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9716\n",
      "Epoch 00051: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.0890 - acc: 0.9716 - val_loss: 0.3040 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9721\n",
      "Epoch 00052: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0861 - acc: 0.9721 - val_loss: 0.2535 - val_acc: 0.9371\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9722\n",
      "Epoch 00053: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0858 - acc: 0.9722 - val_loss: 0.2437 - val_acc: 0.9378\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9727\n",
      "Epoch 00054: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0837 - acc: 0.9727 - val_loss: 0.2470 - val_acc: 0.9352\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9733\n",
      "Epoch 00055: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0802 - acc: 0.9733 - val_loss: 0.2762 - val_acc: 0.9362\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9745\n",
      "Epoch 00056: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.0798 - acc: 0.9745 - val_loss: 0.3114 - val_acc: 0.9229\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9755\n",
      "Epoch 00057: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0768 - acc: 0.9755 - val_loss: 0.2526 - val_acc: 0.9338\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9755\n",
      "Epoch 00058: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0777 - acc: 0.9755 - val_loss: 0.2314 - val_acc: 0.9408\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9765\n",
      "Epoch 00059: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0745 - acc: 0.9766 - val_loss: 0.2804 - val_acc: 0.9306\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9780\n",
      "Epoch 00060: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.0689 - acc: 0.9780 - val_loss: 0.3542 - val_acc: 0.9164\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9754\n",
      "Epoch 00061: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0772 - acc: 0.9753 - val_loss: 0.2323 - val_acc: 0.9406\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9765\n",
      "Epoch 00062: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0720 - acc: 0.9765 - val_loss: 0.2722 - val_acc: 0.9304\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9786\n",
      "Epoch 00063: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0674 - acc: 0.9786 - val_loss: 0.2873 - val_acc: 0.9290\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9808\n",
      "Epoch 00064: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0615 - acc: 0.9808 - val_loss: 0.2720 - val_acc: 0.9376\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9774\n",
      "Epoch 00065: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0689 - acc: 0.9774 - val_loss: 0.2453 - val_acc: 0.9334\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9796\n",
      "Epoch 00066: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0655 - acc: 0.9795 - val_loss: 0.2413 - val_acc: 0.9392\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9801\n",
      "Epoch 00067: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.0610 - acc: 0.9801 - val_loss: 0.2497 - val_acc: 0.9418\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9805\n",
      "Epoch 00068: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0603 - acc: 0.9805 - val_loss: 0.3010 - val_acc: 0.9266\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9805\n",
      "Epoch 00069: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0602 - acc: 0.9805 - val_loss: 0.2751 - val_acc: 0.9373\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9812\n",
      "Epoch 00070: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0583 - acc: 0.9812 - val_loss: 0.3327 - val_acc: 0.9224\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9817\n",
      "Epoch 00071: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0554 - acc: 0.9817 - val_loss: 0.2831 - val_acc: 0.9294\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9817\n",
      "Epoch 00072: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.0568 - acc: 0.9817 - val_loss: 0.3331 - val_acc: 0.9210\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9829\n",
      "Epoch 00073: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0534 - acc: 0.9829 - val_loss: 0.2788 - val_acc: 0.9366\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9825\n",
      "Epoch 00074: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0544 - acc: 0.9825 - val_loss: 0.2318 - val_acc: 0.9457\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9827\n",
      "Epoch 00075: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0525 - acc: 0.9827 - val_loss: 0.2913 - val_acc: 0.9359\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9833\n",
      "Epoch 00076: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.0510 - acc: 0.9832 - val_loss: 0.2725 - val_acc: 0.9397\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9819\n",
      "Epoch 00077: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.0574 - acc: 0.9819 - val_loss: 0.2510 - val_acc: 0.9380\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9836\n",
      "Epoch 00078: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.0486 - acc: 0.9836 - val_loss: 0.2723 - val_acc: 0.9350\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9856\n",
      "Epoch 00079: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0447 - acc: 0.9856 - val_loss: 0.2483 - val_acc: 0.9380\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9843\n",
      "Epoch 00080: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0490 - acc: 0.9843 - val_loss: 0.3150 - val_acc: 0.9269\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9842\n",
      "Epoch 00081: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0489 - acc: 0.9842 - val_loss: 0.2689 - val_acc: 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9853\n",
      "Epoch 00082: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0461 - acc: 0.9853 - val_loss: 0.3357 - val_acc: 0.9264\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9865\n",
      "Epoch 00083: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0443 - acc: 0.9865 - val_loss: 0.2603 - val_acc: 0.9406\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9851\n",
      "Epoch 00084: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0451 - acc: 0.9850 - val_loss: 0.3605 - val_acc: 0.9199\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9852\n",
      "Epoch 00085: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0460 - acc: 0.9852 - val_loss: 0.2993 - val_acc: 0.9313\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9864\n",
      "Epoch 00086: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0436 - acc: 0.9864 - val_loss: 0.2735 - val_acc: 0.9364\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9871\n",
      "Epoch 00087: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0424 - acc: 0.9871 - val_loss: 0.2869 - val_acc: 0.9373\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9847\n",
      "Epoch 00088: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0460 - acc: 0.9847 - val_loss: 0.2740 - val_acc: 0.9364\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9898\n",
      "Epoch 00089: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0346 - acc: 0.9898 - val_loss: 0.3429 - val_acc: 0.9269\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9881\n",
      "Epoch 00090: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0383 - acc: 0.9881 - val_loss: 0.2961 - val_acc: 0.9366\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9863\n",
      "Epoch 00091: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0421 - acc: 0.9863 - val_loss: 0.3125 - val_acc: 0.9308\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9871\n",
      "Epoch 00092: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0401 - acc: 0.9871 - val_loss: 0.2587 - val_acc: 0.9415\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9867\n",
      "Epoch 00093: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0421 - acc: 0.9867 - val_loss: 0.2601 - val_acc: 0.9387\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9886\n",
      "Epoch 00094: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0384 - acc: 0.9886 - val_loss: 0.2744 - val_acc: 0.9411\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9877\n",
      "Epoch 00095: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0387 - acc: 0.9877 - val_loss: 0.3033 - val_acc: 0.9371\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9883\n",
      "Epoch 00096: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0371 - acc: 0.9883 - val_loss: 0.2866 - val_acc: 0.9348\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9882\n",
      "Epoch 00097: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0372 - acc: 0.9882 - val_loss: 0.2654 - val_acc: 0.9429\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9891\n",
      "Epoch 00098: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0358 - acc: 0.9891 - val_loss: 0.2745 - val_acc: 0.9385\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9887\n",
      "Epoch 00099: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0365 - acc: 0.9887 - val_loss: 0.2614 - val_acc: 0.9439\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9884\n",
      "Epoch 00100: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0355 - acc: 0.9884 - val_loss: 0.2665 - val_acc: 0.9408\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9869\n",
      "Epoch 00101: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0391 - acc: 0.9869 - val_loss: 0.3038 - val_acc: 0.9317\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9896\n",
      "Epoch 00102: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0341 - acc: 0.9896 - val_loss: 0.2614 - val_acc: 0.9439\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9878\n",
      "Epoch 00103: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0369 - acc: 0.9878 - val_loss: 0.2788 - val_acc: 0.9418\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9871\n",
      "Epoch 00104: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0420 - acc: 0.9871 - val_loss: 0.2749 - val_acc: 0.9406\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9873\n",
      "Epoch 00105: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0413 - acc: 0.9872 - val_loss: 0.2677 - val_acc: 0.9427\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9882\n",
      "Epoch 00106: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0377 - acc: 0.9881 - val_loss: 0.2749 - val_acc: 0.9394\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9895\n",
      "Epoch 00107: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0341 - acc: 0.9895 - val_loss: 0.2822 - val_acc: 0.9399\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9916\n",
      "Epoch 00108: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0281 - acc: 0.9916 - val_loss: 0.2603 - val_acc: 0.9462\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9912\n",
      "Epoch 00109: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.0294 - acc: 0.9912 - val_loss: 0.2896 - val_acc: 0.9378\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9874\n",
      "Epoch 00110: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.0384 - acc: 0.9873 - val_loss: 0.2709 - val_acc: 0.9434\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9903\n",
      "Epoch 00111: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.0303 - acc: 0.9903 - val_loss: 0.3558 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9908\n",
      "Epoch 00112: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0301 - acc: 0.9908 - val_loss: 0.2674 - val_acc: 0.9443\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9897\n",
      "Epoch 00113: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0325 - acc: 0.9897 - val_loss: 0.2752 - val_acc: 0.9450\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9906\n",
      "Epoch 00114: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.0307 - acc: 0.9906 - val_loss: 0.3096 - val_acc: 0.9287\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9907\n",
      "Epoch 00115: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0301 - acc: 0.9907 - val_loss: 0.2821 - val_acc: 0.9401\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00116: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.4083 - val_acc: 0.9241\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9899\n",
      "Epoch 00117: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0327 - acc: 0.9899 - val_loss: 0.2635 - val_acc: 0.9446\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 00118: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0300 - acc: 0.9907 - val_loss: 0.2727 - val_acc: 0.9383\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9910\n",
      "Epoch 00119: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.0303 - acc: 0.9910 - val_loss: 0.2907 - val_acc: 0.9387\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9906\n",
      "Epoch 00120: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0312 - acc: 0.9906 - val_loss: 0.2791 - val_acc: 0.9401\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9911\n",
      "Epoch 00121: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0274 - acc: 0.9911 - val_loss: 0.3153 - val_acc: 0.9292\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9917\n",
      "Epoch 00122: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0275 - acc: 0.9917 - val_loss: 0.3759 - val_acc: 0.9229\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9879\n",
      "Epoch 00123: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0386 - acc: 0.9879 - val_loss: 0.2844 - val_acc: 0.9404\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9883\n",
      "Epoch 00124: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0359 - acc: 0.9883 - val_loss: 0.2776 - val_acc: 0.9415\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9901\n",
      "Epoch 00125: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0327 - acc: 0.9901 - val_loss: 0.2819 - val_acc: 0.9415\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9908\n",
      "Epoch 00126: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0311 - acc: 0.9908 - val_loss: 0.2970 - val_acc: 0.9362\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9938\n",
      "Epoch 00127: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0225 - acc: 0.9938 - val_loss: 0.2678 - val_acc: 0.9404\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 00128: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0240 - acc: 0.9927 - val_loss: 0.3148 - val_acc: 0.9350\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917\n",
      "Epoch 00129: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.2917 - val_acc: 0.9355\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 00130: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0271 - acc: 0.9915 - val_loss: 0.2864 - val_acc: 0.9401\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9921\n",
      "Epoch 00131: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0252 - acc: 0.9921 - val_loss: 0.2979 - val_acc: 0.9364\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9919\n",
      "Epoch 00132: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0253 - acc: 0.9919 - val_loss: 0.3110 - val_acc: 0.9387\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9911\n",
      "Epoch 00133: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0289 - acc: 0.9911 - val_loss: 0.2540 - val_acc: 0.9460\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9923\n",
      "Epoch 00134: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0252 - acc: 0.9923 - val_loss: 0.3671 - val_acc: 0.9252\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9933\n",
      "Epoch 00135: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0214 - acc: 0.9933 - val_loss: 0.2559 - val_acc: 0.9450\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9917\n",
      "Epoch 00136: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0258 - acc: 0.9917 - val_loss: 0.3270 - val_acc: 0.9329\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9921\n",
      "Epoch 00137: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.2975 - val_acc: 0.9383\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9917\n",
      "Epoch 00138: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0265 - acc: 0.9917 - val_loss: 0.2997 - val_acc: 0.9399\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9936\n",
      "Epoch 00139: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0214 - acc: 0.9936 - val_loss: 0.2883 - val_acc: 0.9369\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9929\n",
      "Epoch 00140: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0231 - acc: 0.9929 - val_loss: 0.2598 - val_acc: 0.9448\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9932\n",
      "Epoch 00141: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.2913 - val_acc: 0.9411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9928\n",
      "Epoch 00142: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.0236 - acc: 0.9928 - val_loss: 0.3167 - val_acc: 0.9380\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9910\n",
      "Epoch 00143: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.0272 - acc: 0.9910 - val_loss: 0.3249 - val_acc: 0.9378\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9936\n",
      "Epoch 00144: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.2982 - val_acc: 0.9408\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 00145: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0231 - acc: 0.9929 - val_loss: 0.3029 - val_acc: 0.9376\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9910\n",
      "Epoch 00146: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.0278 - acc: 0.9910 - val_loss: 0.3888 - val_acc: 0.9227\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9936\n",
      "Epoch 00147: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.0230 - acc: 0.9935 - val_loss: 0.3294 - val_acc: 0.9345\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9922\n",
      "Epoch 00148: val_loss did not improve from 0.22327\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.0251 - acc: 0.9922 - val_loss: 0.2778 - val_acc: 0.9429\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 360us/sample - loss: 0.2762 - acc: 0.9192\n",
      "Loss: 0.27623994896964 Accuracy: 0.9192108\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9018 - acc: 0.4293\n",
      "Epoch 00001: val_loss improved from inf to 1.13764, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/001-1.1376.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 1.9017 - acc: 0.4293 - val_loss: 1.1376 - val_acc: 0.6420\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0091 - acc: 0.6816\n",
      "Epoch 00002: val_loss improved from 1.13764 to 0.58048, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/002-0.5805.hdf5\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 1.0090 - acc: 0.6816 - val_loss: 0.5805 - val_acc: 0.8274\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7070 - acc: 0.7819\n",
      "Epoch 00003: val_loss improved from 0.58048 to 0.44527, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/003-0.4453.hdf5\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.7070 - acc: 0.7819 - val_loss: 0.4453 - val_acc: 0.8668\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8314\n",
      "Epoch 00004: val_loss improved from 0.44527 to 0.36280, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/004-0.3628.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.5358 - acc: 0.8314 - val_loss: 0.3628 - val_acc: 0.8952\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8633\n",
      "Epoch 00005: val_loss improved from 0.36280 to 0.32003, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/005-0.3200.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.4384 - acc: 0.8634 - val_loss: 0.3200 - val_acc: 0.9045\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8847\n",
      "Epoch 00006: val_loss improved from 0.32003 to 0.28950, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/006-0.2895.hdf5\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.3680 - acc: 0.8847 - val_loss: 0.2895 - val_acc: 0.9138\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8995\n",
      "Epoch 00007: val_loss improved from 0.28950 to 0.25144, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/007-0.2514.hdf5\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.3215 - acc: 0.8995 - val_loss: 0.2514 - val_acc: 0.9241\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9117\n",
      "Epoch 00008: val_loss improved from 0.25144 to 0.24520, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/008-0.2452.hdf5\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.2802 - acc: 0.9117 - val_loss: 0.2452 - val_acc: 0.9285\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9212\n",
      "Epoch 00009: val_loss improved from 0.24520 to 0.23280, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/009-0.2328.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.2515 - acc: 0.9212 - val_loss: 0.2328 - val_acc: 0.9317\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9293\n",
      "Epoch 00010: val_loss improved from 0.23280 to 0.21195, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/010-0.2120.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.2264 - acc: 0.9293 - val_loss: 0.2120 - val_acc: 0.9383\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9345\n",
      "Epoch 00011: val_loss did not improve from 0.21195\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.2030 - acc: 0.9344 - val_loss: 0.2305 - val_acc: 0.9364\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9403\n",
      "Epoch 00012: val_loss improved from 0.21195 to 0.20505, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/012-0.2051.hdf5\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1864 - acc: 0.9403 - val_loss: 0.2051 - val_acc: 0.9378\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9445\n",
      "Epoch 00013: val_loss improved from 0.20505 to 0.19721, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/013-0.1972.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1726 - acc: 0.9445 - val_loss: 0.1972 - val_acc: 0.9422\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9521\n",
      "Epoch 00014: val_loss improved from 0.19721 to 0.18058, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/014-0.1806.hdf5\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1516 - acc: 0.9521 - val_loss: 0.1806 - val_acc: 0.9457\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9540\n",
      "Epoch 00015: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1448 - acc: 0.9540 - val_loss: 0.1994 - val_acc: 0.9406\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9573\n",
      "Epoch 00016: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.1318 - acc: 0.9573 - val_loss: 0.2007 - val_acc: 0.9439\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9609\n",
      "Epoch 00017: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1212 - acc: 0.9609 - val_loss: 0.2018 - val_acc: 0.9387\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9643\n",
      "Epoch 00018: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1095 - acc: 0.9643 - val_loss: 0.2117 - val_acc: 0.9411\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9664\n",
      "Epoch 00019: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1033 - acc: 0.9664 - val_loss: 0.2296 - val_acc: 0.9320\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9689\n",
      "Epoch 00020: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0974 - acc: 0.9689 - val_loss: 0.1845 - val_acc: 0.9446\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9719\n",
      "Epoch 00021: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0872 - acc: 0.9719 - val_loss: 0.1910 - val_acc: 0.9462\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9736\n",
      "Epoch 00022: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0803 - acc: 0.9735 - val_loss: 0.1911 - val_acc: 0.9429\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9734\n",
      "Epoch 00023: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0838 - acc: 0.9734 - val_loss: 0.2197 - val_acc: 0.9394\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9770\n",
      "Epoch 00024: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0731 - acc: 0.9770 - val_loss: 0.2041 - val_acc: 0.9464\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9787\n",
      "Epoch 00025: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0676 - acc: 0.9787 - val_loss: 0.2123 - val_acc: 0.9418\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9790\n",
      "Epoch 00026: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0657 - acc: 0.9789 - val_loss: 0.2101 - val_acc: 0.9453\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9793\n",
      "Epoch 00027: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0658 - acc: 0.9793 - val_loss: 0.1876 - val_acc: 0.9483\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9824\n",
      "Epoch 00028: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0568 - acc: 0.9824 - val_loss: 0.2018 - val_acc: 0.9469\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9833\n",
      "Epoch 00029: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0518 - acc: 0.9833 - val_loss: 0.2779 - val_acc: 0.9234\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9822\n",
      "Epoch 00030: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.0560 - acc: 0.9822 - val_loss: 0.2428 - val_acc: 0.9420\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9828\n",
      "Epoch 00031: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0556 - acc: 0.9828 - val_loss: 0.1969 - val_acc: 0.9467\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9855\n",
      "Epoch 00032: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0458 - acc: 0.9855 - val_loss: 0.2064 - val_acc: 0.9478\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.0472 - acc: 0.9860 - val_loss: 0.2146 - val_acc: 0.9474\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9863\n",
      "Epoch 00034: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0445 - acc: 0.9863 - val_loss: 0.1824 - val_acc: 0.9527\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9894\n",
      "Epoch 00035: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0362 - acc: 0.9893 - val_loss: 0.2568 - val_acc: 0.9394\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9858\n",
      "Epoch 00036: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0465 - acc: 0.9858 - val_loss: 0.2260 - val_acc: 0.9450\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9890\n",
      "Epoch 00037: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0358 - acc: 0.9889 - val_loss: 0.1953 - val_acc: 0.9515\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9873\n",
      "Epoch 00038: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0403 - acc: 0.9873 - val_loss: 0.2409 - val_acc: 0.9439\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9898\n",
      "Epoch 00039: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0335 - acc: 0.9898 - val_loss: 0.3305 - val_acc: 0.9224\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9914\n",
      "Epoch 00040: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0297 - acc: 0.9914 - val_loss: 0.1928 - val_acc: 0.9509\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9897\n",
      "Epoch 00041: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0336 - acc: 0.9897 - val_loss: 0.2224 - val_acc: 0.9488\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9908\n",
      "Epoch 00042: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0302 - acc: 0.9908 - val_loss: 0.2487 - val_acc: 0.9425\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9894\n",
      "Epoch 00043: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0344 - acc: 0.9894 - val_loss: 0.2190 - val_acc: 0.9441\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9920\n",
      "Epoch 00044: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0263 - acc: 0.9920 - val_loss: 0.2287 - val_acc: 0.9436\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9920\n",
      "Epoch 00045: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0265 - acc: 0.9920 - val_loss: 0.2609 - val_acc: 0.9355\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9930\n",
      "Epoch 00046: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0246 - acc: 0.9930 - val_loss: 0.3582 - val_acc: 0.9248\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9901\n",
      "Epoch 00047: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0314 - acc: 0.9901 - val_loss: 0.1935 - val_acc: 0.9548\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9928\n",
      "Epoch 00048: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0234 - acc: 0.9928 - val_loss: 0.2504 - val_acc: 0.9418\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9936\n",
      "Epoch 00049: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0200 - acc: 0.9936 - val_loss: 0.2424 - val_acc: 0.9462\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9914\n",
      "Epoch 00050: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0288 - acc: 0.9914 - val_loss: 0.1855 - val_acc: 0.9576\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9932\n",
      "Epoch 00051: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.2347 - val_acc: 0.9504\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9928\n",
      "Epoch 00052: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0231 - acc: 0.9928 - val_loss: 0.2251 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9930\n",
      "Epoch 00053: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0225 - acc: 0.9930 - val_loss: 0.2225 - val_acc: 0.9462\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0199 - acc: 0.9938 - val_loss: 0.2312 - val_acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0197 - acc: 0.9940 - val_loss: 0.2350 - val_acc: 0.9481\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9907\n",
      "Epoch 00056: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0307 - acc: 0.9907 - val_loss: 0.2073 - val_acc: 0.9546\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9945\n",
      "Epoch 00057: val_loss did not improve from 0.18058\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0192 - acc: 0.9945 - val_loss: 0.2371 - val_acc: 0.9478\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9948\n",
      "Epoch 00058: val_loss improved from 0.18058 to 0.17734, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_7_conv_checkpoint/058-0.1773.hdf5\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0196 - acc: 0.9948 - val_loss: 0.1773 - val_acc: 0.9564\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9951\n",
      "Epoch 00059: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0163 - acc: 0.9951 - val_loss: 0.2560 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 00060: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0181 - acc: 0.9943 - val_loss: 0.2642 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9916\n",
      "Epoch 00061: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0273 - acc: 0.9916 - val_loss: 0.2262 - val_acc: 0.9529\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9952\n",
      "Epoch 00062: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0156 - acc: 0.9952 - val_loss: 0.2046 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9957\n",
      "Epoch 00063: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0155 - acc: 0.9957 - val_loss: 0.1999 - val_acc: 0.9562\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9947\n",
      "Epoch 00064: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0164 - acc: 0.9947 - val_loss: 0.1933 - val_acc: 0.9536\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9945\n",
      "Epoch 00065: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0178 - acc: 0.9945 - val_loss: 0.2102 - val_acc: 0.9506\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9964\n",
      "Epoch 00066: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0131 - acc: 0.9964 - val_loss: 0.1914 - val_acc: 0.9525\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00067: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.1929 - val_acc: 0.9534\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9934\n",
      "Epoch 00068: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.0216 - acc: 0.9934 - val_loss: 0.2348 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9945\n",
      "Epoch 00069: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0172 - acc: 0.9945 - val_loss: 0.2061 - val_acc: 0.9532\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9962\n",
      "Epoch 00070: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0133 - acc: 0.9962 - val_loss: 0.2596 - val_acc: 0.9448\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9957\n",
      "Epoch 00071: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.1869 - val_acc: 0.9562\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 00072: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0153 - acc: 0.9954 - val_loss: 0.2583 - val_acc: 0.9497\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9949\n",
      "Epoch 00073: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.0160 - acc: 0.9949 - val_loss: 0.2181 - val_acc: 0.9520\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 00074: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0137 - acc: 0.9959 - val_loss: 0.2248 - val_acc: 0.9511\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9949\n",
      "Epoch 00075: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0165 - acc: 0.9949 - val_loss: 0.2503 - val_acc: 0.9485\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 00076: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0138 - acc: 0.9955 - val_loss: 0.2255 - val_acc: 0.9502\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 00077: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.2785 - val_acc: 0.9441\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00078: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.2021 - val_acc: 0.9550\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9970\n",
      "Epoch 00079: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0109 - acc: 0.9969 - val_loss: 0.2411 - val_acc: 0.9490\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9915\n",
      "Epoch 00080: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0297 - acc: 0.9915 - val_loss: 0.1906 - val_acc: 0.9576\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
      "Epoch 00081: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0099 - acc: 0.9970 - val_loss: 0.2020 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00082: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.2129 - val_acc: 0.9534\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 00083: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 0.1950 - val_acc: 0.9583\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9972\n",
      "Epoch 00084: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.2118 - val_acc: 0.9527\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9941\n",
      "Epoch 00085: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0197 - acc: 0.9941 - val_loss: 0.1839 - val_acc: 0.9578\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9974\n",
      "Epoch 00086: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0091 - acc: 0.9974 - val_loss: 0.2369 - val_acc: 0.9502\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9967\n",
      "Epoch 00087: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0122 - acc: 0.9967 - val_loss: 0.2414 - val_acc: 0.9520\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00088: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.2165 - val_acc: 0.9539\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9956\n",
      "Epoch 00089: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0133 - acc: 0.9956 - val_loss: 0.2189 - val_acc: 0.9532\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9943\n",
      "Epoch 00090: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0191 - acc: 0.9943 - val_loss: 0.2083 - val_acc: 0.9529\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9973\n",
      "Epoch 00091: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0092 - acc: 0.9973 - val_loss: 0.2024 - val_acc: 0.9569\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9975\n",
      "Epoch 00092: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 0.2374 - val_acc: 0.9518\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9964\n",
      "Epoch 00093: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0126 - acc: 0.9964 - val_loss: 0.2201 - val_acc: 0.9560\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9967\n",
      "Epoch 00094: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0109 - acc: 0.9967 - val_loss: 0.2709 - val_acc: 0.9450\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9963\n",
      "Epoch 00095: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0127 - acc: 0.9963 - val_loss: 0.2365 - val_acc: 0.9499\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00096: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.2173 - val_acc: 0.9536\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9960\n",
      "Epoch 00097: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.3530 - val_acc: 0.9327\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9927\n",
      "Epoch 00098: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0247 - acc: 0.9927 - val_loss: 0.1919 - val_acc: 0.9578\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9980\n",
      "Epoch 00099: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0075 - acc: 0.9980 - val_loss: 0.2175 - val_acc: 0.9576\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 00100: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.2116 - val_acc: 0.9574\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00101: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0100 - acc: 0.9968 - val_loss: 0.2256 - val_acc: 0.9522\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 00102: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0142 - acc: 0.9954 - val_loss: 0.2143 - val_acc: 0.9518\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
      "Epoch 00103: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2744 - val_acc: 0.9467\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00104: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.2212 - val_acc: 0.9574\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9955\n",
      "Epoch 00105: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0144 - acc: 0.9955 - val_loss: 0.2551 - val_acc: 0.9522\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 00106: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.2504 - val_acc: 0.9513\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9974\n",
      "Epoch 00107: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.2658 - val_acc: 0.9488\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9960\n",
      "Epoch 00108: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0140 - acc: 0.9960 - val_loss: 0.2210 - val_acc: 0.9576\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 00109: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.2429 - val_acc: 0.9502\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 00110: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0120 - acc: 0.9965 - val_loss: 0.2466 - val_acc: 0.9520\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00111: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1942 - val_acc: 0.9588\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00112: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0058 - acc: 0.9985 - val_loss: 0.1813 - val_acc: 0.9595\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00113: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0128 - acc: 0.9961 - val_loss: 0.2321 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9961\n",
      "Epoch 00114: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0125 - acc: 0.9960 - val_loss: 0.2138 - val_acc: 0.9567\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9955\n",
      "Epoch 00115: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0166 - acc: 0.9955 - val_loss: 0.2721 - val_acc: 0.9448\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
      "Epoch 00116: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.2374 - val_acc: 0.9569\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9981\n",
      "Epoch 00117: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.2248 - val_acc: 0.9567\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9974\n",
      "Epoch 00118: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2276 - val_acc: 0.9536\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9962\n",
      "Epoch 00119: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0118 - acc: 0.9962 - val_loss: 0.2322 - val_acc: 0.9560\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9971\n",
      "Epoch 00120: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0090 - acc: 0.9971 - val_loss: 0.2212 - val_acc: 0.9562\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9975\n",
      "Epoch 00121: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0072 - acc: 0.9975 - val_loss: 0.2808 - val_acc: 0.9492\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 00122: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0113 - acc: 0.9962 - val_loss: 0.2401 - val_acc: 0.9557\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 00123: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.0091 - acc: 0.9970 - val_loss: 0.2432 - val_acc: 0.9548\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00124: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 0.2233 - val_acc: 0.9543\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 00125: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2127 - val_acc: 0.9564\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9967\n",
      "Epoch 00126: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0117 - acc: 0.9967 - val_loss: 0.2465 - val_acc: 0.9548\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 00127: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.2423 - val_acc: 0.9513\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9962\n",
      "Epoch 00128: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0134 - acc: 0.9962 - val_loss: 0.2239 - val_acc: 0.9569\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9978\n",
      "Epoch 00129: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0073 - acc: 0.9978 - val_loss: 0.2665 - val_acc: 0.9527\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9960\n",
      "Epoch 00130: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.2400 - val_acc: 0.9557\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
      "Epoch 00131: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0071 - acc: 0.9979 - val_loss: 0.2321 - val_acc: 0.9571\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00132: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.2272 - val_acc: 0.9564\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 00133: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0083 - acc: 0.9976 - val_loss: 0.2071 - val_acc: 0.9564\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9974\n",
      "Epoch 00134: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 0.2072 - val_acc: 0.9637\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 00135: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.2537 - val_acc: 0.9522\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 00136: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2214 - val_acc: 0.9576\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00137: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.2373 - val_acc: 0.9529\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00138: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0058 - acc: 0.9985 - val_loss: 0.2294 - val_acc: 0.9564\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9983\n",
      "Epoch 00139: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0055 - acc: 0.9982 - val_loss: 0.2443 - val_acc: 0.9553\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9965\n",
      "Epoch 00140: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0123 - acc: 0.9965 - val_loss: 0.2283 - val_acc: 0.9576\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 00141: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2437 - val_acc: 0.9581\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00142: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.2221 - val_acc: 0.9567\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 00143: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0074 - acc: 0.9977 - val_loss: 0.2717 - val_acc: 0.9518\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 00144: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0097 - acc: 0.9971 - val_loss: 0.2713 - val_acc: 0.9506\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 00145: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.2667 - val_acc: 0.9511\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 00146: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.2754 - val_acc: 0.9476\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 00147: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 0.2256 - val_acc: 0.9557\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9964\n",
      "Epoch 00148: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0112 - acc: 0.9964 - val_loss: 0.2134 - val_acc: 0.9578\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985\n",
      "Epoch 00149: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.2393 - val_acc: 0.9539\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9958\n",
      "Epoch 00150: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0151 - acc: 0.9958 - val_loss: 0.2150 - val_acc: 0.9557\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 00151: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.2872 - val_acc: 0.9506\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9981\n",
      "Epoch 00152: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0060 - acc: 0.9981 - val_loss: 0.2098 - val_acc: 0.9592\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 00153: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2842 - val_acc: 0.9529\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 00154: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0060 - acc: 0.9983 - val_loss: 0.2915 - val_acc: 0.9490\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00155: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.2494 - val_acc: 0.9483\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9971\n",
      "Epoch 00156: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.2904 - val_acc: 0.9506\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9957\n",
      "Epoch 00157: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.2173 - val_acc: 0.9585\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 00158: val_loss did not improve from 0.17734\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0050 - acc: 0.9988 - val_loss: 0.2087 - val_acc: 0.9599\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 425us/sample - loss: 0.2332 - acc: 0.9445\n",
      "Loss: 0.23323770014536283 Accuracy: 0.9445483\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4324 - acc: 0.5681\n",
      "Epoch 00001: val_loss improved from inf to 0.94750, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/001-0.9475.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.4325 - acc: 0.5680 - val_loss: 0.9475 - val_acc: 0.6993\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.7941\n",
      "Epoch 00002: val_loss improved from 0.94750 to 0.43859, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/002-0.4386.hdf5\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.6472 - acc: 0.7941 - val_loss: 0.4386 - val_acc: 0.8593\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8590\n",
      "Epoch 00003: val_loss improved from 0.43859 to 0.32921, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/003-0.3292.hdf5\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.4363 - acc: 0.8590 - val_loss: 0.3292 - val_acc: 0.8980\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8925\n",
      "Epoch 00004: val_loss improved from 0.32921 to 0.26613, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/004-0.2661.hdf5\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.3349 - acc: 0.8925 - val_loss: 0.2661 - val_acc: 0.9217\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9160\n",
      "Epoch 00005: val_loss did not improve from 0.26613\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.2649 - acc: 0.9160 - val_loss: 0.2785 - val_acc: 0.9161\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9288\n",
      "Epoch 00006: val_loss did not improve from 0.26613\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.2186 - acc: 0.9288 - val_loss: 0.2964 - val_acc: 0.9045\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9371\n",
      "Epoch 00007: val_loss improved from 0.26613 to 0.26155, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/007-0.2616.hdf5\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.1883 - acc: 0.9370 - val_loss: 0.2616 - val_acc: 0.9234\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9507\n",
      "Epoch 00008: val_loss improved from 0.26155 to 0.23927, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/008-0.2393.hdf5\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.1565 - acc: 0.9507 - val_loss: 0.2393 - val_acc: 0.9257\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9553\n",
      "Epoch 00009: val_loss improved from 0.23927 to 0.23748, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/009-0.2375.hdf5\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.1363 - acc: 0.9553 - val_loss: 0.2375 - val_acc: 0.9257\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9639\n",
      "Epoch 00010: val_loss improved from 0.23748 to 0.22286, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/010-0.2229.hdf5\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.1143 - acc: 0.9640 - val_loss: 0.2229 - val_acc: 0.9306\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9680\n",
      "Epoch 00011: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 22s 586us/sample - loss: 0.1006 - acc: 0.9680 - val_loss: 0.2332 - val_acc: 0.9276\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9732\n",
      "Epoch 00012: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0854 - acc: 0.9732 - val_loss: 0.2382 - val_acc: 0.9290\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9714\n",
      "Epoch 00013: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0886 - acc: 0.9713 - val_loss: 0.2508 - val_acc: 0.9297\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9777\n",
      "Epoch 00014: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0703 - acc: 0.9776 - val_loss: 0.2554 - val_acc: 0.9259\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9764\n",
      "Epoch 00015: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0765 - acc: 0.9764 - val_loss: 0.2625 - val_acc: 0.9280\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9840\n",
      "Epoch 00016: val_loss did not improve from 0.22286\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0533 - acc: 0.9839 - val_loss: 0.2356 - val_acc: 0.9362\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9846\n",
      "Epoch 00017: val_loss improved from 0.22286 to 0.21848, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/017-0.2185.hdf5\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0499 - acc: 0.9846 - val_loss: 0.2185 - val_acc: 0.9394\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9847\n",
      "Epoch 00018: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0485 - acc: 0.9847 - val_loss: 0.2392 - val_acc: 0.9313\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9826\n",
      "Epoch 00019: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 584us/sample - loss: 0.0558 - acc: 0.9826 - val_loss: 0.2725 - val_acc: 0.9290\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9895\n",
      "Epoch 00020: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0348 - acc: 0.9895 - val_loss: 0.2322 - val_acc: 0.9329\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9863\n",
      "Epoch 00021: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0447 - acc: 0.9863 - val_loss: 0.2422 - val_acc: 0.9348\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9914\n",
      "Epoch 00022: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 21s 584us/sample - loss: 0.0301 - acc: 0.9914 - val_loss: 0.2960 - val_acc: 0.9278\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9899\n",
      "Epoch 00023: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0320 - acc: 0.9899 - val_loss: 0.2327 - val_acc: 0.9350\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9897\n",
      "Epoch 00024: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 588us/sample - loss: 0.0335 - acc: 0.9897 - val_loss: 0.2331 - val_acc: 0.9357\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9920\n",
      "Epoch 00025: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 21s 580us/sample - loss: 0.0269 - acc: 0.9920 - val_loss: 0.2838 - val_acc: 0.9292\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9892\n",
      "Epoch 00026: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.2498 - val_acc: 0.9334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9928\n",
      "Epoch 00027: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0248 - acc: 0.9928 - val_loss: 0.2460 - val_acc: 0.9371\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9928\n",
      "Epoch 00028: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0249 - acc: 0.9928 - val_loss: 0.3359 - val_acc: 0.9243\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9929\n",
      "Epoch 00029: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0240 - acc: 0.9929 - val_loss: 0.2765 - val_acc: 0.9311\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9923\n",
      "Epoch 00030: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0241 - acc: 0.9923 - val_loss: 0.2715 - val_acc: 0.9359\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9922\n",
      "Epoch 00031: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0250 - acc: 0.9922 - val_loss: 0.2515 - val_acc: 0.9387\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9939\n",
      "Epoch 00032: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0190 - acc: 0.9939 - val_loss: 0.2464 - val_acc: 0.9401\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9948\n",
      "Epoch 00033: val_loss did not improve from 0.21848\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0177 - acc: 0.9948 - val_loss: 0.3087 - val_acc: 0.9255\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9907\n",
      "Epoch 00034: val_loss improved from 0.21848 to 0.21253, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/034-0.2125.hdf5\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0309 - acc: 0.9907 - val_loss: 0.2125 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00035: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 0.2898 - val_acc: 0.9336\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9956\n",
      "Epoch 00036: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0149 - acc: 0.9956 - val_loss: 0.2717 - val_acc: 0.9331\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 00037: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0233 - acc: 0.9925 - val_loss: 0.2225 - val_acc: 0.9450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9966\n",
      "Epoch 00038: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 577us/sample - loss: 0.0129 - acc: 0.9966 - val_loss: 0.2405 - val_acc: 0.9418\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 00039: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 584us/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.2800 - val_acc: 0.9355\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9956\n",
      "Epoch 00040: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0146 - acc: 0.9956 - val_loss: 0.2704 - val_acc: 0.9415\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9953\n",
      "Epoch 00041: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 584us/sample - loss: 0.0156 - acc: 0.9953 - val_loss: 0.2448 - val_acc: 0.9406\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9930\n",
      "Epoch 00042: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0236 - acc: 0.9930 - val_loss: 0.2668 - val_acc: 0.9366\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9949\n",
      "Epoch 00043: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0186 - acc: 0.9949 - val_loss: 0.2307 - val_acc: 0.9434\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9959\n",
      "Epoch 00044: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 588us/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.3093 - val_acc: 0.9317\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0098 - acc: 0.9973 - val_loss: 0.2879 - val_acc: 0.9378\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 00046: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0145 - acc: 0.9953 - val_loss: 0.3285 - val_acc: 0.9299\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9946\n",
      "Epoch 00047: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0192 - acc: 0.9946 - val_loss: 0.2394 - val_acc: 0.9462\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9972\n",
      "Epoch 00048: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 583us/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.2630 - val_acc: 0.9455\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
      "Epoch 00049: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0099 - acc: 0.9970 - val_loss: 0.2635 - val_acc: 0.9418\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00050: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 580us/sample - loss: 0.0122 - acc: 0.9963 - val_loss: 0.3164 - val_acc: 0.9276\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9932\n",
      "Epoch 00051: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 578us/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 0.2142 - val_acc: 0.9515\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 00052: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 586us/sample - loss: 0.0076 - acc: 0.9981 - val_loss: 0.2706 - val_acc: 0.9378\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9946\n",
      "Epoch 00053: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 584us/sample - loss: 0.0184 - acc: 0.9946 - val_loss: 0.2505 - val_acc: 0.9441\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9973\n",
      "Epoch 00054: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 578us/sample - loss: 0.0088 - acc: 0.9973 - val_loss: 0.2499 - val_acc: 0.9427\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9970\n",
      "Epoch 00055: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 586us/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.3249 - val_acc: 0.9350\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9929\n",
      "Epoch 00056: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0239 - acc: 0.9929 - val_loss: 0.2267 - val_acc: 0.9497\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9961\n",
      "Epoch 00057: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 588us/sample - loss: 0.0135 - acc: 0.9961 - val_loss: 0.2433 - val_acc: 0.9474\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 00058: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.2392 - val_acc: 0.9483\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 580us/sample - loss: 0.0082 - acc: 0.9978 - val_loss: 0.3379 - val_acc: 0.9366\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9964\n",
      "Epoch 00060: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 0.2870 - val_acc: 0.9397\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0187 - acc: 0.9946 - val_loss: 0.2238 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 00062: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 21s 584us/sample - loss: 0.0162 - acc: 0.9955 - val_loss: 0.2655 - val_acc: 0.9427\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00063: val_loss did not improve from 0.21253\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.2546 - val_acc: 0.9441\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9981\n",
      "Epoch 00064: val_loss improved from 0.21253 to 0.21194, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_8_conv_checkpoint/064-0.2119.hdf5\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0072 - acc: 0.9981 - val_loss: 0.2119 - val_acc: 0.9532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9971\n",
      "Epoch 00065: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.2737 - val_acc: 0.9441\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 00066: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 579us/sample - loss: 0.0126 - acc: 0.9962 - val_loss: 0.3262 - val_acc: 0.9294\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 00067: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0147 - acc: 0.9954 - val_loss: 0.2539 - val_acc: 0.9469\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
      "Epoch 00068: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2481 - val_acc: 0.9439\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
      "Epoch 00069: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.2883 - val_acc: 0.9406\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9960\n",
      "Epoch 00070: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0125 - acc: 0.9960 - val_loss: 0.2581 - val_acc: 0.9434\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9978\n",
      "Epoch 00071: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0070 - acc: 0.9978 - val_loss: 0.2851 - val_acc: 0.9418\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9952\n",
      "Epoch 00072: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0160 - acc: 0.9952 - val_loss: 0.2339 - val_acc: 0.9497\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.2572 - val_acc: 0.9492\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00074: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0072 - acc: 0.9982 - val_loss: 0.2883 - val_acc: 0.9383\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00075: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.3890 - val_acc: 0.9271\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9975\n",
      "Epoch 00076: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0076 - acc: 0.9975 - val_loss: 0.2597 - val_acc: 0.9425\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9960\n",
      "Epoch 00077: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0140 - acc: 0.9960 - val_loss: 0.2454 - val_acc: 0.9509\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9986\n",
      "Epoch 00078: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2168 - val_acc: 0.9564\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9970\n",
      "Epoch 00079: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0104 - acc: 0.9970 - val_loss: 0.2290 - val_acc: 0.9509\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9981\n",
      "Epoch 00080: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 586us/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.2519 - val_acc: 0.9471\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00081: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2969 - val_acc: 0.9418\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9967\n",
      "Epoch 00082: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 582us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.2999 - val_acc: 0.9387\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9952\n",
      "Epoch 00083: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0154 - acc: 0.9952 - val_loss: 0.2890 - val_acc: 0.9413\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
      "Epoch 00084: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.2413 - val_acc: 0.9525\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
      "Epoch 00085: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.2350 - val_acc: 0.9539\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
      "Epoch 00086: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.3168 - val_acc: 0.9313\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00087: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.2958 - val_acc: 0.9415\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9961\n",
      "Epoch 00088: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.2789 - val_acc: 0.9455\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9966\n",
      "Epoch 00089: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0110 - acc: 0.9966 - val_loss: 0.2283 - val_acc: 0.9515\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 00090: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.2625 - val_acc: 0.9436\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00091: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.2538 - val_acc: 0.9509\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9960\n",
      "Epoch 00092: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0149 - acc: 0.9960 - val_loss: 0.2201 - val_acc: 0.9553\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9981\n",
      "Epoch 00093: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0059 - acc: 0.9980 - val_loss: 0.2796 - val_acc: 0.9432\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 00094: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.2656 - val_acc: 0.9453\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 00095: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.2388 - val_acc: 0.9525\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00096: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0034 - acc: 0.9992 - val_loss: 0.2463 - val_acc: 0.9502\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9976\n",
      "Epoch 00097: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0077 - acc: 0.9976 - val_loss: 0.2977 - val_acc: 0.9390\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 00098: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.2320 - val_acc: 0.9553\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9978\n",
      "Epoch 00099: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0075 - acc: 0.9978 - val_loss: 0.3366 - val_acc: 0.9352\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 00100: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.2460 - val_acc: 0.9546\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9983\n",
      "Epoch 00101: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0045 - acc: 0.9983 - val_loss: 0.3113 - val_acc: 0.9371\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968\n",
      "Epoch 00102: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.2518 - val_acc: 0.9515\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00103: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.2580 - val_acc: 0.9539\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9971\n",
      "Epoch 00104: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.0090 - acc: 0.9971 - val_loss: 0.2610 - val_acc: 0.9529\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00105: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.2909 - val_acc: 0.9439\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9980\n",
      "Epoch 00106: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0065 - acc: 0.9980 - val_loss: 0.2686 - val_acc: 0.9476\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 00107: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 0.2519 - val_acc: 0.9497\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 00108: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0143 - acc: 0.9954 - val_loss: 0.2259 - val_acc: 0.9541\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 00109: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0039 - acc: 0.9990 - val_loss: 0.2339 - val_acc: 0.9539\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9988\n",
      "Epoch 00110: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.2656 - val_acc: 0.9467\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00111: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.3552 - val_acc: 0.9376\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9971\n",
      "Epoch 00112: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.2834 - val_acc: 0.9413\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00113: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.3030 - val_acc: 0.9397\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00114: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.3402 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9971\n",
      "Epoch 00115: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0104 - acc: 0.9971 - val_loss: 0.2307 - val_acc: 0.9506\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 00116: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.2376 - val_acc: 0.9536\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 00117: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 0.2554 - val_acc: 0.9529\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 00118: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.2883 - val_acc: 0.9434\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 00119: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0076 - acc: 0.9977 - val_loss: 0.2855 - val_acc: 0.9436\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 00120: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0047 - acc: 0.9984 - val_loss: 0.2707 - val_acc: 0.9485\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00121: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.2754 - val_acc: 0.9483\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00122: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 584us/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.2975 - val_acc: 0.9453\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9960\n",
      "Epoch 00123: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0143 - acc: 0.9959 - val_loss: 0.2686 - val_acc: 0.9481\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 00124: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.2542 - val_acc: 0.9529\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9973\n",
      "Epoch 00125: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0086 - acc: 0.9973 - val_loss: 0.2223 - val_acc: 0.9546\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00126: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 578us/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.2159 - val_acc: 0.9569\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9974\n",
      "Epoch 00127: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 583us/sample - loss: 0.0083 - acc: 0.9974 - val_loss: 0.2255 - val_acc: 0.9534\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971\n",
      "Epoch 00128: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.0102 - acc: 0.9971 - val_loss: 0.2311 - val_acc: 0.9509\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00129: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.2527 - val_acc: 0.9513\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00130: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 585us/sample - loss: 0.0029 - acc: 0.9993 - val_loss: 0.2817 - val_acc: 0.9439\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00131: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.2770 - val_acc: 0.9441\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00132: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.2620 - val_acc: 0.9497\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9983\n",
      "Epoch 00133: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0049 - acc: 0.9983 - val_loss: 0.2689 - val_acc: 0.9446\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 00134: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.2637 - val_acc: 0.9497\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
      "Epoch 00135: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0068 - acc: 0.9982 - val_loss: 0.2476 - val_acc: 0.9522\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00136: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 602us/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2807 - val_acc: 0.9464\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00137: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.3228 - val_acc: 0.9399\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 00138: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 589us/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.2895 - val_acc: 0.9481\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00139: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.2467 - val_acc: 0.9520\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 00140: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.2647 - val_acc: 0.9518\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00141: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 586us/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.2569 - val_acc: 0.9511\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 00142: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 588us/sample - loss: 0.0048 - acc: 0.9985 - val_loss: 0.3486 - val_acc: 0.9359\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9961\n",
      "Epoch 00143: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0143 - acc: 0.9961 - val_loss: 0.2582 - val_acc: 0.9532\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9973\n",
      "Epoch 00144: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.2233 - val_acc: 0.9571\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 00145: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.2248 - val_acc: 0.9553\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00146: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.2530 - val_acc: 0.9518\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9969\n",
      "Epoch 00147: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0092 - acc: 0.9969 - val_loss: 0.2670 - val_acc: 0.9495\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00148: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2375 - val_acc: 0.9525\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
      "Epoch 00149: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0034 - acc: 0.9989 - val_loss: 0.2457 - val_acc: 0.9534\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
      "Epoch 00150: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0053 - acc: 0.9984 - val_loss: 0.3020 - val_acc: 0.9453\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 00151: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.2527 - val_acc: 0.9522\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9970\n",
      "Epoch 00152: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0112 - acc: 0.9970 - val_loss: 0.2335 - val_acc: 0.9555\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00153: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.2273 - val_acc: 0.9564\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9985\n",
      "Epoch 00154: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0046 - acc: 0.9985 - val_loss: 0.2626 - val_acc: 0.9529\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
      "Epoch 00155: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 592us/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.2461 - val_acc: 0.9541\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00156: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 590us/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.2616 - val_acc: 0.9492\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9985\n",
      "Epoch 00157: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0047 - acc: 0.9985 - val_loss: 0.2851 - val_acc: 0.9422\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9988\n",
      "Epoch 00158: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.2435 - val_acc: 0.9515\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 00159: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 593us/sample - loss: 0.0098 - acc: 0.9968 - val_loss: 0.2495 - val_acc: 0.9532\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9962\n",
      "Epoch 00160: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.0133 - acc: 0.9962 - val_loss: 0.2380 - val_acc: 0.9560\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993\n",
      "Epoch 00161: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.2497 - val_acc: 0.9555\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 00162: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.0023 - acc: 0.9992 - val_loss: 0.2570 - val_acc: 0.9520\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9982\n",
      "Epoch 00163: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.0050 - acc: 0.9982 - val_loss: 0.2822 - val_acc: 0.9453\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00164: val_loss did not improve from 0.21194\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.0037 - acc: 0.9989 - val_loss: 0.2457 - val_acc: 0.9555\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 462us/sample - loss: 0.3019 - acc: 0.9310\n",
      "Loss: 0.3018609445128176 Accuracy: 0.9310488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,592\n",
      "Trainable params: 682,576\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 326us/sample - loss: 2.2044 - acc: 0.3171\n",
      "Loss: 2.20443851611077 Accuracy: 0.31713396\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_37 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,472\n",
      "Trainable params: 455,424\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 322us/sample - loss: 1.6162 - acc: 0.4974\n",
      "Loss: 1.6161952246510476 Accuracy: 0.49740395\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 306,128\n",
      "Trainable params: 306,016\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 377us/sample - loss: 1.1109 - acc: 0.6725\n",
      "Loss: 1.1108930741143745 Accuracy: 0.67248183\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,800\n",
      "Trainable params: 214,560\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 380us/sample - loss: 0.8090 - acc: 0.7807\n",
      "Loss: 0.8090206719880783 Accuracy: 0.78068537\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 186,768\n",
      "Trainable params: 186,272\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 418us/sample - loss: 0.4895 - acc: 0.8737\n",
      "Loss: 0.4894733119357536 Accuracy: 0.8737279\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 302,736\n",
      "Trainable params: 301,728\n",
      "Non-trainable params: 1,008\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 452us/sample - loss: 0.2762 - acc: 0.9192\n",
      "Loss: 0.27623994896964 Accuracy: 0.9192108\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 927,888\n",
      "Trainable params: 925,856\n",
      "Non-trainable params: 2,032\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 467us/sample - loss: 0.2332 - acc: 0.9445\n",
      "Loss: 0.23323770014536283 Accuracy: 0.9445483\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_64 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 2, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,521,680\n",
      "Trainable params: 3,517,600\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 524us/sample - loss: 0.3019 - acc: 0.9310\n",
      "Loss: 0.3018609445128176 Accuracy: 0.9310488\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
