{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,592\n",
      "Trainable params: 682,576\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,472\n",
      "Trainable params: 455,424\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 306,128\n",
      "Trainable params: 306,016\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,800\n",
      "Trainable params: 214,560\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 186,768\n",
      "Trainable params: 186,272\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 302,736\n",
      "Trainable params: 301,728\n",
      "Non-trainable params: 1,008\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 927,888\n",
      "Trainable params: 925,856\n",
      "Non-trainable params: 2,032\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 2, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,521,680\n",
      "Trainable params: 3,517,600\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.8060 - acc: 0.1843\n",
      "Epoch 00001: val_loss improved from inf to 2.32255, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/001-2.3225.hdf5\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 2.8063 - acc: 0.1844 - val_loss: 2.3225 - val_acc: 0.2639\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.2411 - acc: 0.3018\n",
      "Epoch 00002: val_loss improved from 2.32255 to 2.22095, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/002-2.2210.hdf5\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 2.2411 - acc: 0.3018 - val_loss: 2.2210 - val_acc: 0.3168\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9925 - acc: 0.3735\n",
      "Epoch 00003: val_loss improved from 2.22095 to 2.18127, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_1_conv_checkpoint/003-2.1813.hdf5\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 1.9930 - acc: 0.3733 - val_loss: 2.1813 - val_acc: 0.3336\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.8343 - acc: 0.4265\n",
      "Epoch 00004: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.8342 - acc: 0.4264 - val_loss: 2.1848 - val_acc: 0.3231\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7241 - acc: 0.4590\n",
      "Epoch 00005: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.7245 - acc: 0.4589 - val_loss: 2.2401 - val_acc: 0.3208\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6292 - acc: 0.4889\n",
      "Epoch 00006: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.6296 - acc: 0.4887 - val_loss: 2.2303 - val_acc: 0.3291\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5473 - acc: 0.5163\n",
      "Epoch 00007: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.5473 - acc: 0.5162 - val_loss: 2.2381 - val_acc: 0.3263\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4686 - acc: 0.5376\n",
      "Epoch 00008: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.4684 - acc: 0.5378 - val_loss: 2.2146 - val_acc: 0.3454\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4175 - acc: 0.5553\n",
      "Epoch 00009: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.4171 - acc: 0.5555 - val_loss: 2.2112 - val_acc: 0.3431\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.5662\n",
      "Epoch 00010: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.3774 - acc: 0.5663 - val_loss: 2.2536 - val_acc: 0.3424\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3329 - acc: 0.5804\n",
      "Epoch 00011: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 1.3331 - acc: 0.5802 - val_loss: 2.3370 - val_acc: 0.3315\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2843 - acc: 0.5943\n",
      "Epoch 00012: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.2841 - acc: 0.5942 - val_loss: 2.2503 - val_acc: 0.3580\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2606 - acc: 0.6017\n",
      "Epoch 00013: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.2612 - acc: 0.6016 - val_loss: 2.2601 - val_acc: 0.3629\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2166 - acc: 0.6132\n",
      "Epoch 00014: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 1.2173 - acc: 0.6130 - val_loss: 2.3253 - val_acc: 0.3387\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1963 - acc: 0.6216\n",
      "Epoch 00015: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.1963 - acc: 0.6217 - val_loss: 2.2926 - val_acc: 0.3501\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1674 - acc: 0.6294\n",
      "Epoch 00016: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.1680 - acc: 0.6289 - val_loss: 2.3228 - val_acc: 0.3485\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1440 - acc: 0.6371\n",
      "Epoch 00017: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.1440 - acc: 0.6371 - val_loss: 2.3642 - val_acc: 0.3387\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1197 - acc: 0.6417\n",
      "Epoch 00018: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.1199 - acc: 0.6418 - val_loss: 2.4647 - val_acc: 0.3368\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1017 - acc: 0.6454\n",
      "Epoch 00019: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.1017 - acc: 0.6454 - val_loss: 2.4245 - val_acc: 0.3466\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6491\n",
      "Epoch 00020: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 1.0983 - acc: 0.6489 - val_loss: 2.3837 - val_acc: 0.3613\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0646 - acc: 0.6557\n",
      "Epoch 00021: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 1.0644 - acc: 0.6559 - val_loss: 2.3840 - val_acc: 0.3524\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0625 - acc: 0.6566\n",
      "Epoch 00022: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.0628 - acc: 0.6565 - val_loss: 2.4156 - val_acc: 0.3615\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0337 - acc: 0.6694\n",
      "Epoch 00023: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.0336 - acc: 0.6694 - val_loss: 2.5101 - val_acc: 0.3392\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0306 - acc: 0.6665\n",
      "Epoch 00024: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.0306 - acc: 0.6665 - val_loss: 2.4547 - val_acc: 0.3606\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0120 - acc: 0.6710\n",
      "Epoch 00025: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 1.0119 - acc: 0.6710 - val_loss: 2.5188 - val_acc: 0.3403\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0070 - acc: 0.6732\n",
      "Epoch 00026: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 1.0070 - acc: 0.6731 - val_loss: 2.5267 - val_acc: 0.3562\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9864 - acc: 0.6793\n",
      "Epoch 00027: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.9870 - acc: 0.6794 - val_loss: 2.5369 - val_acc: 0.3492\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.6835\n",
      "Epoch 00028: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.9836 - acc: 0.6834 - val_loss: 2.5407 - val_acc: 0.3585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9686 - acc: 0.6859\n",
      "Epoch 00029: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.9685 - acc: 0.6858 - val_loss: 2.6140 - val_acc: 0.3354\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.6895\n",
      "Epoch 00030: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.9642 - acc: 0.6894 - val_loss: 2.5368 - val_acc: 0.3550\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9541 - acc: 0.6914\n",
      "Epoch 00031: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9543 - acc: 0.6914 - val_loss: 2.5265 - val_acc: 0.3569\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9454 - acc: 0.6916\n",
      "Epoch 00032: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9459 - acc: 0.6915 - val_loss: 2.6244 - val_acc: 0.3447\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.6930\n",
      "Epoch 00033: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.9436 - acc: 0.6931 - val_loss: 2.6014 - val_acc: 0.3566\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.6963\n",
      "Epoch 00034: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.9354 - acc: 0.6963 - val_loss: 2.5521 - val_acc: 0.3594\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9217 - acc: 0.6984\n",
      "Epoch 00035: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9216 - acc: 0.6984 - val_loss: 2.5943 - val_acc: 0.3576\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9191 - acc: 0.6998\n",
      "Epoch 00036: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9199 - acc: 0.6996 - val_loss: 2.5908 - val_acc: 0.3545\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9138 - acc: 0.7033\n",
      "Epoch 00037: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9135 - acc: 0.7035 - val_loss: 2.5704 - val_acc: 0.3662\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9071 - acc: 0.7072\n",
      "Epoch 00038: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.9068 - acc: 0.7072 - val_loss: 2.5974 - val_acc: 0.3576\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.7056\n",
      "Epoch 00039: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.9053 - acc: 0.7054 - val_loss: 2.6555 - val_acc: 0.3494\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8951 - acc: 0.7127\n",
      "Epoch 00040: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.8955 - acc: 0.7125 - val_loss: 2.6183 - val_acc: 0.3550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.7080\n",
      "Epoch 00041: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8922 - acc: 0.7081 - val_loss: 2.7006 - val_acc: 0.3625\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7152\n",
      "Epoch 00042: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8827 - acc: 0.7149 - val_loss: 2.6508 - val_acc: 0.3594\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8858 - acc: 0.7114\n",
      "Epoch 00043: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8856 - acc: 0.7115 - val_loss: 2.6935 - val_acc: 0.3587\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8671 - acc: 0.7186\n",
      "Epoch 00044: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8675 - acc: 0.7184 - val_loss: 2.7539 - val_acc: 0.3524\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8719 - acc: 0.7154\n",
      "Epoch 00045: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8724 - acc: 0.7152 - val_loss: 2.6786 - val_acc: 0.3620\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8598 - acc: 0.7224\n",
      "Epoch 00046: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8605 - acc: 0.7222 - val_loss: 2.7966 - val_acc: 0.3429\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8562 - acc: 0.7190\n",
      "Epoch 00047: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.8572 - acc: 0.7187 - val_loss: 2.9426 - val_acc: 0.3168\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8532 - acc: 0.7207\n",
      "Epoch 00048: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8533 - acc: 0.7208 - val_loss: 2.7318 - val_acc: 0.3580\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7225\n",
      "Epoch 00049: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8454 - acc: 0.7225 - val_loss: 2.7073 - val_acc: 0.3634\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7204\n",
      "Epoch 00050: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8485 - acc: 0.7204 - val_loss: 2.7477 - val_acc: 0.3594\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8309 - acc: 0.7307\n",
      "Epoch 00051: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.8323 - acc: 0.7303 - val_loss: 2.6843 - val_acc: 0.3711\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8383 - acc: 0.7259\n",
      "Epoch 00052: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.8390 - acc: 0.7256 - val_loss: 2.8407 - val_acc: 0.3454\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8417 - acc: 0.7234\n",
      "Epoch 00053: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8419 - acc: 0.7235 - val_loss: 2.7521 - val_acc: 0.3606\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7315\n",
      "Epoch 00054: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8323 - acc: 0.7316 - val_loss: 2.8461 - val_acc: 0.3545\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7309\n",
      "Epoch 00055: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.8233 - acc: 0.7309 - val_loss: 2.7580 - val_acc: 0.3559\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7299\n",
      "Epoch 00056: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8224 - acc: 0.7300 - val_loss: 2.8302 - val_acc: 0.3466\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8249 - acc: 0.7340\n",
      "Epoch 00057: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8243 - acc: 0.7341 - val_loss: 2.8719 - val_acc: 0.3429\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7307\n",
      "Epoch 00058: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8113 - acc: 0.7307 - val_loss: 2.9736 - val_acc: 0.3366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7365\n",
      "Epoch 00059: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.8157 - acc: 0.7362 - val_loss: 2.8362 - val_acc: 0.3578\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.7362\n",
      "Epoch 00060: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8095 - acc: 0.7359 - val_loss: 2.8281 - val_acc: 0.3685\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.7369\n",
      "Epoch 00061: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8120 - acc: 0.7369 - val_loss: 2.8297 - val_acc: 0.3629\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7369\n",
      "Epoch 00062: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.8113 - acc: 0.7368 - val_loss: 2.8180 - val_acc: 0.3643\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7930 - acc: 0.7395\n",
      "Epoch 00063: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7930 - acc: 0.7395 - val_loss: 2.8464 - val_acc: 0.3536\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8068 - acc: 0.7382\n",
      "Epoch 00064: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8069 - acc: 0.7379 - val_loss: 2.9083 - val_acc: 0.3473\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8071 - acc: 0.7403\n",
      "Epoch 00065: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.8073 - acc: 0.7401 - val_loss: 2.8399 - val_acc: 0.3520\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7420\n",
      "Epoch 00066: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.7942 - acc: 0.7419 - val_loss: 2.8009 - val_acc: 0.3638\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7435\n",
      "Epoch 00067: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7905 - acc: 0.7434 - val_loss: 2.8179 - val_acc: 0.3622\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7460\n",
      "Epoch 00068: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7823 - acc: 0.7462 - val_loss: 2.9263 - val_acc: 0.3482\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7408\n",
      "Epoch 00069: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7948 - acc: 0.7407 - val_loss: 2.8840 - val_acc: 0.3608\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7447\n",
      "Epoch 00070: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7879 - acc: 0.7445 - val_loss: 2.8473 - val_acc: 0.3659\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7457\n",
      "Epoch 00071: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7833 - acc: 0.7457 - val_loss: 2.8260 - val_acc: 0.3692\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7826 - acc: 0.7436\n",
      "Epoch 00072: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7832 - acc: 0.7432 - val_loss: 2.8821 - val_acc: 0.3578\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7480\n",
      "Epoch 00073: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7765 - acc: 0.7478 - val_loss: 2.8524 - val_acc: 0.3569\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.7500\n",
      "Epoch 00074: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7701 - acc: 0.7498 - val_loss: 2.8917 - val_acc: 0.3590\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7475\n",
      "Epoch 00075: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7773 - acc: 0.7473 - val_loss: 2.8621 - val_acc: 0.3785\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.7491\n",
      "Epoch 00076: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7668 - acc: 0.7489 - val_loss: 2.9173 - val_acc: 0.3541\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.7479\n",
      "Epoch 00077: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.7701 - acc: 0.7480 - val_loss: 3.3506 - val_acc: 0.3175\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.7533\n",
      "Epoch 00078: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.7593 - acc: 0.7532 - val_loss: 2.9225 - val_acc: 0.3627\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7489\n",
      "Epoch 00079: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7663 - acc: 0.7485 - val_loss: 3.0031 - val_acc: 0.3399\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7623 - acc: 0.7539\n",
      "Epoch 00080: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7621 - acc: 0.7538 - val_loss: 2.9824 - val_acc: 0.3601\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7570 - acc: 0.7525\n",
      "Epoch 00081: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7567 - acc: 0.7526 - val_loss: 2.8864 - val_acc: 0.3583\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7544\n",
      "Epoch 00082: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7526 - acc: 0.7544 - val_loss: 2.8750 - val_acc: 0.3650\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7545 - acc: 0.7561\n",
      "Epoch 00083: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.7551 - acc: 0.7558 - val_loss: 2.9078 - val_acc: 0.3557\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7538\n",
      "Epoch 00084: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7511 - acc: 0.7540 - val_loss: 2.9540 - val_acc: 0.3541\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7531 - acc: 0.7546\n",
      "Epoch 00085: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7526 - acc: 0.7548 - val_loss: 2.8928 - val_acc: 0.3692\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7596\n",
      "Epoch 00086: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7397 - acc: 0.7595 - val_loss: 2.9318 - val_acc: 0.3587\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7456 - acc: 0.7571\n",
      "Epoch 00087: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.7453 - acc: 0.7570 - val_loss: 3.0327 - val_acc: 0.3494\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7556 - acc: 0.7556\n",
      "Epoch 00088: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.7551 - acc: 0.7558 - val_loss: 3.0457 - val_acc: 0.3510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.7596\n",
      "Epoch 00089: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.7444 - acc: 0.7596 - val_loss: 2.9824 - val_acc: 0.3629\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7393 - acc: 0.7575\n",
      "Epoch 00090: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.7403 - acc: 0.7571 - val_loss: 2.9719 - val_acc: 0.3580\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7606\n",
      "Epoch 00091: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.7350 - acc: 0.7607 - val_loss: 2.9954 - val_acc: 0.3569\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7620\n",
      "Epoch 00092: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.7317 - acc: 0.7621 - val_loss: 2.8878 - val_acc: 0.3706\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7308 - acc: 0.7619\n",
      "Epoch 00093: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.7316 - acc: 0.7618 - val_loss: 2.8991 - val_acc: 0.3597\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.7594\n",
      "Epoch 00094: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7370 - acc: 0.7592 - val_loss: 2.9355 - val_acc: 0.3620\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.7633\n",
      "Epoch 00095: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7322 - acc: 0.7633 - val_loss: 2.9548 - val_acc: 0.3643\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7678\n",
      "Epoch 00096: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7213 - acc: 0.7676 - val_loss: 2.9459 - val_acc: 0.3620\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7652\n",
      "Epoch 00097: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7325 - acc: 0.7650 - val_loss: 2.9866 - val_acc: 0.3562\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7264 - acc: 0.7627\n",
      "Epoch 00098: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7263 - acc: 0.7626 - val_loss: 3.0693 - val_acc: 0.3594\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7196 - acc: 0.7619\n",
      "Epoch 00099: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.7201 - acc: 0.7620 - val_loss: 3.0105 - val_acc: 0.3673\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7614\n",
      "Epoch 00100: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7260 - acc: 0.7616 - val_loss: 2.9637 - val_acc: 0.3552\n",
      "Epoch 101/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7331 - acc: 0.7614\n",
      "Epoch 00101: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7332 - acc: 0.7615 - val_loss: 3.2245 - val_acc: 0.3480\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.7624\n",
      "Epoch 00102: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.7277 - acc: 0.7622 - val_loss: 3.0865 - val_acc: 0.3443\n",
      "Epoch 103/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7649\n",
      "Epoch 00103: val_loss did not improve from 2.18127\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7320 - acc: 0.7650 - val_loss: 3.0043 - val_acc: 0.3613\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 199us/sample - loss: 2.2417 - acc: 0.3151\n",
      "Loss: 2.241665267251115 Accuracy: 0.3150571\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.8560 - acc: 0.2086\n",
      "Epoch 00001: val_loss improved from inf to 2.21674, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/001-2.2167.hdf5\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 2.8546 - acc: 0.2088 - val_loss: 2.2167 - val_acc: 0.2944\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0723 - acc: 0.3571\n",
      "Epoch 00002: val_loss improved from 2.21674 to 1.79946, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/002-1.7995.hdf5\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 2.0718 - acc: 0.3573 - val_loss: 1.7995 - val_acc: 0.4386\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8083 - acc: 0.4293\n",
      "Epoch 00003: val_loss improved from 1.79946 to 1.69779, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/003-1.6978.hdf5\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 1.8080 - acc: 0.4295 - val_loss: 1.6978 - val_acc: 0.4750\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6378 - acc: 0.4834\n",
      "Epoch 00004: val_loss improved from 1.69779 to 1.61857, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/004-1.6186.hdf5\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 1.6380 - acc: 0.4833 - val_loss: 1.6186 - val_acc: 0.5003\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5063 - acc: 0.5239\n",
      "Epoch 00005: val_loss improved from 1.61857 to 1.54269, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/005-1.5427.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 1.5060 - acc: 0.5240 - val_loss: 1.5427 - val_acc: 0.5379\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4020 - acc: 0.5582\n",
      "Epoch 00006: val_loss did not improve from 1.54269\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 1.4018 - acc: 0.5582 - val_loss: 1.5653 - val_acc: 0.5139\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3217 - acc: 0.5813\n",
      "Epoch 00007: val_loss did not improve from 1.54269\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 1.3213 - acc: 0.5815 - val_loss: 1.5579 - val_acc: 0.5087\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2504 - acc: 0.6000\n",
      "Epoch 00008: val_loss did not improve from 1.54269\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 1.2505 - acc: 0.6000 - val_loss: 1.5578 - val_acc: 0.5316\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1860 - acc: 0.6210\n",
      "Epoch 00009: val_loss improved from 1.54269 to 1.51581, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/009-1.5158.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 1.1858 - acc: 0.6211 - val_loss: 1.5158 - val_acc: 0.5390\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1256 - acc: 0.6406\n",
      "Epoch 00010: val_loss improved from 1.51581 to 1.50172, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/010-1.5017.hdf5\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 1.1258 - acc: 0.6404 - val_loss: 1.5017 - val_acc: 0.5437\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.6516\n",
      "Epoch 00011: val_loss improved from 1.50172 to 1.49514, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/011-1.4951.hdf5\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 1.0766 - acc: 0.6515 - val_loss: 1.4951 - val_acc: 0.5458\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0413 - acc: 0.6647\n",
      "Epoch 00012: val_loss did not improve from 1.49514\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 1.0411 - acc: 0.6647 - val_loss: 1.5186 - val_acc: 0.5446\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9974 - acc: 0.6764\n",
      "Epoch 00013: val_loss did not improve from 1.49514\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.9976 - acc: 0.6763 - val_loss: 1.5563 - val_acc: 0.5337\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9605 - acc: 0.6917\n",
      "Epoch 00014: val_loss did not improve from 1.49514\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.9610 - acc: 0.6916 - val_loss: 1.6446 - val_acc: 0.5199\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.6978\n",
      "Epoch 00015: val_loss did not improve from 1.49514\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.9281 - acc: 0.6977 - val_loss: 1.5126 - val_acc: 0.5563\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7105\n",
      "Epoch 00016: val_loss did not improve from 1.49514\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.8910 - acc: 0.7104 - val_loss: 1.5731 - val_acc: 0.5416\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8789 - acc: 0.7142\n",
      "Epoch 00017: val_loss improved from 1.49514 to 1.48399, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_2_conv_checkpoint/017-1.4840.hdf5\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.8790 - acc: 0.7142 - val_loss: 1.4840 - val_acc: 0.5723\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8554 - acc: 0.7219\n",
      "Epoch 00018: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.8556 - acc: 0.7219 - val_loss: 1.5921 - val_acc: 0.5372\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7303\n",
      "Epoch 00019: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.8229 - acc: 0.7304 - val_loss: 1.5790 - val_acc: 0.5495\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7382\n",
      "Epoch 00020: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.8018 - acc: 0.7382 - val_loss: 1.5119 - val_acc: 0.5735\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.7389\n",
      "Epoch 00021: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.7926 - acc: 0.7390 - val_loss: 1.5390 - val_acc: 0.5630\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7444\n",
      "Epoch 00022: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.7737 - acc: 0.7443 - val_loss: 1.5772 - val_acc: 0.5502\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7483 - acc: 0.7554\n",
      "Epoch 00023: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.7480 - acc: 0.7555 - val_loss: 1.6673 - val_acc: 0.5250\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7427 - acc: 0.7536\n",
      "Epoch 00024: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.7426 - acc: 0.7535 - val_loss: 1.5805 - val_acc: 0.5546\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7136 - acc: 0.7664\n",
      "Epoch 00025: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.7136 - acc: 0.7664 - val_loss: 1.6651 - val_acc: 0.5257\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7133 - acc: 0.7639\n",
      "Epoch 00026: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.7134 - acc: 0.7639 - val_loss: 1.4869 - val_acc: 0.5800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.7721\n",
      "Epoch 00027: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.6908 - acc: 0.7721 - val_loss: 1.6715 - val_acc: 0.5441\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.7728\n",
      "Epoch 00028: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.6829 - acc: 0.7725 - val_loss: 1.5618 - val_acc: 0.5653\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6718 - acc: 0.7759\n",
      "Epoch 00029: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.6717 - acc: 0.7758 - val_loss: 1.6246 - val_acc: 0.5500\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6566 - acc: 0.7824\n",
      "Epoch 00030: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.6565 - acc: 0.7825 - val_loss: 1.6046 - val_acc: 0.5623\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6447 - acc: 0.7868\n",
      "Epoch 00031: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.6447 - acc: 0.7867 - val_loss: 1.6911 - val_acc: 0.5369\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.7871\n",
      "Epoch 00032: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.6452 - acc: 0.7872 - val_loss: 2.2054 - val_acc: 0.4503\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6203 - acc: 0.7947\n",
      "Epoch 00033: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.6205 - acc: 0.7947 - val_loss: 1.5761 - val_acc: 0.5679\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6135 - acc: 0.7937\n",
      "Epoch 00034: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.6134 - acc: 0.7938 - val_loss: 1.5559 - val_acc: 0.5791\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6085 - acc: 0.7967\n",
      "Epoch 00035: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.6087 - acc: 0.7966 - val_loss: 1.5808 - val_acc: 0.5679\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.8034\n",
      "Epoch 00036: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.5965 - acc: 0.8033 - val_loss: 1.5705 - val_acc: 0.5728\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.8028\n",
      "Epoch 00037: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5878 - acc: 0.8027 - val_loss: 1.6139 - val_acc: 0.5658\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.8053\n",
      "Epoch 00038: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.5854 - acc: 0.8053 - val_loss: 1.6781 - val_acc: 0.5514\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.8067\n",
      "Epoch 00039: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5762 - acc: 0.8068 - val_loss: 1.6456 - val_acc: 0.5607\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5681 - acc: 0.8111\n",
      "Epoch 00040: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.5680 - acc: 0.8111 - val_loss: 1.8038 - val_acc: 0.5346\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.8119\n",
      "Epoch 00041: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5617 - acc: 0.8118 - val_loss: 1.6291 - val_acc: 0.5537\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8142\n",
      "Epoch 00042: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.5547 - acc: 0.8143 - val_loss: 1.6393 - val_acc: 0.5721\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8180\n",
      "Epoch 00043: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5529 - acc: 0.8179 - val_loss: 1.6437 - val_acc: 0.5688\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8234\n",
      "Epoch 00044: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.5323 - acc: 0.8233 - val_loss: 1.6744 - val_acc: 0.5462\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8221\n",
      "Epoch 00045: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5328 - acc: 0.8218 - val_loss: 1.8583 - val_acc: 0.5190\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8232\n",
      "Epoch 00046: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5298 - acc: 0.8232 - val_loss: 1.6328 - val_acc: 0.5691\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8255\n",
      "Epoch 00047: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5240 - acc: 0.8256 - val_loss: 1.6190 - val_acc: 0.5807\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8276\n",
      "Epoch 00048: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5152 - acc: 0.8277 - val_loss: 1.7050 - val_acc: 0.5493\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8341\n",
      "Epoch 00049: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.4998 - acc: 0.8342 - val_loss: 1.6564 - val_acc: 0.5609\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.8320\n",
      "Epoch 00050: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.4991 - acc: 0.8319 - val_loss: 1.6759 - val_acc: 0.5712\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4951 - acc: 0.8342\n",
      "Epoch 00051: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.4947 - acc: 0.8344 - val_loss: 1.5920 - val_acc: 0.5844\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8357\n",
      "Epoch 00052: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4932 - acc: 0.8358 - val_loss: 1.7530 - val_acc: 0.5504\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8394\n",
      "Epoch 00053: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.4895 - acc: 0.8393 - val_loss: 1.6650 - val_acc: 0.5721\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8396\n",
      "Epoch 00054: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.4822 - acc: 0.8392 - val_loss: 1.7289 - val_acc: 0.5500\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4768 - acc: 0.8402\n",
      "Epoch 00055: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4772 - acc: 0.8399 - val_loss: 1.7321 - val_acc: 0.5537\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8405\n",
      "Epoch 00056: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4800 - acc: 0.8405 - val_loss: 1.6760 - val_acc: 0.5660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8444\n",
      "Epoch 00057: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4620 - acc: 0.8443 - val_loss: 1.8681 - val_acc: 0.5434\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8450\n",
      "Epoch 00058: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.4690 - acc: 0.8449 - val_loss: 1.7675 - val_acc: 0.5558\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8455\n",
      "Epoch 00059: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4621 - acc: 0.8456 - val_loss: 1.6331 - val_acc: 0.5763\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8484\n",
      "Epoch 00060: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.4528 - acc: 0.8485 - val_loss: 1.6490 - val_acc: 0.5768\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8491\n",
      "Epoch 00061: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4541 - acc: 0.8492 - val_loss: 1.7065 - val_acc: 0.5730\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.8491\n",
      "Epoch 00062: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4522 - acc: 0.8491 - val_loss: 1.8532 - val_acc: 0.5446\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8503\n",
      "Epoch 00063: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4441 - acc: 0.8503 - val_loss: 1.6980 - val_acc: 0.5658\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8505\n",
      "Epoch 00064: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4489 - acc: 0.8505 - val_loss: 1.6578 - val_acc: 0.5842\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8541\n",
      "Epoch 00065: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4376 - acc: 0.8542 - val_loss: 1.6583 - val_acc: 0.5833\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8521\n",
      "Epoch 00066: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4364 - acc: 0.8522 - val_loss: 1.7186 - val_acc: 0.5749\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8538\n",
      "Epoch 00067: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.4328 - acc: 0.8538 - val_loss: 1.6839 - val_acc: 0.5809\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.8573\n",
      "Epoch 00068: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4328 - acc: 0.8573 - val_loss: 1.8294 - val_acc: 0.5574\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8567\n",
      "Epoch 00069: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.4289 - acc: 0.8566 - val_loss: 1.7019 - val_acc: 0.5765\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8599\n",
      "Epoch 00070: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4234 - acc: 0.8599 - val_loss: 1.6663 - val_acc: 0.5858\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8635\n",
      "Epoch 00071: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.4108 - acc: 0.8636 - val_loss: 1.6982 - val_acc: 0.5775\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8628\n",
      "Epoch 00072: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4139 - acc: 0.8627 - val_loss: 1.6514 - val_acc: 0.5910\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8651\n",
      "Epoch 00073: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.4092 - acc: 0.8650 - val_loss: 1.7556 - val_acc: 0.5693\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8629\n",
      "Epoch 00074: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4090 - acc: 0.8627 - val_loss: 1.8373 - val_acc: 0.5516\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8661\n",
      "Epoch 00075: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4052 - acc: 0.8661 - val_loss: 1.6630 - val_acc: 0.5872\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8650\n",
      "Epoch 00076: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.4039 - acc: 0.8650 - val_loss: 1.6472 - val_acc: 0.5977\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8682\n",
      "Epoch 00077: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3926 - acc: 0.8681 - val_loss: 1.8892 - val_acc: 0.5486\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8709\n",
      "Epoch 00078: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3895 - acc: 0.8710 - val_loss: 1.6787 - val_acc: 0.5842\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8715\n",
      "Epoch 00079: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3857 - acc: 0.8715 - val_loss: 1.7139 - val_acc: 0.5805\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8701\n",
      "Epoch 00080: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3892 - acc: 0.8702 - val_loss: 1.7451 - val_acc: 0.5733\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8733\n",
      "Epoch 00081: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.3815 - acc: 0.8734 - val_loss: 1.7275 - val_acc: 0.5791\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8736\n",
      "Epoch 00082: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3829 - acc: 0.8736 - val_loss: 1.6827 - val_acc: 0.5858\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8708\n",
      "Epoch 00083: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3851 - acc: 0.8707 - val_loss: 2.2620 - val_acc: 0.4957\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8746\n",
      "Epoch 00084: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3750 - acc: 0.8745 - val_loss: 1.6959 - val_acc: 0.5844\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8797\n",
      "Epoch 00085: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3688 - acc: 0.8797 - val_loss: 1.6968 - val_acc: 0.5844\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8757\n",
      "Epoch 00086: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3719 - acc: 0.8757 - val_loss: 2.0218 - val_acc: 0.5339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8746\n",
      "Epoch 00087: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3756 - acc: 0.8746 - val_loss: 2.2873 - val_acc: 0.4838\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8791\n",
      "Epoch 00088: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3627 - acc: 0.8793 - val_loss: 1.7090 - val_acc: 0.5970\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8805\n",
      "Epoch 00089: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.3593 - acc: 0.8806 - val_loss: 1.7816 - val_acc: 0.5723\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8829\n",
      "Epoch 00090: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3580 - acc: 0.8829 - val_loss: 1.8392 - val_acc: 0.5730\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8787\n",
      "Epoch 00091: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3633 - acc: 0.8787 - val_loss: 1.7051 - val_acc: 0.5872\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8828\n",
      "Epoch 00092: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3563 - acc: 0.8829 - val_loss: 1.6888 - val_acc: 0.5956\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8829\n",
      "Epoch 00093: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3521 - acc: 0.8829 - val_loss: 1.7223 - val_acc: 0.5826\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8848\n",
      "Epoch 00094: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3503 - acc: 0.8848 - val_loss: 1.8294 - val_acc: 0.5716\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8865\n",
      "Epoch 00095: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3455 - acc: 0.8865 - val_loss: 1.9001 - val_acc: 0.5565\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8850\n",
      "Epoch 00096: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3483 - acc: 0.8849 - val_loss: 1.6752 - val_acc: 0.5959\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8871\n",
      "Epoch 00097: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3431 - acc: 0.8872 - val_loss: 1.9166 - val_acc: 0.5432\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8860\n",
      "Epoch 00098: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.3471 - acc: 0.8859 - val_loss: 1.7935 - val_acc: 0.5768\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8886\n",
      "Epoch 00099: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3391 - acc: 0.8885 - val_loss: 1.8272 - val_acc: 0.5677\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8902\n",
      "Epoch 00100: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3374 - acc: 0.8902 - val_loss: 1.7673 - val_acc: 0.5812\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8892\n",
      "Epoch 00101: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3378 - acc: 0.8891 - val_loss: 1.9481 - val_acc: 0.5558\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8902\n",
      "Epoch 00102: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3349 - acc: 0.8901 - val_loss: 1.6968 - val_acc: 0.5996\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8901\n",
      "Epoch 00103: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3316 - acc: 0.8901 - val_loss: 1.6927 - val_acc: 0.5917\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8923\n",
      "Epoch 00104: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3225 - acc: 0.8924 - val_loss: 1.7084 - val_acc: 0.6019\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8933\n",
      "Epoch 00105: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.3291 - acc: 0.8934 - val_loss: 1.6975 - val_acc: 0.6007\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8941\n",
      "Epoch 00106: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3277 - acc: 0.8940 - val_loss: 1.9765 - val_acc: 0.5434\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8962\n",
      "Epoch 00107: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3181 - acc: 0.8962 - val_loss: 1.8175 - val_acc: 0.5788\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8961\n",
      "Epoch 00108: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3189 - acc: 0.8961 - val_loss: 1.7351 - val_acc: 0.5973\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.8950\n",
      "Epoch 00109: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.3228 - acc: 0.8950 - val_loss: 1.8028 - val_acc: 0.5875\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8950\n",
      "Epoch 00110: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3144 - acc: 0.8950 - val_loss: 1.7313 - val_acc: 0.5877\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8999\n",
      "Epoch 00111: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3118 - acc: 0.8997 - val_loss: 2.7517 - val_acc: 0.4573\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8984\n",
      "Epoch 00112: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3105 - acc: 0.8984 - val_loss: 1.7536 - val_acc: 0.5966\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8976\n",
      "Epoch 00113: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3133 - acc: 0.8976 - val_loss: 1.7515 - val_acc: 0.5984\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9016\n",
      "Epoch 00114: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.3062 - acc: 0.9015 - val_loss: 1.7384 - val_acc: 0.5877\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8989\n",
      "Epoch 00115: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.3078 - acc: 0.8989 - val_loss: 1.7419 - val_acc: 0.5907\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9001\n",
      "Epoch 00116: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3048 - acc: 0.9001 - val_loss: 1.7508 - val_acc: 0.5952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9006\n",
      "Epoch 00117: val_loss did not improve from 1.48399\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.3044 - acc: 0.9006 - val_loss: 1.8481 - val_acc: 0.5775\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 227us/sample - loss: 1.5744 - acc: 0.5394\n",
      "Loss: 1.5744021488746256 Accuracy: 0.5393562\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4629 - acc: 0.2697\n",
      "Epoch 00001: val_loss improved from inf to 1.83709, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/001-1.8371.hdf5\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 2.4628 - acc: 0.2698 - val_loss: 1.8371 - val_acc: 0.3976\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7392 - acc: 0.4434\n",
      "Epoch 00002: val_loss improved from 1.83709 to 1.45393, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/002-1.4539.hdf5\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 1.7393 - acc: 0.4434 - val_loss: 1.4539 - val_acc: 0.5518\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5142 - acc: 0.5141\n",
      "Epoch 00003: val_loss improved from 1.45393 to 1.34277, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/003-1.3428.hdf5\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 1.5143 - acc: 0.5143 - val_loss: 1.3428 - val_acc: 0.5742\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3619 - acc: 0.5633-\n",
      "Epoch 00004: val_loss improved from 1.34277 to 1.27948, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/004-1.2795.hdf5\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 1.3621 - acc: 0.5633 - val_loss: 1.2795 - val_acc: 0.6017\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2554 - acc: 0.5997\n",
      "Epoch 00005: val_loss improved from 1.27948 to 1.22204, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/005-1.2220.hdf5\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 1.2553 - acc: 0.5998 - val_loss: 1.2220 - val_acc: 0.6222\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1692 - acc: 0.6308\n",
      "Epoch 00006: val_loss improved from 1.22204 to 1.17805, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/006-1.1781.hdf5\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 1.1692 - acc: 0.6308 - val_loss: 1.1781 - val_acc: 0.6362\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1010 - acc: 0.6494\n",
      "Epoch 00007: val_loss improved from 1.17805 to 1.17450, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/007-1.1745.hdf5\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 1.1008 - acc: 0.6495 - val_loss: 1.1745 - val_acc: 0.6294\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0432 - acc: 0.6699\n",
      "Epoch 00008: val_loss improved from 1.17450 to 1.15969, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/008-1.1597.hdf5\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 1.0434 - acc: 0.6698 - val_loss: 1.1597 - val_acc: 0.6403\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9847 - acc: 0.6888\n",
      "Epoch 00009: val_loss improved from 1.15969 to 1.11742, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/009-1.1174.hdf5\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.9845 - acc: 0.6889 - val_loss: 1.1174 - val_acc: 0.6557\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9410 - acc: 0.7037\n",
      "Epoch 00010: val_loss improved from 1.11742 to 1.08256, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/010-1.0826.hdf5\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.9409 - acc: 0.7038 - val_loss: 1.0826 - val_acc: 0.6671\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9014 - acc: 0.7147\n",
      "Epoch 00011: val_loss did not improve from 1.08256\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.9015 - acc: 0.7147 - val_loss: 1.1194 - val_acc: 0.6518\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.7245\n",
      "Epoch 00012: val_loss did not improve from 1.08256\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.8703 - acc: 0.7246 - val_loss: 1.1324 - val_acc: 0.6543\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.7382\n",
      "Epoch 00013: val_loss improved from 1.08256 to 1.05442, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/013-1.0544.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.8272 - acc: 0.7384 - val_loss: 1.0544 - val_acc: 0.6718\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7459\n",
      "Epoch 00014: val_loss did not improve from 1.05442\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.7966 - acc: 0.7460 - val_loss: 1.0726 - val_acc: 0.6641\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7677 - acc: 0.7568\n",
      "Epoch 00015: val_loss did not improve from 1.05442\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.7679 - acc: 0.7567 - val_loss: 1.0770 - val_acc: 0.6778\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7626- ETA: 1s - loss: 0.7\n",
      "Epoch 00016: val_loss improved from 1.05442 to 1.02877, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_3_conv_checkpoint/016-1.0288.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.7442 - acc: 0.7625 - val_loss: 1.0288 - val_acc: 0.6846\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7311 - acc: 0.7644\n",
      "Epoch 00017: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.7313 - acc: 0.7644 - val_loss: 1.1113 - val_acc: 0.6629\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7753\n",
      "Epoch 00018: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.6999 - acc: 0.7753 - val_loss: 1.1511 - val_acc: 0.6490\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.7808\n",
      "Epoch 00019: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.6859 - acc: 0.7808 - val_loss: 1.1779 - val_acc: 0.6443\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6667 - acc: 0.7843\n",
      "Epoch 00020: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.6669 - acc: 0.7843 - val_loss: 1.2327 - val_acc: 0.6322\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.7925\n",
      "Epoch 00021: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.6505 - acc: 0.7922 - val_loss: 1.0641 - val_acc: 0.6825\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6302 - acc: 0.7963\n",
      "Epoch 00022: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.6302 - acc: 0.7962 - val_loss: 1.1355 - val_acc: 0.6625\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.7999\n",
      "Epoch 00023: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.6178 - acc: 0.7998 - val_loss: 1.1093 - val_acc: 0.6774\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6114 - acc: 0.8026\n",
      "Epoch 00024: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.6118 - acc: 0.8025 - val_loss: 1.1496 - val_acc: 0.6611\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.8062\n",
      "Epoch 00025: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.5952 - acc: 0.8061 - val_loss: 1.1401 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.8117\n",
      "Epoch 00026: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.5803 - acc: 0.8117 - val_loss: 1.2087 - val_acc: 0.6527\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8142\n",
      "Epoch 00027: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.5620 - acc: 0.8142 - val_loss: 1.3361 - val_acc: 0.6150\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8141\n",
      "Epoch 00028: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.5619 - acc: 0.8143 - val_loss: 1.0628 - val_acc: 0.6942\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.8221\n",
      "Epoch 00029: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.5482 - acc: 0.8220 - val_loss: 1.0882 - val_acc: 0.6895\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8230\n",
      "Epoch 00030: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.5383 - acc: 0.8230 - val_loss: 1.1636 - val_acc: 0.6615\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.8264\n",
      "Epoch 00031: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.5311 - acc: 0.8265 - val_loss: 1.2525 - val_acc: 0.6492\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8330\n",
      "Epoch 00032: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.5171 - acc: 0.8329 - val_loss: 1.2999 - val_acc: 0.6490\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.8314\n",
      "Epoch 00033: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.5108 - acc: 0.8314 - val_loss: 1.0694 - val_acc: 0.6935\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8323\n",
      "Epoch 00034: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.5079 - acc: 0.8322 - val_loss: 1.0510 - val_acc: 0.7021\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8362\n",
      "Epoch 00035: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.5009 - acc: 0.8361 - val_loss: 1.1169 - val_acc: 0.6890\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8363\n",
      "Epoch 00036: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.4944 - acc: 0.8362 - val_loss: 1.0706 - val_acc: 0.7030\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8426\n",
      "Epoch 00037: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.4797 - acc: 0.8423 - val_loss: 1.0870 - val_acc: 0.6953\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8404\n",
      "Epoch 00038: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.4794 - acc: 0.8405 - val_loss: 1.1285 - val_acc: 0.6846\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8433\n",
      "Epoch 00039: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4704 - acc: 0.8435 - val_loss: 1.0848 - val_acc: 0.7002\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8503\n",
      "Epoch 00040: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4590 - acc: 0.8504 - val_loss: 1.0730 - val_acc: 0.7000\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8505\n",
      "Epoch 00041: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4518 - acc: 0.8505 - val_loss: 1.2500 - val_acc: 0.6636\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8521\n",
      "Epoch 00042: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 367us/sample - loss: 0.4460 - acc: 0.8520 - val_loss: 1.0647 - val_acc: 0.7105\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.8536\n",
      "Epoch 00043: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.4398 - acc: 0.8535 - val_loss: 1.0875 - val_acc: 0.6983\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8553\n",
      "Epoch 00044: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.4363 - acc: 0.8554 - val_loss: 1.0839 - val_acc: 0.7046\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8563\n",
      "Epoch 00045: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4353 - acc: 0.8563 - val_loss: 1.2410 - val_acc: 0.6618\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8553\n",
      "Epoch 00046: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.4349 - acc: 0.8555 - val_loss: 1.0938 - val_acc: 0.7039\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8628\n",
      "Epoch 00047: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.4150 - acc: 0.8628 - val_loss: 1.3465 - val_acc: 0.6387\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8607\n",
      "Epoch 00048: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.4166 - acc: 0.8606 - val_loss: 1.1050 - val_acc: 0.7035\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8658\n",
      "Epoch 00049: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.4091 - acc: 0.8658 - val_loss: 1.2390 - val_acc: 0.6632\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8662\n",
      "Epoch 00050: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4036 - acc: 0.8661 - val_loss: 1.1971 - val_acc: 0.6848\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8664\n",
      "Epoch 00051: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.4007 - acc: 0.8664 - val_loss: 1.1585 - val_acc: 0.6928\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8673\n",
      "Epoch 00052: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 367us/sample - loss: 0.3976 - acc: 0.8674 - val_loss: 1.0828 - val_acc: 0.7063\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8720\n",
      "Epoch 00053: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3906 - acc: 0.8719 - val_loss: 1.0506 - val_acc: 0.7263\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8721\n",
      "Epoch 00054: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.3857 - acc: 0.8722 - val_loss: 1.1351 - val_acc: 0.6914\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8728\n",
      "Epoch 00055: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.3820 - acc: 0.8728 - val_loss: 1.1251 - val_acc: 0.6981\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8746\n",
      "Epoch 00056: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3786 - acc: 0.8745 - val_loss: 1.1355 - val_acc: 0.6995\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8763\n",
      "Epoch 00057: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.3768 - acc: 0.8763 - val_loss: 1.1151 - val_acc: 0.7060\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8793- ETA: 0s - loss: 0.3663 - acc:\n",
      "Epoch 00058: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.3673 - acc: 0.8791 - val_loss: 1.1163 - val_acc: 0.7093\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8777\n",
      "Epoch 00059: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3701 - acc: 0.8777 - val_loss: 1.0714 - val_acc: 0.7207\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8775\n",
      "Epoch 00060: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3656 - acc: 0.8774 - val_loss: 1.0626 - val_acc: 0.7212\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8832\n",
      "Epoch 00061: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3551 - acc: 0.8832 - val_loss: 1.0756 - val_acc: 0.7256\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8823\n",
      "Epoch 00062: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.3539 - acc: 0.8823 - val_loss: 1.0592 - val_acc: 0.7268\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8842\n",
      "Epoch 00063: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3493 - acc: 0.8841 - val_loss: 1.0901 - val_acc: 0.7142\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8850\n",
      "Epoch 00064: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3462 - acc: 0.8849 - val_loss: 1.3922 - val_acc: 0.6494\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8845\n",
      "Epoch 00065: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3478 - acc: 0.8846 - val_loss: 1.0707 - val_acc: 0.7179\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8847\n",
      "Epoch 00066: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3436 - acc: 0.8846 - val_loss: 1.0610 - val_acc: 0.7214\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8877\n",
      "Epoch 00067: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.3384 - acc: 0.8876 - val_loss: 1.1131 - val_acc: 0.7100\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8899\n",
      "Epoch 00068: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3341 - acc: 0.8900 - val_loss: 1.0820 - val_acc: 0.7191\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8894\n",
      "Epoch 00069: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.3297 - acc: 0.8893 - val_loss: 1.1495 - val_acc: 0.7044\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8921\n",
      "Epoch 00070: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3261 - acc: 0.8923 - val_loss: 1.0746 - val_acc: 0.7165\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8917\n",
      "Epoch 00071: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.3273 - acc: 0.8916 - val_loss: 1.1610 - val_acc: 0.7039\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.8939\n",
      "Epoch 00072: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.3196 - acc: 0.8939 - val_loss: 1.1453 - val_acc: 0.7165\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8948\n",
      "Epoch 00073: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3163 - acc: 0.8949 - val_loss: 1.0791 - val_acc: 0.7207\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8944\n",
      "Epoch 00074: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3185 - acc: 0.8944 - val_loss: 1.0901 - val_acc: 0.7207\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8938\n",
      "Epoch 00075: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3212 - acc: 0.8939 - val_loss: 1.0902 - val_acc: 0.7202\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.8988\n",
      "Epoch 00076: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.3083 - acc: 0.8989 - val_loss: 1.0610 - val_acc: 0.7258\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.8999\n",
      "Epoch 00077: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3027 - acc: 0.8999 - val_loss: 1.2281 - val_acc: 0.6860\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9023\n",
      "Epoch 00078: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.3023 - acc: 0.9023 - val_loss: 1.1592 - val_acc: 0.7051\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.8996\n",
      "Epoch 00079: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3039 - acc: 0.8997 - val_loss: 1.0906 - val_acc: 0.7205\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9014\n",
      "Epoch 00080: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.3017 - acc: 0.9014 - val_loss: 1.7406 - val_acc: 0.5986\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9022\n",
      "Epoch 00081: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2937 - acc: 0.9023 - val_loss: 1.2331 - val_acc: 0.6904\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9033\n",
      "Epoch 00082: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2957 - acc: 0.9033 - val_loss: 1.1716 - val_acc: 0.6997\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9044\n",
      "Epoch 00083: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2909 - acc: 0.9044 - val_loss: 1.0695 - val_acc: 0.7261\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9063\n",
      "Epoch 00084: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.2892 - acc: 0.9063 - val_loss: 1.1877 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9060\n",
      "Epoch 00085: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2849 - acc: 0.9058 - val_loss: 1.0657 - val_acc: 0.7326\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9034\n",
      "Epoch 00086: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2896 - acc: 0.9035 - val_loss: 1.1306 - val_acc: 0.7151\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9090\n",
      "Epoch 00087: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.2799 - acc: 0.9090 - val_loss: 1.1973 - val_acc: 0.6995\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9080\n",
      "Epoch 00088: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.2803 - acc: 0.9080 - val_loss: 1.1670 - val_acc: 0.7046\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9109\n",
      "Epoch 00089: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2719 - acc: 0.9109 - val_loss: 1.0658 - val_acc: 0.7289\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9072\n",
      "Epoch 00090: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2791 - acc: 0.9072 - val_loss: 1.1105 - val_acc: 0.7258\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9098\n",
      "Epoch 00091: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.2743 - acc: 0.9099 - val_loss: 1.1221 - val_acc: 0.7144\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9100\n",
      "Epoch 00092: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.2734 - acc: 0.9100 - val_loss: 1.1377 - val_acc: 0.7221\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9133\n",
      "Epoch 00093: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2650 - acc: 0.9132 - val_loss: 1.0969 - val_acc: 0.7275\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9105\n",
      "Epoch 00094: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2726 - acc: 0.9106 - val_loss: 1.1128 - val_acc: 0.7233\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9137\n",
      "Epoch 00095: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.2633 - acc: 0.9138 - val_loss: 1.3344 - val_acc: 0.6790\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9143\n",
      "Epoch 00096: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2626 - acc: 0.9142 - val_loss: 1.0938 - val_acc: 0.7314\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9142\n",
      "Epoch 00097: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.2603 - acc: 0.9141 - val_loss: 1.1133 - val_acc: 0.7254\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9150\n",
      "Epoch 00098: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.2578 - acc: 0.9150 - val_loss: 1.1036 - val_acc: 0.7265\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9162\n",
      "Epoch 00099: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2588 - acc: 0.9162 - val_loss: 1.2024 - val_acc: 0.7028\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9167\n",
      "Epoch 00100: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2554 - acc: 0.9168 - val_loss: 1.2991 - val_acc: 0.6951\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9184\n",
      "Epoch 00101: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.2527 - acc: 0.9184 - val_loss: 1.1191 - val_acc: 0.7223\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9171\n",
      "Epoch 00102: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2528 - acc: 0.9171 - val_loss: 1.1360 - val_acc: 0.7223\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9180\n",
      "Epoch 00103: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.2502 - acc: 0.9180 - val_loss: 1.1353 - val_acc: 0.7198\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9199\n",
      "Epoch 00104: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2472 - acc: 0.9198 - val_loss: 1.0807 - val_acc: 0.7319\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9186\n",
      "Epoch 00105: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2508 - acc: 0.9186 - val_loss: 1.0766 - val_acc: 0.7321\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9212\n",
      "Epoch 00106: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2423 - acc: 0.9212 - val_loss: 1.1376 - val_acc: 0.7277\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9198\n",
      "Epoch 00107: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.2466 - acc: 0.9197 - val_loss: 1.1215 - val_acc: 0.7249\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9220\n",
      "Epoch 00108: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.2385 - acc: 0.9220 - val_loss: 1.0832 - val_acc: 0.7319\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9212\n",
      "Epoch 00109: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2405 - acc: 0.9212 - val_loss: 1.1653 - val_acc: 0.7102\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9218\n",
      "Epoch 00110: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.2394 - acc: 0.9219 - val_loss: 1.1928 - val_acc: 0.7067\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9222\n",
      "Epoch 00111: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.2358 - acc: 0.9221 - val_loss: 1.1710 - val_acc: 0.7135\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9251\n",
      "Epoch 00112: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2303 - acc: 0.9251 - val_loss: 1.1216 - val_acc: 0.7300\n",
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9233\n",
      "Epoch 00113: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2335 - acc: 0.9234 - val_loss: 1.1483 - val_acc: 0.7200\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9237\n",
      "Epoch 00114: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2305 - acc: 0.9237 - val_loss: 1.1177 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9229\n",
      "Epoch 00115: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.2315 - acc: 0.9229 - val_loss: 1.0990 - val_acc: 0.7356\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9236\n",
      "Epoch 00116: val_loss did not improve from 1.02877\n",
      "36805/36805 [==============================] - 14s 368us/sample - loss: 0.2328 - acc: 0.9237 - val_loss: 1.2976 - val_acc: 0.6928\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 285us/sample - loss: 1.1190 - acc: 0.6567\n",
      "Loss: 1.1190139128164092 Accuracy: 0.6566978\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.5148 - acc: 0.2413\n",
      "Epoch 00001: val_loss improved from inf to 1.88865, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/001-1.8887.hdf5\n",
      "36805/36805 [==============================] - 20s 556us/sample - loss: 2.5134 - acc: 0.2415 - val_loss: 1.8887 - val_acc: 0.3955\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7659 - acc: 0.4333\n",
      "Epoch 00002: val_loss improved from 1.88865 to 1.37810, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/002-1.3781.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.7659 - acc: 0.4333 - val_loss: 1.3781 - val_acc: 0.5642\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5025 - acc: 0.5192\n",
      "Epoch 00003: val_loss improved from 1.37810 to 1.24355, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/003-1.2436.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.5022 - acc: 0.5193 - val_loss: 1.2436 - val_acc: 0.6112\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3357 - acc: 0.5765\n",
      "Epoch 00004: val_loss improved from 1.24355 to 1.13028, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/004-1.1303.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3355 - acc: 0.5766 - val_loss: 1.1303 - val_acc: 0.6473\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2238 - acc: 0.6108\n",
      "Epoch 00005: val_loss improved from 1.13028 to 1.08133, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/005-1.0813.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2242 - acc: 0.6107 - val_loss: 1.0813 - val_acc: 0.6597\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1471 - acc: 0.6417\n",
      "Epoch 00006: val_loss improved from 1.08133 to 1.01184, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/006-1.0118.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 1.1471 - acc: 0.6415 - val_loss: 1.0118 - val_acc: 0.6862\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0736 - acc: 0.6643\n",
      "Epoch 00007: val_loss did not improve from 1.01184\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.0734 - acc: 0.6642 - val_loss: 1.0498 - val_acc: 0.6657\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0157 - acc: 0.6831\n",
      "Epoch 00008: val_loss improved from 1.01184 to 0.94808, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/008-0.9481.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.0158 - acc: 0.6830 - val_loss: 0.9481 - val_acc: 0.7032\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9735 - acc: 0.6967\n",
      "Epoch 00009: val_loss did not improve from 0.94808\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.9735 - acc: 0.6967 - val_loss: 1.0204 - val_acc: 0.6876\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.7145\n",
      "Epoch 00010: val_loss improved from 0.94808 to 0.90355, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/010-0.9036.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.9196 - acc: 0.7145 - val_loss: 0.9036 - val_acc: 0.7279\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8836 - acc: 0.7238\n",
      "Epoch 00011: val_loss did not improve from 0.90355\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.8837 - acc: 0.7238 - val_loss: 0.9860 - val_acc: 0.6937\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8523 - acc: 0.7362\n",
      "Epoch 00012: val_loss improved from 0.90355 to 0.85388, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/012-0.8539.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.8522 - acc: 0.7362 - val_loss: 0.8539 - val_acc: 0.7400\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8253 - acc: 0.7455\n",
      "Epoch 00013: val_loss improved from 0.85388 to 0.85218, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/013-0.8522.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.8252 - acc: 0.7455 - val_loss: 0.8522 - val_acc: 0.7424\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7921 - acc: 0.7545\n",
      "Epoch 00014: val_loss did not improve from 0.85218\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.7922 - acc: 0.7543 - val_loss: 0.9430 - val_acc: 0.7216\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7620 - acc: 0.7637\n",
      "Epoch 00015: val_loss improved from 0.85218 to 0.79718, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/015-0.7972.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.7622 - acc: 0.7637 - val_loss: 0.7972 - val_acc: 0.7598\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7400 - acc: 0.7696\n",
      "Epoch 00016: val_loss did not improve from 0.79718\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.7402 - acc: 0.7696 - val_loss: 0.8729 - val_acc: 0.7389\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7127 - acc: 0.7771\n",
      "Epoch 00017: val_loss did not improve from 0.79718\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.7127 - acc: 0.7771 - val_loss: 0.8545 - val_acc: 0.7466\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6976 - acc: 0.7838\n",
      "Epoch 00018: val_loss did not improve from 0.79718\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.6980 - acc: 0.7838 - val_loss: 0.8563 - val_acc: 0.7452\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.7880\n",
      "Epoch 00019: val_loss did not improve from 0.79718\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.6825 - acc: 0.7879 - val_loss: 0.8080 - val_acc: 0.7636\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6649 - acc: 0.7930\n",
      "Epoch 00020: val_loss did not improve from 0.79718\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.6650 - acc: 0.7930 - val_loss: 0.9479 - val_acc: 0.7188\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.7995\n",
      "Epoch 00021: val_loss improved from 0.79718 to 0.76426, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/021-0.7643.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.6467 - acc: 0.7995 - val_loss: 0.7643 - val_acc: 0.7745\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8036\n",
      "Epoch 00022: val_loss did not improve from 0.76426\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.6315 - acc: 0.8036 - val_loss: 0.7696 - val_acc: 0.7724\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.8096\n",
      "Epoch 00023: val_loss did not improve from 0.76426\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.6134 - acc: 0.8095 - val_loss: 0.7722 - val_acc: 0.7701\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.8135\n",
      "Epoch 00024: val_loss improved from 0.76426 to 0.75722, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/024-0.7572.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5986 - acc: 0.8136 - val_loss: 0.7572 - val_acc: 0.7710\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8160\n",
      "Epoch 00025: val_loss did not improve from 0.75722\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.5883 - acc: 0.8159 - val_loss: 0.8213 - val_acc: 0.7617\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8203\n",
      "Epoch 00026: val_loss improved from 0.75722 to 0.75623, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/026-0.7562.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5771 - acc: 0.8204 - val_loss: 0.7562 - val_acc: 0.7843\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8231\n",
      "Epoch 00027: val_loss did not improve from 0.75623\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.5630 - acc: 0.8230 - val_loss: 0.8771 - val_acc: 0.7405\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.8279\n",
      "Epoch 00028: val_loss did not improve from 0.75623\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.5532 - acc: 0.8279 - val_loss: 0.7898 - val_acc: 0.7661\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8307\n",
      "Epoch 00029: val_loss improved from 0.75623 to 0.72580, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/029-0.7258.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.5396 - acc: 0.8306 - val_loss: 0.7258 - val_acc: 0.7878\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.8333\n",
      "Epoch 00030: val_loss did not improve from 0.72580\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.5327 - acc: 0.8331 - val_loss: 0.8198 - val_acc: 0.7640\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8372\n",
      "Epoch 00031: val_loss did not improve from 0.72580\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5173 - acc: 0.8372 - val_loss: 0.7757 - val_acc: 0.7671\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8387\n",
      "Epoch 00032: val_loss did not improve from 0.72580\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5107 - acc: 0.8387 - val_loss: 0.7552 - val_acc: 0.7808\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8398\n",
      "Epoch 00033: val_loss did not improve from 0.72580\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5055 - acc: 0.8398 - val_loss: 0.7964 - val_acc: 0.7636\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8465\n",
      "Epoch 00034: val_loss improved from 0.72580 to 0.72405, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/034-0.7241.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.4925 - acc: 0.8465 - val_loss: 0.7241 - val_acc: 0.7957\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8475\n",
      "Epoch 00035: val_loss did not improve from 0.72405\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.4833 - acc: 0.8475 - val_loss: 0.7474 - val_acc: 0.7850\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.8483\n",
      "Epoch 00036: val_loss did not improve from 0.72405\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.4809 - acc: 0.8483 - val_loss: 0.8238 - val_acc: 0.7601\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8480\n",
      "Epoch 00037: val_loss did not improve from 0.72405\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.4788 - acc: 0.8481 - val_loss: 0.7302 - val_acc: 0.7918\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8574\n",
      "Epoch 00038: val_loss did not improve from 0.72405\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.4570 - acc: 0.8574 - val_loss: 0.7243 - val_acc: 0.7992\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8561\n",
      "Epoch 00039: val_loss improved from 0.72405 to 0.71547, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/039-0.7155.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.4560 - acc: 0.8561 - val_loss: 0.7155 - val_acc: 0.7922\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8574\n",
      "Epoch 00040: val_loss did not improve from 0.71547\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.4502 - acc: 0.8574 - val_loss: 0.7342 - val_acc: 0.7822\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.8615\n",
      "Epoch 00041: val_loss did not improve from 0.71547\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.4381 - acc: 0.8614 - val_loss: 0.7186 - val_acc: 0.7915\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8631\n",
      "Epoch 00042: val_loss did not improve from 0.71547\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.4320 - acc: 0.8631 - val_loss: 0.7717 - val_acc: 0.7815\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8648\n",
      "Epoch 00043: val_loss improved from 0.71547 to 0.70717, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_BN_4_conv_checkpoint/043-0.7072.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.4246 - acc: 0.8649 - val_loss: 0.7072 - val_acc: 0.7990\n",
      "Epoch 44/500\n",
      " 5440/36805 [===>..........................] - ETA: 12s - loss: 0.4174 - acc: 0.8664"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, accuracy, loss])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
