{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.0122 - acc: 0.1226\n",
      "Epoch 00001: val_loss improved from inf to 2.25628, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/001-2.2563.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 6.0119 - acc: 0.1225 - val_loss: 2.2563 - val_acc: 0.2833\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6644 - acc: 0.1616\n",
      "Epoch 00002: val_loss improved from 2.25628 to 1.88674, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/002-1.8867.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 3.6644 - acc: 0.1616 - val_loss: 1.8867 - val_acc: 0.4165\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0809 - acc: 0.2030\n",
      "Epoch 00003: val_loss improved from 1.88674 to 1.70751, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/003-1.7075.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 3.0808 - acc: 0.2030 - val_loss: 1.7075 - val_acc: 0.4717\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6725 - acc: 0.2451\n",
      "Epoch 00004: val_loss improved from 1.70751 to 1.57048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/004-1.5705.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.6724 - acc: 0.2451 - val_loss: 1.5705 - val_acc: 0.5271\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3789 - acc: 0.2846\n",
      "Epoch 00005: val_loss improved from 1.57048 to 1.44584, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/005-1.4458.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.3788 - acc: 0.2846 - val_loss: 1.4458 - val_acc: 0.5688\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1315 - acc: 0.3324\n",
      "Epoch 00006: val_loss improved from 1.44584 to 1.33783, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/006-1.3378.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.1316 - acc: 0.3324 - val_loss: 1.3378 - val_acc: 0.6031\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9293 - acc: 0.3798\n",
      "Epoch 00007: val_loss improved from 1.33783 to 1.25118, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/007-1.2512.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.9291 - acc: 0.3798 - val_loss: 1.2512 - val_acc: 0.6306\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7954 - acc: 0.4174\n",
      "Epoch 00008: val_loss improved from 1.25118 to 1.16518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/008-1.1652.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.7954 - acc: 0.4175 - val_loss: 1.1652 - val_acc: 0.6627\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6740 - acc: 0.4535\n",
      "Epoch 00009: val_loss improved from 1.16518 to 1.10188, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/009-1.1019.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.6739 - acc: 0.4536 - val_loss: 1.1019 - val_acc: 0.6732\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5652 - acc: 0.4860\n",
      "Epoch 00010: val_loss improved from 1.10188 to 1.03348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/010-1.0335.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.5653 - acc: 0.4859 - val_loss: 1.0335 - val_acc: 0.6995\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4968 - acc: 0.5077\n",
      "Epoch 00011: val_loss improved from 1.03348 to 0.98077, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/011-0.9808.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.4967 - acc: 0.5077 - val_loss: 0.9808 - val_acc: 0.7154\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4284 - acc: 0.5289\n",
      "Epoch 00012: val_loss improved from 0.98077 to 0.93700, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/012-0.9370.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.4283 - acc: 0.5289 - val_loss: 0.9370 - val_acc: 0.7209\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3705 - acc: 0.5458\n",
      "Epoch 00013: val_loss improved from 0.93700 to 0.90487, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/013-0.9049.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.3704 - acc: 0.5458 - val_loss: 0.9049 - val_acc: 0.7335\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3093 - acc: 0.5712\n",
      "Epoch 00014: val_loss improved from 0.90487 to 0.88003, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/014-0.8800.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.3095 - acc: 0.5711 - val_loss: 0.8800 - val_acc: 0.7412\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2618 - acc: 0.5861\n",
      "Epoch 00015: val_loss improved from 0.88003 to 0.85860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/015-0.8586.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.2617 - acc: 0.5861 - val_loss: 0.8586 - val_acc: 0.7519\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2186 - acc: 0.5985\n",
      "Epoch 00016: val_loss improved from 0.85860 to 0.82886, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/016-0.8289.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.2186 - acc: 0.5985 - val_loss: 0.8289 - val_acc: 0.7650\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1867 - acc: 0.6109\n",
      "Epoch 00017: val_loss did not improve from 0.82886\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.1868 - acc: 0.6109 - val_loss: 0.8489 - val_acc: 0.7433\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1580 - acc: 0.6190\n",
      "Epoch 00018: val_loss improved from 0.82886 to 0.81339, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/018-0.8134.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.1580 - acc: 0.6190 - val_loss: 0.8134 - val_acc: 0.7622\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1276 - acc: 0.6295\n",
      "Epoch 00019: val_loss improved from 0.81339 to 0.77746, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/019-0.7775.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.1278 - acc: 0.6295 - val_loss: 0.7775 - val_acc: 0.7766\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1118 - acc: 0.6403\n",
      "Epoch 00020: val_loss improved from 0.77746 to 0.77376, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/020-0.7738.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.1119 - acc: 0.6403 - val_loss: 0.7738 - val_acc: 0.7773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0788 - acc: 0.6493\n",
      "Epoch 00021: val_loss improved from 0.77376 to 0.76157, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/021-0.7616.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0788 - acc: 0.6494 - val_loss: 0.7616 - val_acc: 0.7729\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0559 - acc: 0.6544\n",
      "Epoch 00022: val_loss improved from 0.76157 to 0.72251, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/022-0.7225.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0559 - acc: 0.6544 - val_loss: 0.7225 - val_acc: 0.7911\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0456 - acc: 0.6615\n",
      "Epoch 00023: val_loss improved from 0.72251 to 0.71119, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/023-0.7112.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0455 - acc: 0.6615 - val_loss: 0.7112 - val_acc: 0.7869\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0248 - acc: 0.6702\n",
      "Epoch 00024: val_loss did not improve from 0.71119\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0248 - acc: 0.6702 - val_loss: 0.7245 - val_acc: 0.7873\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0035 - acc: 0.6753\n",
      "Epoch 00025: val_loss improved from 0.71119 to 0.70472, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/025-0.7047.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0034 - acc: 0.6753 - val_loss: 0.7047 - val_acc: 0.7943\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.6814\n",
      "Epoch 00026: val_loss did not improve from 0.70472\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9838 - acc: 0.6813 - val_loss: 0.7109 - val_acc: 0.7859\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9760 - acc: 0.6811\n",
      "Epoch 00027: val_loss improved from 0.70472 to 0.68309, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/027-0.6831.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9760 - acc: 0.6811 - val_loss: 0.6831 - val_acc: 0.7987\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9574 - acc: 0.6915\n",
      "Epoch 00028: val_loss improved from 0.68309 to 0.66638, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/028-0.6664.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9574 - acc: 0.6915 - val_loss: 0.6664 - val_acc: 0.8076\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9424 - acc: 0.6972\n",
      "Epoch 00029: val_loss did not improve from 0.66638\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9425 - acc: 0.6971 - val_loss: 0.6670 - val_acc: 0.8078\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9347 - acc: 0.7000\n",
      "Epoch 00030: val_loss improved from 0.66638 to 0.66490, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/030-0.6649.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9348 - acc: 0.7000 - val_loss: 0.6649 - val_acc: 0.8074\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9240 - acc: 0.7037\n",
      "Epoch 00031: val_loss improved from 0.66490 to 0.64497, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/031-0.6450.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9240 - acc: 0.7037 - val_loss: 0.6450 - val_acc: 0.8141\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9093 - acc: 0.7057\n",
      "Epoch 00032: val_loss did not improve from 0.64497\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9095 - acc: 0.7056 - val_loss: 0.6613 - val_acc: 0.8134\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9019 - acc: 0.7110\n",
      "Epoch 00033: val_loss did not improve from 0.64497\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.9019 - acc: 0.7110 - val_loss: 0.6527 - val_acc: 0.8143\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7198\n",
      "Epoch 00034: val_loss improved from 0.64497 to 0.63009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/034-0.6301.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8811 - acc: 0.7198 - val_loss: 0.6301 - val_acc: 0.8251\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8777 - acc: 0.7190\n",
      "Epoch 00035: val_loss did not improve from 0.63009\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8780 - acc: 0.7189 - val_loss: 0.7242 - val_acc: 0.7838\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8651 - acc: 0.7245\n",
      "Epoch 00036: val_loss did not improve from 0.63009\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8652 - acc: 0.7245 - val_loss: 0.6902 - val_acc: 0.7971\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8531 - acc: 0.7284\n",
      "Epoch 00037: val_loss improved from 0.63009 to 0.61418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/037-0.6142.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8530 - acc: 0.7284 - val_loss: 0.6142 - val_acc: 0.8223\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8522 - acc: 0.7290\n",
      "Epoch 00038: val_loss improved from 0.61418 to 0.61110, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/038-0.6111.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8523 - acc: 0.7290 - val_loss: 0.6111 - val_acc: 0.8262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8420 - acc: 0.7323\n",
      "Epoch 00039: val_loss improved from 0.61110 to 0.60500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/039-0.6050.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8419 - acc: 0.7324 - val_loss: 0.6050 - val_acc: 0.8211\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8292 - acc: 0.7339\n",
      "Epoch 00040: val_loss improved from 0.60500 to 0.58739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/040-0.5874.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8292 - acc: 0.7339 - val_loss: 0.5874 - val_acc: 0.8307\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8287 - acc: 0.7342\n",
      "Epoch 00041: val_loss did not improve from 0.58739\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8287 - acc: 0.7342 - val_loss: 0.6028 - val_acc: 0.8272\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8201 - acc: 0.7393\n",
      "Epoch 00042: val_loss did not improve from 0.58739\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8201 - acc: 0.7393 - val_loss: 0.5997 - val_acc: 0.8190\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7421\n",
      "Epoch 00043: val_loss improved from 0.58739 to 0.58057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/043-0.5806.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8108 - acc: 0.7421 - val_loss: 0.5806 - val_acc: 0.8307\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7468\n",
      "Epoch 00044: val_loss improved from 0.58057 to 0.57503, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/044-0.5750.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8007 - acc: 0.7468 - val_loss: 0.5750 - val_acc: 0.8381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7923 - acc: 0.7471\n",
      "Epoch 00045: val_loss improved from 0.57503 to 0.56308, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/045-0.5631.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7923 - acc: 0.7471 - val_loss: 0.5631 - val_acc: 0.8381\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7856 - acc: 0.7502\n",
      "Epoch 00046: val_loss did not improve from 0.56308\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7856 - acc: 0.7501 - val_loss: 0.5859 - val_acc: 0.8258\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7530\n",
      "Epoch 00047: val_loss did not improve from 0.56308\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7823 - acc: 0.7530 - val_loss: 0.5681 - val_acc: 0.8323\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7513\n",
      "Epoch 00048: val_loss improved from 0.56308 to 0.55522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/048-0.5552.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7798 - acc: 0.7513 - val_loss: 0.5552 - val_acc: 0.8365\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7618 - acc: 0.7574\n",
      "Epoch 00049: val_loss did not improve from 0.55522\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7618 - acc: 0.7574 - val_loss: 0.5608 - val_acc: 0.8304\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7627 - acc: 0.7573\n",
      "Epoch 00050: val_loss did not improve from 0.55522\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7633 - acc: 0.7572 - val_loss: 0.5658 - val_acc: 0.8418\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7620 - acc: 0.7563\n",
      "Epoch 00051: val_loss improved from 0.55522 to 0.54676, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/051-0.5468.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7620 - acc: 0.7563 - val_loss: 0.5468 - val_acc: 0.8388\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7538 - acc: 0.7599\n",
      "Epoch 00052: val_loss improved from 0.54676 to 0.53926, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/052-0.5393.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7537 - acc: 0.7599 - val_loss: 0.5393 - val_acc: 0.8481\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7449 - acc: 0.7646\n",
      "Epoch 00053: val_loss improved from 0.53926 to 0.53195, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/053-0.5319.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7449 - acc: 0.7646 - val_loss: 0.5319 - val_acc: 0.8493\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.7629\n",
      "Epoch 00054: val_loss did not improve from 0.53195\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7448 - acc: 0.7629 - val_loss: 0.5546 - val_acc: 0.8418\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7371 - acc: 0.7655\n",
      "Epoch 00055: val_loss improved from 0.53195 to 0.52987, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/055-0.5299.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7370 - acc: 0.7655 - val_loss: 0.5299 - val_acc: 0.8505\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7331 - acc: 0.7658\n",
      "Epoch 00056: val_loss did not improve from 0.52987\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7330 - acc: 0.7658 - val_loss: 0.5607 - val_acc: 0.8293\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7236 - acc: 0.7699\n",
      "Epoch 00057: val_loss improved from 0.52987 to 0.51918, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/057-0.5192.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7235 - acc: 0.7699 - val_loss: 0.5192 - val_acc: 0.8539\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7689\n",
      "Epoch 00058: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7230 - acc: 0.7689 - val_loss: 0.5345 - val_acc: 0.8465\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7169 - acc: 0.7740\n",
      "Epoch 00059: val_loss did not improve from 0.51918\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7168 - acc: 0.7741 - val_loss: 0.5329 - val_acc: 0.8458\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7130 - acc: 0.7763\n",
      "Epoch 00060: val_loss improved from 0.51918 to 0.51455, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/060-0.5145.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7130 - acc: 0.7764 - val_loss: 0.5145 - val_acc: 0.8516\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7133 - acc: 0.7757\n",
      "Epoch 00061: val_loss did not improve from 0.51455\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7133 - acc: 0.7757 - val_loss: 0.5245 - val_acc: 0.8486\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7072 - acc: 0.7786\n",
      "Epoch 00062: val_loss did not improve from 0.51455\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7072 - acc: 0.7786 - val_loss: 0.5547 - val_acc: 0.8388\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7021 - acc: 0.7765\n",
      "Epoch 00063: val_loss did not improve from 0.51455\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7021 - acc: 0.7765 - val_loss: 0.5236 - val_acc: 0.8409\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6986 - acc: 0.7785\n",
      "Epoch 00064: val_loss improved from 0.51455 to 0.50071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/064-0.5007.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6986 - acc: 0.7785 - val_loss: 0.5007 - val_acc: 0.8572\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.7764\n",
      "Epoch 00065: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.7016 - acc: 0.7764 - val_loss: 0.5243 - val_acc: 0.8465\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.7808\n",
      "Epoch 00066: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6913 - acc: 0.7808 - val_loss: 0.5697 - val_acc: 0.8237\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.7812\n",
      "Epoch 00067: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6935 - acc: 0.7812 - val_loss: 0.5402 - val_acc: 0.8460\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.7847\n",
      "Epoch 00068: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6827 - acc: 0.7846 - val_loss: 0.5329 - val_acc: 0.8409\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.7835\n",
      "Epoch 00069: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6789 - acc: 0.7835 - val_loss: 0.5211 - val_acc: 0.8495\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.7870\n",
      "Epoch 00070: val_loss did not improve from 0.50071\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6775 - acc: 0.7870 - val_loss: 0.5131 - val_acc: 0.8516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.7837\n",
      "Epoch 00071: val_loss improved from 0.50071 to 0.49021, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/071-0.4902.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6798 - acc: 0.7836 - val_loss: 0.4902 - val_acc: 0.8577\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6762 - acc: 0.7868\n",
      "Epoch 00072: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6763 - acc: 0.7868 - val_loss: 0.5159 - val_acc: 0.8493\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6697 - acc: 0.7871\n",
      "Epoch 00073: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6697 - acc: 0.7871 - val_loss: 0.4911 - val_acc: 0.8623\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7907\n",
      "Epoch 00074: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6623 - acc: 0.7907 - val_loss: 0.5170 - val_acc: 0.8484\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.7869\n",
      "Epoch 00075: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6686 - acc: 0.7869 - val_loss: 0.5056 - val_acc: 0.8488\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.7878\n",
      "Epoch 00076: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6644 - acc: 0.7879 - val_loss: 0.4937 - val_acc: 0.8574\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.7927\n",
      "Epoch 00077: val_loss did not improve from 0.49021\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6613 - acc: 0.7926 - val_loss: 0.5048 - val_acc: 0.8484\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.7936\n",
      "Epoch 00078: val_loss improved from 0.49021 to 0.48505, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/078-0.4850.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6534 - acc: 0.7936 - val_loss: 0.4850 - val_acc: 0.8626\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.7931\n",
      "Epoch 00079: val_loss did not improve from 0.48505\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6512 - acc: 0.7931 - val_loss: 0.5442 - val_acc: 0.8351\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7941\n",
      "Epoch 00080: val_loss did not improve from 0.48505\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6489 - acc: 0.7941 - val_loss: 0.4946 - val_acc: 0.8549\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.7970\n",
      "Epoch 00081: val_loss improved from 0.48505 to 0.46746, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/081-0.4675.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.6460 - acc: 0.7970 - val_loss: 0.4675 - val_acc: 0.8647\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.7965\n",
      "Epoch 00082: val_loss did not improve from 0.46746\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6409 - acc: 0.7965 - val_loss: 0.5024 - val_acc: 0.8539\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.7975\n",
      "Epoch 00083: val_loss did not improve from 0.46746\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6367 - acc: 0.7975 - val_loss: 0.4903 - val_acc: 0.8549\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.7980\n",
      "Epoch 00084: val_loss did not improve from 0.46746\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6331 - acc: 0.7980 - val_loss: 0.4925 - val_acc: 0.8498\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.7980\n",
      "Epoch 00085: val_loss did not improve from 0.46746\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6368 - acc: 0.7980 - val_loss: 0.4897 - val_acc: 0.8556\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.8016\n",
      "Epoch 00086: val_loss did not improve from 0.46746\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6309 - acc: 0.8016 - val_loss: 0.4701 - val_acc: 0.8626\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6223 - acc: 0.8021\n",
      "Epoch 00087: val_loss improved from 0.46746 to 0.46599, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/087-0.4660.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6225 - acc: 0.8020 - val_loss: 0.4660 - val_acc: 0.8658\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.7992\n",
      "Epoch 00088: val_loss did not improve from 0.46599\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6332 - acc: 0.7992 - val_loss: 0.5015 - val_acc: 0.8472\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8018\n",
      "Epoch 00089: val_loss did not improve from 0.46599\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6259 - acc: 0.8018 - val_loss: 0.4800 - val_acc: 0.8591\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8014\n",
      "Epoch 00090: val_loss did not improve from 0.46599\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6188 - acc: 0.8014 - val_loss: 0.5171 - val_acc: 0.8414\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6152 - acc: 0.8071\n",
      "Epoch 00091: val_loss did not improve from 0.46599\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6151 - acc: 0.8072 - val_loss: 0.5198 - val_acc: 0.8474\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8055\n",
      "Epoch 00092: val_loss improved from 0.46599 to 0.45439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/092-0.4544.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6137 - acc: 0.8055 - val_loss: 0.4544 - val_acc: 0.8682\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6122 - acc: 0.8064\n",
      "Epoch 00093: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6122 - acc: 0.8064 - val_loss: 0.4950 - val_acc: 0.8528\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.8053\n",
      "Epoch 00094: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6129 - acc: 0.8053 - val_loss: 0.4755 - val_acc: 0.8586\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6072 - acc: 0.8075\n",
      "Epoch 00095: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6073 - acc: 0.8075 - val_loss: 0.4982 - val_acc: 0.8479\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.8091\n",
      "Epoch 00096: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6075 - acc: 0.8090 - val_loss: 0.4730 - val_acc: 0.8584\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.8085\n",
      "Epoch 00097: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6095 - acc: 0.8084 - val_loss: 0.4676 - val_acc: 0.8661\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.8073\n",
      "Epoch 00098: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.6039 - acc: 0.8073 - val_loss: 0.5714 - val_acc: 0.8314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8093\n",
      "Epoch 00099: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5967 - acc: 0.8093 - val_loss: 0.4612 - val_acc: 0.8661\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.8128\n",
      "Epoch 00100: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5968 - acc: 0.8128 - val_loss: 0.4680 - val_acc: 0.8689\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.8125\n",
      "Epoch 00101: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5977 - acc: 0.8125 - val_loss: 0.4805 - val_acc: 0.8558\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5944 - acc: 0.8110\n",
      "Epoch 00102: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5944 - acc: 0.8109 - val_loss: 0.4793 - val_acc: 0.8607\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8138\n",
      "Epoch 00103: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5863 - acc: 0.8138 - val_loss: 0.4605 - val_acc: 0.8670\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8115\n",
      "Epoch 00104: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5907 - acc: 0.8115 - val_loss: 0.4622 - val_acc: 0.8595\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.8116\n",
      "Epoch 00105: val_loss did not improve from 0.45439\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5919 - acc: 0.8115 - val_loss: 0.4920 - val_acc: 0.8549\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.8132\n",
      "Epoch 00106: val_loss improved from 0.45439 to 0.45129, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/106-0.4513.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5915 - acc: 0.8132 - val_loss: 0.4513 - val_acc: 0.8679\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8146\n",
      "Epoch 00107: val_loss did not improve from 0.45129\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5842 - acc: 0.8146 - val_loss: 0.4775 - val_acc: 0.8537\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.8146\n",
      "Epoch 00108: val_loss did not improve from 0.45129\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5820 - acc: 0.8146 - val_loss: 0.4854 - val_acc: 0.8586\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.8200\n",
      "Epoch 00109: val_loss improved from 0.45129 to 0.44338, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/109-0.4434.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5731 - acc: 0.8201 - val_loss: 0.4434 - val_acc: 0.8668\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5743 - acc: 0.8202\n",
      "Epoch 00110: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5742 - acc: 0.8202 - val_loss: 0.4752 - val_acc: 0.8626\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.8199\n",
      "Epoch 00111: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5687 - acc: 0.8199 - val_loss: 0.4899 - val_acc: 0.8544\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.8180\n",
      "Epoch 00112: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5690 - acc: 0.8180 - val_loss: 0.4517 - val_acc: 0.8661\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8191\n",
      "Epoch 00113: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5701 - acc: 0.8191 - val_loss: 0.4698 - val_acc: 0.8588\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8182\n",
      "Epoch 00114: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5693 - acc: 0.8182 - val_loss: 0.4906 - val_acc: 0.8570\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8203\n",
      "Epoch 00115: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5653 - acc: 0.8203 - val_loss: 0.5049 - val_acc: 0.8493\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8226\n",
      "Epoch 00116: val_loss did not improve from 0.44338\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5656 - acc: 0.8225 - val_loss: 0.4611 - val_acc: 0.8572\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.8224\n",
      "Epoch 00117: val_loss improved from 0.44338 to 0.43763, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/117-0.4376.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5655 - acc: 0.8224 - val_loss: 0.4376 - val_acc: 0.8705\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8197\n",
      "Epoch 00118: val_loss improved from 0.43763 to 0.42320, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/118-0.4232.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5623 - acc: 0.8197 - val_loss: 0.4232 - val_acc: 0.8756\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.8215\n",
      "Epoch 00119: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5551 - acc: 0.8215 - val_loss: 0.4546 - val_acc: 0.8642\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8209\n",
      "Epoch 00120: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5596 - acc: 0.8209 - val_loss: 0.4676 - val_acc: 0.8651\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8271\n",
      "Epoch 00121: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5490 - acc: 0.8271 - val_loss: 0.4504 - val_acc: 0.8672\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8262\n",
      "Epoch 00122: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5455 - acc: 0.8262 - val_loss: 0.4252 - val_acc: 0.8733\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8248\n",
      "Epoch 00123: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5520 - acc: 0.8248 - val_loss: 0.4704 - val_acc: 0.8665\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.8233\n",
      "Epoch 00124: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5555 - acc: 0.8233 - val_loss: 0.4372 - val_acc: 0.8730\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.8271\n",
      "Epoch 00125: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5487 - acc: 0.8271 - val_loss: 0.4785 - val_acc: 0.8602\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.8268\n",
      "Epoch 00126: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5458 - acc: 0.8268 - val_loss: 0.4769 - val_acc: 0.8591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8316\n",
      "Epoch 00127: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5316 - acc: 0.8316 - val_loss: 0.4771 - val_acc: 0.8623\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8289\n",
      "Epoch 00128: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5389 - acc: 0.8289 - val_loss: 0.4377 - val_acc: 0.8772\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8273\n",
      "Epoch 00129: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5385 - acc: 0.8274 - val_loss: 0.4551 - val_acc: 0.8710\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.8287\n",
      "Epoch 00130: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5336 - acc: 0.8287 - val_loss: 0.4546 - val_acc: 0.8628\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8281\n",
      "Epoch 00131: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5375 - acc: 0.8281 - val_loss: 0.4812 - val_acc: 0.8565\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.8309\n",
      "Epoch 00132: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5319 - acc: 0.8309 - val_loss: 0.4689 - val_acc: 0.8595\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8280\n",
      "Epoch 00133: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5334 - acc: 0.8280 - val_loss: 0.4437 - val_acc: 0.8700\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8303\n",
      "Epoch 00134: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5353 - acc: 0.8303 - val_loss: 0.5035 - val_acc: 0.8442\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8313\n",
      "Epoch 00135: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5302 - acc: 0.8312 - val_loss: 0.4339 - val_acc: 0.8765\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8322\n",
      "Epoch 00136: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5283 - acc: 0.8322 - val_loss: 0.4709 - val_acc: 0.8633\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8341\n",
      "Epoch 00137: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5235 - acc: 0.8341 - val_loss: 0.4651 - val_acc: 0.8665\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8343\n",
      "Epoch 00138: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5309 - acc: 0.8343 - val_loss: 0.4358 - val_acc: 0.8696\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8341\n",
      "Epoch 00139: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5217 - acc: 0.8342 - val_loss: 0.4410 - val_acc: 0.8693\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8340\n",
      "Epoch 00140: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5187 - acc: 0.8340 - val_loss: 0.4732 - val_acc: 0.8623\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8349\n",
      "Epoch 00141: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5221 - acc: 0.8349 - val_loss: 0.4372 - val_acc: 0.8684\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.8354\n",
      "Epoch 00142: val_loss did not improve from 0.42320\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5160 - acc: 0.8353 - val_loss: 0.4542 - val_acc: 0.8649\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8349\n",
      "Epoch 00143: val_loss improved from 0.42320 to 0.42023, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/143-0.4202.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5168 - acc: 0.8349 - val_loss: 0.4202 - val_acc: 0.8758\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8360\n",
      "Epoch 00144: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5138 - acc: 0.8359 - val_loss: 0.4931 - val_acc: 0.8474\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8360\n",
      "Epoch 00145: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5213 - acc: 0.8361 - val_loss: 0.4334 - val_acc: 0.8784\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8362\n",
      "Epoch 00146: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5182 - acc: 0.8362 - val_loss: 0.4452 - val_acc: 0.8630\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8380\n",
      "Epoch 00147: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5095 - acc: 0.8380 - val_loss: 0.4619 - val_acc: 0.8605\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8343\n",
      "Epoch 00148: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5134 - acc: 0.8343 - val_loss: 0.4461 - val_acc: 0.8651\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8388\n",
      "Epoch 00149: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5072 - acc: 0.8388 - val_loss: 0.4541 - val_acc: 0.8661\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8404\n",
      "Epoch 00150: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5043 - acc: 0.8404 - val_loss: 0.4397 - val_acc: 0.8712\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8383\n",
      "Epoch 00151: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5029 - acc: 0.8383 - val_loss: 0.4385 - val_acc: 0.8714\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8400\n",
      "Epoch 00152: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5021 - acc: 0.8400 - val_loss: 0.4426 - val_acc: 0.8649\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8415\n",
      "Epoch 00153: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5008 - acc: 0.8415 - val_loss: 0.4306 - val_acc: 0.8679\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.8413\n",
      "Epoch 00154: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4956 - acc: 0.8413 - val_loss: 0.4254 - val_acc: 0.8742\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8425\n",
      "Epoch 00155: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4946 - acc: 0.8425 - val_loss: 0.4414 - val_acc: 0.8672\n",
      "Epoch 156/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8421\n",
      "Epoch 00156: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4967 - acc: 0.8421 - val_loss: 0.4704 - val_acc: 0.8551\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8387\n",
      "Epoch 00157: val_loss did not improve from 0.42023\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.5045 - acc: 0.8387 - val_loss: 0.4245 - val_acc: 0.8730\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8407\n",
      "Epoch 00158: val_loss improved from 0.42023 to 0.41789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/158-0.4179.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4995 - acc: 0.8407 - val_loss: 0.4179 - val_acc: 0.8772\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8411\n",
      "Epoch 00159: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4971 - acc: 0.8412 - val_loss: 0.4420 - val_acc: 0.8658\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8431\n",
      "Epoch 00160: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4930 - acc: 0.8431 - val_loss: 0.4523 - val_acc: 0.8635\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8438\n",
      "Epoch 00161: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4923 - acc: 0.8438 - val_loss: 0.4244 - val_acc: 0.8749\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8443\n",
      "Epoch 00162: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4871 - acc: 0.8443 - val_loss: 0.4404 - val_acc: 0.8640\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8496\n",
      "Epoch 00163: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4802 - acc: 0.8496 - val_loss: 0.4189 - val_acc: 0.8747\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8455\n",
      "Epoch 00164: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4816 - acc: 0.8455 - val_loss: 0.4367 - val_acc: 0.8765\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8456\n",
      "Epoch 00165: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4859 - acc: 0.8456 - val_loss: 0.4292 - val_acc: 0.8717\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8445\n",
      "Epoch 00166: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4859 - acc: 0.8445 - val_loss: 0.4563 - val_acc: 0.8612\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8445\n",
      "Epoch 00167: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4838 - acc: 0.8445 - val_loss: 0.4261 - val_acc: 0.8754\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8456\n",
      "Epoch 00168: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4766 - acc: 0.8456 - val_loss: 0.5063 - val_acc: 0.8402\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.8492\n",
      "Epoch 00169: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4701 - acc: 0.8492 - val_loss: 0.4203 - val_acc: 0.8744\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8465\n",
      "Epoch 00170: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4844 - acc: 0.8464 - val_loss: 0.4513 - val_acc: 0.8724\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.8478\n",
      "Epoch 00171: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4777 - acc: 0.8478 - val_loss: 0.4541 - val_acc: 0.8675\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8487\n",
      "Epoch 00172: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4746 - acc: 0.8487 - val_loss: 0.4481 - val_acc: 0.8677\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4726 - acc: 0.8481\n",
      "Epoch 00173: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4726 - acc: 0.8481 - val_loss: 0.4900 - val_acc: 0.8563\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.8513\n",
      "Epoch 00174: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.4700 - acc: 0.8514 - val_loss: 0.4279 - val_acc: 0.8712\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8489\n",
      "Epoch 00175: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4686 - acc: 0.8489 - val_loss: 0.4719 - val_acc: 0.8593\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.8514\n",
      "Epoch 00176: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4689 - acc: 0.8514 - val_loss: 0.4356 - val_acc: 0.8733\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.8490\n",
      "Epoch 00177: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4696 - acc: 0.8490 - val_loss: 0.4583 - val_acc: 0.8602\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.8500\n",
      "Epoch 00178: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4699 - acc: 0.8500 - val_loss: 0.4473 - val_acc: 0.8644\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8514\n",
      "Epoch 00179: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4632 - acc: 0.8514 - val_loss: 0.4379 - val_acc: 0.8700\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4633 - acc: 0.8526\n",
      "Epoch 00180: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4633 - acc: 0.8527 - val_loss: 0.4309 - val_acc: 0.8686\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8528\n",
      "Epoch 00181: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4624 - acc: 0.8528 - val_loss: 0.4282 - val_acc: 0.8735\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8533\n",
      "Epoch 00182: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4569 - acc: 0.8533 - val_loss: 0.4663 - val_acc: 0.8616\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8524\n",
      "Epoch 00183: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4584 - acc: 0.8525 - val_loss: 0.4349 - val_acc: 0.8696\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8521\n",
      "Epoch 00184: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4570 - acc: 0.8521 - val_loss: 0.4696 - val_acc: 0.8551\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8531\n",
      "Epoch 00185: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4578 - acc: 0.8531 - val_loss: 0.4389 - val_acc: 0.8679\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8538\n",
      "Epoch 00186: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4566 - acc: 0.8538 - val_loss: 0.4477 - val_acc: 0.8679\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8549\n",
      "Epoch 00187: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4524 - acc: 0.8549 - val_loss: 0.4491 - val_acc: 0.8644\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8548\n",
      "Epoch 00188: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4563 - acc: 0.8548 - val_loss: 0.4363 - val_acc: 0.8677\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8567\n",
      "Epoch 00189: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4530 - acc: 0.8567 - val_loss: 0.4282 - val_acc: 0.8679\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8558\n",
      "Epoch 00190: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4496 - acc: 0.8558 - val_loss: 0.4367 - val_acc: 0.8737\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4501 - acc: 0.8551\n",
      "Epoch 00191: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4501 - acc: 0.8550 - val_loss: 0.4511 - val_acc: 0.8647\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8553\n",
      "Epoch 00192: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4516 - acc: 0.8553 - val_loss: 0.4452 - val_acc: 0.8623\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8542\n",
      "Epoch 00193: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4503 - acc: 0.8542 - val_loss: 0.4309 - val_acc: 0.8747\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8562\n",
      "Epoch 00194: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4499 - acc: 0.8562 - val_loss: 0.4581 - val_acc: 0.8605\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8582\n",
      "Epoch 00195: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4459 - acc: 0.8582 - val_loss: 0.4367 - val_acc: 0.8677\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8594\n",
      "Epoch 00196: val_loss did not improve from 0.41789\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4406 - acc: 0.8594 - val_loss: 0.4358 - val_acc: 0.8691\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8598\n",
      "Epoch 00197: val_loss improved from 0.41789 to 0.40199, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv_checkpoint/197-0.4020.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4427 - acc: 0.8598 - val_loss: 0.4020 - val_acc: 0.8838\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8594\n",
      "Epoch 00198: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4412 - acc: 0.8594 - val_loss: 0.4520 - val_acc: 0.8630\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8574\n",
      "Epoch 00199: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4428 - acc: 0.8573 - val_loss: 0.5037 - val_acc: 0.8449\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8602\n",
      "Epoch 00200: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4370 - acc: 0.8602 - val_loss: 0.4535 - val_acc: 0.8642\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8585\n",
      "Epoch 00201: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4382 - acc: 0.8585 - val_loss: 0.4464 - val_acc: 0.8649\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8591\n",
      "Epoch 00202: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4374 - acc: 0.8591 - val_loss: 0.4231 - val_acc: 0.8735\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8643\n",
      "Epoch 00203: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4285 - acc: 0.8643 - val_loss: 0.4672 - val_acc: 0.8546\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8578\n",
      "Epoch 00204: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4398 - acc: 0.8578 - val_loss: 0.4589 - val_acc: 0.8607\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8614\n",
      "Epoch 00205: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4349 - acc: 0.8614 - val_loss: 0.4727 - val_acc: 0.8598\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8635\n",
      "Epoch 00206: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4311 - acc: 0.8635 - val_loss: 0.4294 - val_acc: 0.8672\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.8596\n",
      "Epoch 00207: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4323 - acc: 0.8596 - val_loss: 0.4800 - val_acc: 0.8521\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.8615\n",
      "Epoch 00208: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4318 - acc: 0.8615 - val_loss: 0.4309 - val_acc: 0.8677\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8610\n",
      "Epoch 00209: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4352 - acc: 0.8610 - val_loss: 0.4447 - val_acc: 0.8630\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8614\n",
      "Epoch 00210: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4320 - acc: 0.8614 - val_loss: 0.4146 - val_acc: 0.8777\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8644\n",
      "Epoch 00211: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4268 - acc: 0.8644 - val_loss: 0.4205 - val_acc: 0.8735\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8639\n",
      "Epoch 00212: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4248 - acc: 0.8640 - val_loss: 0.4600 - val_acc: 0.8591\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8652\n",
      "Epoch 00213: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4224 - acc: 0.8653 - val_loss: 0.4223 - val_acc: 0.8735\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8627\n",
      "Epoch 00214: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4259 - acc: 0.8627 - val_loss: 0.4399 - val_acc: 0.8691\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8628\n",
      "Epoch 00215: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4258 - acc: 0.8628 - val_loss: 0.4222 - val_acc: 0.8756\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8644\n",
      "Epoch 00216: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4202 - acc: 0.8644 - val_loss: 0.4271 - val_acc: 0.8747\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8650\n",
      "Epoch 00217: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4210 - acc: 0.8650 - val_loss: 0.4128 - val_acc: 0.8763\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8642\n",
      "Epoch 00218: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4161 - acc: 0.8642 - val_loss: 0.4530 - val_acc: 0.8600\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8674\n",
      "Epoch 00219: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4167 - acc: 0.8674 - val_loss: 0.4211 - val_acc: 0.8782\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8668\n",
      "Epoch 00220: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4202 - acc: 0.8668 - val_loss: 0.5216 - val_acc: 0.8404\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8674\n",
      "Epoch 00221: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4148 - acc: 0.8674 - val_loss: 0.4465 - val_acc: 0.8658\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8658\n",
      "Epoch 00222: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4192 - acc: 0.8658 - val_loss: 0.4198 - val_acc: 0.8751\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8649\n",
      "Epoch 00223: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4174 - acc: 0.8649 - val_loss: 0.4325 - val_acc: 0.8710\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8656\n",
      "Epoch 00224: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4155 - acc: 0.8656 - val_loss: 0.4369 - val_acc: 0.8682\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8664\n",
      "Epoch 00225: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4153 - acc: 0.8665 - val_loss: 0.4485 - val_acc: 0.8644\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8698\n",
      "Epoch 00226: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4072 - acc: 0.8698 - val_loss: 0.4486 - val_acc: 0.8644\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8706\n",
      "Epoch 00227: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4060 - acc: 0.8706 - val_loss: 0.4191 - val_acc: 0.8768\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8671\n",
      "Epoch 00228: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4136 - acc: 0.8671 - val_loss: 0.4477 - val_acc: 0.8670\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8698\n",
      "Epoch 00229: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4047 - acc: 0.8698 - val_loss: 0.4376 - val_acc: 0.8616\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8708\n",
      "Epoch 00230: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4062 - acc: 0.8708 - val_loss: 0.4657 - val_acc: 0.8584\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8692\n",
      "Epoch 00231: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4056 - acc: 0.8692 - val_loss: 0.4590 - val_acc: 0.8619\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8700\n",
      "Epoch 00232: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4097 - acc: 0.8700 - val_loss: 0.4801 - val_acc: 0.8563\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8711\n",
      "Epoch 00233: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4053 - acc: 0.8711 - val_loss: 0.4713 - val_acc: 0.8591\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8706\n",
      "Epoch 00234: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4059 - acc: 0.8706 - val_loss: 0.4492 - val_acc: 0.8602\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8721\n",
      "Epoch 00235: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3965 - acc: 0.8722 - val_loss: 0.4883 - val_acc: 0.8514\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8723\n",
      "Epoch 00236: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3989 - acc: 0.8723 - val_loss: 0.4919 - val_acc: 0.8516\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8707\n",
      "Epoch 00237: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4018 - acc: 0.8706 - val_loss: 0.4359 - val_acc: 0.8717\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8704\n",
      "Epoch 00238: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3979 - acc: 0.8704 - val_loss: 0.4127 - val_acc: 0.8789\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8732\n",
      "Epoch 00239: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3999 - acc: 0.8732 - val_loss: 0.4335 - val_acc: 0.8717\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8732\n",
      "Epoch 00240: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3924 - acc: 0.8731 - val_loss: 0.4336 - val_acc: 0.8717\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8708\n",
      "Epoch 00241: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.4018 - acc: 0.8708 - val_loss: 0.4778 - val_acc: 0.8535\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8732\n",
      "Epoch 00242: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3907 - acc: 0.8732 - val_loss: 0.4449 - val_acc: 0.8703\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8725\n",
      "Epoch 00243: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3938 - acc: 0.8725 - val_loss: 0.4504 - val_acc: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8742\n",
      "Epoch 00244: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3948 - acc: 0.8743 - val_loss: 0.4443 - val_acc: 0.8661\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8737\n",
      "Epoch 00245: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3907 - acc: 0.8737 - val_loss: 0.4704 - val_acc: 0.8563\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8731\n",
      "Epoch 00246: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3920 - acc: 0.8731 - val_loss: 0.4626 - val_acc: 0.8607\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8737\n",
      "Epoch 00247: val_loss did not improve from 0.40199\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.3892 - acc: 0.8737 - val_loss: 0.4508 - val_acc: 0.8633\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5020 - acc: 0.8478\n",
      "Loss: 0.5019714470840565 Accuracy: 0.8477674\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1425 - acc: 0.1416\n",
      "Epoch 00001: val_loss improved from inf to 2.08947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/001-2.0895.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 5.1424 - acc: 0.1416 - val_loss: 2.0895 - val_acc: 0.3098\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0519 - acc: 0.2218\n",
      "Epoch 00002: val_loss improved from 2.08947 to 1.51677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/002-1.5168.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 3.0518 - acc: 0.2219 - val_loss: 1.5168 - val_acc: 0.5197\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4456 - acc: 0.2890\n",
      "Epoch 00003: val_loss improved from 1.51677 to 1.32648, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/003-1.3265.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 2.4456 - acc: 0.2890 - val_loss: 1.3265 - val_acc: 0.6017\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0847 - acc: 0.3611\n",
      "Epoch 00004: val_loss improved from 1.32648 to 1.11893, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/004-1.1189.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 2.0848 - acc: 0.3610 - val_loss: 1.1189 - val_acc: 0.6718\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8316 - acc: 0.4248\n",
      "Epoch 00005: val_loss improved from 1.11893 to 0.97964, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/005-0.9796.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.8315 - acc: 0.4248 - val_loss: 0.9796 - val_acc: 0.7228\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6328 - acc: 0.4779\n",
      "Epoch 00006: val_loss improved from 0.97964 to 0.88850, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/006-0.8885.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.6330 - acc: 0.4779 - val_loss: 0.8885 - val_acc: 0.7340\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4700 - acc: 0.5305\n",
      "Epoch 00007: val_loss improved from 0.88850 to 0.81828, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/007-0.8183.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.4701 - acc: 0.5304 - val_loss: 0.8183 - val_acc: 0.7598\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3339 - acc: 0.5690\n",
      "Epoch 00008: val_loss improved from 0.81828 to 0.80999, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/008-0.8100.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.3338 - acc: 0.5690 - val_loss: 0.8100 - val_acc: 0.7503\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2289 - acc: 0.6017\n",
      "Epoch 00009: val_loss improved from 0.80999 to 0.69340, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/009-0.6934.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.2291 - acc: 0.6017 - val_loss: 0.6934 - val_acc: 0.7990\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1426 - acc: 0.6261\n",
      "Epoch 00010: val_loss improved from 0.69340 to 0.65938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/010-0.6594.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.1424 - acc: 0.6261 - val_loss: 0.6594 - val_acc: 0.8034\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0766 - acc: 0.6512\n",
      "Epoch 00011: val_loss improved from 0.65938 to 0.60655, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/011-0.6065.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.0765 - acc: 0.6512 - val_loss: 0.6065 - val_acc: 0.8197\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0081 - acc: 0.6755\n",
      "Epoch 00012: val_loss improved from 0.60655 to 0.57616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/012-0.5762.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.0082 - acc: 0.6755 - val_loss: 0.5762 - val_acc: 0.8288\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9579 - acc: 0.6903\n",
      "Epoch 00013: val_loss did not improve from 0.57616\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9581 - acc: 0.6902 - val_loss: 0.5776 - val_acc: 0.8248\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9175 - acc: 0.7055\n",
      "Epoch 00014: val_loss improved from 0.57616 to 0.53014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/014-0.5301.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9175 - acc: 0.7054 - val_loss: 0.5301 - val_acc: 0.8463\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8731 - acc: 0.7195\n",
      "Epoch 00015: val_loss improved from 0.53014 to 0.52608, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/015-0.5261.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8731 - acc: 0.7195 - val_loss: 0.5261 - val_acc: 0.8451\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8434 - acc: 0.7283\n",
      "Epoch 00016: val_loss improved from 0.52608 to 0.51798, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/016-0.5180.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8433 - acc: 0.7283 - val_loss: 0.5180 - val_acc: 0.8467\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7375\n",
      "Epoch 00017: val_loss did not improve from 0.51798\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8231 - acc: 0.7375 - val_loss: 0.5684 - val_acc: 0.8255\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7959 - acc: 0.7476\n",
      "Epoch 00018: val_loss improved from 0.51798 to 0.50529, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/018-0.5053.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7959 - acc: 0.7476 - val_loss: 0.5053 - val_acc: 0.8516\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7554\n",
      "Epoch 00019: val_loss improved from 0.50529 to 0.46824, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/019-0.4682.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7709 - acc: 0.7554 - val_loss: 0.4682 - val_acc: 0.8633\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7400 - acc: 0.7646\n",
      "Epoch 00020: val_loss improved from 0.46824 to 0.46007, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/020-0.4601.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7400 - acc: 0.7646 - val_loss: 0.4601 - val_acc: 0.8651\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7694\n",
      "Epoch 00021: val_loss did not improve from 0.46007\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7274 - acc: 0.7694 - val_loss: 0.4747 - val_acc: 0.8579\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.7764\n",
      "Epoch 00022: val_loss improved from 0.46007 to 0.44712, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/022-0.4471.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7075 - acc: 0.7764 - val_loss: 0.4471 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7808\n",
      "Epoch 00023: val_loss improved from 0.44712 to 0.44020, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/023-0.4402.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6961 - acc: 0.7808 - val_loss: 0.4402 - val_acc: 0.8644\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.7888\n",
      "Epoch 00024: val_loss improved from 0.44020 to 0.42969, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/024-0.4297.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6735 - acc: 0.7889 - val_loss: 0.4297 - val_acc: 0.8779\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.7921\n",
      "Epoch 00025: val_loss improved from 0.42969 to 0.41328, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/025-0.4133.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6604 - acc: 0.7921 - val_loss: 0.4133 - val_acc: 0.8814\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.7957\n",
      "Epoch 00026: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.6502 - acc: 0.7957 - val_loss: 0.4252 - val_acc: 0.8749\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6411 - acc: 0.8010\n",
      "Epoch 00027: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.6411 - acc: 0.8010 - val_loss: 0.4350 - val_acc: 0.8784\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6173 - acc: 0.8058\n",
      "Epoch 00028: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6174 - acc: 0.8058 - val_loss: 0.4156 - val_acc: 0.8805\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8057\n",
      "Epoch 00029: val_loss improved from 0.41328 to 0.41284, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/029-0.4128.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6243 - acc: 0.8057 - val_loss: 0.4128 - val_acc: 0.8798\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.8102\n",
      "Epoch 00030: val_loss improved from 0.41284 to 0.38444, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/030-0.3844.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6032 - acc: 0.8102 - val_loss: 0.3844 - val_acc: 0.8896\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8140\n",
      "Epoch 00031: val_loss did not improve from 0.38444\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5971 - acc: 0.8140 - val_loss: 0.3931 - val_acc: 0.8866\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.8182\n",
      "Epoch 00032: val_loss improved from 0.38444 to 0.37557, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/032-0.3756.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5868 - acc: 0.8182 - val_loss: 0.3756 - val_acc: 0.8915\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.8166\n",
      "Epoch 00033: val_loss did not improve from 0.37557\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5802 - acc: 0.8165 - val_loss: 0.3818 - val_acc: 0.8847\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8214\n",
      "Epoch 00034: val_loss improved from 0.37557 to 0.36248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/034-0.3625.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5713 - acc: 0.8214 - val_loss: 0.3625 - val_acc: 0.8933\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8240\n",
      "Epoch 00035: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5620 - acc: 0.8240 - val_loss: 0.3955 - val_acc: 0.8826\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5542 - acc: 0.8268\n",
      "Epoch 00036: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5542 - acc: 0.8268 - val_loss: 0.3733 - val_acc: 0.8933\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.8290\n",
      "Epoch 00037: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5488 - acc: 0.8290 - val_loss: 0.3658 - val_acc: 0.8942\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.8295\n",
      "Epoch 00038: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5413 - acc: 0.8295 - val_loss: 0.3711 - val_acc: 0.8870\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.8331\n",
      "Epoch 00039: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5330 - acc: 0.8331 - val_loss: 0.3825 - val_acc: 0.8861\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8388\n",
      "Epoch 00040: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5237 - acc: 0.8388 - val_loss: 0.3687 - val_acc: 0.8933\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.8379\n",
      "Epoch 00041: val_loss did not improve from 0.36248\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5217 - acc: 0.8379 - val_loss: 0.3721 - val_acc: 0.8896\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.8415\n",
      "Epoch 00042: val_loss improved from 0.36248 to 0.35134, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/042-0.3513.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5145 - acc: 0.8415 - val_loss: 0.3513 - val_acc: 0.8961\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8436\n",
      "Epoch 00043: val_loss improved from 0.35134 to 0.34226, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/043-0.3423.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5055 - acc: 0.8436 - val_loss: 0.3423 - val_acc: 0.9022\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8457\n",
      "Epoch 00044: val_loss did not improve from 0.34226\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4982 - acc: 0.8457 - val_loss: 0.3451 - val_acc: 0.8984\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8462\n",
      "Epoch 00045: val_loss did not improve from 0.34226\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4961 - acc: 0.8462 - val_loss: 0.3751 - val_acc: 0.8884\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.8479\n",
      "Epoch 00046: val_loss did not improve from 0.34226\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4927 - acc: 0.8479 - val_loss: 0.3450 - val_acc: 0.8996\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.8469\n",
      "Epoch 00047: val_loss improved from 0.34226 to 0.33830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/047-0.3383.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4896 - acc: 0.8469 - val_loss: 0.3383 - val_acc: 0.8977\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.8531\n",
      "Epoch 00048: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4736 - acc: 0.8531 - val_loss: 0.3453 - val_acc: 0.8998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8517\n",
      "Epoch 00049: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4681 - acc: 0.8517 - val_loss: 0.3509 - val_acc: 0.8982\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.8571\n",
      "Epoch 00050: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4647 - acc: 0.8570 - val_loss: 0.3425 - val_acc: 0.8968\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.8551\n",
      "Epoch 00051: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4581 - acc: 0.8551 - val_loss: 0.3410 - val_acc: 0.9012\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8588\n",
      "Epoch 00052: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4532 - acc: 0.8589 - val_loss: 0.3537 - val_acc: 0.8887\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8606\n",
      "Epoch 00053: val_loss did not improve from 0.33830\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4470 - acc: 0.8606 - val_loss: 0.3577 - val_acc: 0.8884\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4524 - acc: 0.8581\n",
      "Epoch 00054: val_loss improved from 0.33830 to 0.31321, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/054-0.3132.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4525 - acc: 0.8581 - val_loss: 0.3132 - val_acc: 0.9092\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8646\n",
      "Epoch 00055: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4397 - acc: 0.8646 - val_loss: 0.3389 - val_acc: 0.9040\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8605\n",
      "Epoch 00056: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4432 - acc: 0.8605 - val_loss: 0.3212 - val_acc: 0.9005\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8664\n",
      "Epoch 00057: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4346 - acc: 0.8664 - val_loss: 0.3147 - val_acc: 0.9110\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8689\n",
      "Epoch 00058: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4320 - acc: 0.8688 - val_loss: 0.3151 - val_acc: 0.9073\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8657\n",
      "Epoch 00059: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4352 - acc: 0.8657 - val_loss: 0.3209 - val_acc: 0.9057\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.8711\n",
      "Epoch 00060: val_loss did not improve from 0.31321\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4210 - acc: 0.8711 - val_loss: 0.3136 - val_acc: 0.9068\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8687\n",
      "Epoch 00061: val_loss improved from 0.31321 to 0.29486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/061-0.2949.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4144 - acc: 0.8688 - val_loss: 0.2949 - val_acc: 0.9082\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8682\n",
      "Epoch 00062: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4182 - acc: 0.8682 - val_loss: 0.3191 - val_acc: 0.9061\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8694\n",
      "Epoch 00063: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4177 - acc: 0.8694 - val_loss: 0.3268 - val_acc: 0.8996\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8717\n",
      "Epoch 00064: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4080 - acc: 0.8717 - val_loss: 0.3077 - val_acc: 0.9094\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8768\n",
      "Epoch 00065: val_loss improved from 0.29486 to 0.29486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/065-0.2949.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4020 - acc: 0.8768 - val_loss: 0.2949 - val_acc: 0.9136\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8745\n",
      "Epoch 00066: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4006 - acc: 0.8746 - val_loss: 0.3077 - val_acc: 0.9080\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8783\n",
      "Epoch 00067: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3926 - acc: 0.8782 - val_loss: 0.3080 - val_acc: 0.9092\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8777\n",
      "Epoch 00068: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3966 - acc: 0.8777 - val_loss: 0.3038 - val_acc: 0.9115\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8801\n",
      "Epoch 00069: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3856 - acc: 0.8801 - val_loss: 0.2958 - val_acc: 0.9110\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8799\n",
      "Epoch 00070: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3874 - acc: 0.8799 - val_loss: 0.3141 - val_acc: 0.9059\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8811\n",
      "Epoch 00071: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3790 - acc: 0.8810 - val_loss: 0.3105 - val_acc: 0.9073\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8828\n",
      "Epoch 00072: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3785 - acc: 0.8828 - val_loss: 0.2969 - val_acc: 0.9101\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8855\n",
      "Epoch 00073: val_loss did not improve from 0.29486\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3726 - acc: 0.8855 - val_loss: 0.2993 - val_acc: 0.9087\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8822\n",
      "Epoch 00074: val_loss improved from 0.29486 to 0.29418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/074-0.2942.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3726 - acc: 0.8821 - val_loss: 0.2942 - val_acc: 0.9150\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8859\n",
      "Epoch 00075: val_loss improved from 0.29418 to 0.28568, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/075-0.2857.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3687 - acc: 0.8859 - val_loss: 0.2857 - val_acc: 0.9168\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8864\n",
      "Epoch 00076: val_loss did not improve from 0.28568\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3648 - acc: 0.8863 - val_loss: 0.2860 - val_acc: 0.9152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8846\n",
      "Epoch 00077: val_loss did not improve from 0.28568\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3743 - acc: 0.8846 - val_loss: 0.2919 - val_acc: 0.9140\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8882\n",
      "Epoch 00078: val_loss did not improve from 0.28568\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3608 - acc: 0.8881 - val_loss: 0.3029 - val_acc: 0.9108\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8870\n",
      "Epoch 00079: val_loss did not improve from 0.28568\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3623 - acc: 0.8870 - val_loss: 0.3029 - val_acc: 0.9099\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8880\n",
      "Epoch 00080: val_loss improved from 0.28568 to 0.28397, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/080-0.2840.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3573 - acc: 0.8880 - val_loss: 0.2840 - val_acc: 0.9164\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8873\n",
      "Epoch 00081: val_loss did not improve from 0.28397\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3570 - acc: 0.8873 - val_loss: 0.2931 - val_acc: 0.9133\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8902\n",
      "Epoch 00082: val_loss did not improve from 0.28397\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3520 - acc: 0.8903 - val_loss: 0.2845 - val_acc: 0.9178\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8914\n",
      "Epoch 00083: val_loss did not improve from 0.28397\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3445 - acc: 0.8914 - val_loss: 0.2982 - val_acc: 0.9110\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8913\n",
      "Epoch 00084: val_loss did not improve from 0.28397\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3483 - acc: 0.8913 - val_loss: 0.2985 - val_acc: 0.9131\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8970\n",
      "Epoch 00085: val_loss improved from 0.28397 to 0.28144, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/085-0.2814.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3366 - acc: 0.8970 - val_loss: 0.2814 - val_acc: 0.9159\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8953\n",
      "Epoch 00086: val_loss improved from 0.28144 to 0.27730, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/086-0.2773.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3416 - acc: 0.8953 - val_loss: 0.2773 - val_acc: 0.9171\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8956\n",
      "Epoch 00087: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3349 - acc: 0.8956 - val_loss: 0.2935 - val_acc: 0.9113\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8939\n",
      "Epoch 00088: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3388 - acc: 0.8940 - val_loss: 0.2838 - val_acc: 0.9189\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8956\n",
      "Epoch 00089: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3306 - acc: 0.8956 - val_loss: 0.3055 - val_acc: 0.9108\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8999\n",
      "Epoch 00090: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3256 - acc: 0.8999 - val_loss: 0.3080 - val_acc: 0.9136\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8978\n",
      "Epoch 00091: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3279 - acc: 0.8978 - val_loss: 0.2869 - val_acc: 0.9173\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8980\n",
      "Epoch 00092: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3253 - acc: 0.8980 - val_loss: 0.2887 - val_acc: 0.9164\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9002\n",
      "Epoch 00093: val_loss did not improve from 0.27730\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3236 - acc: 0.9002 - val_loss: 0.2912 - val_acc: 0.9124\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9008\n",
      "Epoch 00094: val_loss improved from 0.27730 to 0.27674, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/094-0.2767.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3191 - acc: 0.9008 - val_loss: 0.2767 - val_acc: 0.9175\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9005\n",
      "Epoch 00095: val_loss did not improve from 0.27674\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3148 - acc: 0.9005 - val_loss: 0.2958 - val_acc: 0.9106\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8985\n",
      "Epoch 00096: val_loss did not improve from 0.27674\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3170 - acc: 0.8985 - val_loss: 0.2889 - val_acc: 0.9103\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9015\n",
      "Epoch 00097: val_loss did not improve from 0.27674\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3136 - acc: 0.9015 - val_loss: 0.2803 - val_acc: 0.9187\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9010\n",
      "Epoch 00098: val_loss improved from 0.27674 to 0.26622, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/098-0.2662.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3162 - acc: 0.9010 - val_loss: 0.2662 - val_acc: 0.9229\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9022\n",
      "Epoch 00099: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3130 - acc: 0.9021 - val_loss: 0.2680 - val_acc: 0.9196\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9044\n",
      "Epoch 00100: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3082 - acc: 0.9044 - val_loss: 0.2703 - val_acc: 0.9203\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.9030\n",
      "Epoch 00101: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3080 - acc: 0.9029 - val_loss: 0.2891 - val_acc: 0.9147\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9021\n",
      "Epoch 00102: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3084 - acc: 0.9020 - val_loss: 0.2836 - val_acc: 0.9182\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9039\n",
      "Epoch 00103: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3047 - acc: 0.9039 - val_loss: 0.2882 - val_acc: 0.9150\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9070\n",
      "Epoch 00104: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3000 - acc: 0.9071 - val_loss: 0.2879 - val_acc: 0.9106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9080\n",
      "Epoch 00105: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2906 - acc: 0.9080 - val_loss: 0.2684 - val_acc: 0.9220\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9100\n",
      "Epoch 00106: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2940 - acc: 0.9100 - val_loss: 0.2757 - val_acc: 0.9164\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9095\n",
      "Epoch 00107: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2915 - acc: 0.9094 - val_loss: 0.2719 - val_acc: 0.9224\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9089\n",
      "Epoch 00108: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2937 - acc: 0.9089 - val_loss: 0.2775 - val_acc: 0.9159\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9109\n",
      "Epoch 00109: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2858 - acc: 0.9109 - val_loss: 0.2879 - val_acc: 0.9182\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9091\n",
      "Epoch 00110: val_loss did not improve from 0.26622\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2840 - acc: 0.9091 - val_loss: 0.2838 - val_acc: 0.9159\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9103\n",
      "Epoch 00111: val_loss improved from 0.26622 to 0.26421, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/111-0.2642.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2839 - acc: 0.9103 - val_loss: 0.2642 - val_acc: 0.9255\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9104\n",
      "Epoch 00112: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2846 - acc: 0.9104 - val_loss: 0.2855 - val_acc: 0.9157\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9103\n",
      "Epoch 00113: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2826 - acc: 0.9102 - val_loss: 0.2839 - val_acc: 0.9106\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9130\n",
      "Epoch 00114: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2814 - acc: 0.9130 - val_loss: 0.2903 - val_acc: 0.9124\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9143\n",
      "Epoch 00115: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2750 - acc: 0.9143 - val_loss: 0.2782 - val_acc: 0.9164\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9161\n",
      "Epoch 00116: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2704 - acc: 0.9161 - val_loss: 0.2738 - val_acc: 0.9175\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9119\n",
      "Epoch 00117: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2780 - acc: 0.9119 - val_loss: 0.2678 - val_acc: 0.9213\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9128\n",
      "Epoch 00118: val_loss did not improve from 0.26421\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2755 - acc: 0.9128 - val_loss: 0.2721 - val_acc: 0.9217\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9165\n",
      "Epoch 00119: val_loss improved from 0.26421 to 0.26360, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/119-0.2636.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2701 - acc: 0.9165 - val_loss: 0.2636 - val_acc: 0.9224\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9129\n",
      "Epoch 00120: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2723 - acc: 0.9129 - val_loss: 0.2651 - val_acc: 0.9259\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9160\n",
      "Epoch 00121: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2663 - acc: 0.9159 - val_loss: 0.2740 - val_acc: 0.9199\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9177\n",
      "Epoch 00122: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2660 - acc: 0.9177 - val_loss: 0.2899 - val_acc: 0.9154\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9171\n",
      "Epoch 00123: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2624 - acc: 0.9171 - val_loss: 0.2817 - val_acc: 0.9154\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9180\n",
      "Epoch 00124: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2593 - acc: 0.9180 - val_loss: 0.2816 - val_acc: 0.9180\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9170\n",
      "Epoch 00125: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2613 - acc: 0.9170 - val_loss: 0.2770 - val_acc: 0.9175\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9195\n",
      "Epoch 00126: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2543 - acc: 0.9195 - val_loss: 0.2787 - val_acc: 0.9154\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9205\n",
      "Epoch 00127: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2523 - acc: 0.9205 - val_loss: 0.2727 - val_acc: 0.9187\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9202\n",
      "Epoch 00128: val_loss did not improve from 0.26360\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2506 - acc: 0.9202 - val_loss: 0.2729 - val_acc: 0.9217\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9226\n",
      "Epoch 00129: val_loss improved from 0.26360 to 0.26281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/129-0.2628.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2548 - acc: 0.9226 - val_loss: 0.2628 - val_acc: 0.9229\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9217\n",
      "Epoch 00130: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2501 - acc: 0.9217 - val_loss: 0.2782 - val_acc: 0.9164\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9224\n",
      "Epoch 00131: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2473 - acc: 0.9224 - val_loss: 0.2670 - val_acc: 0.9213\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9228\n",
      "Epoch 00132: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2461 - acc: 0.9228 - val_loss: 0.2707 - val_acc: 0.9203\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9210\n",
      "Epoch 00133: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2477 - acc: 0.9210 - val_loss: 0.2753 - val_acc: 0.9185\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9225\n",
      "Epoch 00134: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2434 - acc: 0.9225 - val_loss: 0.2658 - val_acc: 0.9234\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9240\n",
      "Epoch 00135: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2437 - acc: 0.9240 - val_loss: 0.2632 - val_acc: 0.9234\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9247\n",
      "Epoch 00136: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2339 - acc: 0.9247 - val_loss: 0.2934 - val_acc: 0.9138\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9252\n",
      "Epoch 00137: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2370 - acc: 0.9252 - val_loss: 0.2771 - val_acc: 0.9178\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9237\n",
      "Epoch 00138: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2384 - acc: 0.9237 - val_loss: 0.2719 - val_acc: 0.9180\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9239\n",
      "Epoch 00139: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2372 - acc: 0.9240 - val_loss: 0.2792 - val_acc: 0.9168\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9254\n",
      "Epoch 00140: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2343 - acc: 0.9254 - val_loss: 0.2635 - val_acc: 0.9241\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9270\n",
      "Epoch 00141: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2296 - acc: 0.9270 - val_loss: 0.2777 - val_acc: 0.9185\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9246\n",
      "Epoch 00142: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2353 - acc: 0.9246 - val_loss: 0.2707 - val_acc: 0.9185\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9292\n",
      "Epoch 00143: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2248 - acc: 0.9291 - val_loss: 0.2788 - val_acc: 0.9196\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9257\n",
      "Epoch 00144: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2301 - acc: 0.9257 - val_loss: 0.2853 - val_acc: 0.9173\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9275\n",
      "Epoch 00145: val_loss did not improve from 0.26281\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2294 - acc: 0.9275 - val_loss: 0.2830 - val_acc: 0.9154\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9261\n",
      "Epoch 00146: val_loss improved from 0.26281 to 0.25750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv_checkpoint/146-0.2575.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2320 - acc: 0.9261 - val_loss: 0.2575 - val_acc: 0.9266\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9304\n",
      "Epoch 00147: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2202 - acc: 0.9304 - val_loss: 0.2855 - val_acc: 0.9161\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9295\n",
      "Epoch 00148: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2225 - acc: 0.9295 - val_loss: 0.3001 - val_acc: 0.9106\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9300\n",
      "Epoch 00149: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2236 - acc: 0.9300 - val_loss: 0.2711 - val_acc: 0.9171\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9306\n",
      "Epoch 00150: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2207 - acc: 0.9306 - val_loss: 0.2902 - val_acc: 0.9117\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9316\n",
      "Epoch 00151: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2174 - acc: 0.9316 - val_loss: 0.2660 - val_acc: 0.9206\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9311\n",
      "Epoch 00152: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2177 - acc: 0.9311 - val_loss: 0.2729 - val_acc: 0.9189\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9340\n",
      "Epoch 00153: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2126 - acc: 0.9339 - val_loss: 0.2636 - val_acc: 0.9229\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9314\n",
      "Epoch 00154: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2174 - acc: 0.9314 - val_loss: 0.2732 - val_acc: 0.9224\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9328\n",
      "Epoch 00155: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2132 - acc: 0.9328 - val_loss: 0.2695 - val_acc: 0.9185\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9295\n",
      "Epoch 00156: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2184 - acc: 0.9295 - val_loss: 0.2794 - val_acc: 0.9217\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9327\n",
      "Epoch 00157: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2158 - acc: 0.9327 - val_loss: 0.2794 - val_acc: 0.9173\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9351\n",
      "Epoch 00158: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2087 - acc: 0.9351 - val_loss: 0.2691 - val_acc: 0.9203\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9322\n",
      "Epoch 00159: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2103 - acc: 0.9322 - val_loss: 0.2789 - val_acc: 0.9229\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9346\n",
      "Epoch 00160: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2081 - acc: 0.9346 - val_loss: 0.2826 - val_acc: 0.9136\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9337\n",
      "Epoch 00161: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2089 - acc: 0.9337 - val_loss: 0.3104 - val_acc: 0.9087\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9329\n",
      "Epoch 00162: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2122 - acc: 0.9329 - val_loss: 0.2664 - val_acc: 0.9224\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9359\n",
      "Epoch 00163: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2026 - acc: 0.9359 - val_loss: 0.3037 - val_acc: 0.9080\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9347\n",
      "Epoch 00164: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2050 - acc: 0.9347 - val_loss: 0.2828 - val_acc: 0.9154\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9368\n",
      "Epoch 00165: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2013 - acc: 0.9368 - val_loss: 0.2670 - val_acc: 0.9217\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9340\n",
      "Epoch 00166: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2079 - acc: 0.9341 - val_loss: 0.2663 - val_acc: 0.9238\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9398\n",
      "Epoch 00167: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1928 - acc: 0.9397 - val_loss: 0.2833 - val_acc: 0.9096\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9365\n",
      "Epoch 00168: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2044 - acc: 0.9365 - val_loss: 0.2745 - val_acc: 0.9206\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9377\n",
      "Epoch 00169: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1988 - acc: 0.9377 - val_loss: 0.2749 - val_acc: 0.9150\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9366\n",
      "Epoch 00170: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1995 - acc: 0.9366 - val_loss: 0.2765 - val_acc: 0.9182\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9368\n",
      "Epoch 00171: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1982 - acc: 0.9368 - val_loss: 0.2696 - val_acc: 0.9194\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9385\n",
      "Epoch 00172: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1936 - acc: 0.9385 - val_loss: 0.2964 - val_acc: 0.9159\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9397\n",
      "Epoch 00173: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1916 - acc: 0.9397 - val_loss: 0.2946 - val_acc: 0.9106\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9404\n",
      "Epoch 00174: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1895 - acc: 0.9404 - val_loss: 0.2851 - val_acc: 0.9173\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9370\n",
      "Epoch 00175: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1932 - acc: 0.9370 - val_loss: 0.2867 - val_acc: 0.9192\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9412\n",
      "Epoch 00176: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1850 - acc: 0.9412 - val_loss: 0.2847 - val_acc: 0.9154\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9390\n",
      "Epoch 00177: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1928 - acc: 0.9390 - val_loss: 0.2803 - val_acc: 0.9215\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9405\n",
      "Epoch 00178: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1842 - acc: 0.9406 - val_loss: 0.2820 - val_acc: 0.9182\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9401\n",
      "Epoch 00179: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1880 - acc: 0.9401 - val_loss: 0.3008 - val_acc: 0.9122\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9438\n",
      "Epoch 00180: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1792 - acc: 0.9438 - val_loss: 0.2785 - val_acc: 0.9199\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9401\n",
      "Epoch 00181: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1862 - acc: 0.9401 - val_loss: 0.2913 - val_acc: 0.9159\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9424\n",
      "Epoch 00182: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1816 - acc: 0.9424 - val_loss: 0.2727 - val_acc: 0.9236\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9437\n",
      "Epoch 00183: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1797 - acc: 0.9437 - val_loss: 0.2992 - val_acc: 0.9166\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9438\n",
      "Epoch 00184: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1798 - acc: 0.9438 - val_loss: 0.2696 - val_acc: 0.9203\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9449\n",
      "Epoch 00185: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1782 - acc: 0.9449 - val_loss: 0.3155 - val_acc: 0.9026\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9436\n",
      "Epoch 00186: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1814 - acc: 0.9436 - val_loss: 0.2813 - val_acc: 0.9161\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9398\n",
      "Epoch 00187: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1890 - acc: 0.9397 - val_loss: 0.2766 - val_acc: 0.9182\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9441\n",
      "Epoch 00188: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1772 - acc: 0.9441 - val_loss: 0.2662 - val_acc: 0.9229\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9454\n",
      "Epoch 00189: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1707 - acc: 0.9454 - val_loss: 0.2961 - val_acc: 0.9143\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9465\n",
      "Epoch 00190: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1725 - acc: 0.9465 - val_loss: 0.2864 - val_acc: 0.9168\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9450\n",
      "Epoch 00191: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1742 - acc: 0.9450 - val_loss: 0.2769 - val_acc: 0.9182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9448\n",
      "Epoch 00192: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1730 - acc: 0.9448 - val_loss: 0.3019 - val_acc: 0.9057\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9452\n",
      "Epoch 00193: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1745 - acc: 0.9452 - val_loss: 0.2823 - val_acc: 0.9189\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9461\n",
      "Epoch 00194: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1700 - acc: 0.9461 - val_loss: 0.3093 - val_acc: 0.9087\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9459\n",
      "Epoch 00195: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1731 - acc: 0.9459 - val_loss: 0.2851 - val_acc: 0.9159\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9492\n",
      "Epoch 00196: val_loss did not improve from 0.25750\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1644 - acc: 0.9492 - val_loss: 0.2764 - val_acc: 0.9201\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 991us/sample - loss: 0.3377 - acc: 0.8974\n",
      "Loss: 0.3376808849698163 Accuracy: 0.89740396\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3531 - acc: 0.2138\n",
      "Epoch 00001: val_loss improved from inf to 1.59617, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/001-1.5962.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 4.3527 - acc: 0.2139 - val_loss: 1.5962 - val_acc: 0.4875\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2400 - acc: 0.4093\n",
      "Epoch 00002: val_loss improved from 1.59617 to 0.80921, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/002-0.8092.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 2.2398 - acc: 0.4094 - val_loss: 0.8092 - val_acc: 0.7358\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6083 - acc: 0.5392\n",
      "Epoch 00003: val_loss improved from 0.80921 to 0.59721, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/003-0.5972.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.6083 - acc: 0.5392 - val_loss: 0.5972 - val_acc: 0.8134\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2817 - acc: 0.6202\n",
      "Epoch 00004: val_loss improved from 0.59721 to 0.48736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/004-0.4874.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.2816 - acc: 0.6202 - val_loss: 0.4874 - val_acc: 0.8421\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0817 - acc: 0.6775\n",
      "Epoch 00005: val_loss improved from 0.48736 to 0.41354, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/005-0.4135.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.0816 - acc: 0.6775 - val_loss: 0.4135 - val_acc: 0.8728\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9464 - acc: 0.7135\n",
      "Epoch 00006: val_loss improved from 0.41354 to 0.40673, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/006-0.4067.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.9464 - acc: 0.7135 - val_loss: 0.4067 - val_acc: 0.8749\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8404 - acc: 0.7450\n",
      "Epoch 00007: val_loss improved from 0.40673 to 0.38550, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/007-0.3855.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.8404 - acc: 0.7450 - val_loss: 0.3855 - val_acc: 0.8789\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7680\n",
      "Epoch 00008: val_loss improved from 0.38550 to 0.33809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/008-0.3381.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.7608 - acc: 0.7680 - val_loss: 0.3381 - val_acc: 0.8984\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.7864\n",
      "Epoch 00009: val_loss did not improve from 0.33809\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.6954 - acc: 0.7863 - val_loss: 0.3853 - val_acc: 0.8784\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6439 - acc: 0.7998\n",
      "Epoch 00010: val_loss improved from 0.33809 to 0.28927, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/010-0.2893.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.6439 - acc: 0.7998 - val_loss: 0.2893 - val_acc: 0.9106\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8178\n",
      "Epoch 00011: val_loss did not improve from 0.28927\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5933 - acc: 0.8178 - val_loss: 0.3200 - val_acc: 0.8977\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.8263\n",
      "Epoch 00012: val_loss did not improve from 0.28927\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5602 - acc: 0.8263 - val_loss: 0.3303 - val_acc: 0.8961\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.8381\n",
      "Epoch 00013: val_loss did not improve from 0.28927\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5249 - acc: 0.8381 - val_loss: 0.2893 - val_acc: 0.9071\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8441\n",
      "Epoch 00014: val_loss improved from 0.28927 to 0.25215, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/014-0.2521.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5001 - acc: 0.8441 - val_loss: 0.2521 - val_acc: 0.9231\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8525\n",
      "Epoch 00015: val_loss did not improve from 0.25215\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4744 - acc: 0.8525 - val_loss: 0.2655 - val_acc: 0.9203\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8568\n",
      "Epoch 00016: val_loss improved from 0.25215 to 0.25026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/016-0.2503.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4581 - acc: 0.8568 - val_loss: 0.2503 - val_acc: 0.9222\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8654\n",
      "Epoch 00017: val_loss did not improve from 0.25026\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4338 - acc: 0.8654 - val_loss: 0.2507 - val_acc: 0.9215\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.8689\n",
      "Epoch 00018: val_loss improved from 0.25026 to 0.22526, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/018-0.2253.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4232 - acc: 0.8689 - val_loss: 0.2253 - val_acc: 0.9352\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8753\n",
      "Epoch 00019: val_loss improved from 0.22526 to 0.22520, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/019-0.2252.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4007 - acc: 0.8753 - val_loss: 0.2252 - val_acc: 0.9322\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8777\n",
      "Epoch 00020: val_loss improved from 0.22520 to 0.21627, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/020-0.2163.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3860 - acc: 0.8777 - val_loss: 0.2163 - val_acc: 0.9357\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8811\n",
      "Epoch 00021: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3818 - acc: 0.8810 - val_loss: 0.2256 - val_acc: 0.9304\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8880\n",
      "Epoch 00022: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3647 - acc: 0.8880 - val_loss: 0.2208 - val_acc: 0.9369\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8904\n",
      "Epoch 00023: val_loss did not improve from 0.21627\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3544 - acc: 0.8904 - val_loss: 0.2182 - val_acc: 0.9350\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8946\n",
      "Epoch 00024: val_loss improved from 0.21627 to 0.19985, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/024-0.1998.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3370 - acc: 0.8946 - val_loss: 0.1998 - val_acc: 0.9376\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.8957\n",
      "Epoch 00025: val_loss did not improve from 0.19985\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3284 - acc: 0.8957 - val_loss: 0.2082 - val_acc: 0.9392\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8997\n",
      "Epoch 00026: val_loss improved from 0.19985 to 0.18999, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/026-0.1900.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3204 - acc: 0.8997 - val_loss: 0.1900 - val_acc: 0.9418\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9024\n",
      "Epoch 00027: val_loss did not improve from 0.18999\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3116 - acc: 0.9024 - val_loss: 0.2017 - val_acc: 0.9404\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9055\n",
      "Epoch 00028: val_loss did not improve from 0.18999\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3046 - acc: 0.9055 - val_loss: 0.2009 - val_acc: 0.9394\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9079\n",
      "Epoch 00029: val_loss improved from 0.18999 to 0.18983, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/029-0.1898.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2962 - acc: 0.9079 - val_loss: 0.1898 - val_acc: 0.9420\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9077\n",
      "Epoch 00030: val_loss did not improve from 0.18983\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2950 - acc: 0.9076 - val_loss: 0.1958 - val_acc: 0.9401\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9083\n",
      "Epoch 00031: val_loss improved from 0.18983 to 0.17835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/031-0.1783.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2912 - acc: 0.9083 - val_loss: 0.1783 - val_acc: 0.9485\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9123\n",
      "Epoch 00032: val_loss did not improve from 0.17835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2753 - acc: 0.9123 - val_loss: 0.1842 - val_acc: 0.9418\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9136\n",
      "Epoch 00033: val_loss did not improve from 0.17835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2684 - acc: 0.9137 - val_loss: 0.1820 - val_acc: 0.9448\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9182\n",
      "Epoch 00034: val_loss did not improve from 0.17835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2634 - acc: 0.9182 - val_loss: 0.1907 - val_acc: 0.9425\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9185\n",
      "Epoch 00035: val_loss improved from 0.17835 to 0.17008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/035-0.1701.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2549 - acc: 0.9185 - val_loss: 0.1701 - val_acc: 0.9490\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9176\n",
      "Epoch 00036: val_loss did not improve from 0.17008\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2588 - acc: 0.9176 - val_loss: 0.1764 - val_acc: 0.9474\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9228\n",
      "Epoch 00037: val_loss improved from 0.17008 to 0.16989, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/037-0.1699.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2478 - acc: 0.9228 - val_loss: 0.1699 - val_acc: 0.9497\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9244\n",
      "Epoch 00038: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2391 - acc: 0.9244 - val_loss: 0.1767 - val_acc: 0.9478\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9265\n",
      "Epoch 00039: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2360 - acc: 0.9266 - val_loss: 0.1818 - val_acc: 0.9450\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9262\n",
      "Epoch 00040: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2351 - acc: 0.9262 - val_loss: 0.1812 - val_acc: 0.9455\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9288\n",
      "Epoch 00041: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2258 - acc: 0.9288 - val_loss: 0.1820 - val_acc: 0.9457\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9299\n",
      "Epoch 00042: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2179 - acc: 0.9299 - val_loss: 0.1784 - val_acc: 0.9455\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9310\n",
      "Epoch 00043: val_loss did not improve from 0.16989\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2173 - acc: 0.9310 - val_loss: 0.1918 - val_acc: 0.9432\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9332\n",
      "Epoch 00044: val_loss improved from 0.16989 to 0.16284, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/044-0.1628.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2141 - acc: 0.9332 - val_loss: 0.1628 - val_acc: 0.9525\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9349\n",
      "Epoch 00045: val_loss did not improve from 0.16284\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2085 - acc: 0.9349 - val_loss: 0.1794 - val_acc: 0.9446\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9355\n",
      "Epoch 00046: val_loss did not improve from 0.16284\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2030 - acc: 0.9354 - val_loss: 0.1712 - val_acc: 0.9509\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9363\n",
      "Epoch 00047: val_loss did not improve from 0.16284\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2023 - acc: 0.9363 - val_loss: 0.1652 - val_acc: 0.9474\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9382\n",
      "Epoch 00048: val_loss improved from 0.16284 to 0.15835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/048-0.1583.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1972 - acc: 0.9382 - val_loss: 0.1583 - val_acc: 0.9532\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9384\n",
      "Epoch 00049: val_loss did not improve from 0.15835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1904 - acc: 0.9384 - val_loss: 0.1690 - val_acc: 0.9499\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9414\n",
      "Epoch 00050: val_loss did not improve from 0.15835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1873 - acc: 0.9414 - val_loss: 0.1646 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9400\n",
      "Epoch 00051: val_loss did not improve from 0.15835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1858 - acc: 0.9400 - val_loss: 0.1608 - val_acc: 0.9511\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9431\n",
      "Epoch 00052: val_loss did not improve from 0.15835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1789 - acc: 0.9431 - val_loss: 0.1769 - val_acc: 0.9464\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9433\n",
      "Epoch 00053: val_loss did not improve from 0.15835\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1781 - acc: 0.9433 - val_loss: 0.1758 - val_acc: 0.9462\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9438\n",
      "Epoch 00054: val_loss improved from 0.15835 to 0.15417, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv_checkpoint/054-0.1542.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1750 - acc: 0.9438 - val_loss: 0.1542 - val_acc: 0.9529\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9457\n",
      "Epoch 00055: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1702 - acc: 0.9457 - val_loss: 0.1588 - val_acc: 0.9527\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9471\n",
      "Epoch 00056: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1660 - acc: 0.9471 - val_loss: 0.1994 - val_acc: 0.9373\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9462\n",
      "Epoch 00057: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1661 - acc: 0.9462 - val_loss: 0.1829 - val_acc: 0.9420\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9476\n",
      "Epoch 00058: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1631 - acc: 0.9476 - val_loss: 0.1572 - val_acc: 0.9539\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9500\n",
      "Epoch 00059: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1563 - acc: 0.9500 - val_loss: 0.1607 - val_acc: 0.9534\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9515\n",
      "Epoch 00060: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1521 - acc: 0.9515 - val_loss: 0.1761 - val_acc: 0.9478\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9516\n",
      "Epoch 00061: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1517 - acc: 0.9516 - val_loss: 0.1613 - val_acc: 0.9527\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9529\n",
      "Epoch 00062: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1465 - acc: 0.9529 - val_loss: 0.1832 - val_acc: 0.9462\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9530\n",
      "Epoch 00063: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1465 - acc: 0.9530 - val_loss: 0.1648 - val_acc: 0.9534\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9541\n",
      "Epoch 00064: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1410 - acc: 0.9541 - val_loss: 0.1684 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9557\n",
      "Epoch 00065: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1373 - acc: 0.9557 - val_loss: 0.1640 - val_acc: 0.9520\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9560\n",
      "Epoch 00066: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1369 - acc: 0.9560 - val_loss: 0.1618 - val_acc: 0.9502\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9562\n",
      "Epoch 00067: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1377 - acc: 0.9562 - val_loss: 0.1617 - val_acc: 0.9502\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9566\n",
      "Epoch 00068: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1340 - acc: 0.9566 - val_loss: 0.1587 - val_acc: 0.9557\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9576\n",
      "Epoch 00069: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1319 - acc: 0.9576 - val_loss: 0.1609 - val_acc: 0.9527\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9575\n",
      "Epoch 00070: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1308 - acc: 0.9575 - val_loss: 0.1621 - val_acc: 0.9536\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9610\n",
      "Epoch 00071: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1208 - acc: 0.9610 - val_loss: 0.1701 - val_acc: 0.9499\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9615\n",
      "Epoch 00072: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1227 - acc: 0.9616 - val_loss: 0.1816 - val_acc: 0.9474\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9633\n",
      "Epoch 00073: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1154 - acc: 0.9633 - val_loss: 0.1673 - val_acc: 0.9532\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9634\n",
      "Epoch 00074: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1163 - acc: 0.9633 - val_loss: 0.1675 - val_acc: 0.9534\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9621\n",
      "Epoch 00075: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1172 - acc: 0.9621 - val_loss: 0.1700 - val_acc: 0.9499\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9633\n",
      "Epoch 00076: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1133 - acc: 0.9633 - val_loss: 0.1670 - val_acc: 0.9543\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9645\n",
      "Epoch 00077: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1103 - acc: 0.9645 - val_loss: 0.1701 - val_acc: 0.9522\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9646\n",
      "Epoch 00078: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1113 - acc: 0.9646 - val_loss: 0.1590 - val_acc: 0.9557\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9653\n",
      "Epoch 00079: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1082 - acc: 0.9653 - val_loss: 0.1547 - val_acc: 0.9522\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9676\n",
      "Epoch 00080: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1017 - acc: 0.9676 - val_loss: 0.1617 - val_acc: 0.9497\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9670\n",
      "Epoch 00081: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1027 - acc: 0.9670 - val_loss: 0.1721 - val_acc: 0.9504\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9684\n",
      "Epoch 00082: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0971 - acc: 0.9684 - val_loss: 0.2199 - val_acc: 0.9369\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9678\n",
      "Epoch 00083: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0992 - acc: 0.9678 - val_loss: 0.1602 - val_acc: 0.9567\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9685\n",
      "Epoch 00084: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0980 - acc: 0.9685 - val_loss: 0.2019 - val_acc: 0.9373\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9694\n",
      "Epoch 00085: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0951 - acc: 0.9694 - val_loss: 0.1786 - val_acc: 0.9504\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9699\n",
      "Epoch 00086: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0918 - acc: 0.9699 - val_loss: 0.1704 - val_acc: 0.9499\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9707\n",
      "Epoch 00087: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0941 - acc: 0.9707 - val_loss: 0.1664 - val_acc: 0.9546\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9723\n",
      "Epoch 00088: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0887 - acc: 0.9723 - val_loss: 0.1664 - val_acc: 0.9548\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9711\n",
      "Epoch 00089: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0886 - acc: 0.9711 - val_loss: 0.1775 - val_acc: 0.9525\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9729\n",
      "Epoch 00090: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0871 - acc: 0.9729 - val_loss: 0.1663 - val_acc: 0.9529\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9736\n",
      "Epoch 00091: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0846 - acc: 0.9736 - val_loss: 0.2003 - val_acc: 0.9450\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9708\n",
      "Epoch 00092: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0906 - acc: 0.9708 - val_loss: 0.1665 - val_acc: 0.9548\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9738\n",
      "Epoch 00093: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0817 - acc: 0.9738 - val_loss: 0.1708 - val_acc: 0.9522\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9734\n",
      "Epoch 00094: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0802 - acc: 0.9734 - val_loss: 0.1867 - val_acc: 0.9474\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9735\n",
      "Epoch 00095: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0824 - acc: 0.9735 - val_loss: 0.1707 - val_acc: 0.9513\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9747\n",
      "Epoch 00096: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0790 - acc: 0.9746 - val_loss: 0.1830 - val_acc: 0.9499\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9722\n",
      "Epoch 00097: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0835 - acc: 0.9722 - val_loss: 0.1714 - val_acc: 0.9548\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9761\n",
      "Epoch 00098: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0762 - acc: 0.9760 - val_loss: 0.1715 - val_acc: 0.9532\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9755\n",
      "Epoch 00099: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0774 - acc: 0.9754 - val_loss: 0.2287 - val_acc: 0.9387\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9732\n",
      "Epoch 00100: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0829 - acc: 0.9732 - val_loss: 0.1696 - val_acc: 0.9520\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9773\n",
      "Epoch 00101: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0714 - acc: 0.9773 - val_loss: 0.1779 - val_acc: 0.9506\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9779\n",
      "Epoch 00102: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0698 - acc: 0.9779 - val_loss: 0.2037 - val_acc: 0.9448\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9761\n",
      "Epoch 00103: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0738 - acc: 0.9761 - val_loss: 0.1680 - val_acc: 0.9511\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9793\n",
      "Epoch 00104: val_loss did not improve from 0.15417\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0660 - acc: 0.9793 - val_loss: 0.1694 - val_acc: 0.9529\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1841 - acc: 0.9404\n",
      "Loss: 0.18406000310634404 Accuracy: 0.9403946\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1766 - acc: 0.3445\n",
      "Epoch 00001: val_loss improved from inf to 1.09616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/001-1.0962.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 3.1770 - acc: 0.3445 - val_loss: 1.0962 - val_acc: 0.6380\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3318 - acc: 0.6319\n",
      "Epoch 00002: val_loss improved from 1.09616 to 0.42718, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/002-0.4272.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3317 - acc: 0.6319 - val_loss: 0.4272 - val_acc: 0.8744\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.7438\n",
      "Epoch 00003: val_loss improved from 0.42718 to 0.32882, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/003-0.3288.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.9017 - acc: 0.7438 - val_loss: 0.3288 - val_acc: 0.8987\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7904\n",
      "Epoch 00004: val_loss improved from 0.32882 to 0.30448, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/004-0.3045.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7220 - acc: 0.7903 - val_loss: 0.3045 - val_acc: 0.9059\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.8215\n",
      "Epoch 00005: val_loss improved from 0.30448 to 0.24511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/005-0.2451.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6086 - acc: 0.8215 - val_loss: 0.2451 - val_acc: 0.9241\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8441\n",
      "Epoch 00006: val_loss improved from 0.24511 to 0.21680, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/006-0.2168.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5304 - acc: 0.8441 - val_loss: 0.2168 - val_acc: 0.9327\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8542\n",
      "Epoch 00007: val_loss did not improve from 0.21680\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4919 - acc: 0.8542 - val_loss: 0.2887 - val_acc: 0.9119\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4336 - acc: 0.8696\n",
      "Epoch 00008: val_loss improved from 0.21680 to 0.19907, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/008-0.1991.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4337 - acc: 0.8696 - val_loss: 0.1991 - val_acc: 0.9392\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8808\n",
      "Epoch 00009: val_loss did not improve from 0.19907\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3962 - acc: 0.8807 - val_loss: 0.2123 - val_acc: 0.9348\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8906\n",
      "Epoch 00010: val_loss improved from 0.19907 to 0.19057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/010-0.1906.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3621 - acc: 0.8906 - val_loss: 0.1906 - val_acc: 0.9457\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8959\n",
      "Epoch 00011: val_loss did not improve from 0.19057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3467 - acc: 0.8959 - val_loss: 0.2509 - val_acc: 0.9178\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9036\n",
      "Epoch 00012: val_loss improved from 0.19057 to 0.15804, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/012-0.1580.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3150 - acc: 0.9036 - val_loss: 0.1580 - val_acc: 0.9522\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9079\n",
      "Epoch 00013: val_loss improved from 0.15804 to 0.14660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/013-0.1466.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3029 - acc: 0.9078 - val_loss: 0.1466 - val_acc: 0.9581\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9129\n",
      "Epoch 00014: val_loss did not improve from 0.14660\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2837 - acc: 0.9129 - val_loss: 0.1556 - val_acc: 0.9525\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9156\n",
      "Epoch 00015: val_loss did not improve from 0.14660\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2689 - acc: 0.9156 - val_loss: 0.1882 - val_acc: 0.9401\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9211\n",
      "Epoch 00016: val_loss did not improve from 0.14660\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2519 - acc: 0.9210 - val_loss: 0.1664 - val_acc: 0.9478\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9239\n",
      "Epoch 00017: val_loss improved from 0.14660 to 0.12938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/017-0.1294.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2458 - acc: 0.9239 - val_loss: 0.1294 - val_acc: 0.9623\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9279\n",
      "Epoch 00018: val_loss did not improve from 0.12938\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2309 - acc: 0.9279 - val_loss: 0.1487 - val_acc: 0.9562\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9302\n",
      "Epoch 00019: val_loss did not improve from 0.12938\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2231 - acc: 0.9302 - val_loss: 0.1332 - val_acc: 0.9613\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9327\n",
      "Epoch 00020: val_loss improved from 0.12938 to 0.12845, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/020-0.1284.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2102 - acc: 0.9327 - val_loss: 0.1284 - val_acc: 0.9611\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9376\n",
      "Epoch 00021: val_loss did not improve from 0.12845\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1995 - acc: 0.9376 - val_loss: 0.1606 - val_acc: 0.9536\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9391\n",
      "Epoch 00022: val_loss did not improve from 0.12845\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1937 - acc: 0.9391 - val_loss: 0.1341 - val_acc: 0.9618\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9413\n",
      "Epoch 00023: val_loss improved from 0.12845 to 0.12057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/023-0.1206.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1836 - acc: 0.9413 - val_loss: 0.1206 - val_acc: 0.9616\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9438\n",
      "Epoch 00024: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1781 - acc: 0.9437 - val_loss: 0.1427 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9443\n",
      "Epoch 00025: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1779 - acc: 0.9443 - val_loss: 0.1255 - val_acc: 0.9595\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9467\n",
      "Epoch 00026: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1647 - acc: 0.9467 - val_loss: 0.1237 - val_acc: 0.9627\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9483\n",
      "Epoch 00027: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1565 - acc: 0.9482 - val_loss: 0.1279 - val_acc: 0.9604\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9499\n",
      "Epoch 00028: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1561 - acc: 0.9499 - val_loss: 0.1319 - val_acc: 0.9639\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9520\n",
      "Epoch 00029: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1502 - acc: 0.9519 - val_loss: 0.1253 - val_acc: 0.9618\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9541\n",
      "Epoch 00030: val_loss improved from 0.12057 to 0.11181, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/030-0.1118.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1422 - acc: 0.9541 - val_loss: 0.1118 - val_acc: 0.9665\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9527\n",
      "Epoch 00031: val_loss did not improve from 0.11181\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1421 - acc: 0.9527 - val_loss: 0.1275 - val_acc: 0.9634\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9575\n",
      "Epoch 00032: val_loss improved from 0.11181 to 0.10892, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv_checkpoint/032-0.1089.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1300 - acc: 0.9575 - val_loss: 0.1089 - val_acc: 0.9644\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9595\n",
      "Epoch 00033: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1253 - acc: 0.9595 - val_loss: 0.1506 - val_acc: 0.9532\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9598\n",
      "Epoch 00034: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1247 - acc: 0.9598 - val_loss: 0.1299 - val_acc: 0.9618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9593\n",
      "Epoch 00035: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1246 - acc: 0.9593 - val_loss: 0.1243 - val_acc: 0.9604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9623\n",
      "Epoch 00036: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1141 - acc: 0.9623 - val_loss: 0.1091 - val_acc: 0.9660\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9628\n",
      "Epoch 00037: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1138 - acc: 0.9628 - val_loss: 0.1114 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9655\n",
      "Epoch 00038: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1055 - acc: 0.9655 - val_loss: 0.1225 - val_acc: 0.9604\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9662\n",
      "Epoch 00039: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1073 - acc: 0.9662 - val_loss: 0.1349 - val_acc: 0.9611\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9668\n",
      "Epoch 00040: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1011 - acc: 0.9667 - val_loss: 0.1485 - val_acc: 0.9578\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9674\n",
      "Epoch 00041: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1008 - acc: 0.9674 - val_loss: 0.1124 - val_acc: 0.9658\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9704\n",
      "Epoch 00042: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0921 - acc: 0.9704 - val_loss: 0.1355 - val_acc: 0.9639\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9691\n",
      "Epoch 00043: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0938 - acc: 0.9691 - val_loss: 0.1192 - val_acc: 0.9639\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9711\n",
      "Epoch 00044: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0895 - acc: 0.9711 - val_loss: 0.1118 - val_acc: 0.9660\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9708\n",
      "Epoch 00045: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0884 - acc: 0.9708 - val_loss: 0.1447 - val_acc: 0.9602\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9735\n",
      "Epoch 00046: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0819 - acc: 0.9735 - val_loss: 0.1458 - val_acc: 0.9581\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9733\n",
      "Epoch 00047: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0788 - acc: 0.9733 - val_loss: 0.1227 - val_acc: 0.9637\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9743\n",
      "Epoch 00048: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0788 - acc: 0.9743 - val_loss: 0.1460 - val_acc: 0.9576\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9765\n",
      "Epoch 00049: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0722 - acc: 0.9765 - val_loss: 0.1219 - val_acc: 0.9646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9768\n",
      "Epoch 00050: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0720 - acc: 0.9768 - val_loss: 0.1183 - val_acc: 0.9658\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9741\n",
      "Epoch 00051: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0784 - acc: 0.9741 - val_loss: 0.1168 - val_acc: 0.9669\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0672 - acc: 0.9780 - val_loss: 0.1251 - val_acc: 0.9665\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9785\n",
      "Epoch 00053: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0659 - acc: 0.9785 - val_loss: 0.1266 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9785\n",
      "Epoch 00054: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0661 - acc: 0.9785 - val_loss: 0.1221 - val_acc: 0.9651\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9804\n",
      "Epoch 00055: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0618 - acc: 0.9804 - val_loss: 0.1631 - val_acc: 0.9520\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9780\n",
      "Epoch 00056: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0656 - acc: 0.9780 - val_loss: 0.1338 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9808\n",
      "Epoch 00057: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0587 - acc: 0.9808 - val_loss: 0.1190 - val_acc: 0.9681\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9792\n",
      "Epoch 00058: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0623 - acc: 0.9792 - val_loss: 0.1291 - val_acc: 0.9627\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9812\n",
      "Epoch 00059: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0567 - acc: 0.9813 - val_loss: 0.1308 - val_acc: 0.9625\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9817\n",
      "Epoch 00060: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0556 - acc: 0.9817 - val_loss: 0.1192 - val_acc: 0.9648\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9824\n",
      "Epoch 00061: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0540 - acc: 0.9824 - val_loss: 0.1239 - val_acc: 0.9669\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9829\n",
      "Epoch 00062: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0527 - acc: 0.9829 - val_loss: 0.1179 - val_acc: 0.9646\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9831\n",
      "Epoch 00063: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0525 - acc: 0.9831 - val_loss: 0.1189 - val_acc: 0.9660\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9826\n",
      "Epoch 00064: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0540 - acc: 0.9826 - val_loss: 0.1237 - val_acc: 0.9630\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9857\n",
      "Epoch 00065: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0459 - acc: 0.9857 - val_loss: 0.1243 - val_acc: 0.9665\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9849\n",
      "Epoch 00066: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0445 - acc: 0.9849 - val_loss: 0.1534 - val_acc: 0.9604\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9850\n",
      "Epoch 00067: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0452 - acc: 0.9850 - val_loss: 0.1108 - val_acc: 0.9693\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9859\n",
      "Epoch 00068: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0423 - acc: 0.9859 - val_loss: 0.1292 - val_acc: 0.9651\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9858\n",
      "Epoch 00069: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0440 - acc: 0.9858 - val_loss: 0.1421 - val_acc: 0.9609\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9858\n",
      "Epoch 00070: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0427 - acc: 0.9858 - val_loss: 0.1228 - val_acc: 0.9669\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9864\n",
      "Epoch 00071: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0431 - acc: 0.9864 - val_loss: 0.1248 - val_acc: 0.9658\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9830\n",
      "Epoch 00072: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0533 - acc: 0.9830 - val_loss: 0.1275 - val_acc: 0.9665\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9867\n",
      "Epoch 00073: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0396 - acc: 0.9867 - val_loss: 0.1274 - val_acc: 0.9667\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9888\n",
      "Epoch 00074: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0341 - acc: 0.9888 - val_loss: 0.1280 - val_acc: 0.9653\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9896\n",
      "Epoch 00075: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0340 - acc: 0.9896 - val_loss: 0.1157 - val_acc: 0.9681\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9886\n",
      "Epoch 00076: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0362 - acc: 0.9886 - val_loss: 0.1294 - val_acc: 0.9634\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9885\n",
      "Epoch 00077: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0348 - acc: 0.9885 - val_loss: 0.1265 - val_acc: 0.9674\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9897\n",
      "Epoch 00078: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0320 - acc: 0.9897 - val_loss: 0.1447 - val_acc: 0.9630\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9890\n",
      "Epoch 00079: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0332 - acc: 0.9891 - val_loss: 0.1283 - val_acc: 0.9644\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9892\n",
      "Epoch 00080: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.1483 - val_acc: 0.9606\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9898\n",
      "Epoch 00081: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0310 - acc: 0.9898 - val_loss: 0.1274 - val_acc: 0.9651\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9863\n",
      "Epoch 00082: val_loss did not improve from 0.10892\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0405 - acc: 0.9863 - val_loss: 0.1397 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1343 - acc: 0.9578\n",
      "Loss: 0.13427084166467623 Accuracy: 0.9578401\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1900 - acc: 0.4794\n",
      "Epoch 00001: val_loss improved from inf to 0.78139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/001-0.7814.hdf5\n",
      "36805/36805 [==============================] - 122s 3ms/sample - loss: 2.1898 - acc: 0.4794 - val_loss: 0.7814 - val_acc: 0.7529\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8021 - acc: 0.7687\n",
      "Epoch 00002: val_loss improved from 0.78139 to 0.26382, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/002-0.2638.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.8022 - acc: 0.7687 - val_loss: 0.2638 - val_acc: 0.9215\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8388\n",
      "Epoch 00003: val_loss improved from 0.26382 to 0.20736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/003-0.2074.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.5467 - acc: 0.8388 - val_loss: 0.2074 - val_acc: 0.9392\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8692\n",
      "Epoch 00004: val_loss improved from 0.20736 to 0.16888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/004-0.1689.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4457 - acc: 0.8692 - val_loss: 0.1689 - val_acc: 0.9450\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8870\n",
      "Epoch 00005: val_loss did not improve from 0.16888\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3784 - acc: 0.8870 - val_loss: 0.3290 - val_acc: 0.8887\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8984\n",
      "Epoch 00006: val_loss did not improve from 0.16888\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3384 - acc: 0.8984 - val_loss: 0.1691 - val_acc: 0.9429\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9106\n",
      "Epoch 00007: val_loss improved from 0.16888 to 0.12815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/007-0.1281.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2911 - acc: 0.9106 - val_loss: 0.1281 - val_acc: 0.9602\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9180\n",
      "Epoch 00008: val_loss did not improve from 0.12815\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2644 - acc: 0.9180 - val_loss: 0.1496 - val_acc: 0.9576\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9240\n",
      "Epoch 00009: val_loss did not improve from 0.12815\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2440 - acc: 0.9241 - val_loss: 0.1482 - val_acc: 0.9520\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9321\n",
      "Epoch 00010: val_loss improved from 0.12815 to 0.10614, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/010-0.1061.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2181 - acc: 0.9321 - val_loss: 0.1061 - val_acc: 0.9662\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9368\n",
      "Epoch 00011: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1978 - acc: 0.9368 - val_loss: 0.1196 - val_acc: 0.9660\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9409\n",
      "Epoch 00012: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1868 - acc: 0.9409 - val_loss: 0.1247 - val_acc: 0.9641\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9432\n",
      "Epoch 00013: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1770 - acc: 0.9432 - val_loss: 0.1110 - val_acc: 0.9655\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9481\n",
      "Epoch 00014: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1610 - acc: 0.9481 - val_loss: 0.1284 - val_acc: 0.9611\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9504\n",
      "Epoch 00015: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1541 - acc: 0.9504 - val_loss: 0.1163 - val_acc: 0.9646\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9543\n",
      "Epoch 00016: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1416 - acc: 0.9543 - val_loss: 0.1267 - val_acc: 0.9574\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9567\n",
      "Epoch 00017: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1361 - acc: 0.9567 - val_loss: 0.1142 - val_acc: 0.9644\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9609\n",
      "Epoch 00018: val_loss did not improve from 0.10614\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1195 - acc: 0.9609 - val_loss: 0.1140 - val_acc: 0.9660\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9598\n",
      "Epoch 00019: val_loss improved from 0.10614 to 0.10149, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/019-0.1015.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1204 - acc: 0.9598 - val_loss: 0.1015 - val_acc: 0.9669\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9642\n",
      "Epoch 00020: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1118 - acc: 0.9642 - val_loss: 0.1193 - val_acc: 0.9665\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9672\n",
      "Epoch 00021: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1025 - acc: 0.9672 - val_loss: 0.1350 - val_acc: 0.9588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9662\n",
      "Epoch 00022: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1004 - acc: 0.9662 - val_loss: 0.1476 - val_acc: 0.9527\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9624\n",
      "Epoch 00023: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1150 - acc: 0.9624 - val_loss: 0.1369 - val_acc: 0.9599\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9711\n",
      "Epoch 00024: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0868 - acc: 0.9711 - val_loss: 0.1326 - val_acc: 0.9665\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9710\n",
      "Epoch 00025: val_loss did not improve from 0.10149\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0892 - acc: 0.9709 - val_loss: 0.1018 - val_acc: 0.9697\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9730\n",
      "Epoch 00026: val_loss improved from 0.10149 to 0.09626, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/026-0.0963.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0814 - acc: 0.9730 - val_loss: 0.0963 - val_acc: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9750\n",
      "Epoch 00027: val_loss did not improve from 0.09626\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0745 - acc: 0.9750 - val_loss: 0.0979 - val_acc: 0.9709\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9751\n",
      "Epoch 00028: val_loss did not improve from 0.09626\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0750 - acc: 0.9751 - val_loss: 0.1189 - val_acc: 0.9634\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9774\n",
      "Epoch 00029: val_loss did not improve from 0.09626\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0680 - acc: 0.9774 - val_loss: 0.1282 - val_acc: 0.9604\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9789\n",
      "Epoch 00030: val_loss improved from 0.09626 to 0.09099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/030-0.0910.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0635 - acc: 0.9788 - val_loss: 0.0910 - val_acc: 0.9725\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9771\n",
      "Epoch 00031: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0690 - acc: 0.9771 - val_loss: 0.0915 - val_acc: 0.9711\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9762\n",
      "Epoch 00032: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0726 - acc: 0.9762 - val_loss: 0.0966 - val_acc: 0.9711\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9830\n",
      "Epoch 00033: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0531 - acc: 0.9830 - val_loss: 0.1070 - val_acc: 0.9672\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9818\n",
      "Epoch 00034: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0563 - acc: 0.9818 - val_loss: 0.1274 - val_acc: 0.9667\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9837\n",
      "Epoch 00035: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0492 - acc: 0.9837 - val_loss: 0.0919 - val_acc: 0.9720\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9842\n",
      "Epoch 00036: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0472 - acc: 0.9842 - val_loss: 0.1115 - val_acc: 0.9688\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9811\n",
      "Epoch 00037: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0577 - acc: 0.9811 - val_loss: 0.0952 - val_acc: 0.9713\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9861\n",
      "Epoch 00038: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0423 - acc: 0.9860 - val_loss: 0.1232 - val_acc: 0.9651\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9854\n",
      "Epoch 00039: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0444 - acc: 0.9854 - val_loss: 0.1050 - val_acc: 0.9737\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9859\n",
      "Epoch 00040: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0422 - acc: 0.9859 - val_loss: 0.0951 - val_acc: 0.9734\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9860\n",
      "Epoch 00041: val_loss did not improve from 0.09099\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0420 - acc: 0.9860 - val_loss: 0.0944 - val_acc: 0.9732\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9887\n",
      "Epoch 00042: val_loss improved from 0.09099 to 0.08987, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv_checkpoint/042-0.0899.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0368 - acc: 0.9888 - val_loss: 0.0899 - val_acc: 0.9734\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9892\n",
      "Epoch 00043: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0995 - val_acc: 0.9744\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9879\n",
      "Epoch 00044: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0363 - acc: 0.9879 - val_loss: 0.1289 - val_acc: 0.9634\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9892\n",
      "Epoch 00045: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0332 - acc: 0.9891 - val_loss: 0.1045 - val_acc: 0.9697\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9867\n",
      "Epoch 00046: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0400 - acc: 0.9867 - val_loss: 0.1462 - val_acc: 0.9585\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9913\n",
      "Epoch 00047: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0276 - acc: 0.9913 - val_loss: 0.1028 - val_acc: 0.9713\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9908\n",
      "Epoch 00048: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0289 - acc: 0.9908 - val_loss: 0.0999 - val_acc: 0.9709\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 00049: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0266 - acc: 0.9918 - val_loss: 0.1004 - val_acc: 0.9739\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9916\n",
      "Epoch 00050: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0270 - acc: 0.9916 - val_loss: 0.1508 - val_acc: 0.9602\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 00051: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.1670 - val_acc: 0.9609\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9917\n",
      "Epoch 00052: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0275 - acc: 0.9917 - val_loss: 0.1013 - val_acc: 0.9697\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9922\n",
      "Epoch 00053: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0259 - acc: 0.9922 - val_loss: 0.1101 - val_acc: 0.9711\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9907\n",
      "Epoch 00054: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0284 - acc: 0.9907 - val_loss: 0.1012 - val_acc: 0.9753\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9924\n",
      "Epoch 00055: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0233 - acc: 0.9924 - val_loss: 0.1128 - val_acc: 0.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9942\n",
      "Epoch 00056: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0197 - acc: 0.9942 - val_loss: 0.1144 - val_acc: 0.9720\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9918\n",
      "Epoch 00057: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0252 - acc: 0.9918 - val_loss: 0.1078 - val_acc: 0.9709\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9939\n",
      "Epoch 00058: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0198 - acc: 0.9939 - val_loss: 0.1052 - val_acc: 0.9727\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9943\n",
      "Epoch 00059: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0924 - val_acc: 0.9751\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9937\n",
      "Epoch 00060: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0190 - acc: 0.9937 - val_loss: 0.0912 - val_acc: 0.9753\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9945\n",
      "Epoch 00061: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0186 - acc: 0.9945 - val_loss: 0.0967 - val_acc: 0.9739\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9952\n",
      "Epoch 00062: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0174 - acc: 0.9952 - val_loss: 0.1207 - val_acc: 0.9686\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9930\n",
      "Epoch 00063: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0219 - acc: 0.9930 - val_loss: 0.0999 - val_acc: 0.9746\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 00064: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0155 - acc: 0.9953 - val_loss: 0.1368 - val_acc: 0.9653\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 00065: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0181 - acc: 0.9943 - val_loss: 0.1115 - val_acc: 0.9727\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9926\n",
      "Epoch 00066: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0232 - acc: 0.9926 - val_loss: 0.1142 - val_acc: 0.9669\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9941\n",
      "Epoch 00067: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0192 - acc: 0.9941 - val_loss: 0.1304 - val_acc: 0.9676\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9961\n",
      "Epoch 00068: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0138 - acc: 0.9961 - val_loss: 0.1242 - val_acc: 0.9686\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 00069: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0181 - acc: 0.9942 - val_loss: 0.1170 - val_acc: 0.9688\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9945\n",
      "Epoch 00070: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0174 - acc: 0.9945 - val_loss: 0.1706 - val_acc: 0.9571\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9938\n",
      "Epoch 00071: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0207 - acc: 0.9938 - val_loss: 0.1048 - val_acc: 0.9709\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9969\n",
      "Epoch 00072: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0108 - acc: 0.9969 - val_loss: 0.1111 - val_acc: 0.9704\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1469 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9925\n",
      "Epoch 00074: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0243 - acc: 0.9924 - val_loss: 0.1139 - val_acc: 0.9741\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9945\n",
      "Epoch 00075: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0164 - acc: 0.9945 - val_loss: 0.1329 - val_acc: 0.9697\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9949\n",
      "Epoch 00076: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0159 - acc: 0.9949 - val_loss: 0.0936 - val_acc: 0.9772\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9968\n",
      "Epoch 00077: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0983 - val_acc: 0.9751\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9970\n",
      "Epoch 00078: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.1123 - val_acc: 0.9739\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9950\n",
      "Epoch 00079: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0151 - acc: 0.9950 - val_loss: 0.0980 - val_acc: 0.9746\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
      "Epoch 00080: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.1117 - val_acc: 0.9723\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9968\n",
      "Epoch 00081: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0111 - acc: 0.9968 - val_loss: 0.1245 - val_acc: 0.9695\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9951\n",
      "Epoch 00082: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0153 - acc: 0.9951 - val_loss: 0.1303 - val_acc: 0.9697\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9959\n",
      "Epoch 00083: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1598 - val_acc: 0.9658\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9934\n",
      "Epoch 00084: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0221 - acc: 0.9934 - val_loss: 0.0971 - val_acc: 0.9762\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00085: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1277 - val_acc: 0.9693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9971\n",
      "Epoch 00086: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0098 - acc: 0.9971 - val_loss: 0.1069 - val_acc: 0.9746\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9970\n",
      "Epoch 00087: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0106 - acc: 0.9970 - val_loss: 0.1241 - val_acc: 0.9690\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968\n",
      "Epoch 00088: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0110 - acc: 0.9968 - val_loss: 0.1032 - val_acc: 0.9753\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9967\n",
      "Epoch 00089: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.1350 - val_acc: 0.9706\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 00090: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0079 - acc: 0.9980 - val_loss: 0.1365 - val_acc: 0.9655\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9961\n",
      "Epoch 00091: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0122 - acc: 0.9961 - val_loss: 0.1091 - val_acc: 0.9741\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9933\n",
      "Epoch 00092: val_loss did not improve from 0.08987\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0213 - acc: 0.9933 - val_loss: 0.1145 - val_acc: 0.9700\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1374 - acc: 0.9599\n",
      "Loss: 0.13736378238579938 Accuracy: 0.95991695\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5851 - acc: 0.5772\n",
      "Epoch 00001: val_loss improved from inf to 0.63815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/001-0.6382.hdf5\n",
      "36805/36805 [==============================] - 126s 3ms/sample - loss: 1.5849 - acc: 0.5772 - val_loss: 0.6382 - val_acc: 0.8097\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8302\n",
      "Epoch 00002: val_loss improved from 0.63815 to 0.27808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/002-0.2781.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.5622 - acc: 0.8302 - val_loss: 0.2781 - val_acc: 0.9166\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8802\n",
      "Epoch 00003: val_loss improved from 0.27808 to 0.26832, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/003-0.2683.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.3956 - acc: 0.8802 - val_loss: 0.2683 - val_acc: 0.9147\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9030\n",
      "Epoch 00004: val_loss improved from 0.26832 to 0.21517, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/004-0.2152.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.3172 - acc: 0.9030 - val_loss: 0.2152 - val_acc: 0.9301\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9193\n",
      "Epoch 00005: val_loss improved from 0.21517 to 0.19348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/005-0.1935.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2585 - acc: 0.9194 - val_loss: 0.1935 - val_acc: 0.9362\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9279\n",
      "Epoch 00006: val_loss did not improve from 0.19348\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2307 - acc: 0.9279 - val_loss: 0.4674 - val_acc: 0.8775\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9347\n",
      "Epoch 00007: val_loss improved from 0.19348 to 0.12575, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/007-0.1258.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2050 - acc: 0.9347 - val_loss: 0.1258 - val_acc: 0.9618\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9441\n",
      "Epoch 00008: val_loss did not improve from 0.12575\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1738 - acc: 0.9441 - val_loss: 0.1799 - val_acc: 0.9457\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9495\n",
      "Epoch 00009: val_loss did not improve from 0.12575\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1552 - acc: 0.9495 - val_loss: 0.1505 - val_acc: 0.9509\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9552\n",
      "Epoch 00010: val_loss did not improve from 0.12575\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1406 - acc: 0.9552 - val_loss: 0.1584 - val_acc: 0.9488\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9579\n",
      "Epoch 00011: val_loss improved from 0.12575 to 0.11991, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/011-0.1199.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1309 - acc: 0.9579 - val_loss: 0.1199 - val_acc: 0.9620\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9603\n",
      "Epoch 00012: val_loss did not improve from 0.11991\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1237 - acc: 0.9602 - val_loss: 0.1649 - val_acc: 0.9527\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9627\n",
      "Epoch 00013: val_loss did not improve from 0.11991\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1148 - acc: 0.9627 - val_loss: 0.1774 - val_acc: 0.9483\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9677\n",
      "Epoch 00014: val_loss did not improve from 0.11991\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0957 - acc: 0.9677 - val_loss: 0.1464 - val_acc: 0.9548\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9708\n",
      "Epoch 00015: val_loss did not improve from 0.11991\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0884 - acc: 0.9708 - val_loss: 0.1278 - val_acc: 0.9639\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9739\n",
      "Epoch 00016: val_loss improved from 0.11991 to 0.11820, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/016-0.1182.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0802 - acc: 0.9739 - val_loss: 0.1182 - val_acc: 0.9658\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9739\n",
      "Epoch 00017: val_loss improved from 0.11820 to 0.10633, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/017-0.1063.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0819 - acc: 0.9739 - val_loss: 0.1063 - val_acc: 0.9686\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9770\n",
      "Epoch 00018: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0704 - acc: 0.9770 - val_loss: 0.1302 - val_acc: 0.9599\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9779\n",
      "Epoch 00019: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0671 - acc: 0.9779 - val_loss: 0.1720 - val_acc: 0.9502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9718\n",
      "Epoch 00020: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0824 - acc: 0.9718 - val_loss: 0.1257 - val_acc: 0.9618\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9836\n",
      "Epoch 00021: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0507 - acc: 0.9836 - val_loss: 0.1502 - val_acc: 0.9588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9857\n",
      "Epoch 00022: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0457 - acc: 0.9857 - val_loss: 0.1099 - val_acc: 0.9686\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9828\n",
      "Epoch 00023: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0520 - acc: 0.9828 - val_loss: 0.1133 - val_acc: 0.9665\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9845\n",
      "Epoch 00024: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0485 - acc: 0.9845 - val_loss: 0.1258 - val_acc: 0.9620\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9864\n",
      "Epoch 00025: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0436 - acc: 0.9863 - val_loss: 0.1211 - val_acc: 0.9681\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9851\n",
      "Epoch 00026: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0465 - acc: 0.9851 - val_loss: 0.1091 - val_acc: 0.9662\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9870\n",
      "Epoch 00027: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0404 - acc: 0.9870 - val_loss: 0.1134 - val_acc: 0.9658\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9883\n",
      "Epoch 00028: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0359 - acc: 0.9883 - val_loss: 0.1462 - val_acc: 0.9602\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9904\n",
      "Epoch 00029: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0302 - acc: 0.9904 - val_loss: 0.1218 - val_acc: 0.9667\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9883\n",
      "Epoch 00030: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0347 - acc: 0.9883 - val_loss: 0.1490 - val_acc: 0.9653\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9904\n",
      "Epoch 00031: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0288 - acc: 0.9904 - val_loss: 0.1390 - val_acc: 0.9620\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9902\n",
      "Epoch 00032: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0316 - acc: 0.9902 - val_loss: 0.1329 - val_acc: 0.9641\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9916\n",
      "Epoch 00033: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0282 - acc: 0.9916 - val_loss: 0.1079 - val_acc: 0.9704\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9930\n",
      "Epoch 00034: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0232 - acc: 0.9930 - val_loss: 0.1502 - val_acc: 0.9581\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9918\n",
      "Epoch 00035: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0257 - acc: 0.9918 - val_loss: 0.1320 - val_acc: 0.9674\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887\n",
      "Epoch 00036: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.1764 - val_acc: 0.9553\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9878\n",
      "Epoch 00037: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0400 - acc: 0.9878 - val_loss: 0.1098 - val_acc: 0.9702\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9958\n",
      "Epoch 00038: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0155 - acc: 0.9958 - val_loss: 0.1117 - val_acc: 0.9718\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9931\n",
      "Epoch 00039: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.1309 - val_acc: 0.9695\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9931\n",
      "Epoch 00040: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0220 - acc: 0.9931 - val_loss: 0.1205 - val_acc: 0.9693\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9953\n",
      "Epoch 00041: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0166 - acc: 0.9953 - val_loss: 0.1145 - val_acc: 0.9711\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9943\n",
      "Epoch 00042: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.1795 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9928\n",
      "Epoch 00043: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0227 - acc: 0.9928 - val_loss: 0.2045 - val_acc: 0.9511\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9933\n",
      "Epoch 00044: val_loss did not improve from 0.10633\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0210 - acc: 0.9933 - val_loss: 0.1490 - val_acc: 0.9609\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9956\n",
      "Epoch 00045: val_loss improved from 0.10633 to 0.10008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv_checkpoint/045-0.1001.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0159 - acc: 0.9956 - val_loss: 0.1001 - val_acc: 0.9716\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9956\n",
      "Epoch 00046: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0150 - acc: 0.9956 - val_loss: 0.1395 - val_acc: 0.9683\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9950\n",
      "Epoch 00047: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0157 - acc: 0.9949 - val_loss: 0.1774 - val_acc: 0.9602\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9885\n",
      "Epoch 00048: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0367 - acc: 0.9885 - val_loss: 0.1416 - val_acc: 0.9630\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9907\n",
      "Epoch 00049: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0286 - acc: 0.9907 - val_loss: 0.1028 - val_acc: 0.9751\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9967\n",
      "Epoch 00050: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0123 - acc: 0.9967 - val_loss: 0.1203 - val_acc: 0.9700\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 00051: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.1353 - val_acc: 0.9672\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00052: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1155 - val_acc: 0.9725\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9961\n",
      "Epoch 00053: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0118 - acc: 0.9961 - val_loss: 0.1345 - val_acc: 0.9690\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9962\n",
      "Epoch 00054: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0124 - acc: 0.9963 - val_loss: 0.1231 - val_acc: 0.9686\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9937\n",
      "Epoch 00055: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0197 - acc: 0.9937 - val_loss: 0.1464 - val_acc: 0.9667\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9962\n",
      "Epoch 00056: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0120 - acc: 0.9962 - val_loss: 0.1393 - val_acc: 0.9669\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0089 - acc: 0.9974 - val_loss: 0.1213 - val_acc: 0.9711\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0134 - acc: 0.9961 - val_loss: 0.1394 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.2559 - val_acc: 0.9369\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9978\n",
      "Epoch 00060: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0089 - acc: 0.9978 - val_loss: 0.1128 - val_acc: 0.9695\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0098 - acc: 0.9969 - val_loss: 0.1330 - val_acc: 0.9716\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00062: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.1440 - val_acc: 0.9667\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 00063: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0147 - acc: 0.9954 - val_loss: 0.1375 - val_acc: 0.9683\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 00064: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0074 - acc: 0.9978 - val_loss: 0.1138 - val_acc: 0.9720\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9971\n",
      "Epoch 00065: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0094 - acc: 0.9971 - val_loss: 0.2071 - val_acc: 0.9597\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00066: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.1320 - val_acc: 0.9702\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9959\n",
      "Epoch 00067: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0128 - acc: 0.9959 - val_loss: 0.1554 - val_acc: 0.9604\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9958\n",
      "Epoch 00068: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0142 - acc: 0.9958 - val_loss: 0.1066 - val_acc: 0.9755\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 00069: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1519 - val_acc: 0.9674\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9975\n",
      "Epoch 00070: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 0.1145 - val_acc: 0.9723\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9969\n",
      "Epoch 00071: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.1420 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9964\n",
      "Epoch 00072: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0109 - acc: 0.9964 - val_loss: 0.1792 - val_acc: 0.9625\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9976\n",
      "Epoch 00073: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1244 - val_acc: 0.9730\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 00074: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.1251 - val_acc: 0.9695\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9966\n",
      "Epoch 00075: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0111 - acc: 0.9966 - val_loss: 0.1373 - val_acc: 0.9690\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 00076: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2049 - val_acc: 0.9532\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9965\n",
      "Epoch 00077: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0109 - acc: 0.9965 - val_loss: 0.1176 - val_acc: 0.9720\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 00078: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 0.1501 - val_acc: 0.9688\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 00079: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0083 - acc: 0.9973 - val_loss: 0.1117 - val_acc: 0.9739\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9977\n",
      "Epoch 00080: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 0.1561 - val_acc: 0.9676\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 00081: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.1617 - val_acc: 0.9620\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9981\n",
      "Epoch 00082: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0071 - acc: 0.9981 - val_loss: 0.1224 - val_acc: 0.9730\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9967\n",
      "Epoch 00083: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0108 - acc: 0.9967 - val_loss: 0.1447 - val_acc: 0.9711\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00084: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1394 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 00085: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 0.1235 - val_acc: 0.9711\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 00086: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1177 - val_acc: 0.9751\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 00087: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1335 - val_acc: 0.9725\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9982\n",
      "Epoch 00088: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0055 - acc: 0.9982 - val_loss: 0.1312 - val_acc: 0.9716\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00089: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1155 - val_acc: 0.9744\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9967\n",
      "Epoch 00090: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0101 - acc: 0.9967 - val_loss: 0.1230 - val_acc: 0.9737\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9983\n",
      "Epoch 00091: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 0.1363 - val_acc: 0.9686\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 00092: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0097 - acc: 0.9971 - val_loss: 0.1516 - val_acc: 0.9690\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9978\n",
      "Epoch 00093: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0068 - acc: 0.9978 - val_loss: 0.1226 - val_acc: 0.9716\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9980\n",
      "Epoch 00094: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1490 - val_acc: 0.9683\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9971\n",
      "Epoch 00095: val_loss did not improve from 0.10008\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0085 - acc: 0.9971 - val_loss: 0.1469 - val_acc: 0.9651\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1427 - acc: 0.9622\n",
      "Loss: 0.142686597131742 Accuracy: 0.9622015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5020 - acc: 0.8478\n",
      "Loss: 0.5019714470840565 Accuracy: 0.8477674\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3377 - acc: 0.8974\n",
      "Loss: 0.3376808849698163 Accuracy: 0.89740396\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1841 - acc: 0.9404\n",
      "Loss: 0.18406000310634404 Accuracy: 0.9403946\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1343 - acc: 0.9578\n",
      "Loss: 0.13427084166467623 Accuracy: 0.9578401\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1374 - acc: 0.9599\n",
      "Loss: 0.13736378238579938 Accuracy: 0.95991695\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1427 - acc: 0.9622\n",
      "Loss: 0.142686597131742 Accuracy: 0.9622015\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5598 - acc: 0.8260\n",
      "Loss: 0.5598432956207331 Accuracy: 0.8259605\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3513 - acc: 0.8970\n",
      "Loss: 0.3512570066863008 Accuracy: 0.8969886\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1978 - acc: 0.9385\n",
      "Loss: 0.19780471715786005 Accuracy: 0.93852544\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1840 - acc: 0.9506\n",
      "Loss: 0.18404149841749315 Accuracy: 0.9505711\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1691 - acc: 0.9591\n",
      "Loss: 0.16912954397768418 Accuracy: 0.9590862\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2031 - acc: 0.9539\n",
      "Loss: 0.20312970329407526 Accuracy: 0.9538941\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
