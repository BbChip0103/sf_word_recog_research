{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3081 - acc: 0.2699\n",
      "Epoch 00001: val_loss improved from inf to 1.96079, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_3_conv_checkpoint/001-1.9608.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 2.3081 - acc: 0.2700 - val_loss: 1.9608 - val_acc: 0.3864\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7628 - acc: 0.4603\n",
      "Epoch 00002: val_loss improved from 1.96079 to 1.71499, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_3_conv_checkpoint/002-1.7150.hdf5\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 1.7628 - acc: 0.4603 - val_loss: 1.7150 - val_acc: 0.4701\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5066 - acc: 0.5391\n",
      "Epoch 00003: val_loss improved from 1.71499 to 1.59954, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_3_conv_checkpoint/003-1.5995.hdf5\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 1.5066 - acc: 0.5391 - val_loss: 1.5995 - val_acc: 0.5027\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3348 - acc: 0.5934\n",
      "Epoch 00004: val_loss improved from 1.59954 to 1.57889, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_3_conv_checkpoint/004-1.5789.hdf5\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 1.3347 - acc: 0.5935 - val_loss: 1.5789 - val_acc: 0.4976\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2086 - acc: 0.6305\n",
      "Epoch 00005: val_loss improved from 1.57889 to 1.55227, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_3_conv_checkpoint/005-1.5523.hdf5\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 1.2086 - acc: 0.6305 - val_loss: 1.5523 - val_acc: 0.5199\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1086 - acc: 0.6626\n",
      "Epoch 00006: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 1.1085 - acc: 0.6626 - val_loss: 1.5647 - val_acc: 0.5073\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0181 - acc: 0.6886\n",
      "Epoch 00007: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 1.0181 - acc: 0.6886 - val_loss: 1.5528 - val_acc: 0.5150\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.7160\n",
      "Epoch 00008: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.9339 - acc: 0.7159 - val_loss: 1.5632 - val_acc: 0.5236\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8680 - acc: 0.7357\n",
      "Epoch 00009: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.8679 - acc: 0.7357 - val_loss: 1.6170 - val_acc: 0.5160\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7579\n",
      "Epoch 00010: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.8008 - acc: 0.7579 - val_loss: 1.6430 - val_acc: 0.5115\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7744\n",
      "Epoch 00011: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.7438 - acc: 0.7744 - val_loss: 1.6785 - val_acc: 0.5169\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.7946\n",
      "Epoch 00012: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.6794 - acc: 0.7946 - val_loss: 1.6996 - val_acc: 0.5162\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.8101\n",
      "Epoch 00013: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.6285 - acc: 0.8101 - val_loss: 1.7397 - val_acc: 0.5201\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.8257\n",
      "Epoch 00014: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.5782 - acc: 0.8257 - val_loss: 1.7771 - val_acc: 0.5236\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8375\n",
      "Epoch 00015: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.5366 - acc: 0.8375 - val_loss: 1.8427 - val_acc: 0.5120\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8546\n",
      "Epoch 00016: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4899 - acc: 0.8546 - val_loss: 1.8526 - val_acc: 0.5201\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4521 - acc: 0.8651\n",
      "Epoch 00017: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.4520 - acc: 0.8651 - val_loss: 1.9096 - val_acc: 0.5236\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8757\n",
      "Epoch 00018: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.4177 - acc: 0.8757 - val_loss: 1.9259 - val_acc: 0.5232\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8867\n",
      "Epoch 00019: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.3848 - acc: 0.8867 - val_loss: 2.0122 - val_acc: 0.5234\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8935\n",
      "Epoch 00020: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.3602 - acc: 0.8935 - val_loss: 2.0308 - val_acc: 0.5241\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.9054\n",
      "Epoch 00021: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3265 - acc: 0.9054 - val_loss: 2.0944 - val_acc: 0.5274\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9129\n",
      "Epoch 00022: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.3041 - acc: 0.9129 - val_loss: 2.1678 - val_acc: 0.5215\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9182\n",
      "Epoch 00023: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.2824 - acc: 0.9182 - val_loss: 2.2367 - val_acc: 0.5127\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9204\n",
      "Epoch 00024: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2679 - acc: 0.9204 - val_loss: 2.2798 - val_acc: 0.5122\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9270\n",
      "Epoch 00025: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2493 - acc: 0.9270 - val_loss: 2.2869 - val_acc: 0.5215\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9320\n",
      "Epoch 00026: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2334 - acc: 0.9319 - val_loss: 2.3445 - val_acc: 0.5260\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9366\n",
      "Epoch 00027: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2205 - acc: 0.9366 - val_loss: 2.3829 - val_acc: 0.5246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9406\n",
      "Epoch 00028: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.2085 - acc: 0.9406 - val_loss: 2.4315 - val_acc: 0.5241\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9441\n",
      "Epoch 00029: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.1931 - acc: 0.9441 - val_loss: 2.4184 - val_acc: 0.5292\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9474\n",
      "Epoch 00030: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.1856 - acc: 0.9474 - val_loss: 2.5564 - val_acc: 0.5176\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9516\n",
      "Epoch 00031: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.1733 - acc: 0.9516 - val_loss: 2.5059 - val_acc: 0.5306\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9547\n",
      "Epoch 00032: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.1622 - acc: 0.9547 - val_loss: 2.5626 - val_acc: 0.5295\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9547\n",
      "Epoch 00033: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.1627 - acc: 0.9547 - val_loss: 2.6238 - val_acc: 0.5269\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9583\n",
      "Epoch 00034: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.1491 - acc: 0.9583 - val_loss: 2.6502 - val_acc: 0.5304\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9595\n",
      "Epoch 00035: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.1451 - acc: 0.9595 - val_loss: 2.6540 - val_acc: 0.5304\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9626\n",
      "Epoch 00036: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.1385 - acc: 0.9626 - val_loss: 2.6764 - val_acc: 0.5304\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9640\n",
      "Epoch 00037: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.1329 - acc: 0.9641 - val_loss: 2.7212 - val_acc: 0.5267\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9655\n",
      "Epoch 00038: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.1253 - acc: 0.9655 - val_loss: 2.7522 - val_acc: 0.5313\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9659\n",
      "Epoch 00039: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.1232 - acc: 0.9659 - val_loss: 2.8511 - val_acc: 0.5243\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9670\n",
      "Epoch 00040: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.1199 - acc: 0.9670 - val_loss: 2.8444 - val_acc: 0.5290\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9686\n",
      "Epoch 00041: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.1172 - acc: 0.9686 - val_loss: 2.7754 - val_acc: 0.5313\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9686\n",
      "Epoch 00042: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.1154 - acc: 0.9686 - val_loss: 2.8501 - val_acc: 0.5274\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9737\n",
      "Epoch 00043: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.1026 - acc: 0.9737 - val_loss: 2.8924 - val_acc: 0.5292\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9710\n",
      "Epoch 00044: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.1093 - acc: 0.9710 - val_loss: 2.9059 - val_acc: 0.5332\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9752\n",
      "Epoch 00045: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.0970 - acc: 0.9752 - val_loss: 2.8777 - val_acc: 0.5313\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9755\n",
      "Epoch 00046: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.0972 - acc: 0.9755 - val_loss: 3.0141 - val_acc: 0.5262\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9743\n",
      "Epoch 00047: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 640us/sample - loss: 0.0968 - acc: 0.9743 - val_loss: 2.8863 - val_acc: 0.5392\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9758\n",
      "Epoch 00048: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.0943 - acc: 0.9758 - val_loss: 2.9583 - val_acc: 0.5439\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9752\n",
      "Epoch 00049: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.0940 - acc: 0.9752 - val_loss: 2.9875 - val_acc: 0.5348\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9779\n",
      "Epoch 00050: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.0895 - acc: 0.9779 - val_loss: 2.9897 - val_acc: 0.5351\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9770\n",
      "Epoch 00051: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.0867 - acc: 0.9770 - val_loss: 2.9557 - val_acc: 0.5516\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9798\n",
      "Epoch 00052: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.0814 - acc: 0.9798 - val_loss: 3.0532 - val_acc: 0.5451\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9772\n",
      "Epoch 00053: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.0861 - acc: 0.9772 - val_loss: 3.0365 - val_acc: 0.5462\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9782\n",
      "Epoch 00054: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.0848 - acc: 0.9782 - val_loss: 3.0459 - val_acc: 0.5446\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9812\n",
      "Epoch 00055: val_loss did not improve from 1.55227\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.0751 - acc: 0.9812 - val_loss: 3.1008 - val_acc: 0.5399\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 358us/sample - loss: 1.6197 - acc: 0.4868\n",
      "Loss: 1.6197364272358261 Accuracy: 0.48681206\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3079 - acc: 0.2492\n",
      "Epoch 00001: val_loss improved from inf to 1.80757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/001-1.8076.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 2.3078 - acc: 0.2493 - val_loss: 1.8076 - val_acc: 0.4426\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7094 - acc: 0.4546\n",
      "Epoch 00002: val_loss improved from 1.80757 to 1.57921, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/002-1.5792.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 1.7095 - acc: 0.4545 - val_loss: 1.5792 - val_acc: 0.4962\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5221 - acc: 0.5206\n",
      "Epoch 00003: val_loss improved from 1.57921 to 1.48151, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/003-1.4815.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 1.5220 - acc: 0.5206 - val_loss: 1.4815 - val_acc: 0.5334\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3939 - acc: 0.5698\n",
      "Epoch 00004: val_loss improved from 1.48151 to 1.38505, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/004-1.3851.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 1.3938 - acc: 0.5699 - val_loss: 1.3851 - val_acc: 0.5695\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3006 - acc: 0.6021\n",
      "Epoch 00005: val_loss improved from 1.38505 to 1.33657, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/005-1.3366.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 1.3005 - acc: 0.6022 - val_loss: 1.3366 - val_acc: 0.5844\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2187 - acc: 0.6305\n",
      "Epoch 00006: val_loss improved from 1.33657 to 1.30452, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/006-1.3045.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 1.2187 - acc: 0.6306 - val_loss: 1.3045 - val_acc: 0.5947\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1499 - acc: 0.6504\n",
      "Epoch 00007: val_loss improved from 1.30452 to 1.25693, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/007-1.2569.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.1498 - acc: 0.6505 - val_loss: 1.2569 - val_acc: 0.6117\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0822 - acc: 0.6722\n",
      "Epoch 00008: val_loss improved from 1.25693 to 1.22587, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/008-1.2259.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 1.0821 - acc: 0.6722 - val_loss: 1.2259 - val_acc: 0.6217\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0241 - acc: 0.6884\n",
      "Epoch 00009: val_loss did not improve from 1.22587\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 1.0240 - acc: 0.6884 - val_loss: 1.2311 - val_acc: 0.6229\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9642 - acc: 0.7083\n",
      "Epoch 00010: val_loss did not improve from 1.22587\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.9642 - acc: 0.7084 - val_loss: 1.2305 - val_acc: 0.6233\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.7209\n",
      "Epoch 00011: val_loss improved from 1.22587 to 1.18464, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_4_conv_checkpoint/011-1.1846.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9187 - acc: 0.7209 - val_loss: 1.1846 - val_acc: 0.6322\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7359\n",
      "Epoch 00012: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.8652 - acc: 0.7359 - val_loss: 1.1970 - val_acc: 0.6310\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8174 - acc: 0.7533\n",
      "Epoch 00013: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.8175 - acc: 0.7533 - val_loss: 1.1850 - val_acc: 0.6429\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7722 - acc: 0.7660\n",
      "Epoch 00014: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.7722 - acc: 0.7660 - val_loss: 1.2725 - val_acc: 0.6161\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7305 - acc: 0.7776\n",
      "Epoch 00015: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.7305 - acc: 0.7776 - val_loss: 1.1938 - val_acc: 0.6424\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6876 - acc: 0.7902\n",
      "Epoch 00016: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.6876 - acc: 0.7903 - val_loss: 1.1852 - val_acc: 0.6515\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.8017\n",
      "Epoch 00017: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.6464 - acc: 0.8017 - val_loss: 1.2003 - val_acc: 0.6508\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6147 - acc: 0.8141\n",
      "Epoch 00018: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.6147 - acc: 0.8141 - val_loss: 1.2399 - val_acc: 0.6303\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8232\n",
      "Epoch 00019: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.5805 - acc: 0.8232 - val_loss: 1.2296 - val_acc: 0.6457\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8370\n",
      "Epoch 00020: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.5421 - acc: 0.8370 - val_loss: 1.2347 - val_acc: 0.6546\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8429\n",
      "Epoch 00021: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.5139 - acc: 0.8429 - val_loss: 1.2652 - val_acc: 0.6501\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4869 - acc: 0.8491\n",
      "Epoch 00022: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.4869 - acc: 0.8491 - val_loss: 1.2336 - val_acc: 0.6580\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8595\n",
      "Epoch 00023: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.4586 - acc: 0.8595 - val_loss: 1.2555 - val_acc: 0.6608\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8659\n",
      "Epoch 00024: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.4364 - acc: 0.8659 - val_loss: 1.2828 - val_acc: 0.6622\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8709\n",
      "Epoch 00025: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.4161 - acc: 0.8709 - val_loss: 1.2900 - val_acc: 0.6636\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8818\n",
      "Epoch 00026: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.3854 - acc: 0.8819 - val_loss: 1.3258 - val_acc: 0.6539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8851\n",
      "Epoch 00027: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3681 - acc: 0.8851 - val_loss: 1.3265 - val_acc: 0.6653\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8916\n",
      "Epoch 00028: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3505 - acc: 0.8916 - val_loss: 1.4025 - val_acc: 0.6497\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8956\n",
      "Epoch 00029: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.3354 - acc: 0.8956 - val_loss: 1.3478 - val_acc: 0.6646\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9016\n",
      "Epoch 00030: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.3173 - acc: 0.9016 - val_loss: 1.3856 - val_acc: 0.6594\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9064\n",
      "Epoch 00031: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2968 - acc: 0.9064 - val_loss: 1.4228 - val_acc: 0.6639\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9116\n",
      "Epoch 00032: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2872 - acc: 0.9116 - val_loss: 1.4154 - val_acc: 0.6653\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9136\n",
      "Epoch 00033: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2747 - acc: 0.9136 - val_loss: 1.4384 - val_acc: 0.6597\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9171\n",
      "Epoch 00034: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2659 - acc: 0.9171 - val_loss: 1.4933 - val_acc: 0.6555\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9204\n",
      "Epoch 00035: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2525 - acc: 0.9204 - val_loss: 1.4651 - val_acc: 0.6713\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9241\n",
      "Epoch 00036: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2432 - acc: 0.9241 - val_loss: 1.5480 - val_acc: 0.6557\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9274\n",
      "Epoch 00037: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2329 - acc: 0.9273 - val_loss: 1.5213 - val_acc: 0.6622\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9301\n",
      "Epoch 00038: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2257 - acc: 0.9301 - val_loss: 1.5629 - val_acc: 0.6685\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9335\n",
      "Epoch 00039: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2150 - acc: 0.9335 - val_loss: 1.5667 - val_acc: 0.6629\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9356\n",
      "Epoch 00040: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2064 - acc: 0.9356 - val_loss: 1.6292 - val_acc: 0.6585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9395\n",
      "Epoch 00041: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1996 - acc: 0.9395 - val_loss: 1.5476 - val_acc: 0.6751\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9393\n",
      "Epoch 00042: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1932 - acc: 0.9393 - val_loss: 1.6051 - val_acc: 0.6655\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9404\n",
      "Epoch 00043: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1916 - acc: 0.9404 - val_loss: 1.6023 - val_acc: 0.6683\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9429\n",
      "Epoch 00044: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1841 - acc: 0.9429 - val_loss: 1.6262 - val_acc: 0.6681\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9459\n",
      "Epoch 00045: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1771 - acc: 0.9459 - val_loss: 1.6173 - val_acc: 0.6748\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9470\n",
      "Epoch 00046: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1706 - acc: 0.9470 - val_loss: 1.6449 - val_acc: 0.6739\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9480\n",
      "Epoch 00047: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1668 - acc: 0.9480 - val_loss: 1.6768 - val_acc: 0.6760\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9503\n",
      "Epoch 00048: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.1587 - acc: 0.9503 - val_loss: 1.6794 - val_acc: 0.6622\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9515\n",
      "Epoch 00049: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1560 - acc: 0.9515 - val_loss: 1.6891 - val_acc: 0.6725\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9505\n",
      "Epoch 00050: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.1563 - acc: 0.9504 - val_loss: 1.6668 - val_acc: 0.6778\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9542\n",
      "Epoch 00051: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1521 - acc: 0.9542 - val_loss: 1.6967 - val_acc: 0.6795\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9554\n",
      "Epoch 00052: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1477 - acc: 0.9554 - val_loss: 1.7166 - val_acc: 0.6730\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9554\n",
      "Epoch 00053: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1411 - acc: 0.9554 - val_loss: 1.7284 - val_acc: 0.6751\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9587\n",
      "Epoch 00054: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1378 - acc: 0.9586 - val_loss: 1.7372 - val_acc: 0.6762\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9560\n",
      "Epoch 00055: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1414 - acc: 0.9560 - val_loss: 1.7011 - val_acc: 0.6788\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9589\n",
      "Epoch 00056: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1348 - acc: 0.9588 - val_loss: 1.7367 - val_acc: 0.6730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9615\n",
      "Epoch 00057: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1277 - acc: 0.9616 - val_loss: 1.7512 - val_acc: 0.6799\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9607\n",
      "Epoch 00058: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1311 - acc: 0.9607 - val_loss: 1.7773 - val_acc: 0.6720\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9627\n",
      "Epoch 00059: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1267 - acc: 0.9627 - val_loss: 1.7541 - val_acc: 0.6762\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9625\n",
      "Epoch 00060: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1237 - acc: 0.9625 - val_loss: 1.7599 - val_acc: 0.6804\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9657\n",
      "Epoch 00061: val_loss did not improve from 1.18464\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1141 - acc: 0.9657 - val_loss: 1.8123 - val_acc: 0.6751\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 374us/sample - loss: 1.2622 - acc: 0.6037\n",
      "Loss: 1.2621989315543962 Accuracy: 0.6037383\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2638 - acc: 0.2601\n",
      "Epoch 00001: val_loss improved from inf to 1.73378, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/001-1.7338.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 2.2638 - acc: 0.2602 - val_loss: 1.7338 - val_acc: 0.4400\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6669 - acc: 0.4625\n",
      "Epoch 00002: val_loss improved from 1.73378 to 1.48264, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/002-1.4826.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 1.6668 - acc: 0.4625 - val_loss: 1.4826 - val_acc: 0.5295\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4749 - acc: 0.5367\n",
      "Epoch 00003: val_loss improved from 1.48264 to 1.35563, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/003-1.3556.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 1.4749 - acc: 0.5367 - val_loss: 1.3556 - val_acc: 0.5856\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3570 - acc: 0.5774\n",
      "Epoch 00004: val_loss improved from 1.35563 to 1.25414, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/004-1.2541.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 1.3570 - acc: 0.5774 - val_loss: 1.2541 - val_acc: 0.6252\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2596 - acc: 0.6113\n",
      "Epoch 00005: val_loss improved from 1.25414 to 1.19351, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/005-1.1935.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 1.2596 - acc: 0.6113 - val_loss: 1.1935 - val_acc: 0.6371\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1703 - acc: 0.6434\n",
      "Epoch 00006: val_loss improved from 1.19351 to 1.14492, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/006-1.1449.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.1703 - acc: 0.6434 - val_loss: 1.1449 - val_acc: 0.6513\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0956 - acc: 0.6689\n",
      "Epoch 00007: val_loss improved from 1.14492 to 1.07958, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/007-1.0796.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 1.0955 - acc: 0.6689 - val_loss: 1.0796 - val_acc: 0.6713\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0395 - acc: 0.6837\n",
      "Epoch 00008: val_loss improved from 1.07958 to 1.02805, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/008-1.0281.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 1.0396 - acc: 0.6838 - val_loss: 1.0281 - val_acc: 0.6939\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9788 - acc: 0.7059\n",
      "Epoch 00009: val_loss improved from 1.02805 to 0.99321, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/009-0.9932.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.9792 - acc: 0.7059 - val_loss: 0.9932 - val_acc: 0.6995\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7189\n",
      "Epoch 00010: val_loss did not improve from 0.99321\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.9309 - acc: 0.7189 - val_loss: 1.0085 - val_acc: 0.6972\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.7353\n",
      "Epoch 00011: val_loss improved from 0.99321 to 0.95592, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/011-0.9559.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.8831 - acc: 0.7353 - val_loss: 0.9559 - val_acc: 0.7023\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8388 - acc: 0.7484\n",
      "Epoch 00012: val_loss improved from 0.95592 to 0.94387, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/012-0.9439.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8386 - acc: 0.7483 - val_loss: 0.9439 - val_acc: 0.7207\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7963 - acc: 0.7612\n",
      "Epoch 00013: val_loss improved from 0.94387 to 0.92032, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/013-0.9203.hdf5\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.7963 - acc: 0.7612 - val_loss: 0.9203 - val_acc: 0.7242\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7720\n",
      "Epoch 00014: val_loss did not improve from 0.92032\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.7571 - acc: 0.7722 - val_loss: 0.9322 - val_acc: 0.7242\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.7860\n",
      "Epoch 00015: val_loss improved from 0.92032 to 0.90959, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/015-0.9096.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.7201 - acc: 0.7860 - val_loss: 0.9096 - val_acc: 0.7163\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.7951\n",
      "Epoch 00016: val_loss improved from 0.90959 to 0.87443, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/016-0.8744.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.6861 - acc: 0.7951 - val_loss: 0.8744 - val_acc: 0.7345\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.8046\n",
      "Epoch 00017: val_loss did not improve from 0.87443\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.6527 - acc: 0.8045 - val_loss: 0.8884 - val_acc: 0.7326\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6177 - acc: 0.8150\n",
      "Epoch 00018: val_loss improved from 0.87443 to 0.86960, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/018-0.8696.hdf5\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.6174 - acc: 0.8151 - val_loss: 0.8696 - val_acc: 0.7428\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8241\n",
      "Epoch 00019: val_loss did not improve from 0.86960\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.5890 - acc: 0.8241 - val_loss: 0.8722 - val_acc: 0.7393\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8340\n",
      "Epoch 00020: val_loss did not improve from 0.86960\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5559 - acc: 0.8339 - val_loss: 0.9243 - val_acc: 0.7319\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8374\n",
      "Epoch 00021: val_loss improved from 0.86960 to 0.85011, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/021-0.8501.hdf5\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.5355 - acc: 0.8374 - val_loss: 0.8501 - val_acc: 0.7543\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8462\n",
      "Epoch 00022: val_loss improved from 0.85011 to 0.84781, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_5_conv_checkpoint/022-0.8478.hdf5\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.5058 - acc: 0.8462 - val_loss: 0.8478 - val_acc: 0.7552\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8553\n",
      "Epoch 00023: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.4742 - acc: 0.8553 - val_loss: 0.8838 - val_acc: 0.7414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8616\n",
      "Epoch 00024: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.4556 - acc: 0.8616 - val_loss: 0.8642 - val_acc: 0.7468\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8670\n",
      "Epoch 00025: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.4348 - acc: 0.8670 - val_loss: 0.9588 - val_acc: 0.7372\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8746\n",
      "Epoch 00026: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.4123 - acc: 0.8745 - val_loss: 0.9180 - val_acc: 0.7484\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8774\n",
      "Epoch 00027: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.3971 - acc: 0.8774 - val_loss: 0.9011 - val_acc: 0.7517\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8866\n",
      "Epoch 00028: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.3729 - acc: 0.8865 - val_loss: 0.8813 - val_acc: 0.7573\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8900\n",
      "Epoch 00029: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.3557 - acc: 0.8900 - val_loss: 0.9038 - val_acc: 0.7540\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8951\n",
      "Epoch 00030: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3369 - acc: 0.8951 - val_loss: 0.9159 - val_acc: 0.7563\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9001\n",
      "Epoch 00031: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3243 - acc: 0.9000 - val_loss: 0.9452 - val_acc: 0.7484\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9054\n",
      "Epoch 00032: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3068 - acc: 0.9054 - val_loss: 0.9858 - val_acc: 0.7410\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9098\n",
      "Epoch 00033: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2934 - acc: 0.9097 - val_loss: 1.0591 - val_acc: 0.7312\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9125\n",
      "Epoch 00034: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2799 - acc: 0.9125 - val_loss: 0.9598 - val_acc: 0.7496\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9158\n",
      "Epoch 00035: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2707 - acc: 0.9158 - val_loss: 0.9606 - val_acc: 0.7577\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9211\n",
      "Epoch 00036: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2537 - acc: 0.9210 - val_loss: 1.0447 - val_acc: 0.7480\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9230\n",
      "Epoch 00037: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.2492 - acc: 0.9229 - val_loss: 1.0396 - val_acc: 0.7440\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9269\n",
      "Epoch 00038: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2334 - acc: 0.9269 - val_loss: 1.0111 - val_acc: 0.7531\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9293\n",
      "Epoch 00039: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.2250 - acc: 0.9293 - val_loss: 1.0007 - val_acc: 0.7575\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9319\n",
      "Epoch 00040: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2166 - acc: 0.9318 - val_loss: 1.0595 - val_acc: 0.7477\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9358\n",
      "Epoch 00041: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2070 - acc: 0.9359 - val_loss: 1.0255 - val_acc: 0.7543\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9374\n",
      "Epoch 00042: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2005 - acc: 0.9374 - val_loss: 1.0530 - val_acc: 0.7531\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9378\n",
      "Epoch 00043: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1952 - acc: 0.9378 - val_loss: 1.0445 - val_acc: 0.7619\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9400\n",
      "Epoch 00044: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1900 - acc: 0.9400 - val_loss: 1.0702 - val_acc: 0.7529\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9441\n",
      "Epoch 00045: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1798 - acc: 0.9441 - val_loss: 1.0829 - val_acc: 0.7573\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9456\n",
      "Epoch 00046: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1728 - acc: 0.9456 - val_loss: 1.0783 - val_acc: 0.7615\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9462\n",
      "Epoch 00047: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1699 - acc: 0.9460 - val_loss: 1.1121 - val_acc: 0.7591\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9486\n",
      "Epoch 00048: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1647 - acc: 0.9486 - val_loss: 1.1270 - val_acc: 0.7470\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9493\n",
      "Epoch 00049: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1603 - acc: 0.9494 - val_loss: 1.1252 - val_acc: 0.7577\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9522\n",
      "Epoch 00050: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1519 - acc: 0.9523 - val_loss: 1.1106 - val_acc: 0.7615\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9533\n",
      "Epoch 00051: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1526 - acc: 0.9533 - val_loss: 1.1536 - val_acc: 0.7577\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9544\n",
      "Epoch 00052: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1472 - acc: 0.9544 - val_loss: 1.1576 - val_acc: 0.7573\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9543\n",
      "Epoch 00053: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1443 - acc: 0.9543 - val_loss: 1.1757 - val_acc: 0.7615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9568\n",
      "Epoch 00054: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1388 - acc: 0.9568 - val_loss: 1.1573 - val_acc: 0.7598\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9545\n",
      "Epoch 00055: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1426 - acc: 0.9545 - val_loss: 1.1396 - val_acc: 0.7589\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9581\n",
      "Epoch 00056: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1355 - acc: 0.9580 - val_loss: 1.1791 - val_acc: 0.7575\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9598\n",
      "Epoch 00057: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1304 - acc: 0.9598 - val_loss: 1.1831 - val_acc: 0.7594\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9596\n",
      "Epoch 00058: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1287 - acc: 0.9597 - val_loss: 1.1823 - val_acc: 0.7582\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9610\n",
      "Epoch 00059: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1246 - acc: 0.9610 - val_loss: 1.1828 - val_acc: 0.7675\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9624\n",
      "Epoch 00060: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1201 - acc: 0.9624 - val_loss: 1.1973 - val_acc: 0.7615\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9649\n",
      "Epoch 00061: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1171 - acc: 0.9649 - val_loss: 1.1742 - val_acc: 0.7680\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9637\n",
      "Epoch 00062: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1168 - acc: 0.9637 - val_loss: 1.2839 - val_acc: 0.7559\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9647\n",
      "Epoch 00063: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1149 - acc: 0.9647 - val_loss: 1.2290 - val_acc: 0.7617\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9653\n",
      "Epoch 00064: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1140 - acc: 0.9653 - val_loss: 1.2454 - val_acc: 0.7575\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9652\n",
      "Epoch 00065: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1115 - acc: 0.9652 - val_loss: 1.2273 - val_acc: 0.7640\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9661\n",
      "Epoch 00066: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1088 - acc: 0.9661 - val_loss: 1.2022 - val_acc: 0.7696\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9656\n",
      "Epoch 00067: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1097 - acc: 0.9656 - val_loss: 1.2337 - val_acc: 0.7631\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9677\n",
      "Epoch 00068: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1029 - acc: 0.9677 - val_loss: 1.2856 - val_acc: 0.7647\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1112 - acc: 0.9650 - val_loss: 1.2212 - val_acc: 0.7701\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9674\n",
      "Epoch 00070: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1047 - acc: 0.9674 - val_loss: 1.3328 - val_acc: 0.7605\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9702\n",
      "Epoch 00071: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.1009 - acc: 0.9701 - val_loss: 1.2698 - val_acc: 0.7633\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9691\n",
      "Epoch 00072: val_loss did not improve from 0.84781\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1024 - acc: 0.9691 - val_loss: 1.2929 - val_acc: 0.7596\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 390us/sample - loss: 0.9731 - acc: 0.7128\n",
      "Loss: 0.9731363266056572 Accuracy: 0.7127726\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3417 - acc: 0.2350\n",
      "Epoch 00001: val_loss improved from inf to 1.79170, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/001-1.7917.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 2.3407 - acc: 0.2353 - val_loss: 1.7917 - val_acc: 0.4414\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6884 - acc: 0.4565\n",
      "Epoch 00002: val_loss improved from 1.79170 to 1.47713, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/002-1.4771.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.6883 - acc: 0.4565 - val_loss: 1.4771 - val_acc: 0.5416\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5037 - acc: 0.5177\n",
      "Epoch 00003: val_loss improved from 1.47713 to 1.35061, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/003-1.3506.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.5037 - acc: 0.5177 - val_loss: 1.3506 - val_acc: 0.5809\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3975 - acc: 0.5553\n",
      "Epoch 00004: val_loss improved from 1.35061 to 1.27830, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/004-1.2783.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 1.3976 - acc: 0.5553 - val_loss: 1.2783 - val_acc: 0.5993\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3064 - acc: 0.5894\n",
      "Epoch 00005: val_loss improved from 1.27830 to 1.19542, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/005-1.1954.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 1.3064 - acc: 0.5894 - val_loss: 1.1954 - val_acc: 0.6327\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2286 - acc: 0.6163\n",
      "Epoch 00006: val_loss improved from 1.19542 to 1.11933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/006-1.1193.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 1.2286 - acc: 0.6163 - val_loss: 1.1193 - val_acc: 0.6562\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1453 - acc: 0.6469\n",
      "Epoch 00007: val_loss improved from 1.11933 to 1.06488, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/007-1.0649.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.1455 - acc: 0.6468 - val_loss: 1.0649 - val_acc: 0.6867\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0704 - acc: 0.6726\n",
      "Epoch 00008: val_loss improved from 1.06488 to 0.96757, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/008-0.9676.hdf5\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 1.0703 - acc: 0.6727 - val_loss: 0.9676 - val_acc: 0.7058\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.6935\n",
      "Epoch 00009: val_loss improved from 0.96757 to 0.92185, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/009-0.9218.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 1.0043 - acc: 0.6935 - val_loss: 0.9218 - val_acc: 0.7331\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9371 - acc: 0.7155\n",
      "Epoch 00010: val_loss improved from 0.92185 to 0.88202, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/010-0.8820.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.9370 - acc: 0.7155 - val_loss: 0.8820 - val_acc: 0.7435\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.7314\n",
      "Epoch 00011: val_loss improved from 0.88202 to 0.83224, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/011-0.8322.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.8894 - acc: 0.7314 - val_loss: 0.8322 - val_acc: 0.7466\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7452\n",
      "Epoch 00012: val_loss improved from 0.83224 to 0.77516, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/012-0.7752.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.8405 - acc: 0.7452 - val_loss: 0.7752 - val_acc: 0.7654\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7922 - acc: 0.7621\n",
      "Epoch 00013: val_loss did not improve from 0.77516\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.7919 - acc: 0.7621 - val_loss: 0.7885 - val_acc: 0.7594\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7767\n",
      "Epoch 00014: val_loss improved from 0.77516 to 0.70772, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/014-0.7077.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.7500 - acc: 0.7768 - val_loss: 0.7077 - val_acc: 0.7959\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7169 - acc: 0.7845\n",
      "Epoch 00015: val_loss improved from 0.70772 to 0.69480, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/015-0.6948.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.7168 - acc: 0.7845 - val_loss: 0.6948 - val_acc: 0.7945\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.7938\n",
      "Epoch 00016: val_loss improved from 0.69480 to 0.64667, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/016-0.6467.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.6844 - acc: 0.7938 - val_loss: 0.6467 - val_acc: 0.8083\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6454 - acc: 0.8080\n",
      "Epoch 00017: val_loss improved from 0.64667 to 0.64092, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/017-0.6409.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.6455 - acc: 0.8080 - val_loss: 0.6409 - val_acc: 0.8111\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6202 - acc: 0.8148\n",
      "Epoch 00018: val_loss improved from 0.64092 to 0.63028, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/018-0.6303.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.6203 - acc: 0.8148 - val_loss: 0.6303 - val_acc: 0.8190\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.8230\n",
      "Epoch 00019: val_loss improved from 0.63028 to 0.59544, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/019-0.5954.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.5912 - acc: 0.8230 - val_loss: 0.5954 - val_acc: 0.8309\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8291\n",
      "Epoch 00020: val_loss improved from 0.59544 to 0.57417, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/020-0.5742.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.5664 - acc: 0.8291 - val_loss: 0.5742 - val_acc: 0.8330\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8362\n",
      "Epoch 00021: val_loss did not improve from 0.57417\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.5447 - acc: 0.8362 - val_loss: 0.6284 - val_acc: 0.8192\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8432\n",
      "Epoch 00022: val_loss improved from 0.57417 to 0.55860, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/022-0.5586.hdf5\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.5230 - acc: 0.8431 - val_loss: 0.5586 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8463\n",
      "Epoch 00023: val_loss improved from 0.55860 to 0.53373, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/023-0.5337.hdf5\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.5085 - acc: 0.8464 - val_loss: 0.5337 - val_acc: 0.8463\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.8494\n",
      "Epoch 00024: val_loss did not improve from 0.53373\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.4930 - acc: 0.8494 - val_loss: 0.5414 - val_acc: 0.8416\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.8585\n",
      "Epoch 00025: val_loss improved from 0.53373 to 0.52424, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/025-0.5242.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.4741 - acc: 0.8585 - val_loss: 0.5242 - val_acc: 0.8500\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8632\n",
      "Epoch 00026: val_loss did not improve from 0.52424\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.4598 - acc: 0.8632 - val_loss: 0.5321 - val_acc: 0.8442\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8660\n",
      "Epoch 00027: val_loss improved from 0.52424 to 0.51692, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/027-0.5169.hdf5\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.4417 - acc: 0.8660 - val_loss: 0.5169 - val_acc: 0.8567\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8704\n",
      "Epoch 00028: val_loss improved from 0.51692 to 0.49262, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/028-0.4926.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.4266 - acc: 0.8703 - val_loss: 0.4926 - val_acc: 0.8630\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8723\n",
      "Epoch 00029: val_loss did not improve from 0.49262\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.4190 - acc: 0.8723 - val_loss: 0.5072 - val_acc: 0.8546\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8750\n",
      "Epoch 00030: val_loss did not improve from 0.49262\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.4116 - acc: 0.8750 - val_loss: 0.4941 - val_acc: 0.8647\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8814\n",
      "Epoch 00031: val_loss improved from 0.49262 to 0.48288, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/031-0.4829.hdf5\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.3917 - acc: 0.8813 - val_loss: 0.4829 - val_acc: 0.8710\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8827\n",
      "Epoch 00032: val_loss improved from 0.48288 to 0.47132, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/032-0.4713.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.3820 - acc: 0.8827 - val_loss: 0.4713 - val_acc: 0.8740\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8883\n",
      "Epoch 00033: val_loss did not improve from 0.47132\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.3677 - acc: 0.8883 - val_loss: 0.4846 - val_acc: 0.8696\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8891\n",
      "Epoch 00034: val_loss improved from 0.47132 to 0.47090, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/034-0.4709.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3580 - acc: 0.8891 - val_loss: 0.4709 - val_acc: 0.8710\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8937\n",
      "Epoch 00035: val_loss improved from 0.47090 to 0.46737, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/035-0.4674.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.3486 - acc: 0.8937 - val_loss: 0.4674 - val_acc: 0.8719\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8960\n",
      "Epoch 00036: val_loss did not improve from 0.46737\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.3382 - acc: 0.8960 - val_loss: 0.4678 - val_acc: 0.8724\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8965\n",
      "Epoch 00037: val_loss improved from 0.46737 to 0.46268, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/037-0.4627.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.3333 - acc: 0.8965 - val_loss: 0.4627 - val_acc: 0.8751\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8985\n",
      "Epoch 00038: val_loss improved from 0.46268 to 0.45025, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/038-0.4502.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.3278 - acc: 0.8985 - val_loss: 0.4502 - val_acc: 0.8784\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9036\n",
      "Epoch 00039: val_loss improved from 0.45025 to 0.44965, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/039-0.4496.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.3146 - acc: 0.9037 - val_loss: 0.4496 - val_acc: 0.8761\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9045\n",
      "Epoch 00040: val_loss did not improve from 0.44965\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.3103 - acc: 0.9044 - val_loss: 0.4859 - val_acc: 0.8703\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9050\n",
      "Epoch 00041: val_loss did not improve from 0.44965\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3029 - acc: 0.9050 - val_loss: 0.4547 - val_acc: 0.8751\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9102\n",
      "Epoch 00042: val_loss improved from 0.44965 to 0.44437, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/042-0.4444.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2899 - acc: 0.9103 - val_loss: 0.4444 - val_acc: 0.8831\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9104\n",
      "Epoch 00043: val_loss did not improve from 0.44437\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2843 - acc: 0.9104 - val_loss: 0.4602 - val_acc: 0.8775\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9137\n",
      "Epoch 00044: val_loss did not improve from 0.44437\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.2811 - acc: 0.9137 - val_loss: 0.4583 - val_acc: 0.8751\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9148\n",
      "Epoch 00045: val_loss did not improve from 0.44437\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.2754 - acc: 0.9148 - val_loss: 0.4556 - val_acc: 0.8828\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9161\n",
      "Epoch 00046: val_loss improved from 0.44437 to 0.44135, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/046-0.4414.hdf5\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.2657 - acc: 0.9162 - val_loss: 0.4414 - val_acc: 0.8831\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9168\n",
      "Epoch 00047: val_loss improved from 0.44135 to 0.44084, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/047-0.4408.hdf5\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2607 - acc: 0.9168 - val_loss: 0.4408 - val_acc: 0.8847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9183\n",
      "Epoch 00048: val_loss did not improve from 0.44084\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2592 - acc: 0.9183 - val_loss: 0.4668 - val_acc: 0.8814\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9204\n",
      "Epoch 00049: val_loss did not improve from 0.44084\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2574 - acc: 0.9204 - val_loss: 0.4466 - val_acc: 0.8854\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9246\n",
      "Epoch 00050: val_loss did not improve from 0.44084\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2433 - acc: 0.9246 - val_loss: 0.4443 - val_acc: 0.8919\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9258\n",
      "Epoch 00051: val_loss improved from 0.44084 to 0.43660, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_6_conv_checkpoint/051-0.4366.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2373 - acc: 0.9258 - val_loss: 0.4366 - val_acc: 0.8889\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9257\n",
      "Epoch 00052: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2332 - acc: 0.9257 - val_loss: 0.4510 - val_acc: 0.8845\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9273\n",
      "Epoch 00053: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2299 - acc: 0.9273 - val_loss: 0.4435 - val_acc: 0.8861\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9284\n",
      "Epoch 00054: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2253 - acc: 0.9284 - val_loss: 0.4866 - val_acc: 0.8866\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9317\n",
      "Epoch 00055: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.2169 - acc: 0.9316 - val_loss: 0.4423 - val_acc: 0.8875\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9321\n",
      "Epoch 00056: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.2145 - acc: 0.9322 - val_loss: 0.4505 - val_acc: 0.8887\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9317\n",
      "Epoch 00057: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2140 - acc: 0.9317 - val_loss: 0.4767 - val_acc: 0.8861\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9324\n",
      "Epoch 00058: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2118 - acc: 0.9325 - val_loss: 0.4460 - val_acc: 0.8903\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9372\n",
      "Epoch 00059: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1989 - acc: 0.9372 - val_loss: 0.5020 - val_acc: 0.8777\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9342\n",
      "Epoch 00060: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2030 - acc: 0.9342 - val_loss: 0.4608 - val_acc: 0.8896\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9390\n",
      "Epoch 00061: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1920 - acc: 0.9390 - val_loss: 0.4833 - val_acc: 0.8849\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9391\n",
      "Epoch 00062: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1908 - acc: 0.9391 - val_loss: 0.4558 - val_acc: 0.8935\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9390\n",
      "Epoch 00063: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1880 - acc: 0.9390 - val_loss: 0.4612 - val_acc: 0.8917\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9389\n",
      "Epoch 00064: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1891 - acc: 0.9389 - val_loss: 0.4671 - val_acc: 0.8884\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9409\n",
      "Epoch 00065: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1852 - acc: 0.9409 - val_loss: 0.4959 - val_acc: 0.8856\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9399\n",
      "Epoch 00066: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1834 - acc: 0.9399 - val_loss: 0.4850 - val_acc: 0.8833\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9427\n",
      "Epoch 00067: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1759 - acc: 0.9427 - val_loss: 0.4583 - val_acc: 0.8984\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9450\n",
      "Epoch 00068: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1705 - acc: 0.9450 - val_loss: 0.4846 - val_acc: 0.8896\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9461\n",
      "Epoch 00069: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1658 - acc: 0.9461 - val_loss: 0.4717 - val_acc: 0.8933\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9451\n",
      "Epoch 00070: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1717 - acc: 0.9450 - val_loss: 0.4883 - val_acc: 0.8940\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9472\n",
      "Epoch 00071: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1626 - acc: 0.9472 - val_loss: 0.4984 - val_acc: 0.8903\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9478\n",
      "Epoch 00072: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1641 - acc: 0.9478 - val_loss: 0.4706 - val_acc: 0.8926\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9488\n",
      "Epoch 00073: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1578 - acc: 0.9488 - val_loss: 0.4970 - val_acc: 0.8947\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9486\n",
      "Epoch 00074: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1533 - acc: 0.9486 - val_loss: 0.4844 - val_acc: 0.8919\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9482\n",
      "Epoch 00075: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1588 - acc: 0.9482 - val_loss: 0.4859 - val_acc: 0.8917\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9507\n",
      "Epoch 00076: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1502 - acc: 0.9507 - val_loss: 0.4924 - val_acc: 0.8852\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9501\n",
      "Epoch 00077: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1520 - acc: 0.9501 - val_loss: 0.4799 - val_acc: 0.8952\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9543\n",
      "Epoch 00078: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1424 - acc: 0.9544 - val_loss: 0.4821 - val_acc: 0.8949\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9533\n",
      "Epoch 00079: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1444 - acc: 0.9533 - val_loss: 0.5147 - val_acc: 0.8877\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9518\n",
      "Epoch 00080: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1446 - acc: 0.9518 - val_loss: 0.4804 - val_acc: 0.8956\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9555\n",
      "Epoch 00081: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1401 - acc: 0.9555 - val_loss: 0.4732 - val_acc: 0.8959\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9554\n",
      "Epoch 00082: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1378 - acc: 0.9554 - val_loss: 0.4922 - val_acc: 0.8942\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9565\n",
      "Epoch 00083: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1328 - acc: 0.9565 - val_loss: 0.4998 - val_acc: 0.8926\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9566\n",
      "Epoch 00084: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1325 - acc: 0.9566 - val_loss: 0.4930 - val_acc: 0.8959\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9570\n",
      "Epoch 00085: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1347 - acc: 0.9570 - val_loss: 0.4930 - val_acc: 0.8975\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9563\n",
      "Epoch 00086: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1292 - acc: 0.9563 - val_loss: 0.5055 - val_acc: 0.8912\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9564\n",
      "Epoch 00087: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1315 - acc: 0.9564 - val_loss: 0.5015 - val_acc: 0.8956\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9586\n",
      "Epoch 00088: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1259 - acc: 0.9586 - val_loss: 0.5067 - val_acc: 0.8973\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9599\n",
      "Epoch 00089: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1243 - acc: 0.9599 - val_loss: 0.5069 - val_acc: 0.8952\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9607\n",
      "Epoch 00090: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1244 - acc: 0.9607 - val_loss: 0.4929 - val_acc: 0.8987\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9617\n",
      "Epoch 00091: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1203 - acc: 0.9617 - val_loss: 0.5019 - val_acc: 0.8959\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9622\n",
      "Epoch 00092: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1176 - acc: 0.9622 - val_loss: 0.5326 - val_acc: 0.8912\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9596\n",
      "Epoch 00093: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1199 - acc: 0.9596 - val_loss: 0.5188 - val_acc: 0.8919\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9584\n",
      "Epoch 00094: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1252 - acc: 0.9584 - val_loss: 0.5075 - val_acc: 0.8956\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9635\n",
      "Epoch 00095: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1121 - acc: 0.9635 - val_loss: 0.5097 - val_acc: 0.8912\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9613\n",
      "Epoch 00096: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1179 - acc: 0.9612 - val_loss: 0.5025 - val_acc: 0.8973\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9650\n",
      "Epoch 00097: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1101 - acc: 0.9650 - val_loss: 0.5043 - val_acc: 0.8959\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9649\n",
      "Epoch 00098: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1090 - acc: 0.9649 - val_loss: 0.5048 - val_acc: 0.9010\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9660\n",
      "Epoch 00099: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1049 - acc: 0.9660 - val_loss: 0.4992 - val_acc: 0.9015\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9648\n",
      "Epoch 00100: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1073 - acc: 0.9648 - val_loss: 0.4978 - val_acc: 0.8945\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9647\n",
      "Epoch 00101: val_loss did not improve from 0.43660\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1082 - acc: 0.9647 - val_loss: 0.5276 - val_acc: 0.8901\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 432us/sample - loss: 0.5331 - acc: 0.8534\n",
      "Loss: 0.5330665096563219 Accuracy: 0.8533749\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4974 - acc: 0.1771\n",
      "Epoch 00001: val_loss improved from inf to 1.96736, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/001-1.9674.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 2.4973 - acc: 0.1771 - val_loss: 1.9674 - val_acc: 0.3778\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8140 - acc: 0.4070\n",
      "Epoch 00002: val_loss improved from 1.96736 to 1.50831, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/002-1.5083.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.8136 - acc: 0.4071 - val_loss: 1.5083 - val_acc: 0.5376\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5184 - acc: 0.5088\n",
      "Epoch 00003: val_loss improved from 1.50831 to 1.26655, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/003-1.2665.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.5181 - acc: 0.5088 - val_loss: 1.2665 - val_acc: 0.6226\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3338 - acc: 0.5714\n",
      "Epoch 00004: val_loss improved from 1.26655 to 1.11816, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/004-1.1182.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.3337 - acc: 0.5714 - val_loss: 1.1182 - val_acc: 0.6704\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1917 - acc: 0.6240\n",
      "Epoch 00005: val_loss improved from 1.11816 to 1.00632, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/005-1.0063.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.1918 - acc: 0.6240 - val_loss: 1.0063 - val_acc: 0.6944\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0745 - acc: 0.6619\n",
      "Epoch 00006: val_loss improved from 1.00632 to 0.87638, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/006-0.8764.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0745 - acc: 0.6619 - val_loss: 0.8764 - val_acc: 0.7400\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.6941\n",
      "Epoch 00007: val_loss improved from 0.87638 to 0.80232, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/007-0.8023.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.9765 - acc: 0.6941 - val_loss: 0.8023 - val_acc: 0.7785\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8977 - acc: 0.7220\n",
      "Epoch 00008: val_loss improved from 0.80232 to 0.75750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/008-0.7575.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.8980 - acc: 0.7218 - val_loss: 0.7575 - val_acc: 0.7843\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7423\n",
      "Epoch 00009: val_loss improved from 0.75750 to 0.68616, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/009-0.6862.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.8312 - acc: 0.7424 - val_loss: 0.6862 - val_acc: 0.8036\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7811 - acc: 0.7571\n",
      "Epoch 00010: val_loss improved from 0.68616 to 0.62710, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/010-0.6271.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.7807 - acc: 0.7572 - val_loss: 0.6271 - val_acc: 0.8253\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7308 - acc: 0.7719\n",
      "Epoch 00011: val_loss improved from 0.62710 to 0.61548, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/011-0.6155.hdf5\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.7308 - acc: 0.7719 - val_loss: 0.6155 - val_acc: 0.8260\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6829 - acc: 0.7906\n",
      "Epoch 00012: val_loss improved from 0.61548 to 0.56421, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/012-0.5642.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.6830 - acc: 0.7906 - val_loss: 0.5642 - val_acc: 0.8467\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.8023\n",
      "Epoch 00013: val_loss improved from 0.56421 to 0.54841, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/013-0.5484.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6477 - acc: 0.8023 - val_loss: 0.5484 - val_acc: 0.8416\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8143\n",
      "Epoch 00014: val_loss improved from 0.54841 to 0.52454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/014-0.5245.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.6104 - acc: 0.8143 - val_loss: 0.5245 - val_acc: 0.8579\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8219\n",
      "Epoch 00015: val_loss improved from 0.52454 to 0.50289, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/015-0.5029.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5847 - acc: 0.8219 - val_loss: 0.5029 - val_acc: 0.8612\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.8313\n",
      "Epoch 00016: val_loss improved from 0.50289 to 0.46121, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/016-0.4612.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5497 - acc: 0.8313 - val_loss: 0.4612 - val_acc: 0.8747\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.8385\n",
      "Epoch 00017: val_loss did not improve from 0.46121\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.5235 - acc: 0.8385 - val_loss: 0.4700 - val_acc: 0.8682\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8448\n",
      "Epoch 00018: val_loss improved from 0.46121 to 0.43206, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/018-0.4321.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4997 - acc: 0.8447 - val_loss: 0.4321 - val_acc: 0.8847\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8529\n",
      "Epoch 00019: val_loss did not improve from 0.43206\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.4785 - acc: 0.8529 - val_loss: 0.4423 - val_acc: 0.8765\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8547\n",
      "Epoch 00020: val_loss improved from 0.43206 to 0.41033, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/020-0.4103.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4675 - acc: 0.8547 - val_loss: 0.4103 - val_acc: 0.8863\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8628\n",
      "Epoch 00021: val_loss improved from 0.41033 to 0.38468, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/021-0.3847.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4411 - acc: 0.8627 - val_loss: 0.3847 - val_acc: 0.8921\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8700\n",
      "Epoch 00022: val_loss improved from 0.38468 to 0.38271, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/022-0.3827.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4211 - acc: 0.8700 - val_loss: 0.3827 - val_acc: 0.8940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8739\n",
      "Epoch 00023: val_loss improved from 0.38271 to 0.35739, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/023-0.3574.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4118 - acc: 0.8738 - val_loss: 0.3574 - val_acc: 0.8977\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8760\n",
      "Epoch 00024: val_loss improved from 0.35739 to 0.35142, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/024-0.3514.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4003 - acc: 0.8760 - val_loss: 0.3514 - val_acc: 0.9026\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8797\n",
      "Epoch 00025: val_loss did not improve from 0.35142\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.3860 - acc: 0.8798 - val_loss: 0.3630 - val_acc: 0.8968\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8839\n",
      "Epoch 00026: val_loss improved from 0.35142 to 0.33759, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/026-0.3376.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3726 - acc: 0.8840 - val_loss: 0.3376 - val_acc: 0.9005\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8885\n",
      "Epoch 00027: val_loss did not improve from 0.33759\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3605 - acc: 0.8885 - val_loss: 0.3668 - val_acc: 0.8956\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8917\n",
      "Epoch 00028: val_loss did not improve from 0.33759\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.3511 - acc: 0.8916 - val_loss: 0.3471 - val_acc: 0.9003\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8927\n",
      "Epoch 00029: val_loss improved from 0.33759 to 0.33226, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/029-0.3323.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3434 - acc: 0.8927 - val_loss: 0.3323 - val_acc: 0.9015\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8957\n",
      "Epoch 00030: val_loss improved from 0.33226 to 0.31344, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/030-0.3134.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3330 - acc: 0.8957 - val_loss: 0.3134 - val_acc: 0.9061\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8983\n",
      "Epoch 00031: val_loss improved from 0.31344 to 0.29321, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/031-0.2932.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3232 - acc: 0.8983 - val_loss: 0.2932 - val_acc: 0.9150\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9001\n",
      "Epoch 00032: val_loss did not improve from 0.29321\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3162 - acc: 0.9001 - val_loss: 0.3074 - val_acc: 0.9133\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9049\n",
      "Epoch 00033: val_loss did not improve from 0.29321\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3050 - acc: 0.9050 - val_loss: 0.3057 - val_acc: 0.9143\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9054\n",
      "Epoch 00034: val_loss did not improve from 0.29321\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3009 - acc: 0.9054 - val_loss: 0.2944 - val_acc: 0.9129\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9081\n",
      "Epoch 00035: val_loss did not improve from 0.29321\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2911 - acc: 0.9081 - val_loss: 0.2932 - val_acc: 0.9178\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9094\n",
      "Epoch 00036: val_loss improved from 0.29321 to 0.28560, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/036-0.2856.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2880 - acc: 0.9094 - val_loss: 0.2856 - val_acc: 0.9206\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9129\n",
      "Epoch 00037: val_loss improved from 0.28560 to 0.27740, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/037-0.2774.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2759 - acc: 0.9129 - val_loss: 0.2774 - val_acc: 0.9208\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9118\n",
      "Epoch 00038: val_loss did not improve from 0.27740\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2728 - acc: 0.9118 - val_loss: 0.2954 - val_acc: 0.9157\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9138\n",
      "Epoch 00039: val_loss did not improve from 0.27740\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2700 - acc: 0.9138 - val_loss: 0.2830 - val_acc: 0.9194\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9163\n",
      "Epoch 00040: val_loss did not improve from 0.27740\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2637 - acc: 0.9163 - val_loss: 0.2792 - val_acc: 0.9220\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9165\n",
      "Epoch 00041: val_loss improved from 0.27740 to 0.27243, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/041-0.2724.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2599 - acc: 0.9164 - val_loss: 0.2724 - val_acc: 0.9245\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9186\n",
      "Epoch 00042: val_loss did not improve from 0.27243\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2558 - acc: 0.9186 - val_loss: 0.2797 - val_acc: 0.9222\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9214\n",
      "Epoch 00043: val_loss did not improve from 0.27243\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2481 - acc: 0.9214 - val_loss: 0.2769 - val_acc: 0.9196\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9247\n",
      "Epoch 00044: val_loss did not improve from 0.27243\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.2399 - acc: 0.9247 - val_loss: 0.2769 - val_acc: 0.9194\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9230\n",
      "Epoch 00045: val_loss improved from 0.27243 to 0.26109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/045-0.2611.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2389 - acc: 0.9229 - val_loss: 0.2611 - val_acc: 0.9283\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9245\n",
      "Epoch 00046: val_loss did not improve from 0.26109\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.2349 - acc: 0.9245 - val_loss: 0.2693 - val_acc: 0.9262\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9278\n",
      "Epoch 00047: val_loss improved from 0.26109 to 0.25809, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/047-0.2581.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2296 - acc: 0.9278 - val_loss: 0.2581 - val_acc: 0.9285\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9279\n",
      "Epoch 00048: val_loss did not improve from 0.25809\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2313 - acc: 0.9279 - val_loss: 0.2586 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9296\n",
      "Epoch 00049: val_loss did not improve from 0.25809\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2216 - acc: 0.9295 - val_loss: 0.2845 - val_acc: 0.9189\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9277\n",
      "Epoch 00050: val_loss improved from 0.25809 to 0.24800, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/050-0.2480.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2206 - acc: 0.9277 - val_loss: 0.2480 - val_acc: 0.9317\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9315\n",
      "Epoch 00051: val_loss improved from 0.24800 to 0.24126, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/051-0.2413.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2141 - acc: 0.9315 - val_loss: 0.2413 - val_acc: 0.9327\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9310\n",
      "Epoch 00052: val_loss did not improve from 0.24126\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2142 - acc: 0.9309 - val_loss: 0.2535 - val_acc: 0.9297\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9310\n",
      "Epoch 00053: val_loss did not improve from 0.24126\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2123 - acc: 0.9310 - val_loss: 0.2568 - val_acc: 0.9301\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9345\n",
      "Epoch 00054: val_loss improved from 0.24126 to 0.23772, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/054-0.2377.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.2037 - acc: 0.9344 - val_loss: 0.2377 - val_acc: 0.9343\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9340\n",
      "Epoch 00055: val_loss did not improve from 0.23772\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2027 - acc: 0.9340 - val_loss: 0.2492 - val_acc: 0.9352\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9349\n",
      "Epoch 00056: val_loss did not improve from 0.23772\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2012 - acc: 0.9349 - val_loss: 0.2528 - val_acc: 0.9334\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9344\n",
      "Epoch 00057: val_loss did not improve from 0.23772\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2021 - acc: 0.9344 - val_loss: 0.2476 - val_acc: 0.9285\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9362\n",
      "Epoch 00058: val_loss did not improve from 0.23772\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1937 - acc: 0.9362 - val_loss: 0.2745 - val_acc: 0.9252\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9382\n",
      "Epoch 00059: val_loss did not improve from 0.23772\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1913 - acc: 0.9382 - val_loss: 0.2387 - val_acc: 0.9348\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9385\n",
      "Epoch 00060: val_loss improved from 0.23772 to 0.23660, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/060-0.2366.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1868 - acc: 0.9385 - val_loss: 0.2366 - val_acc: 0.9362\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9400\n",
      "Epoch 00061: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1859 - acc: 0.9400 - val_loss: 0.2385 - val_acc: 0.9359\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9405\n",
      "Epoch 00062: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1844 - acc: 0.9405 - val_loss: 0.2415 - val_acc: 0.9352\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9380\n",
      "Epoch 00063: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1854 - acc: 0.9380 - val_loss: 0.2436 - val_acc: 0.9329\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9394\n",
      "Epoch 00064: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1839 - acc: 0.9394 - val_loss: 0.2375 - val_acc: 0.9350\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9435\n",
      "Epoch 00065: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1743 - acc: 0.9435 - val_loss: 0.2579 - val_acc: 0.9304\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9432\n",
      "Epoch 00066: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1718 - acc: 0.9431 - val_loss: 0.2646 - val_acc: 0.9271\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9420\n",
      "Epoch 00067: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1747 - acc: 0.9420 - val_loss: 0.2492 - val_acc: 0.9334\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9432\n",
      "Epoch 00068: val_loss did not improve from 0.23660\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1713 - acc: 0.9432 - val_loss: 0.2570 - val_acc: 0.9278\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9452\n",
      "Epoch 00069: val_loss improved from 0.23660 to 0.23418, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/069-0.2342.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1665 - acc: 0.9452 - val_loss: 0.2342 - val_acc: 0.9373\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9461\n",
      "Epoch 00070: val_loss did not improve from 0.23418\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1653 - acc: 0.9461 - val_loss: 0.2491 - val_acc: 0.9352\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9466\n",
      "Epoch 00071: val_loss improved from 0.23418 to 0.22829, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/071-0.2283.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1637 - acc: 0.9466 - val_loss: 0.2283 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9471\n",
      "Epoch 00072: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1645 - acc: 0.9471 - val_loss: 0.2454 - val_acc: 0.9376\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9484\n",
      "Epoch 00073: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1576 - acc: 0.9483 - val_loss: 0.2424 - val_acc: 0.9385\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9477\n",
      "Epoch 00074: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1588 - acc: 0.9476 - val_loss: 0.2365 - val_acc: 0.9376\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9489\n",
      "Epoch 00075: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1549 - acc: 0.9489 - val_loss: 0.2441 - val_acc: 0.9362\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9498\n",
      "Epoch 00076: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1524 - acc: 0.9498 - val_loss: 0.2362 - val_acc: 0.9383\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9498\n",
      "Epoch 00077: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1531 - acc: 0.9498 - val_loss: 0.2448 - val_acc: 0.9387\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9507\n",
      "Epoch 00078: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1509 - acc: 0.9507 - val_loss: 0.2406 - val_acc: 0.9397\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9506\n",
      "Epoch 00079: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1507 - acc: 0.9506 - val_loss: 0.2436 - val_acc: 0.9399\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9498\n",
      "Epoch 00080: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1484 - acc: 0.9498 - val_loss: 0.2412 - val_acc: 0.9408\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9522\n",
      "Epoch 00081: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1438 - acc: 0.9522 - val_loss: 0.2344 - val_acc: 0.9422\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9535\n",
      "Epoch 00082: val_loss did not improve from 0.22829\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1418 - acc: 0.9535 - val_loss: 0.2344 - val_acc: 0.9404\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9533\n",
      "Epoch 00083: val_loss improved from 0.22829 to 0.22453, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_7_conv_checkpoint/083-0.2245.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1404 - acc: 0.9532 - val_loss: 0.2245 - val_acc: 0.9415\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9549\n",
      "Epoch 00084: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1375 - acc: 0.9549 - val_loss: 0.2295 - val_acc: 0.9429\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9530\n",
      "Epoch 00085: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1404 - acc: 0.9530 - val_loss: 0.2357 - val_acc: 0.9448\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9550\n",
      "Epoch 00086: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1340 - acc: 0.9548 - val_loss: 0.2327 - val_acc: 0.9441\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9554\n",
      "Epoch 00087: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1343 - acc: 0.9554 - val_loss: 0.2335 - val_acc: 0.9422\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9543\n",
      "Epoch 00088: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1335 - acc: 0.9543 - val_loss: 0.2359 - val_acc: 0.9418\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9542\n",
      "Epoch 00089: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1355 - acc: 0.9541 - val_loss: 0.2369 - val_acc: 0.9413\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9563\n",
      "Epoch 00090: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1314 - acc: 0.9563 - val_loss: 0.2291 - val_acc: 0.9427\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9567\n",
      "Epoch 00091: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1304 - acc: 0.9567 - val_loss: 0.2439 - val_acc: 0.9371\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9570\n",
      "Epoch 00092: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1280 - acc: 0.9570 - val_loss: 0.2468 - val_acc: 0.9418\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9574\n",
      "Epoch 00093: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1273 - acc: 0.9574 - val_loss: 0.2364 - val_acc: 0.9420\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9581\n",
      "Epoch 00094: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1262 - acc: 0.9579 - val_loss: 0.2469 - val_acc: 0.9411\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9586\n",
      "Epoch 00095: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1222 - acc: 0.9586 - val_loss: 0.2504 - val_acc: 0.9392\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9590\n",
      "Epoch 00096: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1234 - acc: 0.9591 - val_loss: 0.2360 - val_acc: 0.9441\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9595\n",
      "Epoch 00097: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1190 - acc: 0.9595 - val_loss: 0.2516 - val_acc: 0.9399\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9595\n",
      "Epoch 00098: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.1215 - acc: 0.9596 - val_loss: 0.2693 - val_acc: 0.9373\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9611\n",
      "Epoch 00099: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1170 - acc: 0.9611 - val_loss: 0.2302 - val_acc: 0.9427\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9609\n",
      "Epoch 00100: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1200 - acc: 0.9608 - val_loss: 0.2421 - val_acc: 0.9450\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9620\n",
      "Epoch 00101: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1155 - acc: 0.9620 - val_loss: 0.2277 - val_acc: 0.9460\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9603\n",
      "Epoch 00102: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1158 - acc: 0.9602 - val_loss: 0.2425 - val_acc: 0.9422\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9621\n",
      "Epoch 00103: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1112 - acc: 0.9621 - val_loss: 0.2419 - val_acc: 0.9408\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9613\n",
      "Epoch 00104: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1130 - acc: 0.9613 - val_loss: 0.2411 - val_acc: 0.9439\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9636\n",
      "Epoch 00105: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1091 - acc: 0.9637 - val_loss: 0.2416 - val_acc: 0.9441\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9626\n",
      "Epoch 00106: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1081 - acc: 0.9626 - val_loss: 0.2712 - val_acc: 0.9357\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9636\n",
      "Epoch 00107: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1108 - acc: 0.9636 - val_loss: 0.2375 - val_acc: 0.9448\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9635\n",
      "Epoch 00108: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1088 - acc: 0.9635 - val_loss: 0.2372 - val_acc: 0.9425\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9643\n",
      "Epoch 00109: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1055 - acc: 0.9643 - val_loss: 0.2419 - val_acc: 0.9462\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9630\n",
      "Epoch 00110: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1075 - acc: 0.9630 - val_loss: 0.2361 - val_acc: 0.9427\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9656\n",
      "Epoch 00111: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1048 - acc: 0.9655 - val_loss: 0.2348 - val_acc: 0.9460\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9662\n",
      "Epoch 00112: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1011 - acc: 0.9663 - val_loss: 0.2476 - val_acc: 0.9425\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9644\n",
      "Epoch 00113: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1062 - acc: 0.9644 - val_loss: 0.2469 - val_acc: 0.9455\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9663\n",
      "Epoch 00114: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0992 - acc: 0.9663 - val_loss: 0.2347 - val_acc: 0.9450\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9665\n",
      "Epoch 00115: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0999 - acc: 0.9666 - val_loss: 0.2499 - val_acc: 0.9467\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9653\n",
      "Epoch 00116: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1013 - acc: 0.9653 - val_loss: 0.2450 - val_acc: 0.9434\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9674\n",
      "Epoch 00117: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0969 - acc: 0.9675 - val_loss: 0.2510 - val_acc: 0.9432\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9668\n",
      "Epoch 00118: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0974 - acc: 0.9667 - val_loss: 0.2568 - val_acc: 0.9432\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9675\n",
      "Epoch 00119: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0969 - acc: 0.9675 - val_loss: 0.2337 - val_acc: 0.9457\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9667\n",
      "Epoch 00120: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0973 - acc: 0.9667 - val_loss: 0.2347 - val_acc: 0.9460\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9678\n",
      "Epoch 00121: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0970 - acc: 0.9679 - val_loss: 0.2338 - val_acc: 0.9460\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9688\n",
      "Epoch 00122: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0927 - acc: 0.9688 - val_loss: 0.2545 - val_acc: 0.9448\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9683\n",
      "Epoch 00123: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0940 - acc: 0.9683 - val_loss: 0.2440 - val_acc: 0.9457\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9670\n",
      "Epoch 00124: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0973 - acc: 0.9670 - val_loss: 0.2613 - val_acc: 0.9450\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9689\n",
      "Epoch 00125: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0929 - acc: 0.9689 - val_loss: 0.2625 - val_acc: 0.9441\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9687\n",
      "Epoch 00126: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0927 - acc: 0.9687 - val_loss: 0.2470 - val_acc: 0.9457\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9708\n",
      "Epoch 00127: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0877 - acc: 0.9708 - val_loss: 0.2456 - val_acc: 0.9455\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9690\n",
      "Epoch 00128: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0895 - acc: 0.9690 - val_loss: 0.2489 - val_acc: 0.9425\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9692\n",
      "Epoch 00129: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0899 - acc: 0.9692 - val_loss: 0.2515 - val_acc: 0.9462\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9706\n",
      "Epoch 00130: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0890 - acc: 0.9706 - val_loss: 0.2619 - val_acc: 0.9387\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9720\n",
      "Epoch 00131: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0844 - acc: 0.9720 - val_loss: 0.2504 - val_acc: 0.9460\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9701\n",
      "Epoch 00132: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0871 - acc: 0.9701 - val_loss: 0.2357 - val_acc: 0.9476\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9710\n",
      "Epoch 00133: val_loss did not improve from 0.22453\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0845 - acc: 0.9710 - val_loss: 0.2729 - val_acc: 0.9436\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 414us/sample - loss: 0.2795 - acc: 0.9229\n",
      "Loss: 0.2795416520763409 Accuracy: 0.92294914\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4347 - acc: 0.2014\n",
      "Epoch 00001: val_loss improved from inf to 1.81125, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/001-1.8112.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 2.4348 - acc: 0.2015 - val_loss: 1.8112 - val_acc: 0.4326\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6887 - acc: 0.4415\n",
      "Epoch 00002: val_loss improved from 1.81125 to 1.32143, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/002-1.3214.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.6887 - acc: 0.4415 - val_loss: 1.3214 - val_acc: 0.5893\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3518 - acc: 0.5498\n",
      "Epoch 00003: val_loss improved from 1.32143 to 1.07407, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/003-1.0741.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.3513 - acc: 0.5500 - val_loss: 1.0741 - val_acc: 0.6615\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1650 - acc: 0.6179\n",
      "Epoch 00004: val_loss improved from 1.07407 to 0.93694, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/004-0.9369.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.1650 - acc: 0.6179 - val_loss: 0.9369 - val_acc: 0.7084\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0466 - acc: 0.6568\n",
      "Epoch 00005: val_loss improved from 0.93694 to 0.84018, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/005-0.8402.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0465 - acc: 0.6569 - val_loss: 0.8402 - val_acc: 0.7410\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9455 - acc: 0.6918\n",
      "Epoch 00006: val_loss improved from 0.84018 to 0.77670, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/006-0.7767.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.9456 - acc: 0.6916 - val_loss: 0.7767 - val_acc: 0.7622\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7106\n",
      "Epoch 00007: val_loss improved from 0.77670 to 0.71080, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/007-0.7108.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8785 - acc: 0.7106 - val_loss: 0.7108 - val_acc: 0.7775\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7324\n",
      "Epoch 00008: val_loss did not improve from 0.71080\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.8176 - acc: 0.7324 - val_loss: 0.7327 - val_acc: 0.7668\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7696 - acc: 0.7492\n",
      "Epoch 00009: val_loss improved from 0.71080 to 0.65025, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/009-0.6503.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7695 - acc: 0.7492 - val_loss: 0.6503 - val_acc: 0.8006\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7181 - acc: 0.7652\n",
      "Epoch 00010: val_loss improved from 0.65025 to 0.61793, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/010-0.6179.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7180 - acc: 0.7652 - val_loss: 0.6179 - val_acc: 0.7987\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.7769\n",
      "Epoch 00011: val_loss improved from 0.61793 to 0.56952, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/011-0.5695.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.6798 - acc: 0.7769 - val_loss: 0.5695 - val_acc: 0.8190\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.7929\n",
      "Epoch 00012: val_loss improved from 0.56952 to 0.52387, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/012-0.5239.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.6395 - acc: 0.7929 - val_loss: 0.5239 - val_acc: 0.8295\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8054\n",
      "Epoch 00013: val_loss improved from 0.52387 to 0.51770, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/013-0.5177.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.5962 - acc: 0.8054 - val_loss: 0.5177 - val_acc: 0.8323\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5724 - acc: 0.8135\n",
      "Epoch 00014: val_loss improved from 0.51770 to 0.45536, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/014-0.4554.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.5723 - acc: 0.8135 - val_loss: 0.4554 - val_acc: 0.8602\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8252\n",
      "Epoch 00015: val_loss improved from 0.45536 to 0.42961, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/015-0.4296.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.5375 - acc: 0.8252 - val_loss: 0.4296 - val_acc: 0.8658\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8347\n",
      "Epoch 00016: val_loss did not improve from 0.42961\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.5122 - acc: 0.8347 - val_loss: 0.4578 - val_acc: 0.8542\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4901 - acc: 0.8426\n",
      "Epoch 00017: val_loss improved from 0.42961 to 0.40840, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/017-0.4084.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.4902 - acc: 0.8426 - val_loss: 0.4084 - val_acc: 0.8735\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8500\n",
      "Epoch 00018: val_loss improved from 0.40840 to 0.36736, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/018-0.3674.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.4615 - acc: 0.8500 - val_loss: 0.3674 - val_acc: 0.8880\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8572\n",
      "Epoch 00019: val_loss did not improve from 0.36736\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.4430 - acc: 0.8572 - val_loss: 0.3779 - val_acc: 0.8821\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8630\n",
      "Epoch 00020: val_loss improved from 0.36736 to 0.36508, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/020-0.3651.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.4253 - acc: 0.8630 - val_loss: 0.3651 - val_acc: 0.8877\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8687\n",
      "Epoch 00021: val_loss improved from 0.36508 to 0.32149, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/021-0.3215.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.4050 - acc: 0.8687 - val_loss: 0.3215 - val_acc: 0.9001\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8747\n",
      "Epoch 00022: val_loss did not improve from 0.32149\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.3914 - acc: 0.8746 - val_loss: 0.3685 - val_acc: 0.8833\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8810\n",
      "Epoch 00023: val_loss improved from 0.32149 to 0.30284, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/023-0.3028.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.3741 - acc: 0.8810 - val_loss: 0.3028 - val_acc: 0.9057\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8833\n",
      "Epoch 00024: val_loss did not improve from 0.30284\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.3619 - acc: 0.8833 - val_loss: 0.3383 - val_acc: 0.8975\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8858\n",
      "Epoch 00025: val_loss improved from 0.30284 to 0.27652, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/025-0.2765.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.3509 - acc: 0.8858 - val_loss: 0.2765 - val_acc: 0.9168\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8940\n",
      "Epoch 00026: val_loss did not improve from 0.27652\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.3311 - acc: 0.8940 - val_loss: 0.2811 - val_acc: 0.9140\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8955\n",
      "Epoch 00027: val_loss improved from 0.27652 to 0.27465, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/027-0.2747.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.3256 - acc: 0.8955 - val_loss: 0.2747 - val_acc: 0.9143\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9000\n",
      "Epoch 00028: val_loss improved from 0.27465 to 0.26057, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/028-0.2606.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.3109 - acc: 0.9000 - val_loss: 0.2606 - val_acc: 0.9252\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9037\n",
      "Epoch 00029: val_loss did not improve from 0.26057\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2976 - acc: 0.9036 - val_loss: 0.2747 - val_acc: 0.9168\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9036\n",
      "Epoch 00030: val_loss improved from 0.26057 to 0.23156, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/030-0.2316.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.2990 - acc: 0.9037 - val_loss: 0.2316 - val_acc: 0.9276\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9103\n",
      "Epoch 00031: val_loss did not improve from 0.23156\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.2791 - acc: 0.9103 - val_loss: 0.2339 - val_acc: 0.9290\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9132\n",
      "Epoch 00032: val_loss did not improve from 0.23156\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.2707 - acc: 0.9132 - val_loss: 0.2362 - val_acc: 0.9306\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9133\n",
      "Epoch 00033: val_loss improved from 0.23156 to 0.22832, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/033-0.2283.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.2659 - acc: 0.9134 - val_loss: 0.2283 - val_acc: 0.9334\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9189\n",
      "Epoch 00034: val_loss improved from 0.22832 to 0.21801, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/034-0.2180.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2564 - acc: 0.9189 - val_loss: 0.2180 - val_acc: 0.9350\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9209\n",
      "Epoch 00035: val_loss improved from 0.21801 to 0.21751, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/035-0.2175.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.2489 - acc: 0.9209 - val_loss: 0.2175 - val_acc: 0.9373\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9214\n",
      "Epoch 00036: val_loss improved from 0.21751 to 0.20675, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/036-0.2067.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.2416 - acc: 0.9213 - val_loss: 0.2067 - val_acc: 0.9385\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9229\n",
      "Epoch 00037: val_loss did not improve from 0.20675\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.2367 - acc: 0.9229 - val_loss: 0.2198 - val_acc: 0.9352\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9271\n",
      "Epoch 00038: val_loss improved from 0.20675 to 0.20090, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/038-0.2009.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.2265 - acc: 0.9271 - val_loss: 0.2009 - val_acc: 0.9394\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9273\n",
      "Epoch 00039: val_loss did not improve from 0.20090\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.2240 - acc: 0.9273 - val_loss: 0.2060 - val_acc: 0.9406\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9305\n",
      "Epoch 00040: val_loss improved from 0.20090 to 0.19419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/040-0.1942.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2139 - acc: 0.9305 - val_loss: 0.1942 - val_acc: 0.9429\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9303\n",
      "Epoch 00041: val_loss improved from 0.19419 to 0.19351, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/041-0.1935.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2114 - acc: 0.9303 - val_loss: 0.1935 - val_acc: 0.9443\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9330\n",
      "Epoch 00042: val_loss did not improve from 0.19351\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.2073 - acc: 0.9330 - val_loss: 0.1949 - val_acc: 0.9429\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9337\n",
      "Epoch 00043: val_loss improved from 0.19351 to 0.18017, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/043-0.1802.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.2028 - acc: 0.9337 - val_loss: 0.1802 - val_acc: 0.9446\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9346\n",
      "Epoch 00044: val_loss did not improve from 0.18017\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.1959 - acc: 0.9346 - val_loss: 0.1937 - val_acc: 0.9411\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9378\n",
      "Epoch 00045: val_loss improved from 0.18017 to 0.17970, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/045-0.1797.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.1902 - acc: 0.9378 - val_loss: 0.1797 - val_acc: 0.9474\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9386\n",
      "Epoch 00046: val_loss improved from 0.17970 to 0.17814, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/046-0.1781.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.1871 - acc: 0.9386 - val_loss: 0.1781 - val_acc: 0.9460\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9395\n",
      "Epoch 00047: val_loss improved from 0.17814 to 0.17283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/047-0.1728.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.1853 - acc: 0.9395 - val_loss: 0.1728 - val_acc: 0.9522\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9423\n",
      "Epoch 00048: val_loss did not improve from 0.17283\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.1758 - acc: 0.9423 - val_loss: 0.1859 - val_acc: 0.9467\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9431\n",
      "Epoch 00049: val_loss did not improve from 0.17283\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.1737 - acc: 0.9431 - val_loss: 0.2135 - val_acc: 0.9364\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9454\n",
      "Epoch 00050: val_loss did not improve from 0.17283\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.1674 - acc: 0.9454 - val_loss: 0.1744 - val_acc: 0.9490\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9432\n",
      "Epoch 00051: val_loss improved from 0.17283 to 0.17116, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/051-0.1712.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1688 - acc: 0.9432 - val_loss: 0.1712 - val_acc: 0.9506\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9469\n",
      "Epoch 00052: val_loss improved from 0.17116 to 0.16656, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/052-0.1666.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.1646 - acc: 0.9469 - val_loss: 0.1666 - val_acc: 0.9485\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9460\n",
      "Epoch 00053: val_loss did not improve from 0.16656\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.1608 - acc: 0.9460 - val_loss: 0.1859 - val_acc: 0.9448\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9474\n",
      "Epoch 00054: val_loss did not improve from 0.16656\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.1581 - acc: 0.9474 - val_loss: 0.1734 - val_acc: 0.9488\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9490\n",
      "Epoch 00055: val_loss improved from 0.16656 to 0.16430, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/055-0.1643.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.1544 - acc: 0.9490 - val_loss: 0.1643 - val_acc: 0.9490\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9510\n",
      "Epoch 00056: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.1511 - acc: 0.9509 - val_loss: 0.1847 - val_acc: 0.9448\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9500\n",
      "Epoch 00057: val_loss improved from 0.16430 to 0.16217, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/057-0.1622.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.1518 - acc: 0.9500 - val_loss: 0.1622 - val_acc: 0.9534\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9518\n",
      "Epoch 00058: val_loss did not improve from 0.16217\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.1438 - acc: 0.9518 - val_loss: 0.1798 - val_acc: 0.9464\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9514\n",
      "Epoch 00059: val_loss did not improve from 0.16217\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.1471 - acc: 0.9514 - val_loss: 0.1679 - val_acc: 0.9492\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9532\n",
      "Epoch 00060: val_loss did not improve from 0.16217\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.1414 - acc: 0.9532 - val_loss: 0.1768 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9523\n",
      "Epoch 00061: val_loss did not improve from 0.16217\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1388 - acc: 0.9523 - val_loss: 0.1633 - val_acc: 0.9515\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9543\n",
      "Epoch 00062: val_loss improved from 0.16217 to 0.16081, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/062-0.1608.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.1359 - acc: 0.9543 - val_loss: 0.1608 - val_acc: 0.9543\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9557\n",
      "Epoch 00063: val_loss improved from 0.16081 to 0.15504, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/063-0.1550.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.1329 - acc: 0.9557 - val_loss: 0.1550 - val_acc: 0.9527\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9561\n",
      "Epoch 00064: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.1307 - acc: 0.9561 - val_loss: 0.1572 - val_acc: 0.9534\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9569\n",
      "Epoch 00065: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.1282 - acc: 0.9570 - val_loss: 0.1756 - val_acc: 0.9490\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9571\n",
      "Epoch 00066: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1271 - acc: 0.9571 - val_loss: 0.1556 - val_acc: 0.9541\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9577\n",
      "Epoch 00067: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.1251 - acc: 0.9577 - val_loss: 0.1566 - val_acc: 0.9546\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9571\n",
      "Epoch 00068: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.1248 - acc: 0.9571 - val_loss: 0.1598 - val_acc: 0.9560\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9603\n",
      "Epoch 00069: val_loss did not improve from 0.15504\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.1208 - acc: 0.9603 - val_loss: 0.1687 - val_acc: 0.9550\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9602\n",
      "Epoch 00070: val_loss improved from 0.15504 to 0.15454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/070-0.1545.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.1192 - acc: 0.9601 - val_loss: 0.1545 - val_acc: 0.9578\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9610\n",
      "Epoch 00071: val_loss did not improve from 0.15454\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.1172 - acc: 0.9610 - val_loss: 0.1579 - val_acc: 0.9560\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9603\n",
      "Epoch 00072: val_loss improved from 0.15454 to 0.15275, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/072-0.1528.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.1176 - acc: 0.9603 - val_loss: 0.1528 - val_acc: 0.9574\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9615\n",
      "Epoch 00073: val_loss improved from 0.15275 to 0.15175, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/073-0.1517.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.1158 - acc: 0.9615 - val_loss: 0.1517 - val_acc: 0.9576\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9609\n",
      "Epoch 00074: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.1135 - acc: 0.9609 - val_loss: 0.1532 - val_acc: 0.9550\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9618\n",
      "Epoch 00075: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.1110 - acc: 0.9618 - val_loss: 0.1559 - val_acc: 0.9532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9616\n",
      "Epoch 00076: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.1124 - acc: 0.9616 - val_loss: 0.1617 - val_acc: 0.9539\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9645\n",
      "Epoch 00077: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.1061 - acc: 0.9645 - val_loss: 0.1567 - val_acc: 0.9555\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9638\n",
      "Epoch 00078: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.1058 - acc: 0.9638 - val_loss: 0.1766 - val_acc: 0.9499\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9664\n",
      "Epoch 00079: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1001 - acc: 0.9664 - val_loss: 0.1632 - val_acc: 0.9536\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9633\n",
      "Epoch 00080: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.1049 - acc: 0.9634 - val_loss: 0.1745 - val_acc: 0.9539\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9658\n",
      "Epoch 00081: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.1005 - acc: 0.9658 - val_loss: 0.1621 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9656\n",
      "Epoch 00082: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1013 - acc: 0.9655 - val_loss: 0.1598 - val_acc: 0.9557\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9659\n",
      "Epoch 00083: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0995 - acc: 0.9659 - val_loss: 0.1604 - val_acc: 0.9571\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9678\n",
      "Epoch 00084: val_loss did not improve from 0.15175\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0952 - acc: 0.9678 - val_loss: 0.1536 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9674\n",
      "Epoch 00085: val_loss improved from 0.15175 to 0.15047, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_8_conv_checkpoint/085-0.1505.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.0941 - acc: 0.9675 - val_loss: 0.1505 - val_acc: 0.9592\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9688\n",
      "Epoch 00086: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0930 - acc: 0.9688 - val_loss: 0.1607 - val_acc: 0.9569\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9683\n",
      "Epoch 00087: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.0923 - acc: 0.9683 - val_loss: 0.1538 - val_acc: 0.9560\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9689\n",
      "Epoch 00088: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0909 - acc: 0.9689 - val_loss: 0.1543 - val_acc: 0.9585\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9693\n",
      "Epoch 00089: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.0898 - acc: 0.9693 - val_loss: 0.1609 - val_acc: 0.9578\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9689\n",
      "Epoch 00090: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0879 - acc: 0.9689 - val_loss: 0.1573 - val_acc: 0.9583\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9693\n",
      "Epoch 00091: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0883 - acc: 0.9693 - val_loss: 0.1577 - val_acc: 0.9557\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9708\n",
      "Epoch 00092: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0874 - acc: 0.9708 - val_loss: 0.1603 - val_acc: 0.9578\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9720\n",
      "Epoch 00093: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.0863 - acc: 0.9720 - val_loss: 0.1623 - val_acc: 0.9562\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9709\n",
      "Epoch 00094: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0850 - acc: 0.9710 - val_loss: 0.1607 - val_acc: 0.9562\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9692\n",
      "Epoch 00095: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.0896 - acc: 0.9692 - val_loss: 0.1551 - val_acc: 0.9583\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9726\n",
      "Epoch 00096: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.0809 - acc: 0.9726 - val_loss: 0.1523 - val_acc: 0.9569\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9718\n",
      "Epoch 00097: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0789 - acc: 0.9718 - val_loss: 0.1617 - val_acc: 0.9562\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9727\n",
      "Epoch 00098: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0805 - acc: 0.9727 - val_loss: 0.1955 - val_acc: 0.9522\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9730\n",
      "Epoch 00099: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0777 - acc: 0.9730 - val_loss: 0.1595 - val_acc: 0.9609\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9720\n",
      "Epoch 00100: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.0781 - acc: 0.9720 - val_loss: 0.1647 - val_acc: 0.9567\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9739\n",
      "Epoch 00101: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.0752 - acc: 0.9739 - val_loss: 0.1581 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9731\n",
      "Epoch 00102: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.0766 - acc: 0.9731 - val_loss: 0.1687 - val_acc: 0.9574\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9740\n",
      "Epoch 00103: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0746 - acc: 0.9741 - val_loss: 0.1789 - val_acc: 0.9557\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9729\n",
      "Epoch 00104: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0787 - acc: 0.9729 - val_loss: 0.1848 - val_acc: 0.9543\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9754\n",
      "Epoch 00105: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0708 - acc: 0.9754 - val_loss: 0.1642 - val_acc: 0.9562\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9743\n",
      "Epoch 00106: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0731 - acc: 0.9743 - val_loss: 0.1604 - val_acc: 0.9576\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9752\n",
      "Epoch 00107: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0724 - acc: 0.9752 - val_loss: 0.1695 - val_acc: 0.9578\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9760\n",
      "Epoch 00108: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.0706 - acc: 0.9760 - val_loss: 0.1657 - val_acc: 0.9585\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9763\n",
      "Epoch 00109: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0715 - acc: 0.9763 - val_loss: 0.1660 - val_acc: 0.9585\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9759\n",
      "Epoch 00110: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.0708 - acc: 0.9759 - val_loss: 0.1875 - val_acc: 0.9541\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9767\n",
      "Epoch 00111: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0664 - acc: 0.9767 - val_loss: 0.1587 - val_acc: 0.9595\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9759\n",
      "Epoch 00112: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0666 - acc: 0.9759 - val_loss: 0.1837 - val_acc: 0.9548\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9773\n",
      "Epoch 00113: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0643 - acc: 0.9773 - val_loss: 0.1989 - val_acc: 0.9488\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9759\n",
      "Epoch 00114: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0690 - acc: 0.9759 - val_loss: 0.1807 - val_acc: 0.9574\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9768\n",
      "Epoch 00115: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0662 - acc: 0.9768 - val_loss: 0.1667 - val_acc: 0.9592\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9781\n",
      "Epoch 00116: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0627 - acc: 0.9780 - val_loss: 0.1820 - val_acc: 0.9553\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9786\n",
      "Epoch 00117: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0618 - acc: 0.9786 - val_loss: 0.1789 - val_acc: 0.9543\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9799\n",
      "Epoch 00118: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0588 - acc: 0.9799 - val_loss: 0.1657 - val_acc: 0.9588\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9773\n",
      "Epoch 00119: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0652 - acc: 0.9773 - val_loss: 0.1627 - val_acc: 0.9574\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9790\n",
      "Epoch 00120: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0607 - acc: 0.9790 - val_loss: 0.1628 - val_acc: 0.9569\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9797\n",
      "Epoch 00121: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.0599 - acc: 0.9797 - val_loss: 0.1832 - val_acc: 0.9546\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9788\n",
      "Epoch 00122: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0619 - acc: 0.9788 - val_loss: 0.1736 - val_acc: 0.9597\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9796\n",
      "Epoch 00123: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0598 - acc: 0.9795 - val_loss: 0.1795 - val_acc: 0.9576\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9790\n",
      "Epoch 00124: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0575 - acc: 0.9790 - val_loss: 0.1705 - val_acc: 0.9557\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9811\n",
      "Epoch 00125: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0546 - acc: 0.9811 - val_loss: 0.1737 - val_acc: 0.9599\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9806\n",
      "Epoch 00126: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0546 - acc: 0.9806 - val_loss: 0.1790 - val_acc: 0.9567\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9799\n",
      "Epoch 00127: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.0580 - acc: 0.9798 - val_loss: 0.1704 - val_acc: 0.9581\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9795\n",
      "Epoch 00128: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.0579 - acc: 0.9795 - val_loss: 0.1731 - val_acc: 0.9555\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9815\n",
      "Epoch 00129: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0546 - acc: 0.9815 - val_loss: 0.1695 - val_acc: 0.9567\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9813\n",
      "Epoch 00130: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0564 - acc: 0.9813 - val_loss: 0.1789 - val_acc: 0.9592\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9817\n",
      "Epoch 00131: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0547 - acc: 0.9817 - val_loss: 0.1787 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9805\n",
      "Epoch 00132: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0568 - acc: 0.9805 - val_loss: 0.1707 - val_acc: 0.9576\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9800\n",
      "Epoch 00133: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0543 - acc: 0.9800 - val_loss: 0.1934 - val_acc: 0.9553\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9820\n",
      "Epoch 00134: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0519 - acc: 0.9820 - val_loss: 0.1719 - val_acc: 0.9588\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9818\n",
      "Epoch 00135: val_loss did not improve from 0.15047\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0525 - acc: 0.9818 - val_loss: 0.1862 - val_acc: 0.9578\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 421us/sample - loss: 0.2012 - acc: 0.9425\n",
      "Loss: 0.2011764725838495 Accuracy: 0.94247144\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2546 - acc: 0.2699\n",
      "Epoch 00001: val_loss improved from inf to 1.49354, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/001-1.4935.hdf5\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 2.2544 - acc: 0.2700 - val_loss: 1.4935 - val_acc: 0.5278\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3655 - acc: 0.5575\n",
      "Epoch 00002: val_loss improved from 1.49354 to 1.00251, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/002-1.0025.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.3655 - acc: 0.5575 - val_loss: 1.0025 - val_acc: 0.6858\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0467 - acc: 0.6535\n",
      "Epoch 00003: val_loss improved from 1.00251 to 0.82951, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/003-0.8295.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.0468 - acc: 0.6535 - val_loss: 0.8295 - val_acc: 0.7345\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8946 - acc: 0.7071\n",
      "Epoch 00004: val_loss improved from 0.82951 to 0.71878, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/004-0.7188.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8948 - acc: 0.7070 - val_loss: 0.7188 - val_acc: 0.7589\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7373\n",
      "Epoch 00005: val_loss improved from 0.71878 to 0.65230, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/005-0.6523.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7907 - acc: 0.7374 - val_loss: 0.6523 - val_acc: 0.7815\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7067 - acc: 0.7683\n",
      "Epoch 00006: val_loss improved from 0.65230 to 0.55928, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/006-0.5593.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7068 - acc: 0.7683 - val_loss: 0.5593 - val_acc: 0.8183\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6406 - acc: 0.7900\n",
      "Epoch 00007: val_loss improved from 0.55928 to 0.51017, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/007-0.5102.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.6404 - acc: 0.7900 - val_loss: 0.5102 - val_acc: 0.8369\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5845 - acc: 0.8101\n",
      "Epoch 00008: val_loss improved from 0.51017 to 0.48417, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/008-0.4842.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.5844 - acc: 0.8101 - val_loss: 0.4842 - val_acc: 0.8423\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8234\n",
      "Epoch 00009: val_loss improved from 0.48417 to 0.43484, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/009-0.4348.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.5363 - acc: 0.8234 - val_loss: 0.4348 - val_acc: 0.8567\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8396\n",
      "Epoch 00010: val_loss improved from 0.43484 to 0.37314, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/010-0.3731.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.4876 - acc: 0.8396 - val_loss: 0.3731 - val_acc: 0.8847\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8520\n",
      "Epoch 00011: val_loss improved from 0.37314 to 0.36392, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/011-0.3639.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4547 - acc: 0.8520 - val_loss: 0.3639 - val_acc: 0.8833\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8654\n",
      "Epoch 00012: val_loss improved from 0.36392 to 0.32186, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/012-0.3219.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.4181 - acc: 0.8655 - val_loss: 0.3219 - val_acc: 0.9022\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8758\n",
      "Epoch 00013: val_loss improved from 0.32186 to 0.30254, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/013-0.3025.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.3862 - acc: 0.8758 - val_loss: 0.3025 - val_acc: 0.9061\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8798\n",
      "Epoch 00014: val_loss improved from 0.30254 to 0.27987, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/014-0.2799.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3687 - acc: 0.8798 - val_loss: 0.2799 - val_acc: 0.9082\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8902\n",
      "Epoch 00015: val_loss improved from 0.27987 to 0.25665, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/015-0.2567.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.3383 - acc: 0.8902 - val_loss: 0.2567 - val_acc: 0.9208\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8982\n",
      "Epoch 00016: val_loss improved from 0.25665 to 0.24057, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/016-0.2406.hdf5\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.3149 - acc: 0.8982 - val_loss: 0.2406 - val_acc: 0.9259\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9052\n",
      "Epoch 00017: val_loss did not improve from 0.24057\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.2960 - acc: 0.9052 - val_loss: 0.2419 - val_acc: 0.9245\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9088\n",
      "Epoch 00018: val_loss improved from 0.24057 to 0.23952, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/018-0.2395.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.2840 - acc: 0.9088 - val_loss: 0.2395 - val_acc: 0.9252\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9137\n",
      "Epoch 00019: val_loss improved from 0.23952 to 0.21371, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/019-0.2137.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2638 - acc: 0.9137 - val_loss: 0.2137 - val_acc: 0.9362\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9183\n",
      "Epoch 00020: val_loss did not improve from 0.21371\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.2535 - acc: 0.9182 - val_loss: 0.2412 - val_acc: 0.9236\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9223\n",
      "Epoch 00021: val_loss improved from 0.21371 to 0.19656, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/021-0.1966.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2413 - acc: 0.9223 - val_loss: 0.1966 - val_acc: 0.9404\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9247\n",
      "Epoch 00022: val_loss did not improve from 0.19656\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2317 - acc: 0.9247 - val_loss: 0.2005 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9288\n",
      "Epoch 00023: val_loss did not improve from 0.19656\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2198 - acc: 0.9288 - val_loss: 0.1974 - val_acc: 0.9369\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9307\n",
      "Epoch 00024: val_loss improved from 0.19656 to 0.18122, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/024-0.1812.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2118 - acc: 0.9308 - val_loss: 0.1812 - val_acc: 0.9411\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9339\n",
      "Epoch 00025: val_loss did not improve from 0.18122\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2022 - acc: 0.9338 - val_loss: 0.1938 - val_acc: 0.9415\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9366\n",
      "Epoch 00026: val_loss improved from 0.18122 to 0.17940, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/026-0.1794.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.1952 - acc: 0.9366 - val_loss: 0.1794 - val_acc: 0.9450\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9383\n",
      "Epoch 00027: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1902 - acc: 0.9382 - val_loss: 0.1801 - val_acc: 0.9422\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9424\n",
      "Epoch 00028: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.1778 - acc: 0.9424 - val_loss: 0.1987 - val_acc: 0.9378\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9423\n",
      "Epoch 00029: val_loss improved from 0.17940 to 0.17580, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/029-0.1758.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1768 - acc: 0.9423 - val_loss: 0.1758 - val_acc: 0.9471\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9435\n",
      "Epoch 00030: val_loss did not improve from 0.17580\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1692 - acc: 0.9436 - val_loss: 0.1779 - val_acc: 0.9460\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9473\n",
      "Epoch 00031: val_loss improved from 0.17580 to 0.17494, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/031-0.1749.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.1578 - acc: 0.9473 - val_loss: 0.1749 - val_acc: 0.9455\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9474\n",
      "Epoch 00032: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.1561 - acc: 0.9474 - val_loss: 0.1895 - val_acc: 0.9406\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9500\n",
      "Epoch 00033: val_loss did not improve from 0.17494\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.1501 - acc: 0.9500 - val_loss: 0.1859 - val_acc: 0.9422\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9524\n",
      "Epoch 00034: val_loss improved from 0.17494 to 0.16916, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/034-0.1692.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1434 - acc: 0.9524 - val_loss: 0.1692 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9539\n",
      "Epoch 00035: val_loss did not improve from 0.16916\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1399 - acc: 0.9539 - val_loss: 0.1724 - val_acc: 0.9478\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9563\n",
      "Epoch 00036: val_loss did not improve from 0.16916\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1335 - acc: 0.9563 - val_loss: 0.1862 - val_acc: 0.9422\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9565\n",
      "Epoch 00037: val_loss improved from 0.16916 to 0.15988, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_025_DO_9_conv_checkpoint/037-0.1599.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1312 - acc: 0.9565 - val_loss: 0.1599 - val_acc: 0.9511\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9583\n",
      "Epoch 00038: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1225 - acc: 0.9583 - val_loss: 0.1734 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9595\n",
      "Epoch 00039: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.1204 - acc: 0.9595 - val_loss: 0.1718 - val_acc: 0.9478\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9608\n",
      "Epoch 00040: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.1160 - acc: 0.9608 - val_loss: 0.1665 - val_acc: 0.9497\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9619\n",
      "Epoch 00041: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1135 - acc: 0.9618 - val_loss: 0.1709 - val_acc: 0.9497\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9624\n",
      "Epoch 00042: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.1110 - acc: 0.9625 - val_loss: 0.1764 - val_acc: 0.9471\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9620\n",
      "Epoch 00043: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1083 - acc: 0.9620 - val_loss: 0.1747 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9652\n",
      "Epoch 00044: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1038 - acc: 0.9652 - val_loss: 0.1700 - val_acc: 0.9511\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9658\n",
      "Epoch 00045: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1003 - acc: 0.9658 - val_loss: 0.1639 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9675\n",
      "Epoch 00046: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0975 - acc: 0.9675 - val_loss: 0.1674 - val_acc: 0.9513\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9682\n",
      "Epoch 00047: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.0953 - acc: 0.9680 - val_loss: 0.1684 - val_acc: 0.9506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9686\n",
      "Epoch 00048: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0917 - acc: 0.9686 - val_loss: 0.1690 - val_acc: 0.9522\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9692\n",
      "Epoch 00049: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.0902 - acc: 0.9692 - val_loss: 0.1901 - val_acc: 0.9478\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9703\n",
      "Epoch 00050: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.0863 - acc: 0.9702 - val_loss: 0.1726 - val_acc: 0.9527\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9704\n",
      "Epoch 00051: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0854 - acc: 0.9704 - val_loss: 0.1610 - val_acc: 0.9546\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9696\n",
      "Epoch 00052: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0894 - acc: 0.9697 - val_loss: 0.1771 - val_acc: 0.9518\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9728\n",
      "Epoch 00053: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.0800 - acc: 0.9728 - val_loss: 0.1766 - val_acc: 0.9518\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9733\n",
      "Epoch 00054: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0766 - acc: 0.9733 - val_loss: 0.1756 - val_acc: 0.9532\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9735\n",
      "Epoch 00055: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.0763 - acc: 0.9735 - val_loss: 0.2047 - val_acc: 0.9464\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9707\n",
      "Epoch 00056: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0847 - acc: 0.9707 - val_loss: 0.1814 - val_acc: 0.9483\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9759\n",
      "Epoch 00057: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.0689 - acc: 0.9759 - val_loss: 0.1714 - val_acc: 0.9546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9772\n",
      "Epoch 00058: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.0680 - acc: 0.9772 - val_loss: 0.1687 - val_acc: 0.9555\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9755\n",
      "Epoch 00059: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.0713 - acc: 0.9755 - val_loss: 0.1873 - val_acc: 0.9518\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9750\n",
      "Epoch 00060: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.0720 - acc: 0.9750 - val_loss: 0.1682 - val_acc: 0.9534\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9772\n",
      "Epoch 00061: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0664 - acc: 0.9772 - val_loss: 0.1892 - val_acc: 0.9539\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9779\n",
      "Epoch 00062: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0653 - acc: 0.9779 - val_loss: 0.1715 - val_acc: 0.9555\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9790\n",
      "Epoch 00063: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0600 - acc: 0.9790 - val_loss: 0.1795 - val_acc: 0.9525\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9782\n",
      "Epoch 00064: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0632 - acc: 0.9782 - val_loss: 0.1733 - val_acc: 0.9534\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9794\n",
      "Epoch 00065: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.0606 - acc: 0.9794 - val_loss: 0.1839 - val_acc: 0.9513\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9781\n",
      "Epoch 00066: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.0627 - acc: 0.9782 - val_loss: 0.1967 - val_acc: 0.9502\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9817\n",
      "Epoch 00067: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0547 - acc: 0.9817 - val_loss: 0.1800 - val_acc: 0.9564\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9809\n",
      "Epoch 00068: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0550 - acc: 0.9809 - val_loss: 0.1912 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9797\n",
      "Epoch 00069: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0600 - acc: 0.9797 - val_loss: 0.1821 - val_acc: 0.9548\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9823\n",
      "Epoch 00070: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0528 - acc: 0.9823 - val_loss: 0.2003 - val_acc: 0.9520\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9813\n",
      "Epoch 00071: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.0559 - acc: 0.9813 - val_loss: 0.1869 - val_acc: 0.9506\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9827\n",
      "Epoch 00072: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.0500 - acc: 0.9827 - val_loss: 0.1840 - val_acc: 0.9557\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9822\n",
      "Epoch 00073: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0505 - acc: 0.9822 - val_loss: 0.2001 - val_acc: 0.9497\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9801\n",
      "Epoch 00074: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.0599 - acc: 0.9801 - val_loss: 0.1881 - val_acc: 0.9529\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9836\n",
      "Epoch 00075: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.0462 - acc: 0.9836 - val_loss: 0.2250 - val_acc: 0.9490\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9829\n",
      "Epoch 00076: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0486 - acc: 0.9829 - val_loss: 0.2028 - val_acc: 0.9557\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9826\n",
      "Epoch 00077: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0485 - acc: 0.9826 - val_loss: 0.1842 - val_acc: 0.9532\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9843\n",
      "Epoch 00078: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0459 - acc: 0.9843 - val_loss: 0.2133 - val_acc: 0.9515\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9842\n",
      "Epoch 00079: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0460 - acc: 0.9842 - val_loss: 0.2093 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9854\n",
      "Epoch 00080: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0434 - acc: 0.9854 - val_loss: 0.2200 - val_acc: 0.9492\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9837\n",
      "Epoch 00081: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.0470 - acc: 0.9837 - val_loss: 0.2114 - val_acc: 0.9560\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9848\n",
      "Epoch 00082: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0456 - acc: 0.9848 - val_loss: 0.2168 - val_acc: 0.9513\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9849\n",
      "Epoch 00083: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0448 - acc: 0.9849 - val_loss: 0.1930 - val_acc: 0.9534\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9863\n",
      "Epoch 00084: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.0400 - acc: 0.9863 - val_loss: 0.2069 - val_acc: 0.9557\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9860\n",
      "Epoch 00085: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0412 - acc: 0.9860 - val_loss: 0.2125 - val_acc: 0.9515\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9850\n",
      "Epoch 00086: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0424 - acc: 0.9850 - val_loss: 0.2131 - val_acc: 0.9548\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9855\n",
      "Epoch 00087: val_loss did not improve from 0.15988\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0409 - acc: 0.9855 - val_loss: 0.2118 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 463us/sample - loss: 0.2478 - acc: 0.9267\n",
      "Loss: 0.24777987390168607 Accuracy: 0.9266874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO_025_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 398us/sample - loss: 1.6197 - acc: 0.4868\n",
      "Loss: 1.6197364272358261 Accuracy: 0.48681206\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 423us/sample - loss: 1.2622 - acc: 0.6037\n",
      "Loss: 1.2621989315543962 Accuracy: 0.6037383\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 442us/sample - loss: 0.9731 - acc: 0.7128\n",
      "Loss: 0.9731363266056572 Accuracy: 0.7127726\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 432us/sample - loss: 0.5331 - acc: 0.8534\n",
      "Loss: 0.5330665096563219 Accuracy: 0.8533749\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 477us/sample - loss: 0.2795 - acc: 0.9229\n",
      "Loss: 0.2795416520763409 Accuracy: 0.92294914\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 491us/sample - loss: 0.2012 - acc: 0.9425\n",
      "Loss: 0.2011764725838495 Accuracy: 0.94247144\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 513us/sample - loss: 0.2478 - acc: 0.9267\n",
      "Loss: 0.24777987390168607 Accuracy: 0.9266874\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO_025_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 475us/sample - loss: 3.2254 - acc: 0.5034\n",
      "Loss: 3.2253897103441345 Accuracy: 0.5034268\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 489us/sample - loss: 1.9695 - acc: 0.6395\n",
      "Loss: 1.9694936931318954 Accuracy: 0.63946\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 533us/sample - loss: 1.5426 - acc: 0.7248\n",
      "Loss: 1.5425674485021414 Accuracy: 0.7248183\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 535us/sample - loss: 0.6166 - acc: 0.8650\n",
      "Loss: 0.6165878357669399 Accuracy: 0.8650052\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 556us/sample - loss: 0.3173 - acc: 0.9267\n",
      "Loss: 0.31728586518628327 Accuracy: 0.9266874\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 564us/sample - loss: 0.2295 - acc: 0.9452\n",
      "Loss: 0.22952146081956004 Accuracy: 0.94517136\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 561us/sample - loss: 0.2987 - acc: 0.9362\n",
      "Loss: 0.29866755732858835 Accuracy: 0.9362409\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
