{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    channel_size = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 32,818,832\n",
      "Trainable params: 32,818,320\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 11,072,400\n",
      "Trainable params: 11,071,376\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,889,296\n",
      "Trainable params: 3,887,760\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,562,000\n",
      "Trainable params: 1,559,952\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,453,968\n",
      "Trainable params: 1,450,896\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,309,072\n",
      "Trainable params: 1,304,976\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,524,624\n",
      "Trainable params: 1,519,504\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,863,056\n",
      "Trainable params: 1,856,912\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,035,536\n",
      "Trainable params: 3,027,344\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1975 - acc: 0.3784\n",
      "Epoch 00001: val_loss improved from inf to 1.40801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/001-1.4080.hdf5\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 2.1974 - acc: 0.3784 - val_loss: 1.4080 - val_acc: 0.5712\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3399 - acc: 0.5916\n",
      "Epoch 00002: val_loss improved from 1.40801 to 1.09588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/002-1.0959.hdf5\n",
      "36805/36805 [==============================] - 286s 8ms/sample - loss: 1.3400 - acc: 0.5916 - val_loss: 1.0959 - val_acc: 0.6765\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0848 - acc: 0.6701\n",
      "Epoch 00003: val_loss improved from 1.09588 to 1.03569, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/003-1.0357.hdf5\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 1.0850 - acc: 0.6700 - val_loss: 1.0357 - val_acc: 0.6685\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9090 - acc: 0.7230\n",
      "Epoch 00004: val_loss improved from 1.03569 to 0.96071, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/004-0.9607.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.9090 - acc: 0.7230 - val_loss: 0.9607 - val_acc: 0.7133\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7645\n",
      "Epoch 00005: val_loss improved from 0.96071 to 0.96024, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/005-0.9602.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.7779 - acc: 0.7645 - val_loss: 0.9602 - val_acc: 0.7375\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6732 - acc: 0.7946\n",
      "Epoch 00006: val_loss improved from 0.96024 to 0.89978, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/006-0.8998.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.6731 - acc: 0.7945 - val_loss: 0.8998 - val_acc: 0.7384\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.8163\n",
      "Epoch 00007: val_loss did not improve from 0.89978\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.5948 - acc: 0.8162 - val_loss: 1.1741 - val_acc: 0.6711\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.8385\n",
      "Epoch 00008: val_loss did not improve from 0.89978\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.5210 - acc: 0.8385 - val_loss: 0.9300 - val_acc: 0.7473\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8607\n",
      "Epoch 00009: val_loss did not improve from 0.89978\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.4489 - acc: 0.8606 - val_loss: 0.9230 - val_acc: 0.7512\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8766\n",
      "Epoch 00010: val_loss did not improve from 0.89978\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.3932 - acc: 0.8765 - val_loss: 1.0268 - val_acc: 0.7417\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8849\n",
      "Epoch 00011: val_loss improved from 0.89978 to 0.87290, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv_checkpoint/011-0.8729.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.3581 - acc: 0.8849 - val_loss: 0.8729 - val_acc: 0.7727\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8992\n",
      "Epoch 00012: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.3104 - acc: 0.8992 - val_loss: 0.9740 - val_acc: 0.7594\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9090\n",
      "Epoch 00013: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.2849 - acc: 0.9090 - val_loss: 1.0162 - val_acc: 0.7512\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9180\n",
      "Epoch 00014: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.2530 - acc: 0.9179 - val_loss: 0.9931 - val_acc: 0.7638\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9239\n",
      "Epoch 00015: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.2294 - acc: 0.9238 - val_loss: 1.1136 - val_acc: 0.7533\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9323\n",
      "Epoch 00016: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.2135 - acc: 0.9323 - val_loss: 1.0312 - val_acc: 0.7692\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9317\n",
      "Epoch 00017: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.2123 - acc: 0.9317 - val_loss: 1.0936 - val_acc: 0.7552\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9404\n",
      "Epoch 00018: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1848 - acc: 0.9404 - val_loss: 1.1076 - val_acc: 0.7577\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9429\n",
      "Epoch 00019: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1711 - acc: 0.9428 - val_loss: 1.1612 - val_acc: 0.7447\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9424\n",
      "Epoch 00020: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1816 - acc: 0.9424 - val_loss: 1.0513 - val_acc: 0.7713\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9540\n",
      "Epoch 00021: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1440 - acc: 0.9541 - val_loss: 0.9922 - val_acc: 0.7871\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9497\n",
      "Epoch 00022: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.1588 - acc: 0.9497 - val_loss: 1.0980 - val_acc: 0.7596\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9506\n",
      "Epoch 00023: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1527 - acc: 0.9506 - val_loss: 1.2385 - val_acc: 0.7580\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9566\n",
      "Epoch 00024: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1363 - acc: 0.9566 - val_loss: 1.0697 - val_acc: 0.7862\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9566\n",
      "Epoch 00025: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1364 - acc: 0.9566 - val_loss: 1.0268 - val_acc: 0.7885\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9592\n",
      "Epoch 00026: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.1285 - acc: 0.9591 - val_loss: 1.0996 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9598\n",
      "Epoch 00027: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.1260 - acc: 0.9597 - val_loss: 1.1688 - val_acc: 0.7636\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9600\n",
      "Epoch 00028: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1276 - acc: 0.9600 - val_loss: 1.0910 - val_acc: 0.7841\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9604\n",
      "Epoch 00029: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1259 - acc: 0.9604 - val_loss: 1.1379 - val_acc: 0.7815\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9624\n",
      "Epoch 00030: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1166 - acc: 0.9623 - val_loss: 1.1302 - val_acc: 0.7859\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9664\n",
      "Epoch 00031: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1094 - acc: 0.9663 - val_loss: 1.1901 - val_acc: 0.7738\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9655\n",
      "Epoch 00032: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1159 - acc: 0.9655 - val_loss: 1.0698 - val_acc: 0.7962\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9688\n",
      "Epoch 00033: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1002 - acc: 0.9688 - val_loss: 1.1412 - val_acc: 0.7855\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9684\n",
      "Epoch 00034: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.1014 - acc: 0.9684 - val_loss: 1.0469 - val_acc: 0.7945\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9705\n",
      "Epoch 00035: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0944 - acc: 0.9705 - val_loss: 1.0866 - val_acc: 0.7862\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9720\n",
      "Epoch 00036: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0880 - acc: 0.9720 - val_loss: 1.1978 - val_acc: 0.7720\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9712\n",
      "Epoch 00037: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0955 - acc: 0.9712 - val_loss: 1.1385 - val_acc: 0.7873\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9694\n",
      "Epoch 00038: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1018 - acc: 0.9694 - val_loss: 1.1015 - val_acc: 0.7978\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9723\n",
      "Epoch 00039: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0887 - acc: 0.9722 - val_loss: 1.1384 - val_acc: 0.7890\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9747\n",
      "Epoch 00040: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0826 - acc: 0.9747 - val_loss: 1.0654 - val_acc: 0.7911\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9736\n",
      "Epoch 00041: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0875 - acc: 0.9736 - val_loss: 1.0935 - val_acc: 0.7969\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9745\n",
      "Epoch 00042: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0833 - acc: 0.9745 - val_loss: 1.0941 - val_acc: 0.8022\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9755\n",
      "Epoch 00043: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0809 - acc: 0.9755 - val_loss: 1.3307 - val_acc: 0.7673\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9743\n",
      "Epoch 00044: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0861 - acc: 0.9742 - val_loss: 1.1946 - val_acc: 0.7894\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9760\n",
      "Epoch 00045: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0817 - acc: 0.9760 - val_loss: 1.2224 - val_acc: 0.7778\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9763\n",
      "Epoch 00046: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0763 - acc: 0.9763 - val_loss: 1.1217 - val_acc: 0.8001\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9773\n",
      "Epoch 00047: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0730 - acc: 0.9773 - val_loss: 1.1842 - val_acc: 0.7990\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9783\n",
      "Epoch 00048: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0737 - acc: 0.9783 - val_loss: 1.1242 - val_acc: 0.7964\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9787\n",
      "Epoch 00049: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0733 - acc: 0.9787 - val_loss: 1.1567 - val_acc: 0.7941\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9796\n",
      "Epoch 00050: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0676 - acc: 0.9796 - val_loss: 1.1879 - val_acc: 0.7925\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9814\n",
      "Epoch 00051: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0641 - acc: 0.9814 - val_loss: 1.1989 - val_acc: 0.7913\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9776\n",
      "Epoch 00052: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0801 - acc: 0.9775 - val_loss: 1.2621 - val_acc: 0.7880\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9775\n",
      "Epoch 00053: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0772 - acc: 0.9775 - val_loss: 1.1295 - val_acc: 0.7980\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9805\n",
      "Epoch 00054: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0637 - acc: 0.9805 - val_loss: 1.1418 - val_acc: 0.8020\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9812\n",
      "Epoch 00055: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0631 - acc: 0.9812 - val_loss: 1.1863 - val_acc: 0.7985\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9820\n",
      "Epoch 00056: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0575 - acc: 0.9820 - val_loss: 1.2341 - val_acc: 0.7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9826\n",
      "Epoch 00057: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.0604 - acc: 0.9826 - val_loss: 1.2493 - val_acc: 0.7880\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9823\n",
      "Epoch 00058: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0615 - acc: 0.9823 - val_loss: 1.2171 - val_acc: 0.7941\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9789\n",
      "Epoch 00059: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0727 - acc: 0.9788 - val_loss: 1.1580 - val_acc: 0.7901\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9809\n",
      "Epoch 00060: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0642 - acc: 0.9809 - val_loss: 1.2272 - val_acc: 0.7913\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9823\n",
      "Epoch 00061: val_loss did not improve from 0.87290\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0598 - acc: 0.9823 - val_loss: 1.1481 - val_acc: 0.8046\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 0.9776 - acc: 0.7290\n",
      "Loss: 0.977601520146165 Accuracy: 0.72897196\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0281 - acc: 0.4079\n",
      "Epoch 00001: val_loss improved from inf to 1.46546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/001-1.4655.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 2.0281 - acc: 0.4079 - val_loss: 1.4655 - val_acc: 0.5630\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1921 - acc: 0.6388\n",
      "Epoch 00002: val_loss improved from 1.46546 to 0.97696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/002-0.9770.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 1.1924 - acc: 0.6388 - val_loss: 0.9770 - val_acc: 0.7018\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9684 - acc: 0.7096\n",
      "Epoch 00003: val_loss did not improve from 0.97696\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.9684 - acc: 0.7096 - val_loss: 1.0019 - val_acc: 0.7028\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8317 - acc: 0.7512\n",
      "Epoch 00004: val_loss improved from 0.97696 to 0.79288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/004-0.7929.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.8317 - acc: 0.7512 - val_loss: 0.7929 - val_acc: 0.7782\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7807\n",
      "Epoch 00005: val_loss improved from 0.79288 to 0.66333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/005-0.6633.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.7439 - acc: 0.7806 - val_loss: 0.6633 - val_acc: 0.8111\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6654 - acc: 0.8013\n",
      "Epoch 00006: val_loss did not improve from 0.66333\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.6655 - acc: 0.8013 - val_loss: 0.7486 - val_acc: 0.7904\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6228 - acc: 0.8122\n",
      "Epoch 00007: val_loss did not improve from 0.66333\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.6230 - acc: 0.8121 - val_loss: 0.7119 - val_acc: 0.7971\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.8321\n",
      "Epoch 00008: val_loss improved from 0.66333 to 0.60524, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/008-0.6052.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.5633 - acc: 0.8321 - val_loss: 0.6052 - val_acc: 0.8241\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8456\n",
      "Epoch 00009: val_loss did not improve from 0.60524\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.5148 - acc: 0.8456 - val_loss: 0.6921 - val_acc: 0.8143\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8526\n",
      "Epoch 00010: val_loss did not improve from 0.60524\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.4783 - acc: 0.8526 - val_loss: 0.6649 - val_acc: 0.8288\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8652\n",
      "Epoch 00011: val_loss did not improve from 0.60524\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.4350 - acc: 0.8651 - val_loss: 0.6354 - val_acc: 0.8265\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8752\n",
      "Epoch 00012: val_loss did not improve from 0.60524\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.4022 - acc: 0.8752 - val_loss: 0.6656 - val_acc: 0.8234\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8857\n",
      "Epoch 00013: val_loss improved from 0.60524 to 0.59768, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv_checkpoint/013-0.5977.hdf5\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.3710 - acc: 0.8857 - val_loss: 0.5977 - val_acc: 0.8432\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8955\n",
      "Epoch 00014: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.3301 - acc: 0.8955 - val_loss: 0.6633 - val_acc: 0.8325\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9041\n",
      "Epoch 00015: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.3045 - acc: 0.9041 - val_loss: 0.8299 - val_acc: 0.7883\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9071\n",
      "Epoch 00016: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.2950 - acc: 0.9071 - val_loss: 0.6325 - val_acc: 0.8323\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9176\n",
      "Epoch 00017: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.2567 - acc: 0.9176 - val_loss: 0.6387 - val_acc: 0.8451\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9218\n",
      "Epoch 00018: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.2464 - acc: 0.9218 - val_loss: 0.6139 - val_acc: 0.8507\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9257\n",
      "Epoch 00019: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.2296 - acc: 0.9257 - val_loss: 0.6988 - val_acc: 0.8421\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9354\n",
      "Epoch 00020: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1999 - acc: 0.9354 - val_loss: 0.7563 - val_acc: 0.8281\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9367\n",
      "Epoch 00021: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1960 - acc: 0.9367 - val_loss: 0.7158 - val_acc: 0.8351\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9395\n",
      "Epoch 00022: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.1900 - acc: 0.9395 - val_loss: 0.6898 - val_acc: 0.8460\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9445\n",
      "Epoch 00023: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1725 - acc: 0.9444 - val_loss: 0.6828 - val_acc: 0.8519\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9468\n",
      "Epoch 00024: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1679 - acc: 0.9468 - val_loss: 0.7366 - val_acc: 0.8395\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9488\n",
      "Epoch 00025: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1603 - acc: 0.9488 - val_loss: 0.6906 - val_acc: 0.8498\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9533\n",
      "Epoch 00026: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1433 - acc: 0.9533 - val_loss: 0.7541 - val_acc: 0.8421\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9549\n",
      "Epoch 00027: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1411 - acc: 0.9549 - val_loss: 0.6735 - val_acc: 0.8556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9576\n",
      "Epoch 00028: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1313 - acc: 0.9576 - val_loss: 0.7071 - val_acc: 0.8477\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9598\n",
      "Epoch 00029: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1298 - acc: 0.9598 - val_loss: 0.7322 - val_acc: 0.8493\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9584\n",
      "Epoch 00030: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1276 - acc: 0.9584 - val_loss: 0.7108 - val_acc: 0.8509\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9590\n",
      "Epoch 00031: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1287 - acc: 0.9590 - val_loss: 0.8081 - val_acc: 0.8328\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9624\n",
      "Epoch 00032: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1144 - acc: 0.9624 - val_loss: 0.7113 - val_acc: 0.8588\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9647\n",
      "Epoch 00033: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1106 - acc: 0.9647 - val_loss: 0.7233 - val_acc: 0.8595\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9655\n",
      "Epoch 00034: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.1100 - acc: 0.9655 - val_loss: 0.6877 - val_acc: 0.8558\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9679\n",
      "Epoch 00035: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1016 - acc: 0.9679 - val_loss: 0.8467 - val_acc: 0.8353\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9686\n",
      "Epoch 00036: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1026 - acc: 0.9686 - val_loss: 0.7801 - val_acc: 0.8421\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9684\n",
      "Epoch 00037: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0979 - acc: 0.9684 - val_loss: 0.7203 - val_acc: 0.8577\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9691\n",
      "Epoch 00038: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0968 - acc: 0.9691 - val_loss: 0.7422 - val_acc: 0.8558\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9685\n",
      "Epoch 00039: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0994 - acc: 0.9685 - val_loss: 0.7076 - val_acc: 0.8546\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9731\n",
      "Epoch 00040: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0858 - acc: 0.9730 - val_loss: 0.7626 - val_acc: 0.8549\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9664\n",
      "Epoch 00041: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1082 - acc: 0.9663 - val_loss: 0.8313 - val_acc: 0.8374\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9719\n",
      "Epoch 00042: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0883 - acc: 0.9719 - val_loss: 0.7946 - val_acc: 0.8446\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9757\n",
      "Epoch 00043: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0790 - acc: 0.9756 - val_loss: 0.7671 - val_acc: 0.8521\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9724\n",
      "Epoch 00044: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0896 - acc: 0.9724 - val_loss: 0.7494 - val_acc: 0.8579\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9777\n",
      "Epoch 00045: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0734 - acc: 0.9776 - val_loss: 0.8349 - val_acc: 0.8493\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9721\n",
      "Epoch 00046: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0883 - acc: 0.9722 - val_loss: 0.7238 - val_acc: 0.8654\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9771\n",
      "Epoch 00047: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0735 - acc: 0.9771 - val_loss: 0.7800 - val_acc: 0.8600\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9765\n",
      "Epoch 00048: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0771 - acc: 0.9765 - val_loss: 0.8282 - val_acc: 0.8477\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9769\n",
      "Epoch 00049: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0768 - acc: 0.9769 - val_loss: 0.8484 - val_acc: 0.8376\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9737\n",
      "Epoch 00050: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0829 - acc: 0.9737 - val_loss: 0.8695 - val_acc: 0.8453\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9744\n",
      "Epoch 00051: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0827 - acc: 0.9744 - val_loss: 0.7866 - val_acc: 0.8539\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9756\n",
      "Epoch 00052: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0782 - acc: 0.9756 - val_loss: 0.8370 - val_acc: 0.8532\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9785\n",
      "Epoch 00053: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0683 - acc: 0.9785 - val_loss: 0.9542 - val_acc: 0.8297\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9777\n",
      "Epoch 00054: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0723 - acc: 0.9777 - val_loss: 0.8445 - val_acc: 0.8458\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9816\n",
      "Epoch 00055: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0594 - acc: 0.9816 - val_loss: 0.8810 - val_acc: 0.8416\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9778\n",
      "Epoch 00056: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0726 - acc: 0.9778 - val_loss: 0.8854 - val_acc: 0.8425\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9786\n",
      "Epoch 00057: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0673 - acc: 0.9786 - val_loss: 0.7462 - val_acc: 0.8689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9827\n",
      "Epoch 00058: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0560 - acc: 0.9827 - val_loss: 0.8125 - val_acc: 0.8567\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9804\n",
      "Epoch 00059: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0633 - acc: 0.9804 - val_loss: 0.8515 - val_acc: 0.8512\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9812\n",
      "Epoch 00060: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0609 - acc: 0.9813 - val_loss: 0.7796 - val_acc: 0.8635\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9808\n",
      "Epoch 00061: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0633 - acc: 0.9808 - val_loss: 0.8366 - val_acc: 0.8495\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9792\n",
      "Epoch 00062: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0703 - acc: 0.9792 - val_loss: 0.7747 - val_acc: 0.8612\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9825\n",
      "Epoch 00063: val_loss did not improve from 0.59768\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0567 - acc: 0.9825 - val_loss: 0.8899 - val_acc: 0.8411\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.6927 - acc: 0.8135\n",
      "Loss: 0.6927284018520502 Accuracy: 0.8134995\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9785 - acc: 0.4026\n",
      "Epoch 00001: val_loss improved from inf to 1.25330, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/001-1.2533.hdf5\n",
      "36805/36805 [==============================] - 316s 9ms/sample - loss: 1.9784 - acc: 0.4027 - val_loss: 1.2533 - val_acc: 0.6000\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1228 - acc: 0.6543\n",
      "Epoch 00002: val_loss improved from 1.25330 to 0.84973, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/002-0.8497.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 1.1230 - acc: 0.6542 - val_loss: 0.8497 - val_acc: 0.7414\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8815 - acc: 0.7336\n",
      "Epoch 00003: val_loss improved from 0.84973 to 0.67047, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/003-0.6705.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.8816 - acc: 0.7336 - val_loss: 0.6705 - val_acc: 0.8081\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.7798\n",
      "Epoch 00004: val_loss improved from 0.67047 to 0.63714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/004-0.6371.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.7363 - acc: 0.7797 - val_loss: 0.6371 - val_acc: 0.8262\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.8108\n",
      "Epoch 00005: val_loss improved from 0.63714 to 0.60032, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/005-0.6003.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.6353 - acc: 0.8107 - val_loss: 0.6003 - val_acc: 0.8321\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8364\n",
      "Epoch 00006: val_loss did not improve from 0.60032\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.5529 - acc: 0.8364 - val_loss: 0.6178 - val_acc: 0.8160\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.8516\n",
      "Epoch 00007: val_loss improved from 0.60032 to 0.43385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/007-0.4338.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.4966 - acc: 0.8516 - val_loss: 0.4338 - val_acc: 0.8765\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8658\n",
      "Epoch 00008: val_loss did not improve from 0.43385\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.4432 - acc: 0.8658 - val_loss: 0.4395 - val_acc: 0.8772\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8781\n",
      "Epoch 00009: val_loss did not improve from 0.43385\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.4078 - acc: 0.8780 - val_loss: 0.4493 - val_acc: 0.8724\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8892\n",
      "Epoch 00010: val_loss did not improve from 0.43385\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.3626 - acc: 0.8891 - val_loss: 0.4893 - val_acc: 0.8703\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8987\n",
      "Epoch 00011: val_loss improved from 0.43385 to 0.37880, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/011-0.3788.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.3333 - acc: 0.8987 - val_loss: 0.3788 - val_acc: 0.8973\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9058\n",
      "Epoch 00012: val_loss did not improve from 0.37880\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.3072 - acc: 0.9058 - val_loss: 0.3808 - val_acc: 0.9015\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9117\n",
      "Epoch 00013: val_loss did not improve from 0.37880\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.2850 - acc: 0.9118 - val_loss: 0.4861 - val_acc: 0.8677\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9202\n",
      "Epoch 00014: val_loss did not improve from 0.37880\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.2589 - acc: 0.9203 - val_loss: 0.3965 - val_acc: 0.8982\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9224\n",
      "Epoch 00015: val_loss did not improve from 0.37880\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.2426 - acc: 0.9225 - val_loss: 0.4053 - val_acc: 0.8928\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9279\n",
      "Epoch 00016: val_loss did not improve from 0.37880\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.2321 - acc: 0.9279 - val_loss: 0.4206 - val_acc: 0.8968\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9354\n",
      "Epoch 00017: val_loss improved from 0.37880 to 0.36650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/017-0.3665.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.2079 - acc: 0.9354 - val_loss: 0.3665 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9388\n",
      "Epoch 00018: val_loss did not improve from 0.36650\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.1927 - acc: 0.9386 - val_loss: 0.4400 - val_acc: 0.8859\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9374\n",
      "Epoch 00019: val_loss did not improve from 0.36650\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.1977 - acc: 0.9373 - val_loss: 0.3918 - val_acc: 0.9115\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9441\n",
      "Epoch 00020: val_loss did not improve from 0.36650\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1727 - acc: 0.9440 - val_loss: 0.4290 - val_acc: 0.8980\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9447\n",
      "Epoch 00021: val_loss improved from 0.36650 to 0.36293, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/021-0.3629.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1700 - acc: 0.9447 - val_loss: 0.3629 - val_acc: 0.9094\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9482\n",
      "Epoch 00022: val_loss did not improve from 0.36293\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.1636 - acc: 0.9482 - val_loss: 0.3844 - val_acc: 0.9026\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9542\n",
      "Epoch 00023: val_loss improved from 0.36293 to 0.36238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/023-0.3624.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.1447 - acc: 0.9542 - val_loss: 0.3624 - val_acc: 0.9136\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9595\n",
      "Epoch 00024: val_loss did not improve from 0.36238\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1271 - acc: 0.9595 - val_loss: 0.4389 - val_acc: 0.9024\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9579\n",
      "Epoch 00025: val_loss did not improve from 0.36238\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.1282 - acc: 0.9579 - val_loss: 0.4095 - val_acc: 0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9601\n",
      "Epoch 00026: val_loss improved from 0.36238 to 0.35368, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv_checkpoint/026-0.3537.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1246 - acc: 0.9601 - val_loss: 0.3537 - val_acc: 0.9196\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9601\n",
      "Epoch 00027: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.1228 - acc: 0.9601 - val_loss: 0.3930 - val_acc: 0.9068\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9638\n",
      "Epoch 00028: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1114 - acc: 0.9638 - val_loss: 0.4073 - val_acc: 0.9066\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9645\n",
      "Epoch 00029: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1083 - acc: 0.9645 - val_loss: 0.3957 - val_acc: 0.9126\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9670\n",
      "Epoch 00030: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.1019 - acc: 0.9670 - val_loss: 0.3583 - val_acc: 0.9210\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9700\n",
      "Epoch 00031: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0925 - acc: 0.9700 - val_loss: 0.3751 - val_acc: 0.9175\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9711\n",
      "Epoch 00032: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0903 - acc: 0.9710 - val_loss: 0.4106 - val_acc: 0.9066\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9697\n",
      "Epoch 00033: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0926 - acc: 0.9697 - val_loss: 0.4316 - val_acc: 0.9147\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9689\n",
      "Epoch 00034: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0943 - acc: 0.9688 - val_loss: 0.4415 - val_acc: 0.9026\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9707\n",
      "Epoch 00035: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0895 - acc: 0.9707 - val_loss: 0.3814 - val_acc: 0.9152\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9754\n",
      "Epoch 00036: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0740 - acc: 0.9754 - val_loss: 0.3979 - val_acc: 0.9159\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9757\n",
      "Epoch 00037: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0762 - acc: 0.9757 - val_loss: 0.4211 - val_acc: 0.9138\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9779\n",
      "Epoch 00038: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0686 - acc: 0.9778 - val_loss: 0.3636 - val_acc: 0.9236\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9757\n",
      "Epoch 00039: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0740 - acc: 0.9757 - val_loss: 0.4955 - val_acc: 0.9012\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9772\n",
      "Epoch 00040: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0685 - acc: 0.9772 - val_loss: 0.3700 - val_acc: 0.9236\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9808\n",
      "Epoch 00041: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0614 - acc: 0.9808 - val_loss: 0.4191 - val_acc: 0.9129\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9745\n",
      "Epoch 00042: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0781 - acc: 0.9745 - val_loss: 0.4383 - val_acc: 0.9122\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9731\n",
      "Epoch 00043: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0825 - acc: 0.9730 - val_loss: 0.3564 - val_acc: 0.9236\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9778\n",
      "Epoch 00044: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0689 - acc: 0.9778 - val_loss: 0.5371 - val_acc: 0.9047\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9806\n",
      "Epoch 00045: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0600 - acc: 0.9805 - val_loss: 0.4339 - val_acc: 0.9196\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9778\n",
      "Epoch 00046: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0697 - acc: 0.9778 - val_loss: 0.4328 - val_acc: 0.9143\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9829\n",
      "Epoch 00047: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0544 - acc: 0.9829 - val_loss: 0.4142 - val_acc: 0.9154\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00048: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0699 - acc: 0.9777 - val_loss: 0.4212 - val_acc: 0.9241\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9799\n",
      "Epoch 00049: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0606 - acc: 0.9799 - val_loss: 0.3794 - val_acc: 0.9259\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9853\n",
      "Epoch 00050: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0477 - acc: 0.9852 - val_loss: 0.4908 - val_acc: 0.9050\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9796\n",
      "Epoch 00051: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0646 - acc: 0.9795 - val_loss: 0.4285 - val_acc: 0.9185\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9804\n",
      "Epoch 00052: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0618 - acc: 0.9804 - val_loss: 0.4247 - val_acc: 0.9252\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9871\n",
      "Epoch 00053: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0426 - acc: 0.9870 - val_loss: 0.5169 - val_acc: 0.9138\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9776\n",
      "Epoch 00054: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0667 - acc: 0.9776 - val_loss: 0.3728 - val_acc: 0.9299\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9865\n",
      "Epoch 00055: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0429 - acc: 0.9865 - val_loss: 0.3827 - val_acc: 0.9227\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9858\n",
      "Epoch 00056: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0448 - acc: 0.9858 - val_loss: 0.3682 - val_acc: 0.9315\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9861\n",
      "Epoch 00057: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0456 - acc: 0.9861 - val_loss: 0.4022 - val_acc: 0.9271\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9850\n",
      "Epoch 00058: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0473 - acc: 0.9849 - val_loss: 0.4617 - val_acc: 0.9136\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9844\n",
      "Epoch 00059: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0502 - acc: 0.9843 - val_loss: 0.4147 - val_acc: 0.9189\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9816\n",
      "Epoch 00060: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0560 - acc: 0.9815 - val_loss: 0.4148 - val_acc: 0.9220\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9813\n",
      "Epoch 00061: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0597 - acc: 0.9813 - val_loss: 0.4290 - val_acc: 0.9199\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9853\n",
      "Epoch 00062: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0481 - acc: 0.9853 - val_loss: 0.3778 - val_acc: 0.9285\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9877\n",
      "Epoch 00063: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0378 - acc: 0.9877 - val_loss: 0.4709 - val_acc: 0.9143\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9892\n",
      "Epoch 00064: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0349 - acc: 0.9892 - val_loss: 0.3908 - val_acc: 0.9290\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9837\n",
      "Epoch 00065: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0538 - acc: 0.9837 - val_loss: 0.3730 - val_acc: 0.9257\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9896\n",
      "Epoch 00066: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0351 - acc: 0.9895 - val_loss: 0.4553 - val_acc: 0.9229\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9830\n",
      "Epoch 00067: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0546 - acc: 0.9829 - val_loss: 0.4552 - val_acc: 0.9154\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9842\n",
      "Epoch 00068: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0532 - acc: 0.9842 - val_loss: 0.4702 - val_acc: 0.9196\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9888\n",
      "Epoch 00069: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0351 - acc: 0.9888 - val_loss: 0.4400 - val_acc: 0.9236\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9905\n",
      "Epoch 00070: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0316 - acc: 0.9905 - val_loss: 0.3731 - val_acc: 0.9352\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9871\n",
      "Epoch 00071: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0404 - acc: 0.9871 - val_loss: 0.4783 - val_acc: 0.9122\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9847\n",
      "Epoch 00072: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0484 - acc: 0.9847 - val_loss: 0.4234 - val_acc: 0.9255\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9896\n",
      "Epoch 00073: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0334 - acc: 0.9895 - val_loss: 0.5610 - val_acc: 0.9106\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9838\n",
      "Epoch 00074: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0531 - acc: 0.9837 - val_loss: 0.5704 - val_acc: 0.9068\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9847\n",
      "Epoch 00075: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0490 - acc: 0.9846 - val_loss: 0.4270 - val_acc: 0.9285\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9887\n",
      "Epoch 00076: val_loss did not improve from 0.35368\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0383 - acc: 0.9886 - val_loss: 0.4407 - val_acc: 0.9154\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.4132 - acc: 0.8966\n",
      "Loss: 0.41316051120760533 Accuracy: 0.8965732\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1789 - acc: 0.3439\n",
      "Epoch 00001: val_loss improved from inf to 1.17821, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/001-1.1782.hdf5\n",
      "36805/36805 [==============================] - 324s 9ms/sample - loss: 2.1789 - acc: 0.3439 - val_loss: 1.1782 - val_acc: 0.6408\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6392\n",
      "Epoch 00002: val_loss improved from 1.17821 to 0.72384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/002-0.7238.hdf5\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 1.1561 - acc: 0.6391 - val_loss: 0.7238 - val_acc: 0.7936\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7980 - acc: 0.7564\n",
      "Epoch 00003: val_loss improved from 0.72384 to 0.49404, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/003-0.4940.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.7981 - acc: 0.7564 - val_loss: 0.4940 - val_acc: 0.8584\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.8212\n",
      "Epoch 00004: val_loss improved from 0.49404 to 0.41863, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/004-0.4186.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.6007 - acc: 0.8212 - val_loss: 0.4186 - val_acc: 0.8791\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8539\n",
      "Epoch 00005: val_loss improved from 0.41863 to 0.35410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/005-0.3541.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.4859 - acc: 0.8539 - val_loss: 0.3541 - val_acc: 0.9052\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8784\n",
      "Epoch 00006: val_loss improved from 0.35410 to 0.30850, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/006-0.3085.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.3996 - acc: 0.8783 - val_loss: 0.3085 - val_acc: 0.9131\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8910\n",
      "Epoch 00007: val_loss improved from 0.30850 to 0.28186, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/007-0.2819.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.3592 - acc: 0.8910 - val_loss: 0.2819 - val_acc: 0.9236\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.9007\n",
      "Epoch 00008: val_loss did not improve from 0.28186\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.3194 - acc: 0.9006 - val_loss: 0.3206 - val_acc: 0.9082\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9134\n",
      "Epoch 00009: val_loss improved from 0.28186 to 0.27239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/009-0.2724.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2764 - acc: 0.9133 - val_loss: 0.2724 - val_acc: 0.9245\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9230\n",
      "Epoch 00010: val_loss improved from 0.27239 to 0.24433, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/010-0.2443.hdf5\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.2460 - acc: 0.9230 - val_loss: 0.2443 - val_acc: 0.9338\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9308\n",
      "Epoch 00011: val_loss did not improve from 0.24433\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2208 - acc: 0.9309 - val_loss: 0.2727 - val_acc: 0.9203\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9357\n",
      "Epoch 00012: val_loss improved from 0.24433 to 0.21411, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/012-0.2141.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2036 - acc: 0.9357 - val_loss: 0.2141 - val_acc: 0.9420\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9427\n",
      "Epoch 00013: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1781 - acc: 0.9427 - val_loss: 0.2504 - val_acc: 0.9257\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9436\n",
      "Epoch 00014: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.1775 - acc: 0.9436 - val_loss: 0.2253 - val_acc: 0.9390\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9469\n",
      "Epoch 00015: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1674 - acc: 0.9469 - val_loss: 0.2257 - val_acc: 0.9399\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9522\n",
      "Epoch 00016: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1474 - acc: 0.9521 - val_loss: 0.2175 - val_acc: 0.9411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9525\n",
      "Epoch 00017: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1473 - acc: 0.9525 - val_loss: 0.2699 - val_acc: 0.9278\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9594\n",
      "Epoch 00018: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.1223 - acc: 0.9594 - val_loss: 0.2322 - val_acc: 0.9399\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9626\n",
      "Epoch 00019: val_loss did not improve from 0.21411\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1164 - acc: 0.9625 - val_loss: 0.2400 - val_acc: 0.9348\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9608\n",
      "Epoch 00020: val_loss improved from 0.21411 to 0.20898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/020-0.2090.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1206 - acc: 0.9608 - val_loss: 0.2090 - val_acc: 0.9455\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9639\n",
      "Epoch 00021: val_loss improved from 0.20898 to 0.19277, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/021-0.1928.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1105 - acc: 0.9639 - val_loss: 0.1928 - val_acc: 0.9474\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9688\n",
      "Epoch 00022: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0962 - acc: 0.9688 - val_loss: 0.2183 - val_acc: 0.9483\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9698\n",
      "Epoch 00023: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0932 - acc: 0.9698 - val_loss: 0.2329 - val_acc: 0.9427\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9700\n",
      "Epoch 00024: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0910 - acc: 0.9700 - val_loss: 0.2108 - val_acc: 0.9469\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9718\n",
      "Epoch 00025: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0854 - acc: 0.9717 - val_loss: 0.2021 - val_acc: 0.9469\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9735\n",
      "Epoch 00026: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0799 - acc: 0.9734 - val_loss: 0.2988 - val_acc: 0.9329\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9739\n",
      "Epoch 00027: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0762 - acc: 0.9739 - val_loss: 0.2341 - val_acc: 0.9453\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9786\n",
      "Epoch 00028: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0670 - acc: 0.9786 - val_loss: 0.2167 - val_acc: 0.9504\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9807\n",
      "Epoch 00029: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0595 - acc: 0.9807 - val_loss: 0.2077 - val_acc: 0.9499\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9762\n",
      "Epoch 00030: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0737 - acc: 0.9762 - val_loss: 0.2073 - val_acc: 0.9483\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9812\n",
      "Epoch 00031: val_loss did not improve from 0.19277\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0572 - acc: 0.9811 - val_loss: 0.3006 - val_acc: 0.9264\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9774\n",
      "Epoch 00032: val_loss improved from 0.19277 to 0.19180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/032-0.1918.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0659 - acc: 0.9774 - val_loss: 0.1918 - val_acc: 0.9506\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9771\n",
      "Epoch 00033: val_loss did not improve from 0.19180\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0685 - acc: 0.9771 - val_loss: 0.2090 - val_acc: 0.9513\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9837\n",
      "Epoch 00034: val_loss did not improve from 0.19180\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0500 - acc: 0.9836 - val_loss: 0.2284 - val_acc: 0.9457\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9808\n",
      "Epoch 00035: val_loss did not improve from 0.19180\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0587 - acc: 0.9808 - val_loss: 0.2045 - val_acc: 0.9555\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9858\n",
      "Epoch 00036: val_loss improved from 0.19180 to 0.19055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/036-0.1906.hdf5\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0445 - acc: 0.9858 - val_loss: 0.1906 - val_acc: 0.9555\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9865\n",
      "Epoch 00037: val_loss did not improve from 0.19055\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0422 - acc: 0.9865 - val_loss: 0.2216 - val_acc: 0.9443\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9845\n",
      "Epoch 00038: val_loss did not improve from 0.19055\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0495 - acc: 0.9845 - val_loss: 0.3113 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9820\n",
      "Epoch 00039: val_loss did not improve from 0.19055\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0562 - acc: 0.9820 - val_loss: 0.2364 - val_acc: 0.9499\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9840\n",
      "Epoch 00040: val_loss did not improve from 0.19055\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0485 - acc: 0.9840 - val_loss: 0.2078 - val_acc: 0.9557\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9897\n",
      "Epoch 00041: val_loss improved from 0.19055 to 0.18875, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv_checkpoint/041-0.1888.hdf5\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0329 - acc: 0.9897 - val_loss: 0.1888 - val_acc: 0.9588\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9895\n",
      "Epoch 00042: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0317 - acc: 0.9895 - val_loss: 0.2404 - val_acc: 0.9495\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9879\n",
      "Epoch 00043: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0378 - acc: 0.9879 - val_loss: 0.2249 - val_acc: 0.9529\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9881\n",
      "Epoch 00044: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0362 - acc: 0.9881 - val_loss: 0.1928 - val_acc: 0.9569\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9896\n",
      "Epoch 00045: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0327 - acc: 0.9895 - val_loss: 0.2782 - val_acc: 0.9418\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9863\n",
      "Epoch 00046: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0420 - acc: 0.9863 - val_loss: 0.1986 - val_acc: 0.9574\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9866\n",
      "Epoch 00047: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0408 - acc: 0.9866 - val_loss: 0.2336 - val_acc: 0.9515\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9919\n",
      "Epoch 00048: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.2362 - val_acc: 0.9478\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9861\n",
      "Epoch 00049: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0421 - acc: 0.9861 - val_loss: 0.2156 - val_acc: 0.9539\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9928\n",
      "Epoch 00050: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0233 - acc: 0.9928 - val_loss: 0.2299 - val_acc: 0.9527\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9924\n",
      "Epoch 00051: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0236 - acc: 0.9924 - val_loss: 0.2767 - val_acc: 0.9436\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9849\n",
      "Epoch 00052: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0472 - acc: 0.9849 - val_loss: 0.2442 - val_acc: 0.9520\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 00053: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0232 - acc: 0.9929 - val_loss: 0.2418 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9899\n",
      "Epoch 00054: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0297 - acc: 0.9899 - val_loss: 0.2195 - val_acc: 0.9546\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915\n",
      "Epoch 00055: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.2173 - val_acc: 0.9534\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9884\n",
      "Epoch 00056: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0346 - acc: 0.9884 - val_loss: 0.2144 - val_acc: 0.9569\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00057: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.2172 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9940\n",
      "Epoch 00058: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0192 - acc: 0.9940 - val_loss: 0.2396 - val_acc: 0.9546\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9927\n",
      "Epoch 00059: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0233 - acc: 0.9927 - val_loss: 0.2681 - val_acc: 0.9469\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9895\n",
      "Epoch 00060: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0311 - acc: 0.9894 - val_loss: 0.2266 - val_acc: 0.9532\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9878\n",
      "Epoch 00061: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0378 - acc: 0.9878 - val_loss: 0.2487 - val_acc: 0.9518\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9936\n",
      "Epoch 00062: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0206 - acc: 0.9936 - val_loss: 0.2184 - val_acc: 0.9527\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9902\n",
      "Epoch 00063: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.2307 - val_acc: 0.9532\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9929\n",
      "Epoch 00064: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0215 - acc: 0.9929 - val_loss: 0.2200 - val_acc: 0.9555\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9957\n",
      "Epoch 00065: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0151 - acc: 0.9956 - val_loss: 0.2559 - val_acc: 0.9515\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9913\n",
      "Epoch 00066: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0282 - acc: 0.9913 - val_loss: 0.2424 - val_acc: 0.9527\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9928\n",
      "Epoch 00067: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0227 - acc: 0.9927 - val_loss: 0.2702 - val_acc: 0.9485\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9894\n",
      "Epoch 00068: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0354 - acc: 0.9894 - val_loss: 0.2391 - val_acc: 0.9546\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00069: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.3189 - val_acc: 0.9464\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 00070: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0161 - acc: 0.9951 - val_loss: 0.2091 - val_acc: 0.9560\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9910\n",
      "Epoch 00071: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0264 - acc: 0.9910 - val_loss: 0.2624 - val_acc: 0.9520\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9930\n",
      "Epoch 00072: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0225 - acc: 0.9930 - val_loss: 0.2190 - val_acc: 0.9590\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 00073: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0127 - acc: 0.9962 - val_loss: 0.2359 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9939\n",
      "Epoch 00074: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0187 - acc: 0.9939 - val_loss: 0.3798 - val_acc: 0.9373\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9905\n",
      "Epoch 00075: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0299 - acc: 0.9904 - val_loss: 0.2294 - val_acc: 0.9581\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 00076: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0294 - acc: 0.9908 - val_loss: 0.2540 - val_acc: 0.9506\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9960\n",
      "Epoch 00077: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0129 - acc: 0.9960 - val_loss: 0.2139 - val_acc: 0.9611\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9959\n",
      "Epoch 00078: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0129 - acc: 0.9958 - val_loss: 0.2597 - val_acc: 0.9583\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9925\n",
      "Epoch 00079: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0268 - acc: 0.9925 - val_loss: 0.2358 - val_acc: 0.9564\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9958\n",
      "Epoch 00080: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.2274 - val_acc: 0.9609\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9960\n",
      "Epoch 00081: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0129 - acc: 0.9960 - val_loss: 0.2592 - val_acc: 0.9527\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9947\n",
      "Epoch 00082: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0157 - acc: 0.9946 - val_loss: 0.2680 - val_acc: 0.9460\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9897\n",
      "Epoch 00083: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0329 - acc: 0.9897 - val_loss: 0.2341 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 00084: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0142 - acc: 0.9958 - val_loss: 0.2284 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9916\n",
      "Epoch 00085: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0245 - acc: 0.9916 - val_loss: 0.2210 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9938\n",
      "Epoch 00086: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0191 - acc: 0.9938 - val_loss: 0.2043 - val_acc: 0.9602\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
      "Epoch 00087: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.2055 - val_acc: 0.9564\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9943\n",
      "Epoch 00088: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0193 - acc: 0.9942 - val_loss: 0.2121 - val_acc: 0.9597\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9916\n",
      "Epoch 00089: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0263 - acc: 0.9916 - val_loss: 0.1904 - val_acc: 0.9592\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 00090: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0128 - acc: 0.9954 - val_loss: 0.2384 - val_acc: 0.9522\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9968\n",
      "Epoch 00091: val_loss did not improve from 0.18875\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0111 - acc: 0.9968 - val_loss: 0.2006 - val_acc: 0.9611\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX9+PH3mclkskz2hCQkQCKyhDVsiiLgvrYoKqI/91bsYm2t/WpdW6212mpba9VatbYuVKUuVeuCqCzW4gIIyA6RBBKyrzNZZj2/P04ySchCgAwB5vN6nnmSuXPvuefeufd8zjn33jNKa40QQggBYBnoDAghhDh8SFAQQggRJEFBCCFEkAQFIYQQQRIUhBBCBElQEEIIESRBQQghRJAEBSGEEEESFIQQQgRFDHQG9ldqaqrOyckZ6GwIIcQRZfXq1VVa67R9zXfEBYWcnBxWrVo10NkQQogjilKqqC/zSfeREEKIIAkKQgghgiQoCCGECDriril0x+v1UlxcTEtLy0Bn5YgVFRVFdnY2NpttoLMihBhAR0VQKC4uJi4ujpycHJRSA52dI47WmurqaoqLi8nNzR3o7AghBtBR0X3U0tJCSkqKBIQDpJQiJSVFWlpCiKMjKAASEA6S7D8hBBxFQWFf/P5m3O4SAgHvQGdFCCEOW2ETFAKBZjyeUrTu/6BQV1fHE088cUDLnnvuudTV1fV5/nvuuYeHH374gNYlhBD7EjZBQSlr63+Bfk+7t6Dg8/l6Xfbdd98lMTGx3/MkhBAHImyCApg+c611v6d82223UVBQQH5+PrfccgvLli1j5syZzJkzhzFjxgBwwQUXMGXKFMaOHctTTz0VXDYnJ4eqqioKCwvJy8tjwYIFjB07ljPPPJPm5uZe17t27VqmT5/OhAkTmDt3LrW1tQA8+uijjBkzhgkTJnDppZcCsHz5cvLz88nPz2fSpEk4nc5+3w9CiCPfUXFLakfbt9+Ey7W2y3St/QQCTVgs0Si1f5vtcOQzYsQjPX7+4IMPsmHDBtauNetdtmwZa9asYcOGDcFbPJ999lmSk5Npbm5m2rRpXHTRRaSkpOyV9+289NJLPP3001xyySW89tprXHHFFT2u96qrruLPf/4zs2fP5he/+AX33nsvjzzyCA8++CA7d+7EbrcHu6YefvhhHn/8cWbMmIHL5SIqKmq/9oEQIjyETUvhUN9dc9xxx3W65//RRx9l4sSJTJ8+nd27d7N9+/Yuy+Tm5pKfnw/AlClTKCws7DH9+vp66urqmD17NgBXX301K1asAGDChAlcfvnlvPjii0REmAA4Y8YMbr75Zh599FHq6uqC04UQoqOjrmToqUbv97fQ1LSBqKhcbLaUbufpT7GxscH/ly1bxocffsjKlSuJiYnh5JNP7vaZALvdHvzfarXus/uoJ++88w4rVqzg7bff5v777+frr7/mtttu47zzzuPdd99lxowZLF68mNGjRx9Q+kKIo1cYtRTMpmrd/xea4+Lieu2jr6+vJykpiZiYGLZs2cJnn3120OtMSEggKSmJTz75BIAXXniB2bNnEwgE2L17N6eccgq//e1vqa+vx+VyUVBQwPjx4/n5z3/OtGnT2LJly0HnQQhx9DnqWgo9a4t//R8UUlJSmDFjBuPGjeOcc87hvPPO6/T52WefzZNPPkleXh6jRo1i+vTp/bLe5557ju9///s0NTVxzDHH8Pe//x2/388VV1xBfX09Wmt+/OMfk5iYyN13383SpUuxWCyMHTuWc845p1/yIIQ4uqhQ3I0TSlOnTtV7/8jO5s2bycvL63U5rQO4XGuIjMzCbs8MZRaPWH3Zj0KII5NSarXWeuq+5gub7qO2W1JD0VIQQoijRdgEBXP3kQrJcwpCCHG0CJugYFiQloIQQvQsrIKCUpaQ3H0khBBHi5AFBaXUEKXUUqXUJqXURqXUT7qZRymlHlVK7VBKrVdKTQ5VfgxpKQghRG9CeUuqD/iZ1nqNUioOWK2UWqK13tRhnnOAEa2v44G/tP4NCfOsggQFIYToSchaClrrUq31mtb/ncBmIGuv2c4HntfGZ0CiUiqE94sePt1HDodjv6YLIcShcEiuKSilcoBJwOd7fZQF7O7wvpiugaMf8yEtBSGE6E3Ig4JSygG8BtyktW44wDSuV0qtUkqtqqysPIjchKalcNttt/H4448H37f9EI7L5eK0005j8uTJjB8/njfffLPPaWqtueWWWxg3bhzjx4/nlVdeAaC0tJRZs2aRn5/PuHHj+OSTT/D7/VxzzTXBef/4xz/2+zYKIcJDSIe5UErZMAFhodb69W5mKQGGdHif3TqtE631U8BTYJ5o7nWlN90Ea7sOnQ1gDzSDDoA1ttvPe5SfD4/0PHT2/Pnzuemmm7jhhhsAWLRoEYsXLyYqKoo33niD+Ph4qqqqmD59OnPmzOnTiK2vv/46a9euZd26dVRVVTFt2jRmzZrFP//5T8466yzuvPNO/H4/TU1NrF27lpKSEjZs2ACwX7/kJoQQHYUsKChT8v0N2Ky1/kMPs70F/Egp9TLmAnO91ro0VHlqf6q5f02aNImKigr27NlDZWUlSUlJDBkyBK/Xyx133MGKFSuwWCyUlJRQXl5ORkbGPtP873//y2WXXYbVaiU9PZ3Zs2fz5ZdfMm3aNL7zne/g9Xq54IILyM/P55hjjuGbb77hxhtv5LzzzuPMM88MyXYKIY5+oWwpzACuBL5WSrVV3e8AhgJorZ8E3gXOBXYATcC1B73WXmr03pZCfL56HI6JB72avc2bN49XX32VsrIy5s+fD8DChQuprKxk9erV2Gw2cnJyuh0ye3/MmjWLFStW8M4773DNNddw8803c9VVV7Fu3ToWL17Mk08+yaJFi3j22Wf7Y7OEEGEmZEFBa/1f9lE112bMiRtClYeuQnf30fz581mwYAFVVVUsX74cMENmDxo0CJvNxtKlSykqKupzejNnzuSvf/0rV199NTU1NaxYsYKHHnqIoqIisrOzWbBgAW63mzVr1nDuuecSGRnJRRddxKhRo3r9tTYhhOhNGA2dHdq7j8aOHYvT6SQrK4vMTHNX7eWXX863v/1txo8fz9SpU/frR23mzp3LypUrmThxIkopfve735GRkcFzzz3HQw89hM1mw+Fw8Pzzz1NSUsK1115LIGC27YEHHgjJNgohjn5hM3Q2gNu9B49nDw7HlEP+85xHAhk6W4ijlwyd3a3Q/dCOEEIcDcIqKITyJzmFEOJoEFZBQVoKQgjRu7AKCm3XEaSlIIQQ3QuroNC+uUfWxXUhhDhUwiooyDUFIYToXVgFhVBdU6irq+OJJ544oGXPPfdcGatICHHYCKugEKqWQm9Bwefz9brsu+++S2JiYr/mRwghDlRYBYVQtRRuu+02CgoKyM/P55ZbbmHZsmXMnDmTOXPmMGbMGAAuuOACpkyZwtixY3nqqaeCy+bk5FBVVUVhYSF5eXksWLCAsWPHcuaZZ9Lc3NxlXW+//TbHH388kyZN4vTTT6e8vBwAl8vFtddey/jx45kwYQKvvfYaAO+//z6TJ09m4sSJnHbaaf263UKIo89RN8xFLyNnA3b8/lFYLFHszwPN+xg5mwcffJANGzawtnXFy5YtY82aNWzYsIHc3FwAnn32WZKTk2lubmbatGlcdNFFpKSkdEpn+/btvPTSSzz99NNccsklvPbaa13GMTrppJP47LPPUErxzDPP8Lvf/Y7f//733HfffSQkJPD1118DUFtbS2VlJQsWLGDFihXk5uZSU1PT940WQoSloy4o9E3o7z467rjjggEB4NFHH+WNN94AYPfu3Wzfvr1LUMjNzSU/Px+AKVOmUFhY2CXd4uJi5s+fT2lpKR6PJ7iODz/8kJdffjk4X1JSEm+//TazZs0KzpOcnNyv2yiEOPocdUGhtxq91gFcrq3Y7dlERu77Nw0ORmxs+w/5LFu2jA8//JCVK1cSExPDySef3O0Q2na7Pfi/1Wrttvvoxhtv5Oabb2bOnDksW7aMe+65JyT5F0KEp7C8ptDfF5rj4uJwOp09fl5fX09SUhIxMTFs2bKFzz777IDXVV9fT1aW+Rnr5557Ljj9jDPO6PSToLW1tUyfPp0VK1awc+dOAOk+EkLsU1gFhba7j/q7+yglJYUZM2Ywbtw4brnlli6fn3322fh8PvLy8rjtttuYPn36Aa/rnnvuYd68eUyZMoXU1NTg9Lvuuova2lrGjRvHxIkTWbp0KWlpaTz11FNceOGFTJw4MfjjP0II0ZOwGjobwOlcg82WRlTUkH3PHGZk6Gwhjl4ydHYPQvlDO0IIcaQLu6AQyp/kFEKII13YBQVpKQghRM/CLihIS0EIIXoWlkFBWgpCCNG9sAsKSklLQQghehJ2QQEUh8OP7DgcjoHOghBCdBF2QUFaCkII0bOwCwqhuKZw2223dRpi4p577uHhhx/G5XJx2mmnMXnyZMaPH8+bb765z7R6GmK7uyGwexouWwghDtRRNyDeTe/fxNqyHsfOJhBoQWsfVmvfu2/yM/J55OyeR9qbP38+N910EzfccAMAixYtYvHixURFRfHGG28QHx9PVVUV06dPZ86cOahexu3ubojtQCDQ7RDY3Q2XLYQQB+OoCwr7th8/pNBHkyZNoqKigj179lBZWUlSUhJDhgzB6/Vyxx13sGLFCiwWCyUlJZSXl5OR0fMIrd0NsV1ZWdntENjdDZcthBAH46gLCr3V6AHc7hI8nlIcjim91tj317x583j11VcpKysLDjy3cOFCKisrWb16NTabjZycnG6HzG7T1yG2hRAiVML0mgL09x1I8+fP5+WXX+bVV19l3rx5gBnmetCgQdhsNpYuXUpRUVGvafQ0xHZPQ2B3N1y2EEIcjLALCm3DZ/f3HUhjx47F6XSSlZVFZmYmAJdffjmrVq1i/PjxPP/884wePbrXNHoaYrunIbC7Gy5bCCEORtgNne3xVOJ2FxEbOwGLJTIUWTxiydDZQhy9ZOjsHrRfR5BnFYQQYm9hFxTaf5LzyGohCSHEoXDUBIW+F/JtmywthY4kSAoh4CgJClFRUVRXV/epYAvVheYjmdaa6upqoqKiBjorQogBFrLnFJRSzwLfAiq01uO6+fxk4E1gZ+uk17XWvzqQdWVnZ1NcXExlZeU+5w0E3Hg8VdhsFqzW6ANZ3VEpKiqK7Ozsgc6GEGKAhfLhtX8AjwHP9zLPJ1rrbx3simw2W/Bp331xOteyevU5jB37Omlpcw921UIIcVQJWfeR1noFUBOq9A+U1RoDQCDQPMA5EUKIw89AX1M4QSm1Tin1nlJq7KFYocViuoz8/qZDsTohhDiiDOTYR2uAYVprl1LqXODfwIjuZlRKXQ9cDzB06NCDWmlbUJCWghBCdDVgLQWtdYPW2tX6/7uATSmV2sO8T2mtp2qtp6alpR3UetsuLktQEEKIrgYsKCilMlTr48VKqeNa81Id6vVK95EQQvQslLekvgScDKQqpYqBXwI2AK31k8DFwA+UUj6gGbhUH4InqJSyoJRdWgpCCNGNkAUFrfVl+/j8Mcwtq4ec1RotQUEIIbox0HcfDQiLJUaCghBCdCNMg0K0XFMQQohuhGVQkO4jIYToXlgGBYtFgoIQQnQnTINCjHQfCSFEN8IyKEj3kRBCdC8sg4J0HwkhRPfCNCjE4PdLUBBCiL2FZVAw3UdyTUEIIfYWlkFBuo+EEKJ7YRsUpPtICCG6CsugYLXGoLUbrQMDnRUhhDishGVQkB/aEUKI7oV1UJAuJCGE6Cwsg4LVGgNIS0EIIfYWlkGhvftIbksVQoiOwjooSPeREEJ0FtZBQbqPhBCis7AMCnJNQQghuheWQaG9+0iuKQghREdhHRSkpSCEEJ2FZVCQ7iMhhOheWAYF6T4SQojuhU9Q2LMHXn8dXC7pPhJCiB6ET1D49FO46CLYuROrVYKCEEJ0p09BQSn1E6VUvDL+ppRao5Q6M9SZ61epqeZvZSVKRQIWCQpCCLGXvrYUvqO1bgDOBJKAK4EHQ5arUEhLM38rK1FKYbXG4vM5BzZPQghxmOlrUFCtf88FXtBab+ww7cjQISgAREZm4vGUDmCGhBDi8NPXoLBaKfUBJigsVkrFAUfWL9SkpJi/rUHBbh+Mx7NnADMkhBCHn4g+zvddIB/4RmvdpJRKBq4NXbZCICICkpM7tBSyaGj43wBnSgghDi99bSmcAGzVWtcppa4A7gLqQ5etEElL69RScLv3oLUe4EwJIcTho69B4S9Ak1JqIvAzoAB4PmS5CpW0NKiqAsBuz0JrNz5fzQBnSgghDh99DQo+barU5wOPaa0fB+JCl60Q6dBSiIwcDIDbXTKQORJCiMNKX4OCUyl1O+ZW1HeUUhbAFrpshUin7qMsANxuudgshBBt+hoU5gNuzPMKZUA28FDIchUqaWlQXQ2BQLCl4PFIS0EIIdr0KSi0BoKFQIJS6ltAi9b6yLym4PdDbS12eyYgLQUhhOior8NcXAJ8AcwDLgE+V0pdHMqMhUSHoS4sFjs2W6pcUxBCiA762n10JzBNa3211voq4Djg7t4WUEo9q5SqUEpt6OFzpZR6VCm1Qym1Xik1ef+yfgC6PNWcJQ+wCSFEB30NChatdUWH99V9WPYfwNm9fH4OMKL1dT3mttfQ2isomGcVpKUghBBt+vpE8/tKqcXAS63v5wPv9raA1nqFUiqnl1nOB55vvdX1M6VUolIqU2sdugGJugSFLJzONSFbnRBCHGn6FBS01rcopS4CZrROekpr/cZBrjsL2N3hfXHrtEMWFCIjB+P1VhAIeLFYjrw7bEX/0BpKS8HjAYsFlILISIiJMS+rtfP8fj80N0Njo3mfkmJGUelOTQ3s2GEOueHDzctmg0AACgth/XooKzPrtVohKgrGjjUvW+shWVcHX34JRUVmpJaUFPPX7zd5aGoy609NNYd4TAzs3AnbtsH27e35BLOe6GiznpgYs0x6unklJUFsrPnc74dvvoEtW0z+o6La57PbzU181dVm+zr+73DAkCEwdKjJZ3OzebW0QGJiexq1tbB6NaxZ055+bKxZPj0dsrPNKz4enE7zqq83+7G8HCpa+y0SE80rKQkyMsxr0KD2fRwImOdVd+wwr927zTSlzCshwSyTnm7y0JZ+VRV4ve1pRESY/RUdbfKUkwPHHAPDhpm8bNpkXi5Xe94zMsx309Bg8l5dbeatqDDfScfvsu1vUhL4fOY3wfbsMXlpbDT7sKkJLr8cfvjD/jz6u+prSwGt9WvAayHMS4+UUtdjupgYOnTogSdkt0Nc3F7PKmg8njKioob0Q04FmEK2stKcBG0nhNVqCoohQ8zJ19hoCq5vvjEn3ODB5pWaagouMAVTQQF8/TVs2GBOuNRUcwJFR8OuXebznTtNoeH1mpfFYk7ctldSUvsLTJ4aGkwet241r44F594iI9u3S2tz0naklMlTWprZTr/fFCQVFabw68hqNQVKRYXJc0/sdpg40eRzy5b92v1ddAxYfr/Zht4oZfah39/3dSQmmkLN5WovsPsiNRVGjzbHyJ49ZvmyMhNEumOxmP2clmbyWV9vgmZDw77XFR9vCvGICLMPAgGzbHk5uN1mno7fpd3eHjx8PlMoNzeb77Spm1/yTUoyQaakxByHe3M4TMAaNMgEmOJiUymoru7++IuPN8EqNrY9INnt+97Og9VrUFBKOYHuDiEFaK11/EGsuwToWBJnt07rQmv9FPAUwNSpUw9usKJunmr2ePaEbVAIBEwtefduc8C73ebVVnhobQ5wl6vrq7HRnLx+v3k1NZnabGFhzyc1mBOnfh8jZ6nWgdnbCrC22nvbydsmPd3U2FJSTO2wrYbodJraXkGBOYlra9sLc6XMCZecDCNHwsyZ5m9MTHvN0OMx29NWS2tbTilzYra1IrQ26ykvN4eV1qbgslhM+scea16pqe017+3bTcEwcSJMmGCCZSDQXvNftw5WrTK16PR0uOIKOO44GDHC7LeqKlOQ2GwmD7Gx5juqrDSvxkbIzTXbdOyx5vM2Wptta2vpVFWZQry83BSQjY3mpbVZ36hR5q/HY+YpKzP/p6R0rt12DDwtLabAq6kxBVlMjPnu2grg8nKTpylTTI1a7TUIv9Zm+3bvNsfZ3gHe0s3VTK/XbEdZmUk/EGj/HhISzDakpHRdV9v6GhrMPklN7bnV13H+qirzfRYWmu9yzBjzV6n2CkHbdiYkmLz3VqC73eYYrakxFYfBg039dSCoUA4I13pN4T9a63HdfHYe8CPMcNzHA49qrY/bV5pTp07Vq1atOvBMTZ9uvqEPPsDp/IrVqyczduxrpKVdeOBpHga0NjXn9etNwdxWQDid7c3ymJj2JnhFhanR7NzZtaDdl7amfmys+d9qNSdfVJSpieXkmFdqqjkhEhLMSbtrl8lbeTlkZZnCPDfXnERtzeXWZwuDwSA3F8aPh7w8k/+mJjOPqzFAdrYOFnhWi7Wn7Ab3T2OjOWljYsxfp9tJRWMFVU1V1DTXMDp1NDmJOajuSg7AF/BR0VhBWkwaNmvX7saiuiKSopOItx9MXenAaa1xepzUt9ST7kgn0hp5SNbZ0/46EF6/F6vFikW1l/z+gJ8dNTvYUrWFSGskcfY4HJEORqWMItoW3SWNLVVbSLAnkBmXGZzW5G1iWeEyviz5kjh7HCnRKaTEpJBgTyDOHkdcZBwaza76XRTVFVHeWM6olFFMz57eKZ29NXoaqW6uxul24vQ48QV8REVEERURhVVZqW6upqKxgorGCuxWO4PjBjM4bjCOSAeVTZWUu8qpaqoioANYLVasykq6I538jHwGxQ4CoMXXwpclX/LJrk+YNngaZww/44D2rVJqtdZ66r7m63P30QFk4CXgZCBVKVUM/JLWoTG01k9iLlSfC+wAmjhUQ3GnpZlqDEfGUBctLe21jp072/tGy8vba+jNzbBxo6mJtWlrBsfFmYLU5YJGfx2xgypJGuQiYZCT5OOayLvAQ0q6m9RUyEnIZXjiSJJj47FaocXfRHnLblz+6mDN2BETwbTsSV0KRa01e5x72FGzg4LaAr6p/Ybi1hPc6rYSaY0kdkQs6WMdjIlO4Vsjv4U9onPV6X+7/8crG17B7Xfj9XsJEEDHZWOLHIWuG0VtWS0ffvMhH+38iK9Kv0J3aMSOSRvDt0d+m2+P/DbTs6d3CRIB7eetna+wrHAZm6s2s6VqC1VNVV3299CEoZycczLDEoZR7iqnrLGMUmcpJc4SylxlBHSAQbGD+O6k73L9lOvJcGTwr43/4rEvH+OLki8AGBw3mNGpozkm8ZhgIRAbGcu26m1sqtzE1uqteP1eom3RREVEkeHIYObQmcweNpvx6ePZWrWVlcUrWVm8kuKGYupb6mlwN9DobcQX8OEP+AnoADarLVgAuX1uylxlNPtMs0ahyHBkMDTBdLc2uBtocDdgs9qYkjmFaYOnMT59PMUNxWys2MjGyo24PC5ibDFE26KJtEbi9Xvx+D14/B58AZ9Zt/bT7G2mrqWOupY6mrxNpMWmkRWXRVZ8FnarnRZfCy2+FizKQlZ8FkPih5DhyGB3/W42VW1iU+UmbBYb07OnMz17OjmJOazcvZKPdn7EZ8WfBfdhVnwWAR3g6/KvafR27V/JcGTwi1m/4LrJ12Gz2thcuZlbltzCO9vfCaYxJXMKbr+b5YXLcfv3s/bT4ZiYOngqE9MnMiF9AinRKSwtXMoHBR/wWfFn+PV+9LPth0xHJlnxWawvX4/H7wHgjpPuOOCg0FchbSmEwkG3FL7zHfjgAyguRusAK1ZEMWTIzzjmmAf6L5N76a42pTW8/fUyHvnfn0luOZ6kbxaw5askysrA7QnQmPwprvSP8PibIcINVg9EtEBEC3ZHCw6VSmr5fJLqTibKbmXUaE3a2I2UJ/2bmLhmxmcdy4jU4URYIli8YzHv7niXVXv6tt/SY9PxBrzUNHc/gmxaTBpXTLiCa/KvwaIs/Gvjv1i0aRFbqto7v63Kis1qCxZieq9eyGOTj+VPZ/+Jc0eci8vj4o6P7uCxLx4j2haNI9KBzWJDKcUe5x4Cuv33nGwWGycOOZETh5xIdISpJXoDXj7d/SkrilbgC/jIdGTynUnfYcHkBQxLHMaSgiXcsuQW1pWvIyU6hby0PPJS8xieNJx0RzppMWkkRCWwrmwdy4qWsaxwGdVN1aTFppHhyCDDkUFWXBbZ8dkMih3EBwUf8M72d9BakxCVQF1LHaNSRnHd5OvwBXxsqdrC5qrN7KrfRbmrPLjtFmVheNJwRqeOJtoWHSw8C2oKKKgtAExh3jZ/akwqI5JHEG+PJyEqgVhbLBGWCKzKitVixev30uI3adgsNjIdmWQ4Moizx1HqLKWovohd9buwKItJw56Ay+ti1Z5VfFP7TXCfxtpiGZM2hqToJJq9zTR5m/D4PURaI4OvCEtEsCYbFRFFUlQSiVGJRNuiqWisoLihmBJnCb6Aj+gIE+x8AR/FDcWUukpNELPYGJkykry0PFp8LazcvZLq5urgdk8ZPIXZw2Zjs9godhZT0lCCRjMxfSL5GfmMSRuDP+DH6XFS01zDE18+wSe7PmF40nBmDJ3BwvULiY2M5dYTbyU2MpbVpatZtWcVFmXhrOFncfaxZzNz6EzcfjfVTdVUNVXR4G7A6XHi8rjQWjM0YSjDEoeRGpPKpspNrNy9ks9KPmNN6RoKagqC301bfs885kxyk3KJi4wjzh5HhCUCt89Ni68Fb8BLakwqg2IHMSh2EG6fmz3OPexx7sHpcQanp8WkYVEW/NqPP+CnxFnCV6VfsbZ8LbvrdzMlcwozh81kxpAZpMSk9Okc7k5fWwrhFxR+/nN45BFTBVeKlSuHkZh4Mnl5z+13Uo2eRmJsMd0U+JovSr5gaeFSlhUu47+7/ktK5GAm6CuxbbmCrZutbM/9P7wj/wXNiRBdB94YMsquJjkmkaKEhTRG7AIggigilDkxo21ROOxRREdGUVRXhNPjZHDcYM4afhb/2/0/tlZvRaGCB1gbi7IwPXs6Zw8/m5zEnGBzOcYWgz3Cjt1+bCE1AAAgAElEQVRqx6/9FNQUsK16G9uqt2GPsDMkfghDEoaQFpMW3Mb6lnpe2fgKb219C2/AG0x/1rBZXDDqAsakjWF48nCGJgwlwtLeEPX6vTR6G2n0NLK2bC0/++BnbK3eyjnHnsPmqs0U1RXxo+N+xG9O+w2OSEdwOY/fQ0FNAVurtxJji2HGkBnERnboJO+grqWO93e8z4vrX+Td7eaO6by0PDZVbiI3MZcHTnuAS8Zess/uDq01fu3vlP+97arfxdOrn6aovoirJl7FabmndZuuL+Cj3FWO0+MkJzGHqIiobtMraShhRdEK1pevJy8tjxOHnMjwpOH92jXTUXVTNZsqNzEkYQhDE4Z26q7pb16/l6qmKlJjUju1MLXWFNQWsLN2J1MHTyUpOmm/0tVa8+72d7n9o9vZVLmJ70/9Pr+c/UvSYtP6exMAcHlcbKjYQEVjBScOOZHUmNSQrCdUJCj05KGH4NZbTed6fDxr1pyAxRJLfv6H+1zU4/ewas8q3tv+Hu8XvM/qPasZnTqaH077IVdOuJJIayQvrn+RRz5/hE2VmwBwNI2jZetMfImbIXcZACpgw6oiOM1+G9eMvIXYodt4veRP/HPDQnwBH2cOP5Mrxl/B+aPP71RAdtTsbebtbW/z4voX+WjnR0zPns7FeRczN28uKdEpFNUXUVBTQKO3kdnDZh9UDaM71U3VLNq4CKUUc0fPJd2Rvl/Le/weHv38Ue5dfi9ZcVk8M+cZThp6Ur/lb1f9Lv625m8sLljMJWMv4YZpN3TprhJHh4AO0OJrIcYWM9BZOaxJUOjJP/4B115rOuaHD2fDhotpatrIccdt7nb2ZYXLeGPzG3yx5wu+Kv0Kt98drHnPGjqLjws/5ouSL0zTnijqvdXYqvPxfvIT2HYeY3PTOPVUOOkkyB67i2U1Cyl3lfGzE38W7O9tU9NcQ0AHjrgayMFocDcQHRHd7YVbIUT/GfALzYetjg+wDR+O3T6Y2tolXWZr8jZx65JbefzLx4mOiGbq4Kn86LgfcUL2CZyaeypJ0UmUlsKYUli0ZxVLnU9S73VhXfMDTh89i/k3Kc46yzzA0m4oJ3J7j1lLjk7u3209AgzUnTpCiO6Fd1DA3IHk9zfg87mIiDBdNWtK13D565ezpWoLP53+U35z2m+CfcFlZbDwb/Dyy/DppyaplJSpnHPKM5x1Fsx90tz1I4QQR6KwDwodH2Cr96Twq+W/4olVTzAodhBLrlzC6cecDpj7/W+7DZ580txHP24c3HcfnHeeeQipuwdqhBDiSBP2QcFuz8IbgN+v/CN/WPUyDe4Grpt0HQ+c/kCwO+eDD2DBAvOE5Q9/CD/4gRmbRgghjjbhFxTaHsPt0FL4wzZ4v/xJzj72bB464yHGDTIPYHu9cNNN8MQTZnyWTz+FE04YyMwLIURohV9QUMq0FqrM06xrq/bwfjl8b9xsnrzoveBsTifMmweLF8PPfga//rWJJUIIcTQLv6AAwUHxtNb834d3khyp+F7emODHpaXmWsH69fD003DddQOYVyGEOITCOii8tOElPiv+jLvGZRCpTXdSRQWceKLpXXr7bTjnnAHOqxBCHELhGRRSU2ks2MzPP/w5kzMnM/eY+ODPct51lxkv79NPzXDFQggRTsLzRsq0NB7KLaW4oZhHznqE6Khs3O4S1q6FZ56BG2+UgCCECE9hGRS8qcn8fqqXi0fNZeawmURF5dLSUsxNN/lJToa77x7oHAohxMAIy+6j1YlNuHxw6eAzAXA48vnkk/NZvtzKE0+0/2yjEEKEm7AMCits5kd1ZkYMB8Bmm8STT+YzenQNCxaE3/hDQgjRJiy7j1Z4dzC6EgY1mN8cePbZHEpLj+HnP39hn7/PKoQQR7OwCwr+gJ9P6tczq4jgU80vvqgYP34jEya8MrCZE0KIARZ2QWF9+XoavK5gUCgshK++gnPO2YHLtR4dot9bFUKII0HYBYUVRSsAmFVihcpK/v1vM/38870EAo00NxcMYO6EEGJghV9Q2LWC3MRchiTlQEEBb7xhhsEeP95cdHa51g5sBoUQYgCFVVDQWrOiaAWzhs2CvDwq15fy3//C3LkQGzsWpWy4XF8NdDaFEGLAhFVQ2FK1haqmKhMUxozhrR15BAImKFgskcTEjJGWghAirIXVDZjLi5YDtLYULLzhT2VYlpf8fPOj8Q5HPrW1iwcyi0IIMaDCqqWwomgFmY5MhicNxzl0LEs4g7mTi1DKfO5w5OPxlOF2lw1sRoUQYoCETVBou54wO2c2Sine3z0WD3bmZnwWnMfhyAegsXHdQGVTCCEGVNgEhZ11OylxljBr6CwA3lgcQ5qlihlNS4LzOBwTAbkDSQgRvsImKASfTxg2C63hvffgW+mrsG7ZGJzHZksiKipHgoIQImyFzYXmy8dfzpi0MeSl5VFTA3V1MGF8E6zZAoEAWEx8dDjyJSgIIcJW2LQUbFYbx2Udh0VZKCw003LGxEBjo/mptVYORz5NTVvx+xsHJqNCCDGAwiYodLRzp/mbMzXV/LNpU/Azc7FZ43J9fegzJoQQAywsg0KwpXByjvln8+bgZ3FxxwOK2toley8mhBBHvbANComJkHhsKqSkdAoKdnsG8fEnUln52sBlUAghBkjYBoWcnNY3Y8Z06j4CSEu7kMbGdTJiqhAi7EhQyMszLQWtg5+npl4IQGXl64c8b0IIMZDCLiho3U1QqKkJ/gobQHR0Dg7HFOlCEkKEnbALClVV5i7UTt1H0G0XktP5OS0txQghRLgIaVBQSp2tlNqqlNqhlLqtm8+vUUpVKqXWtr6uC2V+oMOdRzmtE/LyzN8OF5sB0tIuAqCqSrqQhBDhI2RBQSllBR4HzgHGAJcppcZ0M+srWuv81tczocpPm7agkJvbOiE7GxyOLkEhJmYUMTFj5bqCECKshLKlcBywQ2v9jdbaA7wMnB/C9fVJW1AYNqx1glKmtbBX9xGYLqT6+k/weCoOWf6EEGIghTIoZAG7O7wvbp22t4uUUuuVUq8qpYZ0l5BS6nql1Cql1KrKDheED0RhISQlQUJCh4l5ebB+Pfh8neY1XUgBqqr+fVDrFEKII8VAX2h+G8jRWk8AlgDPdTeT1voprfVUrfXUtLS0g1phpzuP2syda+4+eumlTpNjYycQHX0s5eX/PKh1CiHEkSKUQaEE6Fjzz26dFqS1rtZau1vfPgNMCWF+gB6Cwpw5MGEC/PrX4PcHJyulyMy8nvr65bhc60OdNSGEGHChDApfAiOUUrlKqUjgUuCtjjMopTI7vJ0DdL7a28+6PKPQxmKBu++Gbdtg0aJOH2VmfheLJZqSkj+HMmtCCHFYCFlQ0Fr7gB8BizGF/SKt9Ual1K+UUnNaZ/uxUmqjUmod8GPgmlDlB0wPUVNThzuPOrrwQvPMwn33md9XaGWzJZOefiXl5S/i9VaHMntCCDHgQnpNQWv9rtZ6pNZ6uNb6/tZpv9Bav9X6/+1a67Fa64la61O01ltCmZ8uzyh01NZa2LwZXuv8JHNW1o0EAi2Ulob8jlkhhBhQA32h+ZDqNSgAzJsHo0d3aS04HONITDyVkpLHCQR8PSwshBBHvrAMCsFnFPZmtcJdd8HXX8Orr3b6KDv7x7jdu6mufjOkeRRCiIEUdkEhORni43uZ6dJLzZ1It94Kzc3BySkp3yIqKofi4j+FPJ9CCDFQwi4o9Nh11MZqhUcegaIi+MMfgpOVspKV9WPq6z+RwCCEOGqFVVDYubMPQQHglFPM3Ui/+Q2UtD9akZ39Y1JTL2THjpsoL18YsnwKIfZDSQn84x8DnYujRtgEhbZnFLq9HbU7Dz1khr24/fbgJKWs5OUtJDHxFLZsuYbq6vdCklchxH54+GG49lookF9K7A9hExQqKqClpY8tBYBjjoGbb4YXXoDPPw9OtlqjGDfu38TGTmDjxouor/9fSPIrhOijjz82f5cvH9h8HCXCJijs83bU7txxB2RkwHe/C05ncHJERDwTJryH3Z7N11+fJ0NgCDFQKivNYJYgQaGfSFDoTVycaSls3gxXX93p2YXIyEFMnPghVquDdevOpKlpe39mVwjRF8uWmb85ORIU+knYBIUzzoClS+HYY/dzwdNPh9//Ht54A371q04fRUUNZcKEJYCfdWtOp6VGAoMQh9THH5vK2403mjsGi4oGOkdHvLAJCsnJcPLJEBV1AAv/5CempXDvvfB6519ii40dzYRhrzJuQTFq3Fi8Zd/0S35FP1mzxtxlIA5P779vniYtLz+w5T/+GGbPNpU3kNZCPwiboHBQlIInn4Tjj4crrjC3v7UVNI2NxF16J44dCluVl+aLphHwtQxodg+ZpiZz9f5gbN8OVVX9k5+9vf8+TJkCTz0VmvTFwdHajDe2a1eX0Yn7pLjYjGx86qkwbpyp+UlQOGgSFPoqKgrefNMEhmuvhcsug7IyOP98WLkS9dLLOH99DfH/q6H6pyegw6F2euqpcOaZB14Tb26G6dPh+uv7N19tHnvM/P3Nb8DjCc06xIH7+GNYtQoiIg4sKCxdav6eeqoZ0HLmTAkK/UFrfUS9pkyZogeUz6f1b36jdUSE1jab1kpp/fzz5rNAQDvnTtABhS79x5UDm89QW7NGaxMOtP7nPw8sjeefN8tHRmpdV9e/+du503w3M2eadTz1VP+mr7XWLpfW27b1b5pOZ/+mdzg74wyt09O1vvNO8x3t3r1/y19zjdYpKVr7/eb9H/5wYOmECWCV7kMZKy2F/WW1mgfaPv0Upk6FZ56BK680nylF7Av/w31sIik/foGSt7+H1u13LKG16cq47jrwevsnP83Npmurrq5/0uurZ58Fu90022+91XQl7a+nnjIDUXk88O9+/h3sv/7V1B4XLoRp00xrob/2eZuf/MSMk1VW1j/pvfCC2R933NHpTreQ8Pvhww8PTQuqocE8DLpzZ/u0NWtgyRK46Sa46iozba9BKHultWlpnHKK+Z7BXFuA0LUWAgFz++snnxzd16n6EjkOp9eAtxT6wL9lg/ZkxGi/DV1yV772ehpMDfCyy9pr17/7Xf+s7Ac/MOlNm6Z1bW3/pLkvzc1aJyWZ7Vmxwqz/nnv2L42NG81yv/2t1jk5Wp9zTv/lr6VF69RUrefONe//8x+zrmee6b91VFRobbebdG+77eDTW71a66gorQcNMmlefrnWbvfBp9ud8nKtTzvNrOeSS9pr2gersNCk3dGaNVofe6xZV2qq1p98YqbPn691XFz7MZufr/X06Z2X/etftb7ySvN97m3HDpPmE0+0T/P5tE5I0HrBgvZpgYDWXm/X5QMBrf/8Z63feWff2/Xyy1qfd57WiYnt5++Pf9x/++0QoY8thQEv5Pf3dSQEBa21DlRU6KZTRmsNuvrUBO0fNVxri0Xr++/Xes4craOjtS4o6LBAQOv77jMFTGNj31by1lvmKzz7bNOVdfzxWtfXh2aDOnrpJbPeJUvM+3nzzPbs2tX3NH76U5Pn8nKtb73VdMdVVfVP/l58sXP+AgGtp0zROjdXa4+nf9Zx331mHccfbwqig9nvlZVaDx2q9ZAhZn/cf79J+5RTtN60yQSg/sr3p59qnZVlAtCll5r1/OQnZh91nOdXvzLdY321eLE5BqxWU4C+/LLWjz9uAmdWlvlORo40XYX332/OhVtuaV/+gQdMXgoLzfsvvzTHBJjA0DF/WpvuQNB6y5bO07/1LbMerbX+/HOtx441r8rKzvM98kh7AT9/ftdgprUp9G+5xcwzfLjW112n9XPPmf0FplIUqsAdAhIUDgd+v2686xodsKDdSUpX/ev/dCAQMH2eDofpUw0EzMH3wx+2H6QjRpgTszelpVqnpWk9caKpSb3xhjmJTjxR62XLTP/qZZdpfeGFWhcV7V++6+u7r121OeMMrYcNa68p7dxpTv5LL+168nanuVnr5GRTS9W6/fpEf/X7n3iiKRg61uTefLPv67j/flOw/fvfpva5N7db68xMrc86S+tVq3S3Lb/qaq0//ljrV181Nd5nnum+kPV6Ta3dbjcFYZsXXzRBs+2YABM0Xnyxb/t4by0tJpBFRJgC7quvTDo//Wl7/ktLtb7qqvb1TZzYXkj35u23TWE/YYLWP/+5CQJtaZx9tglqWmtdU6P1qafq4HWkkpL2NAoKzPSHHjKVotGjtc7ONhUGMEGqjc9nKlaDB3fdFw89ZOb/wQ9M4GkLgFOnat3QYOb58EMTvM4/3+yTyEhzPD76qLlGFAiYPFx0kUnrhz/sfD4EAlo/+KD57KyztN68uf+CdkcFBSYo1dT0S3ISFA4jzWuW6PUfn6iXLkWvW3eubmkpMU1XMDWPBQvM/7fcovVHH5kC12LR+vvfNwXUr35lumdefNE0m/1+c7JFRZlumDavvmoO9o6FiMNhLuatXNl7Jr1e0/L49rfNukeO1Pr997vOV1hoLuDu3V10991mnZMna71oUfeFaZuFC828H35o3gcCJhCedlr385eWav1//2daUfuqkX/1lUn7j3/sPD0QMN0TVqspOHoqWN97zywfHW3+Dh1qurg61gjbWiLvvmven3aaCRJt3Rzr15t93rFAB1OIPfus2Tcejwnkp59uPvv737vmZeNGrV94wRwr991nughB61mztP766973Q0eLF5v929Zd1LGb0e9vbzE4HCYQ3X671q+9ZlpAaWlaL19ujrsHHjAtrmOPNQXlf/5jbjKw2cz06mqTps9nvttFi7p2sXg8Jv29vx+tTcE9bZrWP/pR+/ERCGh99dXm/d/+pvVf/tK+LTfe2DWNL75o398LFpgbGN56y3zvp55qCvDkZK3HjGkPEps2aT1jRvtyqalaH3OMOc7/+Meej5VnnjHnCphgO2qUqYT9+tfmONq7dbI/XC6tx43Twa7hfrgRQ4LCYSYQ8Ovdux/Vy5dH6xUrHHrH1pu1//jJ7QfVnXe2H3z19eaAVqprwQLtfZuPPdZ1RatXm5OgtNS837jRHOB2e893Cb3ySnvtLj1d65tuau8HvuACUyC0ufdek6+9a5A+nzlJRo7UwdbOXXeZAqnt5Gsze7bJU8cC4667zL4oK2ufVl1tAkFbt4RSpvb4n/90vx3l5aY7Jzq6+9pVfX177W/u3K4nWlmZ6dMfN85cA3r1VdOFA6ZAqa0139G0aaYAaMv/kiU6eM3i88/N9ZbBg00+163TurjYFKzHH2/my8trv3aQman173/f/fbsze83LZ3kZLOvsrNN18iJJ5puk2uuMcHz3ntNC+Cqq8y+bvs+ugvyWptgduGFpva9dWv79C1bzPfZ8Tg8/nizrpiY9mknnNA/d4/97nftad50U/t0t7t9O8AEj1de6b416/OZCsoHH3Se/sILOthCSUzUevv2zp8HAlpv2KD1009rfe21Jkj8+9/7zvOWLeYuujvuMPuw7bwBs9/OP99cd+spsFRXmxZix0pHIKD1FVeY5W+/3QTd6dMPumtYgsJhqrFxm9648f/ppUst+stnI7Q30a69d/+s+4OmpcW8vF7zWrdO6yefNDWnn/60790IlZWmdtlWyC9fbpatrm6vJU6bZmqubc3glhZz623byX/ssWa9WVmmdtsTn88UpjNmtLdarFYTBEaONAUimFpnR19/3R7o9uwxwSA+3pwYl19uTuKVK00h2NYP/MUX7ftg3TpTq4+ONrXcngQCpmstIsJcY/jnP01hGwiYi912e9da+PPPmxNzzJj26ymPP945zcmTTSHtcJht/eab7tf9yitm3gsuMN0uvXXT9aSqSutf/MIUXnPnmoA1aZJpGUZFmfzFxJj3kyaZmmt3F2v7orbWdAn9/veduyGbm03B+/jjXYP+gSosNHkfM8ak31F1tcnHsmUH1n2mtekecjh6Do79pbbWdB3efru5ZbYtkN14o6nsXXmlOdays9sDyIgR7Re9//IXM+3ee837118359CMGQd1y7IEhcNcU9M3euvWG/TypXb9ySeJuqxsobneECputylIkpPN1z5pkqnNRkSYQqOnwmnXLlODO//89gO8t0K3o4YG01K4806t/9//MwX5xRebWlBbV0NHY8aYGnRkpKkJz5tnumL23o577mkv/MaONf3ODofZnlWr+pa3//63vXk+bpzW11/fHpS6s3RpewstIaHryfnKK+0FWse+8oEQiv7tQ2Xhws4t0/52IEH4YDQ2mkJ+zBjTgszMNHfb5eeb4PDQQ6brcNQoc/ycfro5/s85p3NLetEiExi+970DzooEhSNEY+M2vXr1CXrpUvSGDRdrp3O9bmzcrpuadmqPp5/uxum8QtMFMWGCqb2sXt33ZQOB9m6pUPjjH01N/fvf79q831tdnbmAO316e01sfwtjv9/cJdN2Qn7rW73XQjdtMgHkN7/pPq1XXum/O6hEeHG7tX74YXOb7rBh3R9H//nPQR1ffQ0Kysx75Jg6dapetWrVQGejX2ntZ9euhygs/AVad37AKiZmDMnJZ5KUdBZJSadisUQOUC4PAa3Nr93ZbPu3XFkZpKaa4RIOhM8HH30EJ55oRtwUYqDU1pq/SUn9nrRSarXWeuo+55OgcPhoatqG07kGrX1o7cPrLae29iPq6lagtZuoqGPIzf01gwbNRyl5GF0I0XcSFI4ifn8zNTWLKSz8JY2N63E4JpGVdQMWixkH3GKJJiXlPCwW+wDnVAhxuOprUDjA9rY4lKzWaNLSLiA1dQ7l5f9k58672Lr1uk7zxMaOY/To54iLmzxAuRRCHA0kKBxBlLKQkXEFgwZdQktL+y9MNTZuYPv2G1iz5niGDr2TYcPuOLqvPQghQkaCwhHIYokkJmZE8H1MzAgSE2ezY8dPKCq6l127HiA6egSxsXnYbOl4PHtwu3fj8ZR1uHB9BrGx41FKDeCWCCEONxIUjhI2WzJ5eS+Qnn4ltbUf0dS0GZdrHR5POXZ7Fnb7UGJiRuN0rqGg4P8AsNuHkZFxDZmZ1xIVNWyAt0AIcTiQC81hqKWlmNraD6ioeJna2g8BSEiYRXT0cGy2NCIj04iISCIiIgGrNYHIyDSio4/Fao0d4JwLIQ6UXGgWPYqKyiYz8ztkZn6H5uZCysr+QXX1W9TUvIfXW9XlWYk2dns20dGjiIkZSUzMKKKjRxIVlYPdPhirNV66ooQ4CkhLQXSitcbvb8DrrcXvr8fnq8fjKaOpaRvNzdtoatpKU9NW/P76TstZLNHY7UNJSJhBYuLJJCbOJipqaKd5AgEP1dX/oa5uGSkp3yYp6fRuA0kg4MHnq8XvdxEVlSvPZAjRD6SlIA6IUoqIiAQiIhJ6nEdrjddbRVPTVtzuXXg8pbjdpTQ3b6eq6g3Kyp4FIDJyMHFxU4mLm4LPV0t5+Yt4vVWAlZKSPxMbO47s7J8SEZFMQ8On1Nf/l8bGDfj9ruC6YmPHk5PzS1JT5waDg8dTTktLEQ7HZCyWvh/CWvsBJUFGiF5IUBD7TSlFZKS59rA3rQM0Nn5NXd1ynM4vcTpXUV39NkpFkJp6PhkZ3yExcTYVFYsoLv4DW7d+tzXNSOLippGRcS022yBstmS0DlBS8hgbN15MbOx44uKmUl//X5qbtwMm6GRkXEVGxrXExIzslA+frx6Xay1O5xoaG9fjcq2jsXETFouduLgpxMVNIyFhBsnJ52Cx7OewGoeI212C1eroNUAL0d+k+0iEnM/nBAJdCjetNQ0N/0NrTVzcVKzWqC7Lau2nouIViorux+MpJyFhBgkJJ2G3D6ai4hWqq98F/Fitca0XxxPx+xtpaSkIphEZmUFs7EQcjvH4/Y04nV/icq1Day+RkYMZPPh7ZGZej92e0WndXm8dTU2baWnZiVKRWK2xWK2x2GwpREZmEBGRTCDQQkPDZ9TVLaOh4XMggMUShcUSTWzsWDIyrgl2o2ntp7r6PSoqFhIbO4GsrBuJiHB02eZAwMOuXb+lqOjXREQkMmLEY6SlXdznazaBgPuAn273eutobFyPz1dPcvJZIXnexedzYbHY5An8Q+ywGOZCKXU28CfACjyjtX5wr8/twPPAFKAamK+1LuwtTQkKoiO3u5TKykW0tBTi89Xh89WhVAQOxyQcjsnExU0iMjK9y3KBgJuamiXs2fM4NTXvo1QENltasED3+erwePb0um6lTAvDXJi34HBMwGKJJhBoxu9vCrZokpPPIj5+OmVlz9PS8g0REUn4fLXYbKkMGXIrmZnfQakItPbT2LiRbdt+QFPTRtLS5tHc/A0u12pSUy8gN/fXuN17aGz8mqamrURExGO3D8Fuz8brraG+fgV1dctxu4tJSTmXwYO/T3Ly2Shl7Tb/Xm8dLtdqGhq+wOn8AqfzK9zu9oci7fZssrNvJjNzAVr7qKtbSm3tB/h8ThITZ5KQMJuYmFFdglUg4KW2dgmNjRuJjj6G6OhR2O2Z1NQsoaLiZWpq3msNdo+SlnZJt8FOa01j43paWnZjt2cSGZlFZGRaj9sCpnVYU7OY2tol+P1NKBWBUlaio0eSmXkdkZGpPS7b2LgJt3s3CQmzu62c7M3vb6S09BlKS/+Ow5FPTs7dREcP75D/AG73bmy29D6lZ/LfgMu1Dqs1DodjYr/fuDHgQUGZb28bcAZQDHwJXKa13tRhnh8CE7TW31dKXQrM1VrP7y1dCQqivzU1bae8/Hk8nnICgWYCgRYslhhiY8cQEzOG6OjhaO3H72/E73fh81Xjdpfi8ZQBmoSEmSQknITNltgpXXNn198pK3sWt7uYhISTyMq6kdTUubhca9i585fU1i7ukh+7fQgjR/6FlJTzCAR8FBf/kcLCXxAItATniYhIwe93obU7OM1mG0Ri4izs9mwqKl7G4ynDbh+KwzEBsKCUBb+/Gbe7GLe7uNPNAtHRI4mLm9zaopqA1n527/499fXLsUL8i8YAAAlGSURBVFod+P3NmBaZA6vV0brtYLOl4XDkExs7npiYPFyur6isXNR67airyMjBpKXNo6HhU5zOVSQnn8eIEX8CoKWliJaWndTVLaem5gO83vJOyyoVSXz8dJKSTicp6XQsFjtNTZtpbNxEQ8Pn1NcvR2sfERFJ2GwpwYEl3e5iLJZo0tOvIivrB9hspttTay/V1e9SVvYPnM4vALBa40lNncugQfOw24cEKwmBQAtebzVebxVO5+eUlDyBz1eDwzGZpqZNBAJeMjKuIjHxZGprP6SmZjFebwWgsNuHEB09gri4ySQmziYh4SSs1jgaGzdRV7eM+voVOJ2raWn5JritMTFjSE+/kkGD5hEZmYHFEnPQQeJwCAonAPdorc9qfX87gNb6gQ7zLG6dZ6VSKgIoA9J0L5mSoCCONFr7Wx8iHNzls/r6lTQ0/A+wopQFqzWWtLRLiIjoPIR3U9MO6uo+an1SfTyRkWnBC/5u926s1liio0cGC45AwEt19VuUlv69tQD3o3UAiyUSuz27tYUxBIcjn7i4qdhs3Q/V3NDwOXv2PI3dnklS0pnEx09HqQiam3dQV7e89eaAr1sLxhYslihSUuaQnn45CQkzaGkpoqlpG253EfHxJ5CQcBJKWdDaT3Hxo+zceReBQFOnddpsqSQlnUFS0pnExIzG4ynD4ymhubmAurpluFxrgY5FhJXY2DySk88lNXVOax7bWxSNjRspLn6EsrIXOgXRNrGx44PXpSorX6Wy8nX8/oZev9PU1AsYMuRWEhJOwO0uZdeu37Jnz5No7Q7mPyHhJLzeSpqattPcvK21y9IDWIiIiMfnqwPAbh9KfPxxra3bibjdxZSVvUBDw6cd1qiwWmPJzv4Zubn39Jq3nhwOQeFi4Gyt9XWt768Ejtda/6jDPBta5ylufV/QOk/31QwkKAhxONLaT3PzTiIjBxEREd/n5ZqbC6msfJXIyDTs9qGttepjer1DzOOpam0VBIiNHUt09LF9uvbh8VRQXf1Op+dw4uKm4nBM6lQL9/tbaGj4FJ+vvrUrsBmLxY7NlorNltoaVDO7pO92l+HxlOFwTOg2/35/c+v1p+V4PCXEx5vbt6Ojc3rYNwXU1CzB728ItlITE08mNfXb+9zW7hxVQUEpdT1wPcDQoUOnFBUVIYQQou/6GhRCecN2CTCkw/vs1mndztPafZSAueDcidb6Ka31VK311LS0rrdBCiGE6B+hDApfAiOUUrlKqUjgUuCtveZ5C7i69f+LgY97u54ghBAitEL28JrW2qeU+hGwGHNL6rNa641K/f/27i3EqiqO4/j3V3ZTI7GLlFZmRmVRWhGWFWI92IX0IbtpRNSbkEZRGl0o6CGIrAcpQwkrKcuUIiKqSSQf0jS7apFY6ISmoHYDS+3fw1qzPTOaM0x49uT6fV6cfZnDOov/9n/O2rP/fz1OaiD9NjAHeFnSWmArKXGYmVlNDugTzRHxLvBuh32PNPy8A5hwIMdgZmZd5yIwZmZWcVIwM7OKk4KZmVWcFMzMrPK/q5IqaQvQ3afXjgP+9WnpAnk+2vN87OG5aO9gmI9TI6LTB73+d0nhv5C0oitP9JXC89Ge52MPz0V7Jc2Hl4/MzKzipGBmZpXSksILdQ+gh/F8tOf52MNz0V4x81HUPQUzM9u/0r4pmJnZfhSTFCSNlfSdpLWSptU9nmaSdLKkxZJWS/pG0pS8v7+kDyR9n//dd/utg5SkQyWtkvRO3j5N0rIcI/Nzdd8iSOonaYGkbyWtkXRJqfEh6Z58nXwt6VVJR5YUG0UkhdwveiZwNTAMuEXSsHpH1VS7gHsjYhgwEpic3/80oCUizgBa8nZJpgBrGrafBGZExFBgG3BnLaOqx7PAexFxFnA+aV6Kiw9JA4G7gYsi4lxSheebKSg2ikgKwMXA2ohYF6lJ6mvAuJrH1DQRsTEiPss//0a64AeS5mBuPm0uML6eETafpEHAtcDsvC1gDLAgn1LMfEg6BriCVMqeiPgrIrZTbnz0Ao7Kjb96AxspKDZKSQoDgQ0N2615X3EkDQZGAMuAARGxMR/aBAyoaVh1eAa4H/g7bx8LbI+IXXm7pBg5DdgCvJiX02ZL6kOB8RERPwFPAetJyeAXYCUFxUYpScEASX2BN4GpEfFr47Hc8a6IP0WTdB2wOSJW1j2WHqIXcAHwXESMAP6gw1JRKfGR75uMIyXKk4A+wNhaB9VkpSSFrvSLPqhJOoyUEOZFxMK8+2dJJ+bjJwKb6xpfk40Crpf0I2kpcQxpTb1fXjKAsmKkFWiNiGV5ewEpSZQYH1cBP0TElojYCSwkxUsxsVFKUuhKv+iDVl4vnwOsiYinGw419si+HXir2WOrQ0RMj4hBETGYFAsfRcREYDGpVziUNR+bgA2Szsy7rgRWU2Z8rAdGSuqdr5u2uSgmNop5eE3SNaR15LZ+0U/UPKSmkXQZ8DHwFXvW0B8k3Vd4HTiFVHn2xojYWssgayJpNHBfRFwnaQjpm0N/YBUwKSL+rHN8zSJpOOmm++HAOuAO0ofG4uJD0mPATaS/2lsF3EW6h1BEbBSTFMzMrHOlLB+ZmVkXOCmYmVnFScHMzCpOCmZmVnFSMDOzipOCWRNJGt1WldWsJ3JSMDOzipOC2T5ImiRpuaTPJc3KvRd+lzQj19pvkXR8Pne4pE8kfSlpUVvfAUlDJX0o6QtJn0k6Pb9834beBfPyk7NmPYKTglkHks4mPdE6KiKGA7uBiaTiaCsi4hxgCfBo/pWXgAci4jzSU+Nt++cBMyPifOBSUtVNSFVqp5J6ewwh1dYx6xF6dX6KWXGuBC4EPs0f4o8iFYP7G5ifz3kFWJh7EfSLiCV5/1zgDUlHAwMjYhFAROwAyK+3PCJa8/bnwGBg6YF/W2adc1Iw25uAuRExvd1O6eEO53W3RkxjzZzd+Dq0HsTLR2Z7awFukHQCVL2sTyVdL22VMm8FlkbEL8A2SZfn/bcBS3KHu1ZJ4/NrHCGpd1PfhVk3+BOKWQcRsVrSQ8D7kg4BdgKTSc1nLs7HNpPuO0Aqpfx8/k+/rcIopAQxS9Lj+TUmNPFtmHWLq6SadZGk3yOib93jMDuQvHxkZmYVf1MwM7OKvymYmVnFScHMzCpOCmZmVnFSMDOzipOCmZlVnBTMzKzyD5tJh+vbhWDuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.1987 - acc: 0.9533\n",
      "Loss: 0.1986725314780451 Accuracy: 0.95327103\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1175 - acc: 0.3725\n",
      "Epoch 00001: val_loss improved from inf to 0.93022, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/001-0.9302.hdf5\n",
      "36805/36805 [==============================] - 331s 9ms/sample - loss: 2.1176 - acc: 0.3726 - val_loss: 0.9302 - val_acc: 0.7258\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7089\n",
      "Epoch 00002: val_loss improved from 0.93022 to 0.57737, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/002-0.5774.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.9353 - acc: 0.7090 - val_loss: 0.5774 - val_acc: 0.8246\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.8231\n",
      "Epoch 00003: val_loss improved from 0.57737 to 0.35799, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/003-0.3580.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.5796 - acc: 0.8230 - val_loss: 0.3580 - val_acc: 0.8933\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8683\n",
      "Epoch 00004: val_loss improved from 0.35799 to 0.25806, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/004-0.2581.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.4260 - acc: 0.8683 - val_loss: 0.2581 - val_acc: 0.9224\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8910\n",
      "Epoch 00005: val_loss improved from 0.25806 to 0.24012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/005-0.2401.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.3504 - acc: 0.8909 - val_loss: 0.2401 - val_acc: 0.9334\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9079\n",
      "Epoch 00006: val_loss improved from 0.24012 to 0.22583, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/006-0.2258.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.2951 - acc: 0.9079 - val_loss: 0.2258 - val_acc: 0.9343\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9189\n",
      "Epoch 00007: val_loss did not improve from 0.22583\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.2557 - acc: 0.9189 - val_loss: 0.3264 - val_acc: 0.9043\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9295\n",
      "Epoch 00008: val_loss improved from 0.22583 to 0.18501, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/008-0.1850.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.2237 - acc: 0.9295 - val_loss: 0.1850 - val_acc: 0.9455\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9360\n",
      "Epoch 00009: val_loss improved from 0.18501 to 0.17458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/009-0.1746.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.2024 - acc: 0.9359 - val_loss: 0.1746 - val_acc: 0.9490\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9426\n",
      "Epoch 00010: val_loss improved from 0.17458 to 0.16070, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/010-0.1607.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.1823 - acc: 0.9425 - val_loss: 0.1607 - val_acc: 0.9581\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9465\n",
      "Epoch 00011: val_loss improved from 0.16070 to 0.13703, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/011-0.1370.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.1684 - acc: 0.9466 - val_loss: 0.1370 - val_acc: 0.9583\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9511\n",
      "Epoch 00012: val_loss did not improve from 0.13703\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.1531 - acc: 0.9510 - val_loss: 0.1531 - val_acc: 0.9541\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9509\n",
      "Epoch 00013: val_loss did not improve from 0.13703\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.1549 - acc: 0.9509 - val_loss: 0.1469 - val_acc: 0.9532\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9601\n",
      "Epoch 00014: val_loss did not improve from 0.13703\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.1272 - acc: 0.9601 - val_loss: 0.2124 - val_acc: 0.9392\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9624\n",
      "Epoch 00015: val_loss did not improve from 0.13703\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.1177 - acc: 0.9623 - val_loss: 0.1707 - val_acc: 0.9474\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9599\n",
      "Epoch 00016: val_loss did not improve from 0.13703\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.1221 - acc: 0.9599 - val_loss: 0.1571 - val_acc: 0.9550\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9684\n",
      "Epoch 00017: val_loss improved from 0.13703 to 0.13233, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/017-0.1323.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0977 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9634\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9696\n",
      "Epoch 00018: val_loss did not improve from 0.13233\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0938 - acc: 0.9696 - val_loss: 0.1668 - val_acc: 0.9539\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9738\n",
      "Epoch 00019: val_loss improved from 0.13233 to 0.12181, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/019-0.1218.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0829 - acc: 0.9738 - val_loss: 0.1218 - val_acc: 0.9672\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9743\n",
      "Epoch 00020: val_loss improved from 0.12181 to 0.11472, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/020-0.1147.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0794 - acc: 0.9743 - val_loss: 0.1147 - val_acc: 0.9667\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9757\n",
      "Epoch 00021: val_loss did not improve from 0.11472\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0773 - acc: 0.9756 - val_loss: 0.1325 - val_acc: 0.9630\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9710\n",
      "Epoch 00022: val_loss did not improve from 0.11472\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0929 - acc: 0.9709 - val_loss: 0.1633 - val_acc: 0.9536\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9739\n",
      "Epoch 00023: val_loss did not improve from 0.11472\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0809 - acc: 0.9738 - val_loss: 0.1470 - val_acc: 0.9613\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9796\n",
      "Epoch 00024: val_loss improved from 0.11472 to 0.11285, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/024-0.1128.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0654 - acc: 0.9796 - val_loss: 0.1128 - val_acc: 0.9688\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9829\n",
      "Epoch 00025: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0550 - acc: 0.9829 - val_loss: 0.1619 - val_acc: 0.9576\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9832\n",
      "Epoch 00026: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0543 - acc: 0.9832 - val_loss: 0.1288 - val_acc: 0.9632\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9802\n",
      "Epoch 00027: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0599 - acc: 0.9802 - val_loss: 0.1260 - val_acc: 0.9690\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9843\n",
      "Epoch 00028: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0507 - acc: 0.9843 - val_loss: 0.1519 - val_acc: 0.9618\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9841\n",
      "Epoch 00029: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0496 - acc: 0.9841 - val_loss: 0.1556 - val_acc: 0.9611\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9829\n",
      "Epoch 00030: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0519 - acc: 0.9829 - val_loss: 0.1440 - val_acc: 0.9604\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9841\n",
      "Epoch 00031: val_loss improved from 0.11285 to 0.11273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/031-0.1127.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0496 - acc: 0.9841 - val_loss: 0.1127 - val_acc: 0.9681\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9885\n",
      "Epoch 00032: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0378 - acc: 0.9884 - val_loss: 0.1934 - val_acc: 0.9597\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9808\n",
      "Epoch 00033: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0593 - acc: 0.9808 - val_loss: 0.1349 - val_acc: 0.9662\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9892\n",
      "Epoch 00034: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0349 - acc: 0.9891 - val_loss: 0.1300 - val_acc: 0.9655\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9882\n",
      "Epoch 00035: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0383 - acc: 0.9882 - val_loss: 0.1151 - val_acc: 0.9702\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9907\n",
      "Epoch 00036: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0303 - acc: 0.9907 - val_loss: 0.1805 - val_acc: 0.9567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9873\n",
      "Epoch 00037: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0399 - acc: 0.9873 - val_loss: 0.1514 - val_acc: 0.9623\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9896\n",
      "Epoch 00038: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0330 - acc: 0.9895 - val_loss: 0.1684 - val_acc: 0.9602\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9870\n",
      "Epoch 00039: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0413 - acc: 0.9869 - val_loss: 0.1216 - val_acc: 0.9704\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9881\n",
      "Epoch 00040: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0369 - acc: 0.9881 - val_loss: 0.1377 - val_acc: 0.9669\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9914\n",
      "Epoch 00041: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0283 - acc: 0.9914 - val_loss: 0.1219 - val_acc: 0.9693\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9877\n",
      "Epoch 00042: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0381 - acc: 0.9877 - val_loss: 0.1497 - val_acc: 0.9623\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9925\n",
      "Epoch 00043: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0248 - acc: 0.9925 - val_loss: 0.1218 - val_acc: 0.9688\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9922\n",
      "Epoch 00044: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0254 - acc: 0.9921 - val_loss: 0.1615 - val_acc: 0.9623\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9894\n",
      "Epoch 00045: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0361 - acc: 0.9894 - val_loss: 0.1405 - val_acc: 0.9639\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9905\n",
      "Epoch 00046: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0296 - acc: 0.9905 - val_loss: 0.1674 - val_acc: 0.9562\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9880\n",
      "Epoch 00047: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0376 - acc: 0.9880 - val_loss: 0.1151 - val_acc: 0.9688\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9951\n",
      "Epoch 00048: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0165 - acc: 0.9950 - val_loss: 0.1423 - val_acc: 0.9683\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9919\n",
      "Epoch 00049: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0247 - acc: 0.9919 - val_loss: 0.1556 - val_acc: 0.9658\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9938\n",
      "Epoch 00050: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0196 - acc: 0.9938 - val_loss: 0.2206 - val_acc: 0.9497\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9882\n",
      "Epoch 00051: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0371 - acc: 0.9882 - val_loss: 0.1283 - val_acc: 0.9681\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9948\n",
      "Epoch 00052: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0179 - acc: 0.9948 - val_loss: 0.1527 - val_acc: 0.9639\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9914\n",
      "Epoch 00053: val_loss did not improve from 0.11273\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0268 - acc: 0.9914 - val_loss: 0.1589 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9904\n",
      "Epoch 00054: val_loss improved from 0.11273 to 0.10471, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv_checkpoint/054-0.1047.hdf5\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0309 - acc: 0.9904 - val_loss: 0.1047 - val_acc: 0.9704\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9956\n",
      "Epoch 00055: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0164 - acc: 0.9955 - val_loss: 0.1487 - val_acc: 0.9662\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9915\n",
      "Epoch 00056: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0260 - acc: 0.9915 - val_loss: 0.1223 - val_acc: 0.9676\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9956\n",
      "Epoch 00057: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0149 - acc: 0.9956 - val_loss: 0.1225 - val_acc: 0.9679\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9950\n",
      "Epoch 00058: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0167 - acc: 0.9950 - val_loss: 0.1418 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9919\n",
      "Epoch 00059: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0259 - acc: 0.9918 - val_loss: 0.1442 - val_acc: 0.9634\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9907\n",
      "Epoch 00060: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0278 - acc: 0.9907 - val_loss: 0.1161 - val_acc: 0.9734\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00061: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.1975 - val_acc: 0.9588\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9962\n",
      "Epoch 00062: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0121 - acc: 0.9962 - val_loss: 0.1145 - val_acc: 0.9746\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9947\n",
      "Epoch 00063: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0177 - acc: 0.9946 - val_loss: 0.1833 - val_acc: 0.9611\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9930\n",
      "Epoch 00064: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0208 - acc: 0.9930 - val_loss: 0.1421 - val_acc: 0.9716\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 00065: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0137 - acc: 0.9959 - val_loss: 0.1279 - val_acc: 0.9716\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9944\n",
      "Epoch 00066: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0184 - acc: 0.9944 - val_loss: 0.1443 - val_acc: 0.9674\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 00067: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.1479 - val_acc: 0.9700\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9943\n",
      "Epoch 00068: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0195 - acc: 0.9943 - val_loss: 0.1494 - val_acc: 0.9681\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.3149 - val_acc: 0.9439\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9921\n",
      "Epoch 00070: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0249 - acc: 0.9921 - val_loss: 0.1238 - val_acc: 0.9702\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9963\n",
      "Epoch 00071: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0115 - acc: 0.9963 - val_loss: 0.1105 - val_acc: 0.9739\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 00072: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.1370 - val_acc: 0.9711\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9968\n",
      "Epoch 00073: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0108 - acc: 0.9968 - val_loss: 0.1496 - val_acc: 0.9683\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00074: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1132 - val_acc: 0.9748\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9978\n",
      "Epoch 00075: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0079 - acc: 0.9978 - val_loss: 0.1365 - val_acc: 0.9702\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9968\n",
      "Epoch 00076: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0102 - acc: 0.9968 - val_loss: 0.1805 - val_acc: 0.9592\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9922\n",
      "Epoch 00077: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0250 - acc: 0.9922 - val_loss: 0.1127 - val_acc: 0.9744\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 00078: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0093 - acc: 0.9971 - val_loss: 0.1256 - val_acc: 0.9718\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9933\n",
      "Epoch 00079: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0227 - acc: 0.9932 - val_loss: 0.1649 - val_acc: 0.9679\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9956\n",
      "Epoch 00080: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0138 - acc: 0.9956 - val_loss: 0.1249 - val_acc: 0.9706\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9967\n",
      "Epoch 00081: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0101 - acc: 0.9967 - val_loss: 0.1378 - val_acc: 0.9700\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 00082: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1506 - val_acc: 0.9700\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00083: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0191 - acc: 0.9941 - val_loss: 0.1475 - val_acc: 0.9676\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00084: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0147 - acc: 0.9951 - val_loss: 0.1430 - val_acc: 0.9697\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9951\n",
      "Epoch 00085: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0173 - acc: 0.9951 - val_loss: 0.1459 - val_acc: 0.9709\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9967\n",
      "Epoch 00086: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.1206 - val_acc: 0.9739\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 00087: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0176 - acc: 0.9946 - val_loss: 0.1621 - val_acc: 0.9641\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 00088: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0126 - acc: 0.9962 - val_loss: 0.1349 - val_acc: 0.9704\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9964\n",
      "Epoch 00089: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0125 - acc: 0.9964 - val_loss: 0.1444 - val_acc: 0.9723\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9961\n",
      "Epoch 00090: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1150 - val_acc: 0.9748\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9969\n",
      "Epoch 00091: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0104 - acc: 0.9969 - val_loss: 0.1455 - val_acc: 0.9690\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 00092: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0148 - acc: 0.9954 - val_loss: 0.1576 - val_acc: 0.9669\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 00093: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0073 - acc: 0.9978 - val_loss: 0.1777 - val_acc: 0.9620\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9944\n",
      "Epoch 00094: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.1569 - val_acc: 0.9688\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00095: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.1352 - val_acc: 0.9713\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00096: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1410 - val_acc: 0.9686\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9978\n",
      "Epoch 00097: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1602 - val_acc: 0.9646\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00098: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.1387 - val_acc: 0.9686\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9933\n",
      "Epoch 00099: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0224 - acc: 0.9933 - val_loss: 0.1195 - val_acc: 0.9748\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 00100: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 0.1347 - val_acc: 0.9730\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9942\n",
      "Epoch 00101: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0189 - acc: 0.9942 - val_loss: 0.1397 - val_acc: 0.9730\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9976\n",
      "Epoch 00102: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0074 - acc: 0.9976 - val_loss: 0.1182 - val_acc: 0.9765\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00103: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 314s 9ms/sample - loss: 0.0052 - acc: 0.9987 - val_loss: 0.1514 - val_acc: 0.9618\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9940\n",
      "Epoch 00104: val_loss did not improve from 0.10471\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 0.0178 - acc: 0.9940 - val_loss: 0.1162 - val_acc: 0.9730\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1302 - acc: 0.9639\n",
      "Loss: 0.13020230604235086 Accuracy: 0.96386296\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6270 - acc: 0.5272\n",
      "Epoch 00001: val_loss improved from inf to 0.63462, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/001-0.6346.hdf5\n",
      "36805/36805 [==============================] - 331s 9ms/sample - loss: 1.6271 - acc: 0.5272 - val_loss: 0.6346 - val_acc: 0.8015\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8170\n",
      "Epoch 00002: val_loss improved from 0.63462 to 0.28073, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/002-0.2807.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.5881 - acc: 0.8169 - val_loss: 0.2807 - val_acc: 0.9180\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8789\n",
      "Epoch 00003: val_loss improved from 0.28073 to 0.25927, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/003-0.2593.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.3890 - acc: 0.8789 - val_loss: 0.2593 - val_acc: 0.9189\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9075\n",
      "Epoch 00004: val_loss improved from 0.25927 to 0.22225, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/004-0.2223.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2992 - acc: 0.9074 - val_loss: 0.2223 - val_acc: 0.9315\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9188\n",
      "Epoch 00005: val_loss improved from 0.22225 to 0.20859, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/005-0.2086.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2602 - acc: 0.9188 - val_loss: 0.2086 - val_acc: 0.9401\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9320\n",
      "Epoch 00006: val_loss improved from 0.20859 to 0.17144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/006-0.1714.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.2136 - acc: 0.9319 - val_loss: 0.1714 - val_acc: 0.9497\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9430\n",
      "Epoch 00007: val_loss improved from 0.17144 to 0.15634, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/007-0.1563.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.1851 - acc: 0.9431 - val_loss: 0.1563 - val_acc: 0.9539\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9489\n",
      "Epoch 00008: val_loss improved from 0.15634 to 0.14994, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/008-0.1499.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1617 - acc: 0.9489 - val_loss: 0.1499 - val_acc: 0.9564\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9524\n",
      "Epoch 00009: val_loss did not improve from 0.14994\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1441 - acc: 0.9524 - val_loss: 0.1546 - val_acc: 0.9564\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9596\n",
      "Epoch 00010: val_loss did not improve from 0.14994\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.1222 - acc: 0.9596 - val_loss: 0.2241 - val_acc: 0.9408\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9646\n",
      "Epoch 00011: val_loss did not improve from 0.14994\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1099 - acc: 0.9646 - val_loss: 0.1814 - val_acc: 0.9457\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9638\n",
      "Epoch 00012: val_loss improved from 0.14994 to 0.14540, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/012-0.1454.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.1108 - acc: 0.9637 - val_loss: 0.1454 - val_acc: 0.9546\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9681\n",
      "Epoch 00013: val_loss did not improve from 0.14540\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.1001 - acc: 0.9681 - val_loss: 0.1514 - val_acc: 0.9581\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9685\n",
      "Epoch 00014: val_loss improved from 0.14540 to 0.13519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/014-0.1352.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0964 - acc: 0.9685 - val_loss: 0.1352 - val_acc: 0.9630\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9695\n",
      "Epoch 00015: val_loss improved from 0.13519 to 0.11602, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/015-0.1160.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0938 - acc: 0.9694 - val_loss: 0.1160 - val_acc: 0.9665\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9772\n",
      "Epoch 00016: val_loss did not improve from 0.11602\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0723 - acc: 0.9772 - val_loss: 0.1672 - val_acc: 0.9522\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9757\n",
      "Epoch 00017: val_loss did not improve from 0.11602\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0759 - acc: 0.9757 - val_loss: 0.1246 - val_acc: 0.9679\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9782\n",
      "Epoch 00018: val_loss did not improve from 0.11602\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0678 - acc: 0.9782 - val_loss: 0.1407 - val_acc: 0.9634\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9807\n",
      "Epoch 00019: val_loss did not improve from 0.11602\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0589 - acc: 0.9806 - val_loss: 0.1381 - val_acc: 0.9627\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9759\n",
      "Epoch 00020: val_loss improved from 0.11602 to 0.10793, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/020-0.1079.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0741 - acc: 0.9758 - val_loss: 0.1079 - val_acc: 0.9704\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9839\n",
      "Epoch 00021: val_loss did not improve from 0.10793\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0497 - acc: 0.9839 - val_loss: 0.1623 - val_acc: 0.9548\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9827\n",
      "Epoch 00022: val_loss did not improve from 0.10793\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0542 - acc: 0.9827 - val_loss: 0.1308 - val_acc: 0.9604\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9848\n",
      "Epoch 00023: val_loss did not improve from 0.10793\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0471 - acc: 0.9848 - val_loss: 0.1743 - val_acc: 0.9553\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9867\n",
      "Epoch 00024: val_loss did not improve from 0.10793\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0415 - acc: 0.9867 - val_loss: 0.1128 - val_acc: 0.9693\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9879\n",
      "Epoch 00025: val_loss did not improve from 0.10793\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0385 - acc: 0.9879 - val_loss: 0.1547 - val_acc: 0.9590\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9844\n",
      "Epoch 00026: val_loss improved from 0.10793 to 0.09911, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv_checkpoint/026-0.0991.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0491 - acc: 0.9844 - val_loss: 0.0991 - val_acc: 0.9727\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9849\n",
      "Epoch 00027: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0492 - acc: 0.9849 - val_loss: 0.1341 - val_acc: 0.9662\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9901\n",
      "Epoch 00028: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0327 - acc: 0.9901 - val_loss: 0.1540 - val_acc: 0.9613\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9876\n",
      "Epoch 00029: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0390 - acc: 0.9876 - val_loss: 0.1589 - val_acc: 0.9623\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9859\n",
      "Epoch 00030: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0431 - acc: 0.9859 - val_loss: 0.1351 - val_acc: 0.9641\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9904\n",
      "Epoch 00031: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0301 - acc: 0.9904 - val_loss: 0.1764 - val_acc: 0.9564\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9901\n",
      "Epoch 00032: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0313 - acc: 0.9901 - val_loss: 0.1285 - val_acc: 0.9651\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9905\n",
      "Epoch 00033: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0309 - acc: 0.9905 - val_loss: 0.1244 - val_acc: 0.9665\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9896\n",
      "Epoch 00034: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0328 - acc: 0.9896 - val_loss: 0.1268 - val_acc: 0.9669\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9925\n",
      "Epoch 00035: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0254 - acc: 0.9925 - val_loss: 0.1530 - val_acc: 0.9618\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9895\n",
      "Epoch 00036: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0332 - acc: 0.9895 - val_loss: 0.1984 - val_acc: 0.9478\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9890\n",
      "Epoch 00037: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0350 - acc: 0.9890 - val_loss: 0.1242 - val_acc: 0.9688\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9920\n",
      "Epoch 00038: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0269 - acc: 0.9920 - val_loss: 0.1326 - val_acc: 0.9653\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9899\n",
      "Epoch 00039: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0344 - acc: 0.9899 - val_loss: 0.1007 - val_acc: 0.9755\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9955\n",
      "Epoch 00040: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0160 - acc: 0.9955 - val_loss: 0.1211 - val_acc: 0.9702\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9951\n",
      "Epoch 00041: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0155 - acc: 0.9951 - val_loss: 0.1377 - val_acc: 0.9667\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9918\n",
      "Epoch 00042: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0265 - acc: 0.9918 - val_loss: 0.1329 - val_acc: 0.9681\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9934\n",
      "Epoch 00043: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0211 - acc: 0.9934 - val_loss: 0.1243 - val_acc: 0.9709\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9927\n",
      "Epoch 00044: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0256 - acc: 0.9926 - val_loss: 0.1196 - val_acc: 0.9709\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9910\n",
      "Epoch 00045: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0282 - acc: 0.9910 - val_loss: 0.1132 - val_acc: 0.9723\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9964\n",
      "Epoch 00046: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0129 - acc: 0.9964 - val_loss: 0.1442 - val_acc: 0.9700\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 00047: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0138 - acc: 0.9955 - val_loss: 0.1462 - val_acc: 0.9660\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9948\n",
      "Epoch 00048: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0179 - acc: 0.9948 - val_loss: 0.1239 - val_acc: 0.9697\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9943\n",
      "Epoch 00049: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.1578 - val_acc: 0.9644\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9950\n",
      "Epoch 00050: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0158 - acc: 0.9950 - val_loss: 0.1271 - val_acc: 0.9716\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9963\n",
      "Epoch 00051: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0134 - acc: 0.9963 - val_loss: 0.1368 - val_acc: 0.9665\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9949\n",
      "Epoch 00052: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0168 - acc: 0.9949 - val_loss: 0.1251 - val_acc: 0.9718\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9959\n",
      "Epoch 00053: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0134 - acc: 0.9958 - val_loss: 0.1419 - val_acc: 0.9679\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9906\n",
      "Epoch 00054: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0299 - acc: 0.9906 - val_loss: 0.1414 - val_acc: 0.9651\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9960\n",
      "Epoch 00055: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0126 - acc: 0.9960 - val_loss: 0.1269 - val_acc: 0.9725\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9965\n",
      "Epoch 00056: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.1231 - val_acc: 0.9695\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9971\n",
      "Epoch 00057: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0092 - acc: 0.9971 - val_loss: 0.1151 - val_acc: 0.9739\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9945\n",
      "Epoch 00058: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1438 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00059: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.1680 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9973\n",
      "Epoch 00060: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0104 - acc: 0.9973 - val_loss: 0.1313 - val_acc: 0.9727\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0160 - acc: 0.9951 - val_loss: 0.1376 - val_acc: 0.9665\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9956\n",
      "Epoch 00062: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0141 - acc: 0.9956 - val_loss: 0.1828 - val_acc: 0.9653\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9957\n",
      "Epoch 00063: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0151 - acc: 0.9957 - val_loss: 0.1330 - val_acc: 0.9697\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 00064: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0080 - acc: 0.9974 - val_loss: 0.1567 - val_acc: 0.9672\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9934\n",
      "Epoch 00065: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0219 - acc: 0.9934 - val_loss: 0.1328 - val_acc: 0.9683\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9951\n",
      "Epoch 00066: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0168 - acc: 0.9951 - val_loss: 0.1074 - val_acc: 0.9739\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 00067: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.1075 - val_acc: 0.9727\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9989\n",
      "Epoch 00068: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0042 - acc: 0.9989 - val_loss: 0.1050 - val_acc: 0.9746\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9966\n",
      "Epoch 00069: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.1386 - val_acc: 0.9688\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9954\n",
      "Epoch 00070: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0159 - acc: 0.9954 - val_loss: 0.1093 - val_acc: 0.9732\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 00071: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0067 - acc: 0.9978 - val_loss: 0.1245 - val_acc: 0.9720\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9970\n",
      "Epoch 00072: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0099 - acc: 0.9969 - val_loss: 0.1194 - val_acc: 0.9727\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9947\n",
      "Epoch 00073: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0176 - acc: 0.9947 - val_loss: 0.1186 - val_acc: 0.9716\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9961\n",
      "Epoch 00074: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0130 - acc: 0.9961 - val_loss: 0.1096 - val_acc: 0.9746\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 00075: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.1268 - val_acc: 0.9727\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9972\n",
      "Epoch 00076: val_loss did not improve from 0.09911\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.1632 - val_acc: 0.9653\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1419 - acc: 0.9607\n",
      "Loss: 0.1418630836651441 Accuracy: 0.96074766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_128_DO_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1562000     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,562,000\n",
      "Trainable params: 1,559,952\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.9776 - acc: 0.7290\n",
      "Loss: 0.977601520146165 Accuracy: 0.72897196\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_98_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1453968     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,453,968\n",
      "Trainable params: 1,450,896\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.6927 - acc: 0.8135\n",
      "Loss: 0.6927284018520502 Accuracy: 0.8134995\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_108_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1309072     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,309,072\n",
      "Trainable params: 1,304,976\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.4132 - acc: 0.8966\n",
      "Loss: 0.41316051120760533 Accuracy: 0.8965732\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_120_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1524624     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,524,624\n",
      "Trainable params: 1,519,504\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1987 - acc: 0.9533\n",
      "Loss: 0.1986725314780451 Accuracy: 0.95327103\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_134_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           1863056     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,863,056\n",
      "Trainable params: 1,856,912\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1302 - acc: 0.9639\n",
      "Loss: 0.13020230604235086 Accuracy: 0.96386296\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_150_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           3035536     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,035,536\n",
      "Trainable params: 3,027,344\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1419 - acc: 0.9607\n",
      "Loss: 0.1418630836651441 Accuracy: 0.96074766\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1562000     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,562,000\n",
      "Trainable params: 1,559,952\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 1.3574 - acc: 0.7591\n",
      "Loss: 1.3574142118604509 Accuracy: 0.7590862\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_98_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1453968     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,453,968\n",
      "Trainable params: 1,450,896\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 1.0567 - acc: 0.8150\n",
      "Loss: 1.0567264173694364 Accuracy: 0.81495327\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_108_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1309072     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,309,072\n",
      "Trainable params: 1,304,976\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.5001 - acc: 0.9030\n",
      "Loss: 0.5000750778991485 Accuracy: 0.90301144\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_120_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1524624     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,524,624\n",
      "Trainable params: 1,519,504\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.2222 - acc: 0.9531\n",
      "Loss: 0.2221942367571815 Accuracy: 0.95306337\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_134_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           1863056     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,863,056\n",
      "Trainable params: 1,856,912\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1493 - acc: 0.9672\n",
      "Loss: 0.14931701532588812 Accuracy: 0.96718585\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_150_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           3035536     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,035,536\n",
      "Trainable params: 3,027,344\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1909 - acc: 0.9610\n",
      "Loss: 0.19089845451132248 Accuracy: 0.9609553\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
