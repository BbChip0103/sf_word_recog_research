{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 32,769,296\n",
      "Trainable params: 32,769,040\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 11,005,840\n",
      "Trainable params: 11,005,328\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 491,984\n",
      "Trainable params: 490,832\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 377,616\n",
      "Trainable params: 376,336\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 351,952\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 359,824\n",
      "Trainable params: 358,288\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,080\n",
      "Trainable params: 362,480\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2730 - acc: 0.2958\n",
      "Epoch 00001: val_loss improved from inf to 1.65314, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/001-1.6531.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 2.2728 - acc: 0.2959 - val_loss: 1.6531 - val_acc: 0.4584\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4362 - acc: 0.5385\n",
      "Epoch 00002: val_loss improved from 1.65314 to 1.01421, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/002-1.0142.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 1.4362 - acc: 0.5385 - val_loss: 1.0142 - val_acc: 0.7023\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.6407\n",
      "Epoch 00003: val_loss improved from 1.01421 to 0.86731, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/003-0.8673.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 1.1320 - acc: 0.6406 - val_loss: 0.8673 - val_acc: 0.7426\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9659 - acc: 0.6975\n",
      "Epoch 00004: val_loss improved from 0.86731 to 0.78811, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/004-0.7881.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.9660 - acc: 0.6975 - val_loss: 0.7881 - val_acc: 0.7754\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8633 - acc: 0.7354\n",
      "Epoch 00005: val_loss improved from 0.78811 to 0.73903, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/005-0.7390.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.8633 - acc: 0.7354 - val_loss: 0.7390 - val_acc: 0.7952\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7840 - acc: 0.7603\n",
      "Epoch 00006: val_loss improved from 0.73903 to 0.66350, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/006-0.6635.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.7839 - acc: 0.7603 - val_loss: 0.6635 - val_acc: 0.8174\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.7805\n",
      "Epoch 00007: val_loss did not improve from 0.66350\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.7199 - acc: 0.7805 - val_loss: 0.6675 - val_acc: 0.8171\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.7947\n",
      "Epoch 00008: val_loss did not improve from 0.66350\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.6790 - acc: 0.7947 - val_loss: 0.6817 - val_acc: 0.8085\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6312 - acc: 0.8096\n",
      "Epoch 00009: val_loss improved from 0.66350 to 0.57058, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/009-0.5706.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.6312 - acc: 0.8096 - val_loss: 0.5706 - val_acc: 0.8444\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.8226\n",
      "Epoch 00010: val_loss improved from 0.57058 to 0.55747, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/010-0.5575.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.5956 - acc: 0.8226 - val_loss: 0.5575 - val_acc: 0.8460\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8314\n",
      "Epoch 00011: val_loss improved from 0.55747 to 0.52778, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/011-0.5278.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.5582 - acc: 0.8314 - val_loss: 0.5278 - val_acc: 0.8570\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8398\n",
      "Epoch 00012: val_loss did not improve from 0.52778\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.5348 - acc: 0.8398 - val_loss: 0.5947 - val_acc: 0.8318\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8464\n",
      "Epoch 00013: val_loss improved from 0.52778 to 0.46937, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/013-0.4694.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.5121 - acc: 0.8463 - val_loss: 0.4694 - val_acc: 0.8682\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8555\n",
      "Epoch 00014: val_loss did not improve from 0.46937\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.4870 - acc: 0.8555 - val_loss: 0.4746 - val_acc: 0.8733\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8612\n",
      "Epoch 00015: val_loss did not improve from 0.46937\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.4628 - acc: 0.8613 - val_loss: 0.5140 - val_acc: 0.8572\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.8667\n",
      "Epoch 00016: val_loss did not improve from 0.46937\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.4474 - acc: 0.8667 - val_loss: 0.5927 - val_acc: 0.8432\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8714\n",
      "Epoch 00017: val_loss did not improve from 0.46937\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.4315 - acc: 0.8714 - val_loss: 0.5220 - val_acc: 0.8502\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8745\n",
      "Epoch 00018: val_loss improved from 0.46937 to 0.45859, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/018-0.4586.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.4213 - acc: 0.8744 - val_loss: 0.4586 - val_acc: 0.8728\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8813\n",
      "Epoch 00019: val_loss improved from 0.45859 to 0.43570, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/019-0.4357.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3975 - acc: 0.8812 - val_loss: 0.4357 - val_acc: 0.8852\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8811\n",
      "Epoch 00020: val_loss did not improve from 0.43570\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3976 - acc: 0.8811 - val_loss: 0.4421 - val_acc: 0.8789\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8882\n",
      "Epoch 00021: val_loss did not improve from 0.43570\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3728 - acc: 0.8882 - val_loss: 0.5254 - val_acc: 0.8491\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8900\n",
      "Epoch 00022: val_loss did not improve from 0.43570\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3591 - acc: 0.8900 - val_loss: 0.4434 - val_acc: 0.8831\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8948\n",
      "Epoch 00023: val_loss improved from 0.43570 to 0.40163, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/023-0.4016.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3477 - acc: 0.8949 - val_loss: 0.4016 - val_acc: 0.8901\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8990\n",
      "Epoch 00024: val_loss did not improve from 0.40163\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3370 - acc: 0.8990 - val_loss: 0.5029 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8991\n",
      "Epoch 00025: val_loss did not improve from 0.40163\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3337 - acc: 0.8991 - val_loss: 0.5663 - val_acc: 0.8477\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9029\n",
      "Epoch 00026: val_loss did not improve from 0.40163\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3211 - acc: 0.9028 - val_loss: 0.4558 - val_acc: 0.8758\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9043\n",
      "Epoch 00027: val_loss did not improve from 0.40163\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3090 - acc: 0.9043 - val_loss: 0.4582 - val_acc: 0.8789\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9064\n",
      "Epoch 00028: val_loss improved from 0.40163 to 0.39721, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/028-0.3972.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.3080 - acc: 0.9064 - val_loss: 0.3972 - val_acc: 0.8954\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9116\n",
      "Epoch 00029: val_loss improved from 0.39721 to 0.39277, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/029-0.3928.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2902 - acc: 0.9116 - val_loss: 0.3928 - val_acc: 0.8894\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9127\n",
      "Epoch 00030: val_loss did not improve from 0.39277\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2872 - acc: 0.9127 - val_loss: 0.4100 - val_acc: 0.8873\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9125\n",
      "Epoch 00031: val_loss did not improve from 0.39277\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2854 - acc: 0.9125 - val_loss: 0.4065 - val_acc: 0.8877\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9156\n",
      "Epoch 00032: val_loss did not improve from 0.39277\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2761 - acc: 0.9155 - val_loss: 0.4931 - val_acc: 0.8693\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9183\n",
      "Epoch 00033: val_loss did not improve from 0.39277\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2664 - acc: 0.9183 - val_loss: 0.4254 - val_acc: 0.8880\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9206\n",
      "Epoch 00034: val_loss improved from 0.39277 to 0.38960, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/034-0.3896.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2565 - acc: 0.9206 - val_loss: 0.3896 - val_acc: 0.8982\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9227\n",
      "Epoch 00035: val_loss did not improve from 0.38960\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2519 - acc: 0.9227 - val_loss: 0.4480 - val_acc: 0.8761\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9242\n",
      "Epoch 00036: val_loss did not improve from 0.38960\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2455 - acc: 0.9242 - val_loss: 0.3938 - val_acc: 0.8875\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9260\n",
      "Epoch 00037: val_loss improved from 0.38960 to 0.38302, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/037-0.3830.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2369 - acc: 0.9259 - val_loss: 0.3830 - val_acc: 0.8975\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9267\n",
      "Epoch 00038: val_loss did not improve from 0.38302\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2379 - acc: 0.9267 - val_loss: 0.4096 - val_acc: 0.8919\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9286\n",
      "Epoch 00039: val_loss did not improve from 0.38302\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2331 - acc: 0.9285 - val_loss: 0.3967 - val_acc: 0.8931\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9295\n",
      "Epoch 00040: val_loss improved from 0.38302 to 0.38273, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/040-0.3827.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2285 - acc: 0.9295 - val_loss: 0.3827 - val_acc: 0.8989\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9307\n",
      "Epoch 00041: val_loss did not improve from 0.38273\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2218 - acc: 0.9306 - val_loss: 0.4032 - val_acc: 0.8889\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9316\n",
      "Epoch 00042: val_loss did not improve from 0.38273\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2159 - acc: 0.9316 - val_loss: 0.4072 - val_acc: 0.8861\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9360\n",
      "Epoch 00043: val_loss improved from 0.38273 to 0.37731, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/043-0.3773.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2050 - acc: 0.9360 - val_loss: 0.3773 - val_acc: 0.8963\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9376\n",
      "Epoch 00044: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.2009 - acc: 0.9376 - val_loss: 0.3855 - val_acc: 0.9005\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9393\n",
      "Epoch 00045: val_loss improved from 0.37731 to 0.36985, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/045-0.3698.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1996 - acc: 0.9393 - val_loss: 0.3698 - val_acc: 0.9003\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9387\n",
      "Epoch 00046: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1965 - acc: 0.9387 - val_loss: 0.4013 - val_acc: 0.9031\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9418\n",
      "Epoch 00047: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1887 - acc: 0.9419 - val_loss: 0.3865 - val_acc: 0.8994\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9409\n",
      "Epoch 00048: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1854 - acc: 0.9409 - val_loss: 0.4214 - val_acc: 0.8854\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9410\n",
      "Epoch 00049: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1837 - acc: 0.9410 - val_loss: 0.3939 - val_acc: 0.8954\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9439\n",
      "Epoch 00050: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1762 - acc: 0.9439 - val_loss: 0.4138 - val_acc: 0.8940\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9438\n",
      "Epoch 00051: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1791 - acc: 0.9438 - val_loss: 0.3710 - val_acc: 0.9066\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9463\n",
      "Epoch 00052: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1699 - acc: 0.9462 - val_loss: 0.4433 - val_acc: 0.8926\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9482\n",
      "Epoch 00053: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1674 - acc: 0.9482 - val_loss: 0.4372 - val_acc: 0.8954\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9471\n",
      "Epoch 00054: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1649 - acc: 0.9471 - val_loss: 0.4188 - val_acc: 0.8959\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9483\n",
      "Epoch 00055: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1605 - acc: 0.9483 - val_loss: 0.4638 - val_acc: 0.8859\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9507\n",
      "Epoch 00056: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1566 - acc: 0.9507 - val_loss: 0.4765 - val_acc: 0.8817\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9502\n",
      "Epoch 00057: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1567 - acc: 0.9502 - val_loss: 0.4929 - val_acc: 0.8819\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9484\n",
      "Epoch 00058: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1634 - acc: 0.9484 - val_loss: 0.4155 - val_acc: 0.8880\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9537\n",
      "Epoch 00059: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1428 - acc: 0.9537 - val_loss: 0.4561 - val_acc: 0.8908\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9557\n",
      "Epoch 00060: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1438 - acc: 0.9556 - val_loss: 0.4278 - val_acc: 0.8994\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9509\n",
      "Epoch 00061: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1578 - acc: 0.9508 - val_loss: 0.4367 - val_acc: 0.8940\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9578\n",
      "Epoch 00062: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1372 - acc: 0.9578 - val_loss: 0.3718 - val_acc: 0.9015\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9566\n",
      "Epoch 00063: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1357 - acc: 0.9566 - val_loss: 0.4349 - val_acc: 0.8984\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9569\n",
      "Epoch 00064: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1361 - acc: 0.9569 - val_loss: 0.3794 - val_acc: 0.9054\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9568\n",
      "Epoch 00065: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1346 - acc: 0.9568 - val_loss: 0.4783 - val_acc: 0.8824\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9568\n",
      "Epoch 00066: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1363 - acc: 0.9569 - val_loss: 0.4005 - val_acc: 0.9001\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9594\n",
      "Epoch 00067: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1284 - acc: 0.9594 - val_loss: 0.4092 - val_acc: 0.8945\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9598\n",
      "Epoch 00068: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1245 - acc: 0.9598 - val_loss: 0.3977 - val_acc: 0.8982\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9612\n",
      "Epoch 00069: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1260 - acc: 0.9612 - val_loss: 0.3972 - val_acc: 0.8991\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9608\n",
      "Epoch 00070: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1252 - acc: 0.9608 - val_loss: 0.4150 - val_acc: 0.8982\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9620\n",
      "Epoch 00071: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1184 - acc: 0.9620 - val_loss: 0.4905 - val_acc: 0.8938\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9626\n",
      "Epoch 00072: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1181 - acc: 0.9626 - val_loss: 0.5284 - val_acc: 0.8821\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9639\n",
      "Epoch 00073: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1142 - acc: 0.9639 - val_loss: 0.4377 - val_acc: 0.8980\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9651\n",
      "Epoch 00074: val_loss did not improve from 0.36985\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1101 - acc: 0.9650 - val_loss: 0.4540 - val_acc: 0.8908\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9569\n",
      "Epoch 00075: val_loss improved from 0.36985 to 0.36582, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_6_conv_checkpoint/075-0.3658.hdf5\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1335 - acc: 0.9569 - val_loss: 0.3658 - val_acc: 0.9078\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9636\n",
      "Epoch 00076: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1129 - acc: 0.9636 - val_loss: 0.5020 - val_acc: 0.8849\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9642\n",
      "Epoch 00077: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1091 - acc: 0.9642 - val_loss: 0.4495 - val_acc: 0.8919\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9661\n",
      "Epoch 00078: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1059 - acc: 0.9661 - val_loss: 0.3817 - val_acc: 0.9119\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9663\n",
      "Epoch 00079: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1097 - acc: 0.9663 - val_loss: 0.3935 - val_acc: 0.9061\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9666\n",
      "Epoch 00080: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1037 - acc: 0.9666 - val_loss: 0.3981 - val_acc: 0.9008\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9688\n",
      "Epoch 00081: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1004 - acc: 0.9688 - val_loss: 0.4563 - val_acc: 0.8945\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9676\n",
      "Epoch 00082: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1022 - acc: 0.9676 - val_loss: 0.4449 - val_acc: 0.8991\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9675\n",
      "Epoch 00083: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1018 - acc: 0.9675 - val_loss: 0.5719 - val_acc: 0.8691\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9677\n",
      "Epoch 00084: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.1011 - acc: 0.9677 - val_loss: 0.3994 - val_acc: 0.9057\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9686\n",
      "Epoch 00085: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0986 - acc: 0.9686 - val_loss: 0.4272 - val_acc: 0.8994\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9705\n",
      "Epoch 00086: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0930 - acc: 0.9705 - val_loss: 0.3923 - val_acc: 0.9059\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9693\n",
      "Epoch 00087: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0960 - acc: 0.9693 - val_loss: 0.4149 - val_acc: 0.9012\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9703\n",
      "Epoch 00088: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0940 - acc: 0.9703 - val_loss: 0.4339 - val_acc: 0.9033\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9741\n",
      "Epoch 00089: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0856 - acc: 0.9741 - val_loss: 0.4201 - val_acc: 0.9052\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9707\n",
      "Epoch 00090: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0950 - acc: 0.9707 - val_loss: 0.3964 - val_acc: 0.9117\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9733\n",
      "Epoch 00091: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0827 - acc: 0.9732 - val_loss: 0.5237 - val_acc: 0.8854\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9734\n",
      "Epoch 00092: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0832 - acc: 0.9734 - val_loss: 0.3994 - val_acc: 0.9089\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9734\n",
      "Epoch 00093: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0855 - acc: 0.9734 - val_loss: 0.4620 - val_acc: 0.9005\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9735\n",
      "Epoch 00094: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0864 - acc: 0.9735 - val_loss: 0.4060 - val_acc: 0.9033\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9714\n",
      "Epoch 00095: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0882 - acc: 0.9714 - val_loss: 0.4284 - val_acc: 0.9033\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9760\n",
      "Epoch 00096: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0772 - acc: 0.9760 - val_loss: 0.7111 - val_acc: 0.8619\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9739\n",
      "Epoch 00097: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0804 - acc: 0.9739 - val_loss: 0.4335 - val_acc: 0.9038\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9747\n",
      "Epoch 00098: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0809 - acc: 0.9747 - val_loss: 0.5290 - val_acc: 0.8875\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9696\n",
      "Epoch 00099: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0943 - acc: 0.9696 - val_loss: 0.5418 - val_acc: 0.8856\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9754\n",
      "Epoch 00100: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0781 - acc: 0.9754 - val_loss: 0.4396 - val_acc: 0.9005\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9768\n",
      "Epoch 00101: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0740 - acc: 0.9768 - val_loss: 0.4510 - val_acc: 0.9059\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9770\n",
      "Epoch 00102: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0748 - acc: 0.9770 - val_loss: 0.4597 - val_acc: 0.9022\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9786\n",
      "Epoch 00103: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0697 - acc: 0.9786 - val_loss: 0.4308 - val_acc: 0.9085\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9709\n",
      "Epoch 00104: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0929 - acc: 0.9709 - val_loss: 0.5049 - val_acc: 0.8859\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9766\n",
      "Epoch 00105: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0734 - acc: 0.9766 - val_loss: 0.4474 - val_acc: 0.9026\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9784\n",
      "Epoch 00106: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0694 - acc: 0.9784 - val_loss: 0.4623 - val_acc: 0.8984\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9764\n",
      "Epoch 00107: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0722 - acc: 0.9764 - val_loss: 0.5370 - val_acc: 0.8896\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9782\n",
      "Epoch 00108: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0714 - acc: 0.9782 - val_loss: 0.4521 - val_acc: 0.9036\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9779\n",
      "Epoch 00109: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0713 - acc: 0.9779 - val_loss: 0.5050 - val_acc: 0.8845\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9775\n",
      "Epoch 00110: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0702 - acc: 0.9775 - val_loss: 0.4570 - val_acc: 0.9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9777\n",
      "Epoch 00111: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0684 - acc: 0.9777 - val_loss: 0.5383 - val_acc: 0.8921\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9769\n",
      "Epoch 00112: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0721 - acc: 0.9769 - val_loss: 0.5327 - val_acc: 0.8877\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9784\n",
      "Epoch 00113: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0690 - acc: 0.9784 - val_loss: 0.4931 - val_acc: 0.8901\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9803\n",
      "Epoch 00114: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0639 - acc: 0.9803 - val_loss: 0.4122 - val_acc: 0.9073\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9796\n",
      "Epoch 00115: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0640 - acc: 0.9795 - val_loss: 0.5091 - val_acc: 0.8912\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9774\n",
      "Epoch 00116: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0700 - acc: 0.9774 - val_loss: 0.5634 - val_acc: 0.8761\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9765\n",
      "Epoch 00117: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0703 - acc: 0.9765 - val_loss: 0.4624 - val_acc: 0.9019\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9803\n",
      "Epoch 00118: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0624 - acc: 0.9803 - val_loss: 0.4737 - val_acc: 0.9052\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9811\n",
      "Epoch 00119: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0622 - acc: 0.9811 - val_loss: 0.8639 - val_acc: 0.8348\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9811\n",
      "Epoch 00120: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0602 - acc: 0.9811 - val_loss: 0.4838 - val_acc: 0.8947\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9813\n",
      "Epoch 00121: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0613 - acc: 0.9812 - val_loss: 0.4279 - val_acc: 0.9087\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9773\n",
      "Epoch 00122: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0708 - acc: 0.9773 - val_loss: 0.4791 - val_acc: 0.9047\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9829\n",
      "Epoch 00123: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0565 - acc: 0.9829 - val_loss: 0.4768 - val_acc: 0.8982\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9817\n",
      "Epoch 00124: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0589 - acc: 0.9817 - val_loss: 0.4376 - val_acc: 0.9057\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9823\n",
      "Epoch 00125: val_loss did not improve from 0.36582\n",
      "36805/36805 [==============================] - 183s 5ms/sample - loss: 0.0584 - acc: 0.9823 - val_loss: 0.4313 - val_acc: 0.9050\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.4336 - acc: 0.8829\n",
      "Loss: 0.4336039381389925 Accuracy: 0.882866\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5425 - acc: 0.2341\n",
      "Epoch 00001: val_loss improved from inf to 1.81241, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/001-1.8124.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 2.5424 - acc: 0.2342 - val_loss: 1.8124 - val_acc: 0.4172\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6286 - acc: 0.4703\n",
      "Epoch 00002: val_loss improved from 1.81241 to 1.18490, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/002-1.1849.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 1.6285 - acc: 0.4703 - val_loss: 1.1849 - val_acc: 0.6436\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2562 - acc: 0.5958\n",
      "Epoch 00003: val_loss improved from 1.18490 to 0.89959, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/003-0.8996.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 1.2563 - acc: 0.5958 - val_loss: 0.8996 - val_acc: 0.7424\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0304 - acc: 0.6775\n",
      "Epoch 00004: val_loss improved from 0.89959 to 0.74588, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/004-0.7459.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 1.0304 - acc: 0.6775 - val_loss: 0.7459 - val_acc: 0.7950\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7258\n",
      "Epoch 00005: val_loss improved from 0.74588 to 0.67129, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/005-0.6713.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.8906 - acc: 0.7258 - val_loss: 0.6713 - val_acc: 0.8092\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7651\n",
      "Epoch 00006: val_loss improved from 0.67129 to 0.59752, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/006-0.5975.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.7714 - acc: 0.7651 - val_loss: 0.5975 - val_acc: 0.8360\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6853 - acc: 0.7912\n",
      "Epoch 00007: val_loss improved from 0.59752 to 0.52019, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/007-0.5202.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.6854 - acc: 0.7912 - val_loss: 0.5202 - val_acc: 0.8544\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.8149\n",
      "Epoch 00008: val_loss improved from 0.52019 to 0.49241, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/008-0.4924.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.6185 - acc: 0.8149 - val_loss: 0.4924 - val_acc: 0.8577\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8279\n",
      "Epoch 00009: val_loss improved from 0.49241 to 0.48532, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/009-0.4853.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.5664 - acc: 0.8278 - val_loss: 0.4853 - val_acc: 0.8689\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.8457\n",
      "Epoch 00010: val_loss improved from 0.48532 to 0.43788, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/010-0.4379.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.5180 - acc: 0.8457 - val_loss: 0.4379 - val_acc: 0.8880\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.8545\n",
      "Epoch 00011: val_loss improved from 0.43788 to 0.41901, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/011-0.4190.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4832 - acc: 0.8545 - val_loss: 0.4190 - val_acc: 0.8859\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8643\n",
      "Epoch 00012: val_loss did not improve from 0.41901\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4499 - acc: 0.8643 - val_loss: 0.4342 - val_acc: 0.8726\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8717\n",
      "Epoch 00013: val_loss improved from 0.41901 to 0.36435, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/013-0.3644.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4246 - acc: 0.8716 - val_loss: 0.3644 - val_acc: 0.9050\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8782\n",
      "Epoch 00014: val_loss improved from 0.36435 to 0.35123, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/014-0.3512.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4002 - acc: 0.8783 - val_loss: 0.3512 - val_acc: 0.8998\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8890\n",
      "Epoch 00015: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3776 - acc: 0.8890 - val_loss: 0.3654 - val_acc: 0.9029\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8898\n",
      "Epoch 00016: val_loss improved from 0.35123 to 0.34292, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/016-0.3429.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3613 - acc: 0.8898 - val_loss: 0.3429 - val_acc: 0.9017\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8939\n",
      "Epoch 00017: val_loss improved from 0.34292 to 0.30931, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/017-0.3093.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3455 - acc: 0.8938 - val_loss: 0.3093 - val_acc: 0.9194\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.9007\n",
      "Epoch 00018: val_loss did not improve from 0.30931\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3274 - acc: 0.9007 - val_loss: 0.3444 - val_acc: 0.9022\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.9034\n",
      "Epoch 00019: val_loss did not improve from 0.30931\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3123 - acc: 0.9034 - val_loss: 0.3402 - val_acc: 0.9087\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9080\n",
      "Epoch 00020: val_loss did not improve from 0.30931\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3036 - acc: 0.9080 - val_loss: 0.4533 - val_acc: 0.8684\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9126\n",
      "Epoch 00021: val_loss improved from 0.30931 to 0.28195, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/021-0.2820.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2869 - acc: 0.9126 - val_loss: 0.2820 - val_acc: 0.9208\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9144\n",
      "Epoch 00022: val_loss did not improve from 0.28195\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2808 - acc: 0.9144 - val_loss: 0.3406 - val_acc: 0.9122\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9196\n",
      "Epoch 00023: val_loss did not improve from 0.28195\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2646 - acc: 0.9195 - val_loss: 0.3213 - val_acc: 0.9099\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9215\n",
      "Epoch 00024: val_loss improved from 0.28195 to 0.26524, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/024-0.2652.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2559 - acc: 0.9215 - val_loss: 0.2652 - val_acc: 0.9324\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9243\n",
      "Epoch 00025: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2492 - acc: 0.9243 - val_loss: 0.2813 - val_acc: 0.9255\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9269\n",
      "Epoch 00026: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2360 - acc: 0.9269 - val_loss: 0.2679 - val_acc: 0.9294\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9301\n",
      "Epoch 00027: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2277 - acc: 0.9301 - val_loss: 0.3013 - val_acc: 0.9273\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9311\n",
      "Epoch 00028: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2237 - acc: 0.9312 - val_loss: 0.2715 - val_acc: 0.9271\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9319\n",
      "Epoch 00029: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2192 - acc: 0.9318 - val_loss: 0.2786 - val_acc: 0.9320\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9317\n",
      "Epoch 00030: val_loss did not improve from 0.26524\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2137 - acc: 0.9317 - val_loss: 0.3238 - val_acc: 0.9192\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9368\n",
      "Epoch 00031: val_loss improved from 0.26524 to 0.24018, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/031-0.2402.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2028 - acc: 0.9369 - val_loss: 0.2402 - val_acc: 0.9355\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9399\n",
      "Epoch 00032: val_loss did not improve from 0.24018\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1939 - acc: 0.9399 - val_loss: 0.3185 - val_acc: 0.9103\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9418\n",
      "Epoch 00033: val_loss did not improve from 0.24018\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1871 - acc: 0.9418 - val_loss: 0.2764 - val_acc: 0.9273\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9415\n",
      "Epoch 00034: val_loss did not improve from 0.24018\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1879 - acc: 0.9414 - val_loss: 0.2813 - val_acc: 0.9220\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9427\n",
      "Epoch 00035: val_loss did not improve from 0.24018\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1829 - acc: 0.9427 - val_loss: 0.2617 - val_acc: 0.9343\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9474\n",
      "Epoch 00036: val_loss improved from 0.24018 to 0.22880, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_7_conv_checkpoint/036-0.2288.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1699 - acc: 0.9473 - val_loss: 0.2288 - val_acc: 0.9380\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9489\n",
      "Epoch 00037: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1662 - acc: 0.9488 - val_loss: 0.2726 - val_acc: 0.9259\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9457\n",
      "Epoch 00038: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1745 - acc: 0.9456 - val_loss: 0.2522 - val_acc: 0.9311\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9498\n",
      "Epoch 00039: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1608 - acc: 0.9498 - val_loss: 0.2900 - val_acc: 0.9238\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9533\n",
      "Epoch 00040: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1511 - acc: 0.9533 - val_loss: 0.2427 - val_acc: 0.9341\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9539\n",
      "Epoch 00041: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1470 - acc: 0.9539 - val_loss: 0.2668 - val_acc: 0.9317\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9533\n",
      "Epoch 00042: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1454 - acc: 0.9533 - val_loss: 0.3149 - val_acc: 0.9150\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9544\n",
      "Epoch 00043: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1445 - acc: 0.9544 - val_loss: 0.2815 - val_acc: 0.9229\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9567\n",
      "Epoch 00044: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1402 - acc: 0.9567 - val_loss: 0.3087 - val_acc: 0.9317\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9572\n",
      "Epoch 00045: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1334 - acc: 0.9572 - val_loss: 0.2577 - val_acc: 0.9301\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9610\n",
      "Epoch 00046: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1277 - acc: 0.9610 - val_loss: 0.2637 - val_acc: 0.9341\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9589\n",
      "Epoch 00047: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1293 - acc: 0.9589 - val_loss: 0.3302 - val_acc: 0.9131\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9605\n",
      "Epoch 00048: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1254 - acc: 0.9605 - val_loss: 0.2618 - val_acc: 0.9334\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9604\n",
      "Epoch 00049: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1221 - acc: 0.9604 - val_loss: 0.2572 - val_acc: 0.9357\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9610\n",
      "Epoch 00050: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1229 - acc: 0.9610 - val_loss: 0.3009 - val_acc: 0.9243\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9639\n",
      "Epoch 00051: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1115 - acc: 0.9639 - val_loss: 0.2556 - val_acc: 0.9355\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9633\n",
      "Epoch 00052: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1138 - acc: 0.9633 - val_loss: 0.2620 - val_acc: 0.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9668\n",
      "Epoch 00053: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1042 - acc: 0.9667 - val_loss: 0.3132 - val_acc: 0.9210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9642\n",
      "Epoch 00054: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1109 - acc: 0.9642 - val_loss: 0.2341 - val_acc: 0.9387\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9683\n",
      "Epoch 00055: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1034 - acc: 0.9683 - val_loss: 0.3110 - val_acc: 0.9175\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9675\n",
      "Epoch 00056: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1022 - acc: 0.9675 - val_loss: 0.2668 - val_acc: 0.9297\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9676\n",
      "Epoch 00057: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1022 - acc: 0.9676 - val_loss: 0.2604 - val_acc: 0.9371\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9690\n",
      "Epoch 00058: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0997 - acc: 0.9691 - val_loss: 0.2405 - val_acc: 0.9404\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9712\n",
      "Epoch 00059: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0926 - acc: 0.9712 - val_loss: 0.3234 - val_acc: 0.9238\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9692\n",
      "Epoch 00060: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0970 - acc: 0.9692 - val_loss: 0.2775 - val_acc: 0.9362\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9720\n",
      "Epoch 00061: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0903 - acc: 0.9720 - val_loss: 0.3397 - val_acc: 0.9129\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9733\n",
      "Epoch 00062: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0856 - acc: 0.9733 - val_loss: 0.3044 - val_acc: 0.9236\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9717\n",
      "Epoch 00063: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0880 - acc: 0.9717 - val_loss: 0.3440 - val_acc: 0.9227\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9745\n",
      "Epoch 00064: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0785 - acc: 0.9745 - val_loss: 0.2708 - val_acc: 0.9322\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9707\n",
      "Epoch 00065: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0895 - acc: 0.9707 - val_loss: 0.3617 - val_acc: 0.9171\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9758\n",
      "Epoch 00066: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0767 - acc: 0.9757 - val_loss: 0.3521 - val_acc: 0.9152\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9746\n",
      "Epoch 00067: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0814 - acc: 0.9746 - val_loss: 0.3220 - val_acc: 0.9304\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9773\n",
      "Epoch 00068: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0756 - acc: 0.9772 - val_loss: 0.3138 - val_acc: 0.9290\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9750\n",
      "Epoch 00069: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0797 - acc: 0.9750 - val_loss: 0.2531 - val_acc: 0.9399\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9764\n",
      "Epoch 00070: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0766 - acc: 0.9764 - val_loss: 0.2753 - val_acc: 0.9359\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9781\n",
      "Epoch 00071: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0710 - acc: 0.9780 - val_loss: 0.2793 - val_acc: 0.9380\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9761\n",
      "Epoch 00072: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0747 - acc: 0.9761 - val_loss: 0.3221 - val_acc: 0.9229\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9783\n",
      "Epoch 00073: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0712 - acc: 0.9783 - val_loss: 0.3237 - val_acc: 0.9271\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9783\n",
      "Epoch 00074: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0686 - acc: 0.9783 - val_loss: 0.3170 - val_acc: 0.9283\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9792\n",
      "Epoch 00075: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0638 - acc: 0.9792 - val_loss: 0.2641 - val_acc: 0.9392\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9755\n",
      "Epoch 00076: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0768 - acc: 0.9755 - val_loss: 0.2704 - val_acc: 0.9387\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9827\n",
      "Epoch 00077: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0557 - acc: 0.9826 - val_loss: 0.2843 - val_acc: 0.9364\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9778\n",
      "Epoch 00078: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0712 - acc: 0.9777 - val_loss: 0.2637 - val_acc: 0.9434\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9774\n",
      "Epoch 00079: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0694 - acc: 0.9774 - val_loss: 0.2600 - val_acc: 0.9429\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9810\n",
      "Epoch 00080: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0628 - acc: 0.9810 - val_loss: 0.3143 - val_acc: 0.9238\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9819\n",
      "Epoch 00081: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0581 - acc: 0.9819 - val_loss: 0.2726 - val_acc: 0.9418\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9814\n",
      "Epoch 00082: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0570 - acc: 0.9814 - val_loss: 0.4017 - val_acc: 0.9133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9788\n",
      "Epoch 00083: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0665 - acc: 0.9788 - val_loss: 0.2993 - val_acc: 0.9364\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9821\n",
      "Epoch 00084: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0577 - acc: 0.9820 - val_loss: 0.3160 - val_acc: 0.9317\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9792\n",
      "Epoch 00085: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0640 - acc: 0.9792 - val_loss: 0.2483 - val_acc: 0.9418\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9820\n",
      "Epoch 00086: val_loss did not improve from 0.22880\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0568 - acc: 0.9820 - val_loss: 0.3108 - val_acc: 0.9336\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VcX5wPHv3D37HhIIEFCEECCBAEJRwI2qWKsVXKq12Grrr65VaW1rrba2tVZbl4oWl9Z9KbjUilq1ICigEAh7kDVAIPt2s911fn9MVkggQC4B8n6e5zxJzpl7zpybe+c9M3NmjtJaI4QQQgBYejoDQgghjh8SFIQQQrSQoCCEEKKFBAUhhBAtJCgIIYRoIUFBCCFECwkKQgghWkhQEEII0UKCghBCiBa2ns7A4UpMTNTp6ek9nQ0hhDih5Obmlmmtkw6V7oQLCunp6axcubKnsyGEECcUpVRBV9JJ85EQQogWEhSEEEK0kKAghBCixQnXp9ARn8/Hnj17aGxs7OmsnLBcLhdpaWnY7faezooQogedFEFhz549REVFkZ6ejlKqp7NzwtFaU15ezp49exg0aFBPZ0cI0YNOiuajxsZGEhISJCAcIaUUCQkJUtMSQpwcQQGQgHCU5P0TQsBJFBQOJRBowOMpJBj09XRWhBDiuBWyoKCU6q+UWqiU2qiU2qCUuq2DNFOVUtVKqbym5d5Q5ScYbMTr3YfW3R8UqqqqmDNnzhG99sILL6SqqqrL6e+77z4efvjhIzqWEEIcSihrCn7gTq31cGACcJNSangH6ZZorbOblt+GKjNKmVPVOtjt+z5YUPD7/Qd97YIFC4iNje32PAkhxJEIWVDQWu/TWq9q+t0NbAL6hep4h2Zt+hno9j3ffffdbNu2jezsbGbPns2iRYs488wzufjiixk+3MTBSy65hJycHDIzM5k7d27La9PT0ykrK2Pnzp1kZGRwww03kJmZybRp02hoaDjocfPy8pgwYQKjRo3i0ksvpbKyEoDHH3+c4cOHM2rUKK688koAPvvsM7Kzs8nOzmb06NG43e5ufx+EECe+Y3JLqlIqHRgNfNnB5olKqTXAXuAurfWGoznWli23U1ub18GWIIFAHRZLGEod3mlHRmYzZMijnW5/8MEHWb9+PXl55riLFi1i1apVrF+/vuUWz+eff574+HgaGhoYN24cl112GQkJCfvlfQuvvfYazzzzDJdffjnz58/nmmuu6fS41157LU888QRTpkzh3nvv5f777+fRRx/lwQcfZMeOHTidzpamqYcffpgnn3ySSZMmUVtbi8vlOqz3QAjRO4S8o1kpFQnMB27XWtfst3kVMFBrnQU8AbzTyT5+pJRaqZRaWVpaepQ50kf5+q4ZP358u3v+H3/8cbKyspgwYQK7d+9my5YtB7xm0KBBZGdnA5CTk8POnTs73X91dTVVVVVMmTIFgO9///ssXrwYgFGjRnH11Vfz8ssvY7OZADhp0iTuuOMOHn/8caqqqlrWCyFEWyEtGZRSdkxAeEVr/db+29sGCa31AqXUHKVUota6bL90c4G5AGPHjj1oqd7ZFX0w6KOubg1OZ38cjj6HfzKHKSIiouX3RYsW8cknn7Bs2TLCw8OZOnVqh2MCnE5ny+9Wq/WQzUedef/991m8eDHvvfcev//971m3bh13330306dPZ8GCBUyaNImPPvqIYcOGHdH+hRAnr1DefaSA54BNWuu/dJImpSkdSqnxTfkpD01+TJ9CKDqao6KiDtpGX11dTVxcHOHh4eTn57N8+fKjPmZMTAxxcXEsWbIEgJdeeokpU6YQDAbZvXs3Z511Fn/605+orq6mtraWbdu2MXLkSH7+858zbtw48vPzjzoPQoiTTyhrCpOA7wHrlFLNjfy/BAYAaK2fBmYA/6eU8gMNwJVa65C075i7jxSh6GhOSEhg0qRJjBgxggsuuIDp06e3237++efz9NNPk5GRwdChQ5kwYUK3HPeFF17gxhtvpL6+nsGDB/OPf/yDQCDANddcQ3V1NVprbr31VmJjY/n1r3/NwoULsVgsZGZmcsEFF3RLHoQQJxcVojI4ZMaOHav3f8jOpk2byMjIOORr3e7V2O0JuFwDQpW9E1pX30chxIlHKZWrtR57qHS9ZkQzmCYkrbu/piCEECeLXhcUoPv7FIQQ4mTRq4ICWKSmIIQQB9GrgoJpPpKaghBCdKaXBQULobj7SAghTha9KiiAdDQLIcTB9KqgYGoKx0fzUWRk5GGtF0KIY6FXBQWpKQghxMH1qqBgagq62zub7777bp588smWv5sfhFNbW8s555zDmDFjGDlyJO+++26X96m1Zvbs2YwYMYKRI0fyxhtvALBv3z4mT55MdnY2I0aMYMmSJQQCAWbNmtWS9q9//Wu3np8Qovc4+abKvP12yOto6myway/WoAeskZgpL7ooOxse7Xzq7CuuuILbb7+dm266CYA333yTjz76CJfLxdtvv010dDRlZWVMmDCBiy++uEvPQ37rrbfIy8tjzZo1lJWVMW7cOCZPnsyrr77KN7/5TX71q18RCASor68nLy+PwsJC1q9fD3BYT3ITQoi2Tr6gcFDNhbHmsILCIYwePZqSkhL27t1LaWkpcXFx9O/fH5/Pxy9/+UsWL16MxWKhsLCQ4uJiUlJSDrnPzz//nKuuugqr1UqfPn2YMmUKK1asYNy4cfzgBz/A5/NxySWXkJ2dzeDBg9m+fTu33HIL06dPZ9q0ad12bkKI3uXkCwoHuaIP+CpobNxOeHgmVmtYtx525syZzJs3j6KiIq644goAXnnlFUpLS8nNzcVut5Oent7hlNmHY/LkySxevJj333+fWbNmcccdd3DttdeyZs0aPvroI55++mnefPNNnn/++e44LSFEL9PL+hRC90jOK664gtdff5158+Yxc+ZMwEyZnZycjN1uZ+HChRQUFHR5f2eeeSZvvPEGgUCA0tJSFi9ezPjx4ykoKKBPnz7ccMMNXH/99axatYqysjKCwSCXXXYZDzzwAKtWrer28xNC9A4nX03hoEwMDMWo5szMTNxuN/369SM1NRWAq6++mm9961uMHDmSsWPHHtZDbS699FKWLVtGVlYWSikeeughUlJSeOGFF/jzn/+M3W4nMjKSF198kcLCQq677jqCQXNef/zjH7v9/IQQvUOvmjo7EKinvn4jLtcp2O1xocriCUumzhbi5CVTZ3eo+XSPjwFsQghxvOlVQaH1kZwygE0IITrSy4JC6PoUhBDiZNCrgkLr6UpNQQghOtKrgoIZSSzPVBBCiM70qqAApglJ+hSEEKJjvS4ogJXubj6qqqpizpw5R/TaCy+8UOYqEkIcN3pdUDA1he5tPjpYUPD7/Qd97YIFC4iNje3W/AghxJHqhUGh+2sKd999N9u2bSM7O5vZs2ezaNEizjzzTC6++GKGDx8OwCWXXEJOTg6ZmZnMnTu35bXp6emUlZWxc+dOMjIyuOGGG8jMzGTatGk0NDQccKz33nuP008/ndGjR3PuuedSXFwMQG1tLddddx0jR45k1KhRzJ8/H4APP/yQMWPGkJWVxTnnnNOt5y2EOPmcdNNcHGTmbACCwf5orbFaO0+zv0PMnM2DDz7I+vXryWs68KJFi1i1ahXr169n0KBBADz//PPEx8fT0NDAuHHjuOyyy0hISGi3ny1btvDaa6/xzDPPcPnllzN//nyuueaadmnOOOMMli9fjlKKZ599loceeohHHnmE3/3ud8TExLBu3ToAKisrKS0t5YYbbmDx4sUMGjSIioqKrp+0EKJXOumCwqEpjsWI5vHjx7cEBIDHH3+ct99+G4Ddu3ezZcuWA4LCoEGDyM7OBiAnJ4edO3cesN89e/ZwxRVXsG/fPrxeb8sxPvnkE15//fWWdHFxcbz33ntMnjy5JU18fHy3nqMQ4uRz0gWFg13RAzQ2luD3VxEZmRXSfERERLT8vmjRIj755BOWLVtGeHg4U6dO7XAKbafT2fK71WrtsPnolltu4Y477uDiiy9m0aJF3HfffSHJvxCid+p1fQrQ/bekRkVF4Xa7O91eXV1NXFwc4eHh5Ofns3z58iM+VnV1Nf369QPghRdeaFl/3nnntXskaGVlJRMmTGDx4sXs2LEDQJqPhBCH1OuCguloDtKds8MmJCQwadIkRowYwezZsw/Yfv755+P3+8nIyODuu+9mwoQJR3ys++67j5kzZ5KTk0NiYmLL+nvuuYfKykpGjBhBVlYWCxcuJCkpiblz5/Kd73yHrKyslof/CCFEZ3rV1NkAXm8RHs8eIiNHt3nojgCZOluIk5lMnd0pmSlVCCE6E7KgoJTqr5RaqJTaqJTaoJS6rYM0Sin1uFJqq1JqrVJqTKjy03pMmSlVCCE6E8q7j/zAnVrrVUqpKCBXKfWx1npjmzQXAEOaltOBp5p+hlDontMshBAnupDVFLTW+7TWq5p+dwObgH77Jfs28KI2lgOxSqnUUOUJpKYghBAHc0z6FJRS6cBo4Mv9NvUDdrf5ew8HBg6UUj9SSq1USq0sLS09yrxIn4IQQnQm5EFBKRUJzAdu11rXHMk+tNZztdZjtdZjk5KSjjJH8qAdIYToTEiDglLKjgkIr2it3+ogSSHQv83faU3rQpin5ppCzzYfRUZG9ujxhRCiI6G8+0gBzwGbtNZ/6STZv4Frm+5CmgBUa633hSpPJl/S0SyEEJ0JZU1hEvA94GylVF7TcqFS6kal1I1NaRYA24GtwDPAT0KYnybd39F89913t5ti4r777uPhhx+mtraWc845hzFjxjBy5EjefffdQ+6rsym2O5oCu7PpsoUQ4kiddCOab//wdvKKDjJ3NhAI1KKUHYvFedB0zbJTsnn0/M5n2lu9ejW33347n332GQDDhw/no48+IjU1lfr6eqKjoykrK2PChAls2bIFpRSRkZHU1tYesK+Kiop2U2x/9tlnBINBxowZ024K7Pj4eH7+85/j8Xh4tGkWwMrKSuLi4rp0Th2REc1CnLy6OqL5pJslteu6LxiOHj2akpIS9u7dS2lpKXFxcfTv3x+fz8cvf/lLFi9ejMViobCwkOLiYlJSUjrdV0dTbJeWlnY4BXZH02ULIcTROOmCwsGu6JvV1q7Dao0gLGxwtx135syZzJs3j6KiopaJ51555RVKS0vJzc3FbreTnp7e4ZTZzbo6xbYQQoRKL5z7yHQ2d/c4hSuuuILXX3+defPmMXPmTMBMc52cnIzdbmfhwoUUFBQcdB+dTbHd2RTYHU2XLYQQR6OXBgUL3f30tczMTNxuN/369SM11QzKvvrqq1m5ciUjR47kxRdfZNiwYQfdR2dTbHc2BXZH02ULIcTROOk6mruivn4LWvuIiBje3dk7oUlHsxAnL5k6+yBCUVMQQoiTQa8MCtD9fQpCCHEyOGmCwuE0gyll6fFpLo43J1ozohAiNE6KoOByuSgvLz94wdbQAHv3gt/fNNVFQArCJlprysvLcblcPZ0VIUQPOynGKaSlpbFnzx4OOq12fT2UlkJqKn5LA35/FU7nxpbnK/R2LpeLtLS0ns6GEKKHnRRBwW63t4z27dRnn8EFF8Cnn1I4NJ8tW27iG98owuHoc2wyKYQQJ4Dec5ncNDUEFRVYrWba6kDgwLmHhBCiN+t9QaG8HKs1CgC/392DGRJCiONP7wsKUlMQQohO9Z6gEBZmloqKlppCICA1BSGEaKv3BAWAhAQoL8dmk6AghBAd6V1BIT5emo+EEOIgel9QaNPRLDUFIYRor3cFhYSEdjUFuftICCHa611Boan5yGJxoJRDmo+EEGI/vS8olJeD1litUdJ8JIQQ++ldQSEhAXw+qKvDao2UmoIQQuyndwWFNqOabTapKQghxP56V1BISDA/mzqbJSgIIUR7vSsotJnqwm7vg8ezr2fzI4QQx5neGRTKy3G5BuLxFMiDdoQQoo3eFRTaNB+5XAMJBGrx+yt7Nk9CCHEc6V1BIS7O/GwKCgCNjQU9mCEhhDi+9K6g4HJBeDiUl+N0SlAQQoj99a6gAC1TXTTXFDweCQpCCNEsZEFBKfW8UqpEKbW+k+1TlVLVSqm8puXeUOWlnaZRzXZ7IhZLmNQUhBCiDVsI9/1P4G/AiwdJs0RrfVEI83CgppqCUgqXa6AEBSGEaCNkNQWt9WKgIlT7P2JNk+IBOJ0SFIQQoq2e7lOYqJRao5T6QCmVeUyO2DwpHrSMVRBCCGH0ZFBYBQzUWmcBTwDvdJZQKfUjpdRKpdTK0tLSoztqU/MRWuNyDcTnKyMQqDu6fQohxEmix4KC1rpGa13b9PsCwK6USuwk7Vyt9Vit9dikpKSjO3B8PPj9UFvbZqzCrqPbpxBCnCR6LCgopVKUUqrp9/FNeSkP+YHbTHUhYxWEEKK9kN19pJR6DZgKJCql9gC/AewAWuungRnA/yml/EADcKU+FhMRtZ3qIkXGKgghRFshCwpa66sOsf1vmFtWj612NYUslLJJTUEIIZr09N1Hx16bmoJSVpzONAkKQgjRpPcFhTbPVAAzVsHjkY5mIYSA3hwU2oxVkJqCEEIYvS8oOBwQGdlSUzAD2AoJBn09nDEhhOh5XQoKSqnblFLRynhOKbVKKTUt1JkLmf1GNUMQj6ewZ/MkhBDHga7WFH6gta4BpgFxwPeAB0OWq1Dbb/4jkNtShRACuh4UVNPPC4GXtNYb2qw78TRPdQHyBDYhhGijq0EhVyn1X0xQ+EgpFQUEQ5etEGvTfOR09gckKAghBHR98NoPgWxgu9a6XikVD1wXumyFWJuagtXqwuFIkaAghBB0vaYwEdista5SSl0D3ANUhy5bIdbcp9A0q4YZqyBBQQghuhoUngLqlVJZwJ3ANg7+RLXjW3w8BAJQUwPIWAUhhGjW1aDgb5qs7tvA37TWTwJRoctWiLWZ6gKag8IutD5xu0mEEKI7dDUouJVSv8Dcivq+UspC04ynJ6QORjVr7cHrLenBTAkhRM/ralC4AvBgxisUAWnAn0OWq1DrYP4jkLEKQgjRpaDQFAheAWKUUhcBjVrrE7dPYb/mo7CwQQA0NGztqRwJIcRxoavTXFwOfAXMBC4HvlRKzQhlxkJqv+ajsLDTUMqJ2726BzMlhBA9r6vjFH4FjNNalwAopZKAT4B5ocpYSO3XfGSx2ImMzKK2NrcHMyWEED2vq30KluaA0KT8MF57/LHbISqqJSgAREXl4HavkjuQhBC9WlcL9g+VUh8ppWYppWYB7wMLQpetY6DNVBdggkIgUENDw7YezJQQQvSsLjUfaa1nK6UuAyY1rZqrtX47dNk6BtpMdQEQGZkDgNudS3j4kJ7KlRBC9Kiu9imgtZ4PzA9hXo6tlBTYvbvlz4iITJRyUlubS58+V/ZgxoQQouccNCgopdyA7mgToLXW0SHJ1bEwbBj8738QDILF0tTZPAq3WzqbhRC910H7FLTWUVrr6A6WqBM6IABkZEBjIxS0Dlhr7WzuKA4KIcTJ78S9g+hoDRtmfubnt6yKjMwhEKiWzmYhRK8lQWHTppZVUVGms1nGKwgheqveGxQSE83SpqZgOpsd0q8ghOi1em9QAFNbaBMULBaHdDYLIXq13h0UMjLaNR+B6Vdwu3Ols1kI0Sv17qAwbBiUlZmliRnZLJ3NQojeqXcHhYwM87NNE1JU1FhAOpuFEL1T7w4KHdyWKp3NQojeLGRBQSn1vFKqRCm1vpPtSin1uFJqq1JqrVJqTKjy0qkBA8Dlks5mIYRoEsqawj+B8w+y/QJgSNPyI+CpEOalY1YrDB16QGdzVNQ43O4VBIOeY54lIYToSSELClrrxUDFQZJ8G3hRG8uBWKVUaqjy06n9bksFSEi4iEDATWXlp8c8O0II0ZN6sk+hH7C7zd97mtYdWxkZsGMHNDS0rIqLOwerNZrS0reOeXaEEKIndXnq7J6klPoRpomJAQMGdO/Ohw0DrWHLFhg1CgCLxUlCwkWUlb1DMPg0FssJ8TYJcVLSGurrzfyVERHgdIJS7bf7fCZNQ4NZtDZpIyIgLAw8HqitNUub6z+0hkAAvF6zDzCPWklKgrg4s8/8fNPCvHOnWZeWZpb4eNMCbbGY/FRXt97hXlnZuk+v1+zXajVL84MfY2LMorV5tEt5OVRVQXg4xMaaYzkcZr9VVWbJyYGpU0P7fvdkaVcI9G/zd1rTugNorecCcwHGjh3bvaPK2t6B1BQUAJKSvkNJyatUVy8mLu7sbj2kEEfL5zMFkdVqfnq9plApKzOFh8sF0dGm0LFYoKQEiorMT7/fFEw2m1maNReuDQ2mAG5oaC1I3W4zy3xkpClow8PN9ubCyu02r2lerFZTGIeFmWPV1LQWbjabKXSTk03BV1pqHm2ye7dJ43CYgt/hMIVyTY3JczOr1RSqSrUeLxRjTa1WEzCOJ3feeXIHhX8DNyulXgdOB6q11vuOeS5OO818uvbrbI6PPx+LJYzS0rckKPQiwaApmMrLTWHo97cuFkv7gtTrNYvHA3V1pmB0u01B1vyaQMCsKy01BXJlpSnwIiPNYrO1Xt02Nprjg/lINl/FBgJmX3V15vVVVSZts+a0oeJ0mkLYYjF5qKtr3RYdba5qo6JMAHC5zM9AwLyPRUUm0DRfGffvb7aVlkJenjmfxESzftQoEyTavq9hYa1X1GFh5tjNQQrM8Vwuk8fwcJMmPNxsa85rfb1J0/yeh4W1r2lYrSYA2e3mfSwvN/krLTVpMzLMMniwOac9e8xSUWHSB4NmiYlpnVKt+Sq/eb/Q+r/0+VqDZHW12ZaQYJbYWPNZaP4/e71mv7GxZomMDN3/uVnIgoJS6jVgKpColNoD/AawA2itn8Y84/lCYCtQD1wXqrwcVFgYpKcf0NlstUYQH38+ZWVvMWTI4yjVu4d09JRg0HyBKitbv+B1da2FdHPVvbkQ8XjMF23v3talucmgsdF8KePiTNU/Pt6kLyoyS3GxKRCaC+bu4nJBnz7m6jg+3uS1uBi2bTMFRHNh5nKZAqq5gFfKFHbNzQ7p6SbvsbGmMG4OGsGgKXwSE1sLlub3ofkqu08f87DB5GRTSPn95th+f/sC0m4Hm9NHIxVgb2BYykAcDtXufIJB834257cnuD1uvAEvcWFxWLrxu+n2uNForMqKRVlw2pzt9h+X4KPSsh0dno+vrgSLsmC1WHFabHwjfSpp0WkH7HNrxVYSwhKIi4xrWdenT+d5iIqCpCTNjqodlNaVMihpOFHOqG47x0MJWVDQWl91iO0auClUxz8sHcyBBJCUdBllZW9TU/MlMTETeyBjx58aTw0v5L0AwLDEYQxNHEq/qH5UNlayt6aYnSUlVLuD+Osj8NVF4nFH4PdZ8fsUPp+izldHuW8Plf5CqgNF9FEjGKSm4lDhNDTAzsJG8urfoyB8Pr6iYXgW3olu3O8LYWsEWwM0xmIeAghYvdB/KZzyX+izBiJKsESVwsgKwhpOI6n6PFLqzyWuMYfS+hIKqndRWbEHBxGk2jMYNGQIkyY5cSXtoyZ6GUX2pdRbigm3RRFhiyLKHkt29Lmc4hpLIKDQun0zR3i4xm3ZzfaGVRQ2bMFptxPhCCfCEYYn2EhpXQnFdcW4vW5mZMzgwiEXotqUxlsrtvJM7jPsrtmN2+vG7XFjtViZ1H8SZ6WfxcT+E6n31bOkYAmLdi7i0+I8nFYnUc4ooh3RDIobRNagcxjXbxw2i43KhkpeWfcKz656ln2effwk+SecO+4W4sPiW45ZWFPIF7u/YEPJBjaUbmBj6UYK3YXUeGpa0pyWcBrXjrqWa0ZdQ0pkCkt3L+W/2/7L0j1LOTv9bH468adEOw/9vC2P38Pa4rV8VfgV4fZwvj3s2+3ysr8GXwP/2/E/ar21uGwuXDYXtd5aPt/1OYt3LSavKI+gDmKz2EgKTyIhPAGP30Ott5Zaby1JEUnMyJjB5ZmXMyZ1TMt7Xe+rp7y+HG/AiyfgocHXQF5RHot3LWZxwWJ2Vu08IC8R9giindE4bU721OzBH/QfkAbAqqxcMuwSbhl/C2P7juWNDW8wN3cuXxZ+SZwrjoenPcx12de15GVT6SYeWPIAm8s20zeqL32j+pIQlsCG0g0s27OMkrqSln0PjhvMqD6juHrk1cwYPuOQ7/fRUCfaxG9jx47VK1eu7N6d3nknzJljLkEtrVcFfn81X3yRRL9+t3LqqQ937zFDoKi2iHfz3yUjKYMzB5zZrtA5lA+2fMCSXUsYmZTF0NjR9LGdyr69lpa23l3FbpYGHmeV8xE8lsruzbjPBQVToDYFNfxttKMGhz8Rr62MCN2HC12/47yk69hn/ZIv6v7J55VvUB9wE2aNpI9zAFG2BLbWraIhUIdN2TgtbgRpsSn0iUwi1hVLXlEey/Ys6/TLDGBRFhLDE1u+iE6rk9SoVGq9tbg9bjwBM2YlIzGD72d9nzMHnsnmss2sL1nPupJ1rC5aTVl9Waf7B4hxxmBRFiobKxmRPILZ35hNemw6f13+V97NfxebxcaAmAGmoHdGU+etY3XRaoI6iMPqwBfwodG4bC6y+mQR1EHcXjc1nhr2uve2HGNM6hiW7VlGo7+RMaljSIlMYcGWBUTYI7h+zPVorfl4+8dsKtvUcu6nxJ1CZnImA2MGkhCWQHxYPBrNvI3z+KzgMwDCbGE0+BuwWWxkJGawrmQd8WHx/HzSz7lm1DWs3reaJbuWsHT3Umo8NditdhxWBx6/h3Ul6/AGvC3vhc1i49zB53JZxmUMiBlAhD2CCEcEO6t28uaGN3nv6/eo9dYe8B66bC4mpk1k8sDJxLniKGkKthUNFbhsLiIdkUTYI8gvz+eT7Z/gD/oZGDMQu9VOca0Jyh1JCk9i8sDJjO07FofVQSAYIKADNPobcXvMe9zgb2BgzECGJQ5jWOIwUqNS0VoT0AHcHjcvr32ZZ1c/S0VDBTaLDX/QT0ZiBrOyZ/Gfr//Dkl1LmDJwCvdMvocX1rzAq+teJdwezqT+kyiqLWKvey9l9WWcGn8qE/tPZGLaRFIjU9lQuoG1xWtZU7yGWVmz+PkZPz/o56wzSqlcrfXYQ6aToAA8+yxi7W5bAAAgAElEQVTccIO5NTU9vd2mtWsvpL4+n9NP33ZYhezRqPfVs3DHQuLD4ukf05/UyFQsykK9r57KxkqqG6tNldXqxGlzkleUx7OrnuW9r99rKfhOjT+V67Kv4+wB32RLcSEbi7ewrWI7w+3n07/hWxQWtjavbPJ9xNbx08HSplfNGwH1ieCJBk8UJOZDeAXOgotI2vAb4qxp2FI2oxPy0ZGFxDkTSQhLpk9EMjFRdhwRtVjDa7E467DaglisQaw2TYQjjP4xaQyITSM5MpGV+77io+0f8NG2D9hbW8h3Mr7D90Z9j7PSz2Ll3pXc8d87WLp7KZGOSGq9tUTYI5iZOZMRSSPYXbObXdW7KKotYnTKaKadMo2zBp3V4ZWr2+Pms4LP2Fi6kdTIVPrH9Kd/dH/cXjebSjexqWwTu2t2MzJ5JN/o/w3GpI7BYXW0vL6yoZJ/bfwXL6x5gaW7l7asd9lcZCZlkp2SzZjUMeSk5jAscRgBHaDB10CDvwGH1UFyRDIumwtfwMfr61/noaUPsb7EDPaPc8Xxk3E/4ebxN5MSmdIu39WN1SzZtYTFBYuJckQxNX0q4/uNx2lztktXVl/Gp9s/5ePtH/NV4VecMeAMrh9zPWNSzUQB60vW89AXD/HqulexW+1MHjiZ8wafx1npZ5GZnInL5ur087izaicvr32Zsvoyzhl0DlPTpxLljCJ3by6/XvhrPtj6QUtam8VGTmoOfSL74Av48AV9WJSFrD5ZjO83nnF9x1FWX8abG97kzY1vdnhlnhCWwGUZlzFj+Az6RffD4/fQ6G/EZrExqs+oA869MxUNFbyT/w7vb3kfu8VOSmQKKZEpxIfF47K5Wr4/QxOGMixxWLd8vxt8Dby2/jXyivK4PPNyJvWfhFKKoA7y3Krn+NknP6OqsYowWxg3j7+Zn036GYnhiS2vD+pgtzaHtSVB4XB8/jmceSb85z8wfXq7Tfv2PcfmzdeTk7OKqKjR3XvcDuQV5XHV/KvIL2vt42hu3/QFfZ2+LtaexKSIWQz1XEPevjWs4nmqYhe1T+Rzgb0RPvwLLP8piYkQn7GObVMnERMYzHf9n+AL30WFI49y6zqCrgpw1hCwuUmNTmD2pNmM7XvIz1S30lozf9N83tr0FtNOmcaM4TOIdByD3raD+Lr8azaWbmR40nBOiTsFq+XwG9a11ny49UOK64qZOXwmEY6IEOT0QBUNFYTbww8aBA7XF7u+4IvdXzCu7zhOTzudcHt4l16ntWZT2SYqGiqo99VT560jxhXDmQPOxG61d1v+jidFtUW8//X7TD9t+gEXAKEmQeFw1NWZHrqf/AT+8pd2m7zeUpYuTWXAgNkMHvzHI9r95rLNPLniSV5a+xK+gM9UcR0R9I3qy/mnnM/006Yzqs8oHlv+GHd/ejeJ4Yk8fv7jhNnD2F6+m3W7drOn0E/prngKNsdRsisGVABsHrB6oDYFtl4AAXNlGxtr7rRNzdyGrX8uA6LTOSV2CAkxYTxVfA2LSuZz+/g7uWvST5n43EQCOsCX13/ZYSeZEOLkIEHhcJ1/vmk+2rz5gE1r115EbW0eEycWoFTXrgp3V+9m0c5FvLT2JT7e/jEOq4PLMi6jb1Tfls6wr8u/ZsXeFQBEOaJxe2vIcn6bCSXPUrApka+/NgNmmu+GiY2FM86AiRPNXSjN95pHRJi7GZKTzc/Y2PZ3lLQVCAa47cPbeHLFk0Q7owkEAyy5bgmjU0NfCxJC9JyuBgUZqtts+nS49VbYuhVOPbXdppSUWWzcOJOKio9JSGid409rzcq9K9lYupHyhnLK68spdBeyZNcStlduB6BfVD8eOOsBbsi5geSIZMDcGrl+PawsgUG7ili89wOKwhZCwWTWrPohW8IVQ4fC+PFwzTUwZAhkZUFmZrt+8CNitVh54oIn6BfVjz98/gden/G6BAQhRAupKTTbvh1OOQUefRRuu63dpmDQw9KlfYmLO4/MzNdxe9y8su4Vnlr5FGuL17aksyorSRFJTEibwJSBU5iaPpWRySOxKCuffw4vvQRffQUbNrSO0ExOhgkT4PTTITsbhg83M3ofbeHfFYFg4Ijaw4UQJx6pKRyuwYNNQ/z77x8QFCwWJ336XM2O3X/n9aKf8+hXc6j11pKdks3fL/o75w4+l4SwBKKcUe3uHCguhscehWeeMWPjoqJg0iRTKRkzxizp6Z039YSaBAQhxP4kKLQ1fTo88YQZAttmPLnWmtzaQfx0hZeixoeYMXwGd028i/H9xre7jU1r+PJLE1c++ACaKzQTJ8I//gEzZ5r2fyGEOF5JUGhr+nR45BH45BO45BLK6st4b/N7vLzuZf63438MjnTy9ISB/Pib/zrgpYsXwy9/CV98YZp+Jk6EBx6ASy4xfQFCCHEikKDQ1hlnQHQ0n338LL+pepQlu5YQ1EEGxAzgL9P+wrf7Btm14y5qa9cTGTkCMJN6/eIX8OGHkJoKTz4JV15p5rgRQogTjQSFtux21lw8notiF5BQNYBfnfkrLhl2CaNTRqOUwustZffOuykq+ifR0Q/zq1/Bc8+ZW0Afeghuuql1hkYhhDgRSVBoY697LxcNzSW2QrP09Ln0nTit3XaHI4no6Et47LFwXnxRU1+v+OlP4de/NoFBCCFOdBIUmtR56/jWa9+iyuLl89egb/xXsF9QqK6G2257miVLEjjrrAKeemogQ4f2UIaFECIE5CEBmEmorn7ravKK8nhj5ptkDRgP//53uzS7dpnbSZcti+c3v/kjDzwwgVNPre+hHAshRGhIUADe3vQ2725+l0emPcKFQy40w4hXrGgJDKtXmwFmu3fDhx8qbr/9TLzeIgoL/9bDORdCiO7V60c0a62Z+NxEyurL2HzzZjOgy+eD0aOhtpYVL+VzznQXsbGwYAGMMDcdsXbtdGpqljFhwg5stphuy48QQoRCV0c09/qawue7PufLwi+5Y+IdrSN87XaYM4eNBeFc8M0ACQmwdGlrQAAYNOgB/P5Kdu8+/h++I4QQXdXrg8LDyx4mISyBWdmz2q3fOWAy08I/x95QwyfP7iRtv1mlo6JGk5R0Obt3/xWvtwQhhDgZ9OqgkF+Wz783/5ubxt3U7sEgRUVw7rlQ74jlvxHf4ZSHftz6NPU2Bg36LcFgIzt33n8ssy2EECHTq4PCI0sfwWVzcfP4m9utv/562LcPFnxgYeQfroL//hfmzz/g9eHhQ+nX7yfs3TuHqqrPjlW2hRAiZHptUCiqLeLFtS8yK2sWSRFJLev/9z8zod1995k7jvjJT8yc1rffDu4DH/o9ePAfcblOIT//Ovz+Ax80LoQQJ5JeGxT+9tXf8AV83DHxjpZ1wSDcdRcMHAi33NK00maDp56CwkK4/8BmIqs1gmHD/kFj4062b//5Mcq9EEKERq8NCm9teotzB5/LkIQhLetefdWMSfj978HV9rnmEybADTeYB/CsW3fAvmJjzyQt7Xb27p1DZeWnxyD3QggRGr0yKNR6a8kvy2di2sSWdQ0NZurrMWPgqqs6eNEf/2gejPx//9f60OQ2Bg36PWFhp5Gf/wN8vqoQ5l4IIUKnVwaFNUVr0Ghy+ua0rHv8cTNi+eGHO3kUZkKCmQr1iy/gxRcP2Gy1hjFs2At4vXvZsGEGwaA3hGcghBCh0SuDQu6+XAByUk1QqKyEP/zBPGPnrLMO8sLvf99MgDR7NlRUHLA5JmYCQ4c+S1XVp3z99Y2caKPFhRCi1waFPhF96BvVF4A334SaGnPH0UFZLDBnjgkInSROSfk+Awf+mqKif7Br1x+6Nd9CCBFqvTMo7M0lp29Oy/OVX34Zhg+HnJxDvBBg1Cj48Y9NcFi/vsMk6en3k5x8NTt23ENx8avdmHMhhAitXhcU6rx1bCrb1NJ0tGMHfP65mRi1KUYc2u9+B9HRZuxCB01ESimGDXuOmJjJ5OfPorz8w248AyGECJ1eFxTWFK8hqIMtQeHVpgv57373MHaSkAC//S18+im8/XaHSSwWJyNGvEtERCYbNnyH6uovjjLnQggReiENCkqp85VSm5VSW5VSd3ewfZZSqlQplde0XB/K/IBpOgLI6ZuD1qbpaPJkM2DtsNx4o5k29c47zf2sHbDbYxk16kOczjTWrp1Obe2ao8y9EEKEVsiCglLKCjwJXAAMB65SSg3vIOkbWuvspuXZUOWnWe6+XJIjkukX1Y9VqyA/3zQdHTabDR57DHbuhFtvhfqOn8LmcPQhK+tjbLYo1qz5JrW1HfdDCCHE8SCUNYXxwFat9XattRd4Hfh2CI/XJbn7cslJNZ3ML78MDgfMmHGEOzv7bPjpT+HZZ02tYcGCDpO5XAMZNepjAFatOp3i4teO8IBCCBFaoQwK/YDdbf7e07Ruf5cppdYqpeYppfqHMD/U++rZWLqRnNQc/H547TUzNiEu7ih2+pe/wMKF4HSanV16KcydC4sXQ0lJS0d0RMQwxo5dRWTkaDZt+i5bttwiA9yEEMednu5ofg9I11qPAj4GXugokVLqR0qplUqplaWlpUd8sDVFTZ3MfXP49FMoLj7CpqP9TZ0Ka9aYSZM+/dTcsjplCvTpYzosamoAcDr7kp29kLS0Oygs/BurV59JQ8PObsiAEEJ0j1AGhUKg7ZV/WtO6Flrrcq21p+nPZ4EORwporedqrcdqrccmJSV1lKRL2o5kfuUViI2FCy884t2153CYyZOqqkw/w4cfwgMPwPLlcNFFLX0OFoudU099hOHD/0V9fT65uaMpLX2rmzIhhBBHJ5RBYQUwRCk1SCnlAK4E/t02gVIqtc2fFwObQpgfcvflkhSeRFp0GkuWwHnn7TcbanewWMytTN/8JvzqV+b2pi++MM1KHk9LsuTkGYwdu5qwsFPZsOEyvv76ZgKBxm7OzEF4vfCvf3U4uZ8QovcKWVDQWvuBm4GPMIX9m1rrDUqp3yqlLm5KdqtSaoNSag1wKzArVPkBWLVvFTl9c6iqUuzcaWZEDbkrroBnnjFPb7vySlMYNwkLG8zo0V+QlvZT9u59khUrMikpmXds5kx66im4/HL4+OPQH0sIccIIaZ+C1nqB1vo0rfUpWuvfN627V2v976bff6G1ztRaZ2mtz9Ja54cqLw2+BjaUbGBMyhjy8sy60aNDdbT9/OAH5vbVd94xtYjf/Ab27gWtsazdyKnPOTnzxoH0fbOejRtnsnr1mdTUfBm6/GhtOsMBPvkkdMcRQpxwerqj+ZhZW7yWgA6Q0zeH1avNuuzsY5iBW2+Fjz4ykeh3v4MBA2DQIPP3n/+MtT5I/ycrGK7up6FhK6tWTWDDhsupr9/a/XlZuhQ2bjT9IJ/KQ4GEEK16TVAodBcSYY8gJ9UEhdRUc3PQMTVtmhnLsGWLmTdp1ChzxV5UBCtXoiIjSf7lR5w+bjMDB95Lefn7rFiRwddf34zXW9x9+Zg7F6KizBiLvDwoK+u+fQshOhcMmj7G43lafa31CbXk5OToI+UP+HUwGNSZmVpfeOER7yZ0XnhBa9D6iSe01lo3Nu7TmzffqBcutOrPPgvXW7fO1h5P8dEdo6JCa5dL6xtv1PqLL8zx3nyzGzIvhDikl14y37m33z7mhwZW6i6Usb2mpgBgtVhpbFTk5x/D/oTD8b3vmdrEL34BBQU4nSmcdtpTjB+/kaSk77B79yMsXz6Ibdt+RkPD9iM7xssvQ2Mj/OhHMG6cqTFIE5IQx8bLL5uff/1rz+bjIHpVUADzCIRA4DgNCkrB3/9uqpY33GBG1wHh4aeRkfES48ZtoG/12fjmPszqBaewatUZ7N37d3y+yq7tX2tzJ9TYseYNsNvNIDsJCqKnFBeDz9fTuTg2SkvNjR1paWbGg1WrejpHHbL1dAaOteZO5uMyKACkp8Of/gQ332w6PsaONdNnNDQQ8c47nLp5MwDaYaP4kk1sn3kjW5JuIT7+Qvr0uYqEhG9htYZ3vO8vv4R161rvPAI45xz4z3+goOAIpooV4iisWAETJpgbHsaMgfHj4YILTG35ZPSvf5kr0tdeM+f56KMdPu+9p/W6msLq1eb5OOnpPZ2Tg7jpJnMVcf/9YLWan488Yu5YevJJ+PJL1DXX0md+NROvcTDqpSxqy79i48YrWbq0D/n5P6Sm5iv0Sy+ZSftmzDD7vOsuiIw04yWanXuu+dm2thAIwPPPd/pkOSHaKS83d9Z11Hm6dClce61Js7/77zfTCvzkJ6aW/PTTZtDnFVecnDc/vPoqZGbCGWeY29Rffx327evpXB2oKx0Px9NyNB3NWmt9+ulaT558VLs49srKtK6qOnD9tm1az5qlNejgmDG6auVLetOmH+jFH4frPRejNWjv4GQdHHaa1vHxpoPrpz9tv49gUOs+fbT+7ndb191zj0kLWp99ttbvvKO13x/acwwGta6tDe0xjjefftpyU8Fxz+/X2uc7cL3Pp/U3vmE+K1dfrXVdXeu2+fPNTQ2g9Q9+0P51K1aY9b//fes6j0frBx7Q2m7XOilJ63nzuifvgYD5bJWWar1rl9Z795rP27FUUND+fLdu1VoprX/1q2OWBbrY0dzjhfzhLkd195Ff67AwrW+77Yh3cXx6912t4+K0jorS+qmndHDsGK1B7702WS/6BL1woUWvXn2OLtw5R9fX79DB/b8QV11lAkMwqPWCBeZjcc01Wj/4oNb9+5u/R43SurIyNPn3erW+7DKtY2O1Liw8cPvWraZQKSgIzfEP165dJlAeTcGya5fW0dHmvX3vve7Lm9Zar11rCsKONDQcWb6/+12tBw40FyJt3XuvOYcZM0whN2qU1lu2aP3YY+bviRO1/vGPTZolS1pf961vmc9sdXXH+R9jPsO6f3+tL7rIFJ4ff3z4+f7sM60TElovcpqXiAits7K0njlT66VLD3+/B1NZqfUnn7R/n//0J3Pctu/ft79t8lZfb/7esEHrP/9Z6+JO7jBcv77jwNxFEhQ6sGGDOeN//vOId3H82rlT6wkTzAlGRWn91ls6GAxqt3uN3r79Hr18+RC9cCF64UL0kiVxevXqs/TWrT/T1dUrdPDZZ83r/vMfU6PIymr9oPp8Wr/8stY2m7mP93BqDF6v1h98oHVj48HTXHqpOb7FYgqQtoJBradNay0gNm8+/PemO+3ebQpHMPmuqGjd5nZrPXu21qedpnVeXuf7CAa1Pu88UzANHap1376HDrgbN2pdUnLo/D36qMnbr3994LbiYq2Tk7W++OKD/0/213zrslJaDxig9Y4dZv2iReZ/9v3vm78//NB8fpprB5deaj5HtbXmdSNGmP93bq7Z/tvfdn5Mr1frOXNMMMrM1NpqNa95442u5/uDD8xV4LBhplB+/HGtn3lG67/9Tevbbzef5+RkrWNiTOHQHRobW2tOd97ZGhiyskwzRVsLF+qWGtbYsa0Ba/r0AwP3rl3mounmm484axIUOvDKK+aM16w54l0c37xerf/+d62//vqATc0BYs+ep3R+/o/0ypXj9KJFDr1wIXr1O6dqDTrocupgdLS50tvfnDnmzetqdXfnTnOVCFoPH671l192nN/vfMekeewx84G3WrXOz29N8957Zvv//Z9pUkhO1nr16i6+IYfB7db6xRdbg2FHSkpMARMVpfXPfmaaOQYO1HrZMq3/9S+t09JMXqOjTUG/a1fH+3nqKZNuzhytV64057x/80pb77xjjjVggNbbt3eebt48U3BHRpqCef+a1fXXtxauF1xgag2HEgyaQi4lxVzpx8ZqPWiQCXppaVoPGaJ1TU1r+h07TJPjXXe1v4B4911z3IceMlfIsbEdN4l2pr7efJ6io03N8VDmzzfvWXb2wYNpQYGpJQ8a1PkVelcFg1pfe605z/PPNz+vu87UfJo/4/unb64RZWVp/de/an3//ebvF15oTef3mzbvyMiunXsnJCh04K67tHY6TVkktPZ6K3Vh4d91bu43dH1fc5Wy7nfopUvT9OrVZ+mCgj/rxsY9JnEwaAoVMAWg1lpv2mT6Hy680DQ1bdhg0r3zTmtz1m9/awoPi8VcRe/da6rrzz3X+sVp/rIUF5sP/mWXmb89HlPoDB1q/mn5+aa2EBOj9R//aK7ErrhC63POMV+ojpoiDjzpA9cVFJhmj+arNI/nwDRVVeYL7HKZJgmtTaBLT9ctV3hZWeaqeu1aU3hlZh5YA9i2zdQQzjuv9WrwF78wr//wwwOP+8YbppY2Zox5TwcObL1Sb+uLL0zeJk4075PLZZoFm61YYQLGnXeaq2WlTB7a9gF05O23Td7+/vfW/cTEmP+n3W6CWlddfHFrLeK++7r+umY7d5pgkpPTvqbj8Zj37tlntf7DH8zATKvVvBddafL86itTo5g48cBAuWePCd7TppnP85VXmppORx56yJzb/feb/+1vfmP+Tkoy79e+fQe+Zu9e0yzULBDQ+owz2jel/u53Zj8vvnjoczkICQodOOcc83kSB2p88TFd9cfv6x077tMbN16rV6wY3dTcpPTq1efoPXue0jWlS3VwwummUMvJ0S1NPqee2lowNl8t5+S0XtVUVbUGlLaLy2Wq9G3dd5/Ztny51o88Yn5///3W7QUFJkg0v37IENMsASag3Hrrge3eWpsv6W23ae1wmKu3devM+qVLTe0jOlrrW24x+5k5s/1V7rZtWk+aZArntnnR2hQ6N91kOozbtvd++qkpNKdMMVe5e/ZovXix+cJHR7evRTQ0aJ2RYQLexo2tHe4vvWTe3zPOMAFv1aoDA0NDgzmHhATzfygtNeubbxZYutQUNBMnmivi5sD5z3+awDBhggno8+ebYNY2aPp8pmY0bFj7c1u+3NQcDreTfOdOrcPDTVA50v6p5iB1++0mGDz9dGtzXvMSFaX1JZeY2l9XzZunW2pQt91mLg5OO611n0OGmGaeqCjdcgPGiy+avoO8PK1ff928n5df3r7p57HHTPpzz+16Xr7+2gSp6dPN/89qNcc+ShIU9hMMmubOH/7wiF7eK9XVfa23b/+NXrbslJb+iKXzrLohzaHrM+N12T3f1CVrntRud54O7iowV5OXXqr1L3/ZcZv1woVaP/ywaRLaurXj/omaGlNIjx9vCo/zzz8wjc9n7shq++VbscJ0jtvtJljMmdO6PRDQ+kc/Mh/3s84yXzjQeupUU3UcPNgUxlqb/IG5q2vfPtOkZbeb1xxOe7bWre2VFkv7QqujK75ly9qni442hczZZ7e/Kys311xFJiaaIKKUSZ+Y2L7Zz+3WOjXVtGP/858mzT/+0f6Yr75qCve2eUtJMbW7khLz/+xsSoYj7WT/9FNTkB6NW281+UpNNT9PP93UTnfuPHjz36H8+c/mf9DcCX3ZZabmsXFj6/lWVZkaQd++B17k5OR0XPNaurTzpsTO/PWvrZ+DQYO6Vgs+hK4GBWXSnjjGjh2rV65cediv27XLjM3629/MLfui67TWNDZux+1eTW2tWerr82ls3AmYz4/DkUJ8/AUkJEwnJmYKDkfikR/wySfN4D2r1Qy2y8jo+msLC+H6682T75qflz17Nvzzn+bJeA88ABUV5p74J56A4cPNoKKEhNZ93H8/3HefOT7AD39opjvv2/fwz2XePPjqKxg82CxDh3Y+SHD9ejOQprDQTK0eEQH33gthYe3T5eaa/MTHw6mnmmXKFOi33yPQX3gBZs0yzw/PyoJly8xDoPZXXQ1bt0J+vpmG4cMPzdOnHA4YMQI+/9yMIzheeDxw/vng98M995jBbt2Vv7o6CA8/9P58PjOxZWmpWdxuuPji9p+joxEImP/p8uXm/Z8w4ah3qZTK1VqPPWS63hIU3n0XLrnETFD4jW+EIGO9UCDQSGPjNtzuXMrLF1BZ+RF+fxUADkdfIiOziIgYhdPZD4ejT9OSgsPRF5stqvMde71w1llmINO99x5+xoJBM7fML35hCvbGRlPQ//rX7b/szZ/9/QsArc3ztjdvNoXO0KGHn4fjQTBoRgnn5prR7OPHd+11mzaZ53/85z/w1ltdf53oXs2P9u2mOf4lKOxn0yYzuvxnPzODekX3Cwb91NQso6bmS+rq1lBbu4b6+k2Yh/C1Z7VG4nD0xensj8s1EJdrIGFhQ4iJmYTLNcAUzEd79bdypakWXn453Hnn0e3rRLV7t6mBXHBBT+dE9DAJCuK4oHUAn68cr7e4aSnC692H17sXj2cvHs8uGhsL8Hpbh/s7nQOJjZ1MdPQEIiPHEBmZhdUadpCjCCEOpatBoddNiCeOLaWsOBzJOBzJwMhO0wUCjdTX51NdvYTq6sVUVHxEcfFLTVutRERkEB4+jLCw0wgPH4rdnoDW/qYliMORjNOZhsPRD6vVdUzOTYiTkQQFcVywWl1ERWUTFZVNWtotaK3xeHbjdufidudSW5tHbe0aSkvfBgIH3ZfLlU58/PnEx19IXNzZWK0RB6QJBOqoq9uI3Z5IWNigEJ2VECceCQriuKSUwuUagMs1gKSkS1vWB4M+Ghu34/dXo5QdpcxH2OstxuPZg8ezh9raXIqKXmLv3qdRyoHLNQC7PQm7PQmlbNTVraehYQvNd06Fh2eSmHgxCQkXERU1FovF0ROnLMRxQfoUxEkpGPRQXf05FRX/pbGxAJ+vDJ+vlGDQQ0TE8JY7oxobCygv/zdVVYuBAEo5iYoaQ3T06UREjMDhSMFuN3dOWa1RWK1hKOVAHU+3aArRBdLRLMRh8Pkqqaz8lJqa5bjdX+J2ryQYbOwktcJicTXVVOxYLA5stjgcjlQcjhSczn6EhQ0hPLy5/yNZgojocdLRLMRhsNvjSE6eQXLyDMA0U3k8hfh8zXdMFRMI1BEMNhAMNhAINKC1D619BINe/P4KvN4iamqW4vEUorW3Zd9WayQu1yBcrnSczv5AkECgnmCwAbDgcCRjtyc3/UzEbk/AZkldq5IAAAqnSURBVEvAZotq05nux+lMw2aL6Zk3SPQaEhSE6IDFYicsLJ2wsPTDfq3WARobd9PQsJn6+s00NGynsXEHjY07qK7+HKVsWCxhWCxhQACvt5RAoLoruSIqagyxsWcRHT0BrQMEAm4CgVpsthiiosYTHj4UpSzt8hEI1BIWdqrclSW6RIKCEN1MKWtLQImP/2aXXhMMevB6S/D5yvH7K/D5ygkE3Chla1qs1NVtpKpqIXv2PIrWHT/s3mqNJjIyC5+vnIaGrW1qLBbCwk4lImI4dnsiSjmwWJwo1b5T3WJxYrPFYLVGY7PFEhZ2KuHhp7V77ncw6MfnK8Nuj5dO+ZOQBAUhjgMWixOXqz8uV/9DpLyfQKCeurqNWCxOrNYobLYovN4Samq+xO3+itraNYSFDSEhYTphYUOwWiOor8+nrm4D9fWb8Pu/Ihj0oLWHYNALNPd36HbNXm05nQOx2WLxevfh85Vi7tyyEhY2iLCw0wgLO6VpnEhfnM6+gIVAoIZAwI3WfiIiRhERMQKLxQ6A3++mpmYpbvdqIiNHERs7tV3gET1HgoIQJxirNZzo6Pb9hXZ7AhERGaSmzjqqfWsdJBBw4/fXNNU2tlBfn099/SYCATfR0ac3dagn4fUWUV//NQ0NX1NdvYRAwH3QfVssLiIjx6C1D7d7FW3HmyjlJDZ2ChERw/F6i/B4CvF6i7DZYpv6YwbhcCTh85W1jI4Hhd0ej80Wj92e0GbKlHQsljB8vtKmpQKrNaKpvyYRuz0eqzWqpZmtI42NBVRUfITNFk9i4iVYLO2LSvM+1TU1A55cxajcfSSE6BZ+vxuvdx8eTyFAUy0mGtC43atxu7+ipuYrlLISGzuZmJgpREWNxu1eRUXFh1RUfEBjYwFOZ18cjr44HCn4/VU0Nm6nsbEArX0oZWvqlO/TdMxKfL4KAoGaw8ytaspfbNOI+344nWkoZaOy8hPq6ze0pHS5BtO//2xSUr5PXd0GSkpep7T0DTyePWZPyoHVGo7T2b9pxP0QXC5Tc3I6++F09sNmi23X11NXt7Hl/dDaT2TkaKKixhARMRK/v4KGhq00NGxt6g8aSkREBi7X/7d3t7FVnnUcx7+/QtlKkWIBlwmyMlh0aDbmCJmiyzJ8weYiM9ncdDOL0fhmxs1odBgf4hKjS4y4F4tuYRqmxE2RRWIWn9hC3AthDOYD4DKcDEqYMEd5KGm70/P3xXX1rJT2tCscTg/37/OGcz/0cPXiOvzPfd339f93IE0a97+PH0k1s/NGRD+l0jEmT24b9ht+udxHb+9+enr20tOzl3K5t7Jgsbm5nf7+7rxW5bV8v+YopdIxSqWuysLHvr4D9Pd309Z2LTNn3kh7+w2cPPki+/Z9j+PHtyJNIaIPqZn29hW0tS2jXO6jXD5JqXSc3t5XOHnyJXp6/j1sEkhooqlpChHlyjRdChbNeUquuqamC5k3bxUdHePIHIwfSTWz84g0iebmt494vKlpCi0tC2hpWXBGf09E+ZSg09p6ObNmraSrazOHDj3O9OlLmTXr41XbUi6XKkEmrbI/QH//CcrlvkowaG29gunTl9LSshAQvb0HOHFiO93dO3PqlYX5ibHWyvRdd/dupk07O2m0q6nplYKkFcCDwCRgTUR8f8jxC4DHgKuB/wG3RcTeau/pKwUzs7durFcKI99pOfMGTAIeAm4AFgGflLRoyGmfBY5ExEJgNfBArdpjZmajq1lQAJYCeyLi5UjXTI8DK4ecsxJYm1+vB5bL+QDMzOqmlkFhDrB/0HZn3jfsOZHuzBwFTityKunzkrZJ2nb48Og3ZMzMbHxqGRTOmoh4JCKWRMSS2bNn17s5ZmbnrVoGhQPA4OWZc/O+Yc9RSozfRrrhbGZmdVDLoPAccJmk+UoJVm4HNg45ZyNwV359C/B0NNrCCTOz80jN1ilEREnSF4A/kB5J/WlE7JR0P7AtIjYCjwI/l7QHeJ0UOMzMrE5qungtIp4Cnhqy71uDXvcAt9ayDWZmNnYNl+ZC0mHglXH++CzgtbPYnPON+6c698/I3DfVTYT+uSQiRn1Sp+GCwpmQtG0sK/qKyv1TnftnZO6b6hqpfxrikVQzMzs3HBTMzKyiaEHhkXo3YIJz/1Tn/hmZ+6a6humfQt1TMDOz6op2pWBmZlUUJihIWiHpRUl7JN1X7/bUk6R3SXpG0i5JOyXdk/e3S/qTpJfynyNXEikASZMk7ZD0u7w9X9KWPIaeyCv1C0nSDEnrJf1L0m5JH/D4SSR9KX+u/inpl5IubKSxU4igMMbaDkVSAr4cEYuAa4C7c3/cB2yKiMuATXm7yO4Bdg/afgBYnet/HCHVAymqB4HfR8R7gCtJ/VT48SNpDvBFYElEvI+UzeF2GmjsFCIoMLbaDoUREQcjYnt+fZz0gZ7DqfUt1gI316eF9SdpLvBRYE3eFnA9qe4HFLh/JLUB15LS1BARfRHRhcfPgMlAS07yORU4SAONnaIEhbHUdigkSR3AVcAW4KKIOJgPvQpcVKdmTQQ/Ar4KlPP2TKAr3qzIXuQxNB84DPwsT6+tkdSKxw8RcQD4AbCPFAyOAs/TQGOnKEHBhiFpGvAb4N6IODb4WM5WW8hH0yTdBByKiOfr3ZYJajLwfuDHEXEV0M2QqaKijp98H2UlKXC+E2gFVtS1UW9RUYLCWGo7FIqkZlJAWBcRG/Lu/0q6OB+/GDhUr/bV2TLgY5L2kqYaryfNoc/IUwJQ7DHUCXRGxJa8vZ4UJDx+4CPAfyLicES8AWwgjaeGGTtFCQpjqe1QGHl+/FFgd0T8cNChwfUt7gJ+e67bNhFExKqImBsRHaSx8nRE3AE8Q6r7AcXun1eB/ZLenXctB3bh8QNp2ugaSVPz52ygbxpm7BRm8ZqkG0nzxAO1Hb5b5ybVjaQPAX8B/sGbc+ZfJ91X+BUwj5SJ9hMR8XpdGjlBSLoO+EpE3CTpUtKVQzuwA7gzInrr2b56kbSYdBN+CvAy8BnSl8zCjx9J3wFuIz3ltwP4HOkeQkOMncIEBTMzG11Rpo/MzGwMHBTMzKzCQcHMzCocFMzMrMJBwczMKhwUzM4hSdcNZF01m4gcFMzMrMJBwWwYku6UtFXSC5IezrUVTkhanXPlb5I0O5+7WNJfJf1d0pMDdQQkLZT0Z0l/k7Rd0oL89tMG1SJYl1e+mk0IDgpmQ0i6nLQidVlELAb6gTtIyc22RcR7gc3At/OPPAZ8LSKuIK0SH9i/DngoIq4EPkjKmgkpK+29pNoel5Jy45hNCJNHP8WscJYDVwPP5S/xLaTkbmXgiXzOL4ANubbAjIjYnPevBX4t6W3AnIh4EiAiegDy+22NiM68/QLQATxb+1/LbHQOCmanE7A2IladslP65pDzxpsjZnDOm378ObQJxNNHZqfbBNwi6R1QqV19CenzMpDp8lPAsxFxFDgi6cN5/6eBzbmiXaekm/N7XCBp6jn9LczGwd9QzIaIiF2SvgH8UVIT8AZwN6mYzNJ87BDpvgOkVMg/yf/pD2QMhRQgHpZ0f36PW8/hr2E2Ls6SajZGkk5ExLR6t8Osljx9ZGZmFb5SMDOzCl8pmJlZhYOCmZlVOCiYmVmFg4KZmVU4KJiZWYWDgpmZVfwfysDhEGmkxcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2752 - acc: 0.9202\n",
      "Loss: 0.27517747267508186 Accuracy: 0.9202492\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5533 - acc: 0.2488\n",
      "Epoch 00001: val_loss improved from inf to 1.78258, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/001-1.7826.hdf5\n",
      "36805/36805 [==============================] - 194s 5ms/sample - loss: 2.5532 - acc: 0.2488 - val_loss: 1.7826 - val_acc: 0.4948\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6142 - acc: 0.4838\n",
      "Epoch 00002: val_loss improved from 1.78258 to 0.99419, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/002-0.9942.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.6142 - acc: 0.4837 - val_loss: 0.9942 - val_acc: 0.7098\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2090 - acc: 0.6142\n",
      "Epoch 00003: val_loss improved from 0.99419 to 0.81421, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/003-0.8142.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.2089 - acc: 0.6143 - val_loss: 0.8142 - val_acc: 0.7668\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.6977\n",
      "Epoch 00004: val_loss improved from 0.81421 to 0.58856, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/004-0.5886.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.9737 - acc: 0.6978 - val_loss: 0.5886 - val_acc: 0.8444\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8016 - acc: 0.7545\n",
      "Epoch 00005: val_loss improved from 0.58856 to 0.55615, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/005-0.5561.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.8015 - acc: 0.7546 - val_loss: 0.5561 - val_acc: 0.8376\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.7924\n",
      "Epoch 00006: val_loss improved from 0.55615 to 0.44957, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/006-0.4496.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.6798 - acc: 0.7924 - val_loss: 0.4496 - val_acc: 0.8770\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8222\n",
      "Epoch 00007: val_loss improved from 0.44957 to 0.43147, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/007-0.4315.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.5853 - acc: 0.8222 - val_loss: 0.4315 - val_acc: 0.8798\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8456\n",
      "Epoch 00008: val_loss improved from 0.43147 to 0.31758, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/008-0.3176.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.5182 - acc: 0.8456 - val_loss: 0.3176 - val_acc: 0.9138\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.8614\n",
      "Epoch 00009: val_loss improved from 0.31758 to 0.31669, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/009-0.3167.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4613 - acc: 0.8614 - val_loss: 0.3167 - val_acc: 0.9113\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8718\n",
      "Epoch 00010: val_loss improved from 0.31669 to 0.29353, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/010-0.2935.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4266 - acc: 0.8717 - val_loss: 0.2935 - val_acc: 0.9126\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8835\n",
      "Epoch 00011: val_loss improved from 0.29353 to 0.26336, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/011-0.2634.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3894 - acc: 0.8835 - val_loss: 0.2634 - val_acc: 0.9229\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8935\n",
      "Epoch 00012: val_loss improved from 0.26336 to 0.25851, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/012-0.2585.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3540 - acc: 0.8935 - val_loss: 0.2585 - val_acc: 0.9278\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8977\n",
      "Epoch 00013: val_loss improved from 0.25851 to 0.25391, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/013-0.2539.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3339 - acc: 0.8977 - val_loss: 0.2539 - val_acc: 0.9273\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9048\n",
      "Epoch 00014: val_loss improved from 0.25391 to 0.23289, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/014-0.2329.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3109 - acc: 0.9048 - val_loss: 0.2329 - val_acc: 0.9378\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9120\n",
      "Epoch 00015: val_loss improved from 0.23289 to 0.22090, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/015-0.2209.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2904 - acc: 0.9120 - val_loss: 0.2209 - val_acc: 0.9371\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9163\n",
      "Epoch 00016: val_loss did not improve from 0.22090\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2737 - acc: 0.9163 - val_loss: 0.2225 - val_acc: 0.9373\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9209\n",
      "Epoch 00017: val_loss improved from 0.22090 to 0.20775, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/017-0.2077.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2598 - acc: 0.9209 - val_loss: 0.2077 - val_acc: 0.9446\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9251\n",
      "Epoch 00018: val_loss did not improve from 0.20775\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2452 - acc: 0.9251 - val_loss: 0.2258 - val_acc: 0.9327\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9292\n",
      "Epoch 00019: val_loss improved from 0.20775 to 0.20424, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/019-0.2042.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2307 - acc: 0.9291 - val_loss: 0.2042 - val_acc: 0.9387\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9332\n",
      "Epoch 00020: val_loss improved from 0.20424 to 0.17558, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/020-0.1756.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2207 - acc: 0.9332 - val_loss: 0.1756 - val_acc: 0.9506\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9357\n",
      "Epoch 00021: val_loss did not improve from 0.17558\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2128 - acc: 0.9357 - val_loss: 0.1907 - val_acc: 0.9425\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9361\n",
      "Epoch 00022: val_loss did not improve from 0.17558\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2063 - acc: 0.9360 - val_loss: 0.1941 - val_acc: 0.9436\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9393\n",
      "Epoch 00023: val_loss improved from 0.17558 to 0.17202, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/023-0.1720.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1985 - acc: 0.9393 - val_loss: 0.1720 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9429\n",
      "Epoch 00024: val_loss did not improve from 0.17202\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1837 - acc: 0.9429 - val_loss: 0.1749 - val_acc: 0.9478\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9453\n",
      "Epoch 00025: val_loss did not improve from 0.17202\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1782 - acc: 0.9453 - val_loss: 0.1853 - val_acc: 0.9457\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9485\n",
      "Epoch 00026: val_loss did not improve from 0.17202\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1707 - acc: 0.9484 - val_loss: 0.1847 - val_acc: 0.9439\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9489\n",
      "Epoch 00027: val_loss did not improve from 0.17202\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1670 - acc: 0.9489 - val_loss: 0.1847 - val_acc: 0.9471\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9511\n",
      "Epoch 00028: val_loss improved from 0.17202 to 0.16860, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/028-0.1686.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1584 - acc: 0.9510 - val_loss: 0.1686 - val_acc: 0.9476\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9517\n",
      "Epoch 00029: val_loss did not improve from 0.16860\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1548 - acc: 0.9517 - val_loss: 0.1952 - val_acc: 0.9432\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9550\n",
      "Epoch 00030: val_loss did not improve from 0.16860\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1455 - acc: 0.9550 - val_loss: 0.1810 - val_acc: 0.9522\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9552\n",
      "Epoch 00031: val_loss improved from 0.16860 to 0.15092, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/031-0.1509.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1437 - acc: 0.9553 - val_loss: 0.1509 - val_acc: 0.9590\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9593\n",
      "Epoch 00032: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1303 - acc: 0.9593 - val_loss: 0.1547 - val_acc: 0.9574\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9603\n",
      "Epoch 00033: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1297 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9569\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9606\n",
      "Epoch 00034: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1270 - acc: 0.9606 - val_loss: 0.1637 - val_acc: 0.9557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9622\n",
      "Epoch 00035: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1201 - acc: 0.9622 - val_loss: 0.1574 - val_acc: 0.9557\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9649\n",
      "Epoch 00036: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1142 - acc: 0.9649 - val_loss: 0.1782 - val_acc: 0.9541\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9651\n",
      "Epoch 00037: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1152 - acc: 0.9651 - val_loss: 0.1560 - val_acc: 0.9536\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9653\n",
      "Epoch 00038: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1120 - acc: 0.9653 - val_loss: 0.1751 - val_acc: 0.9534\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9670\n",
      "Epoch 00039: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1038 - acc: 0.9670 - val_loss: 0.2001 - val_acc: 0.9450\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9662\n",
      "Epoch 00040: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1046 - acc: 0.9662 - val_loss: 0.2906 - val_acc: 0.9229\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9672\n",
      "Epoch 00041: val_loss did not improve from 0.15092\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1020 - acc: 0.9672 - val_loss: 0.1724 - val_acc: 0.9560\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9694\n",
      "Epoch 00042: val_loss improved from 0.15092 to 0.14243, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_8_conv_checkpoint/042-0.1424.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0989 - acc: 0.9694 - val_loss: 0.1424 - val_acc: 0.9627\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9705\n",
      "Epoch 00043: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0944 - acc: 0.9705 - val_loss: 0.1900 - val_acc: 0.9422\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9706\n",
      "Epoch 00044: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0931 - acc: 0.9706 - val_loss: 0.1430 - val_acc: 0.9632\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9737\n",
      "Epoch 00045: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0844 - acc: 0.9737 - val_loss: 0.1589 - val_acc: 0.9604\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9738\n",
      "Epoch 00046: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0839 - acc: 0.9738 - val_loss: 0.2036 - val_acc: 0.9411\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9726\n",
      "Epoch 00047: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0876 - acc: 0.9726 - val_loss: 0.1558 - val_acc: 0.9590\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9733\n",
      "Epoch 00048: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0843 - acc: 0.9733 - val_loss: 0.1536 - val_acc: 0.9553\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9750\n",
      "Epoch 00049: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0799 - acc: 0.9749 - val_loss: 0.1995 - val_acc: 0.9453\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9733\n",
      "Epoch 00050: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0846 - acc: 0.9732 - val_loss: 0.1676 - val_acc: 0.9532\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9752\n",
      "Epoch 00051: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0772 - acc: 0.9752 - val_loss: 0.1793 - val_acc: 0.9509\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9738\n",
      "Epoch 00052: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0827 - acc: 0.9738 - val_loss: 0.1456 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9764\n",
      "Epoch 00053: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0746 - acc: 0.9764 - val_loss: 0.1478 - val_acc: 0.9599\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9800\n",
      "Epoch 00054: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0651 - acc: 0.9800 - val_loss: 0.1438 - val_acc: 0.9592\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9802\n",
      "Epoch 00055: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0658 - acc: 0.9802 - val_loss: 0.1665 - val_acc: 0.9576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9801\n",
      "Epoch 00056: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0653 - acc: 0.9800 - val_loss: 0.1805 - val_acc: 0.9529\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9712\n",
      "Epoch 00057: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0885 - acc: 0.9711 - val_loss: 0.1546 - val_acc: 0.9585\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9767\n",
      "Epoch 00058: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0744 - acc: 0.9767 - val_loss: 0.1609 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9810\n",
      "Epoch 00059: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0595 - acc: 0.9810 - val_loss: 0.1455 - val_acc: 0.9634\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9833\n",
      "Epoch 00060: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0557 - acc: 0.9833 - val_loss: 0.1581 - val_acc: 0.9546\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9834\n",
      "Epoch 00061: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0549 - acc: 0.9834 - val_loss: 0.1545 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9821\n",
      "Epoch 00062: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0567 - acc: 0.9821 - val_loss: 0.1923 - val_acc: 0.9504\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9803\n",
      "Epoch 00063: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0629 - acc: 0.9803 - val_loss: 0.1728 - val_acc: 0.9562\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9820\n",
      "Epoch 00064: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0574 - acc: 0.9820 - val_loss: 0.1646 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9853\n",
      "Epoch 00065: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0487 - acc: 0.9853 - val_loss: 0.2883 - val_acc: 0.9394\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9802\n",
      "Epoch 00066: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0636 - acc: 0.9802 - val_loss: 0.1527 - val_acc: 0.9623\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9824\n",
      "Epoch 00067: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0570 - acc: 0.9824 - val_loss: 0.1579 - val_acc: 0.9604\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9844\n",
      "Epoch 00068: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0502 - acc: 0.9844 - val_loss: 0.2130 - val_acc: 0.9457\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9848\n",
      "Epoch 00069: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0491 - acc: 0.9848 - val_loss: 0.1513 - val_acc: 0.9613\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9859\n",
      "Epoch 00070: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0450 - acc: 0.9859 - val_loss: 0.1477 - val_acc: 0.9648\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9849\n",
      "Epoch 00071: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0477 - acc: 0.9849 - val_loss: 0.1819 - val_acc: 0.9532\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9835\n",
      "Epoch 00072: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0521 - acc: 0.9834 - val_loss: 0.3937 - val_acc: 0.9017\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9817\n",
      "Epoch 00073: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0575 - acc: 0.9816 - val_loss: 0.1750 - val_acc: 0.9541\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9872\n",
      "Epoch 00074: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0424 - acc: 0.9872 - val_loss: 0.1685 - val_acc: 0.9613\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9880\n",
      "Epoch 00075: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0398 - acc: 0.9880 - val_loss: 0.2178 - val_acc: 0.9532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9879\n",
      "Epoch 00076: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0406 - acc: 0.9879 - val_loss: 0.2158 - val_acc: 0.9525\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9871\n",
      "Epoch 00077: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0422 - acc: 0.9870 - val_loss: 0.1873 - val_acc: 0.9571\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9844\n",
      "Epoch 00078: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0494 - acc: 0.9844 - val_loss: 0.1876 - val_acc: 0.9555\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9898\n",
      "Epoch 00079: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0361 - acc: 0.9898 - val_loss: 0.1856 - val_acc: 0.9562\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9886\n",
      "Epoch 00080: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0371 - acc: 0.9886 - val_loss: 0.2746 - val_acc: 0.9418\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9887\n",
      "Epoch 00081: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0380 - acc: 0.9887 - val_loss: 0.2172 - val_acc: 0.9429\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9834\n",
      "Epoch 00082: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0527 - acc: 0.9834 - val_loss: 0.1680 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9870\n",
      "Epoch 00083: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0400 - acc: 0.9870 - val_loss: 0.1794 - val_acc: 0.9578\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9906\n",
      "Epoch 00084: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0310 - acc: 0.9906 - val_loss: 0.1502 - val_acc: 0.9627\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9900\n",
      "Epoch 00085: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0341 - acc: 0.9900 - val_loss: 0.1730 - val_acc: 0.9583\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9908\n",
      "Epoch 00086: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0324 - acc: 0.9908 - val_loss: 0.2566 - val_acc: 0.9427\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9901\n",
      "Epoch 00087: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0338 - acc: 0.9901 - val_loss: 0.1722 - val_acc: 0.9555\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9871\n",
      "Epoch 00088: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0416 - acc: 0.9871 - val_loss: 0.1811 - val_acc: 0.9590\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9908\n",
      "Epoch 00089: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0307 - acc: 0.9908 - val_loss: 0.1879 - val_acc: 0.9569\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9888\n",
      "Epoch 00090: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0353 - acc: 0.9888 - val_loss: 0.2553 - val_acc: 0.9432\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 00091: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0294 - acc: 0.9908 - val_loss: 0.1967 - val_acc: 0.9506\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9908\n",
      "Epoch 00092: val_loss did not improve from 0.14243\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0306 - acc: 0.9908 - val_loss: 0.3002 - val_acc: 0.9369\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.1976 - acc: 0.9450\n",
      "Loss: 0.19761540082632442 Accuracy: 0.94496363\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5867 - acc: 0.2312\n",
      "Epoch 00001: val_loss improved from inf to 1.91536, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/001-1.9154.hdf5\n",
      "36805/36805 [==============================] - 198s 5ms/sample - loss: 2.5867 - acc: 0.2312 - val_loss: 1.9154 - val_acc: 0.4782\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6852 - acc: 0.4652\n",
      "Epoch 00002: val_loss improved from 1.91536 to 1.01362, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/002-1.0136.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.6853 - acc: 0.4652 - val_loss: 1.0136 - val_acc: 0.7258\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2503 - acc: 0.6115\n",
      "Epoch 00003: val_loss improved from 1.01362 to 0.74239, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/003-0.7424.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.2504 - acc: 0.6115 - val_loss: 0.7424 - val_acc: 0.8029\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0001 - acc: 0.6947\n",
      "Epoch 00004: val_loss improved from 0.74239 to 0.61300, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/004-0.6130.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.0001 - acc: 0.6947 - val_loss: 0.6130 - val_acc: 0.8637\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8448 - acc: 0.7465\n",
      "Epoch 00005: val_loss improved from 0.61300 to 0.49012, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/005-0.4901.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.8449 - acc: 0.7465 - val_loss: 0.4901 - val_acc: 0.8784\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7802\n",
      "Epoch 00006: val_loss improved from 0.49012 to 0.40166, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/006-0.4017.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.7370 - acc: 0.7802 - val_loss: 0.4017 - val_acc: 0.9052\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8061\n",
      "Epoch 00007: val_loss improved from 0.40166 to 0.39413, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/007-0.3941.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.6485 - acc: 0.8060 - val_loss: 0.3941 - val_acc: 0.9033\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8278\n",
      "Epoch 00008: val_loss improved from 0.39413 to 0.31286, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/008-0.3129.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.5830 - acc: 0.8278 - val_loss: 0.3129 - val_acc: 0.9236\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.8457\n",
      "Epoch 00009: val_loss improved from 0.31286 to 0.29902, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/009-0.2990.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.5275 - acc: 0.8457 - val_loss: 0.2990 - val_acc: 0.9273\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8580\n",
      "Epoch 00010: val_loss improved from 0.29902 to 0.26987, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/010-0.2699.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4883 - acc: 0.8580 - val_loss: 0.2699 - val_acc: 0.9313\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.8690\n",
      "Epoch 00011: val_loss improved from 0.26987 to 0.26300, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/011-0.2630.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4472 - acc: 0.8690 - val_loss: 0.2630 - val_acc: 0.9287\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8789\n",
      "Epoch 00012: val_loss improved from 0.26300 to 0.23657, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/012-0.2366.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.4190 - acc: 0.8789 - val_loss: 0.2366 - val_acc: 0.9380\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8833\n",
      "Epoch 00013: val_loss improved from 0.23657 to 0.21660, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/013-0.2166.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4003 - acc: 0.8833 - val_loss: 0.2166 - val_acc: 0.9392\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8900\n",
      "Epoch 00014: val_loss did not improve from 0.21660\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3711 - acc: 0.8900 - val_loss: 0.2755 - val_acc: 0.9220\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8977\n",
      "Epoch 00015: val_loss did not improve from 0.21660\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3484 - acc: 0.8977 - val_loss: 0.2188 - val_acc: 0.9406\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.9012\n",
      "Epoch 00016: val_loss did not improve from 0.21660\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3324 - acc: 0.9011 - val_loss: 0.2325 - val_acc: 0.9348\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.9063\n",
      "Epoch 00017: val_loss improved from 0.21660 to 0.19493, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/017-0.1949.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.3188 - acc: 0.9063 - val_loss: 0.1949 - val_acc: 0.9495\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.9133\n",
      "Epoch 00018: val_loss improved from 0.19493 to 0.17644, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/018-0.1764.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2957 - acc: 0.9133 - val_loss: 0.1764 - val_acc: 0.9513\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9156\n",
      "Epoch 00019: val_loss did not improve from 0.17644\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2868 - acc: 0.9156 - val_loss: 0.2121 - val_acc: 0.9378\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9181\n",
      "Epoch 00020: val_loss did not improve from 0.17644\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2777 - acc: 0.9181 - val_loss: 0.1887 - val_acc: 0.9397\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9221\n",
      "Epoch 00021: val_loss improved from 0.17644 to 0.17268, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/021-0.1727.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.2630 - acc: 0.9221 - val_loss: 0.1727 - val_acc: 0.9495\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9261\n",
      "Epoch 00022: val_loss improved from 0.17268 to 0.15926, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/022-0.1593.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2470 - acc: 0.9260 - val_loss: 0.1593 - val_acc: 0.9532\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9275\n",
      "Epoch 00023: val_loss did not improve from 0.15926\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2425 - acc: 0.9275 - val_loss: 0.1597 - val_acc: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9324\n",
      "Epoch 00024: val_loss did not improve from 0.15926\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2295 - acc: 0.9324 - val_loss: 0.1690 - val_acc: 0.9495\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9313\n",
      "Epoch 00025: val_loss did not improve from 0.15926\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2292 - acc: 0.9313 - val_loss: 0.1597 - val_acc: 0.9550\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9358\n",
      "Epoch 00026: val_loss improved from 0.15926 to 0.14628, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/026-0.1463.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2136 - acc: 0.9358 - val_loss: 0.1463 - val_acc: 0.9576\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9375\n",
      "Epoch 00027: val_loss did not improve from 0.14628\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2078 - acc: 0.9375 - val_loss: 0.1467 - val_acc: 0.9592\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9439\n",
      "Epoch 00028: val_loss did not improve from 0.14628\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1924 - acc: 0.9439 - val_loss: 0.1710 - val_acc: 0.9518\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9425\n",
      "Epoch 00029: val_loss improved from 0.14628 to 0.13924, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/029-0.1392.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1904 - acc: 0.9425 - val_loss: 0.1392 - val_acc: 0.9599\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9444\n",
      "Epoch 00030: val_loss did not improve from 0.13924\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1861 - acc: 0.9444 - val_loss: 0.1532 - val_acc: 0.9527\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9453\n",
      "Epoch 00031: val_loss did not improve from 0.13924\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1825 - acc: 0.9453 - val_loss: 0.1723 - val_acc: 0.9474\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9449\n",
      "Epoch 00032: val_loss improved from 0.13924 to 0.13635, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/032-0.1364.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1832 - acc: 0.9449 - val_loss: 0.1364 - val_acc: 0.9611\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9505\n",
      "Epoch 00033: val_loss did not improve from 0.13635\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1676 - acc: 0.9505 - val_loss: 0.1627 - val_acc: 0.9532\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9506\n",
      "Epoch 00034: val_loss did not improve from 0.13635\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1638 - acc: 0.9506 - val_loss: 0.1483 - val_acc: 0.9560\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9540\n",
      "Epoch 00035: val_loss did not improve from 0.13635\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1563 - acc: 0.9540 - val_loss: 0.1447 - val_acc: 0.9557\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9554\n",
      "Epoch 00036: val_loss improved from 0.13635 to 0.13555, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/036-0.1356.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1464 - acc: 0.9554 - val_loss: 0.1356 - val_acc: 0.9590\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9561\n",
      "Epoch 00037: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1475 - acc: 0.9560 - val_loss: 0.1491 - val_acc: 0.9588\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9555\n",
      "Epoch 00038: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1452 - acc: 0.9555 - val_loss: 0.1579 - val_acc: 0.9548\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9585\n",
      "Epoch 00039: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1383 - acc: 0.9585 - val_loss: 0.1531 - val_acc: 0.9564\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9601\n",
      "Epoch 00040: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1360 - acc: 0.9601 - val_loss: 0.1454 - val_acc: 0.9560\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9581\n",
      "Epoch 00041: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1335 - acc: 0.9581 - val_loss: 0.1879 - val_acc: 0.9450\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9605\n",
      "Epoch 00042: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1289 - acc: 0.9605 - val_loss: 0.1485 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9614\n",
      "Epoch 00043: val_loss improved from 0.13555 to 0.13376, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/043-0.1338.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1305 - acc: 0.9614 - val_loss: 0.1338 - val_acc: 0.9609\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9637\n",
      "Epoch 00044: val_loss improved from 0.13376 to 0.12221, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_BN_9_conv_checkpoint/044-0.1222.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1199 - acc: 0.9636 - val_loss: 0.1222 - val_acc: 0.9644\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9617\n",
      "Epoch 00045: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1255 - acc: 0.9617 - val_loss: 0.1449 - val_acc: 0.9632\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9648\n",
      "Epoch 00046: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1155 - acc: 0.9648 - val_loss: 0.1407 - val_acc: 0.9571\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9659\n",
      "Epoch 00047: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1079 - acc: 0.9659 - val_loss: 0.1568 - val_acc: 0.9564\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9676\n",
      "Epoch 00048: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1072 - acc: 0.9676 - val_loss: 0.1425 - val_acc: 0.9611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9664\n",
      "Epoch 00049: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1102 - acc: 0.9664 - val_loss: 0.1289 - val_acc: 0.9646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9688\n",
      "Epoch 00050: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1003 - acc: 0.9687 - val_loss: 0.1420 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9658\n",
      "Epoch 00051: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1123 - acc: 0.9658 - val_loss: 0.1366 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9698\n",
      "Epoch 00052: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1002 - acc: 0.9698 - val_loss: 0.1228 - val_acc: 0.9665\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9716\n",
      "Epoch 00053: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0951 - acc: 0.9716 - val_loss: 0.1555 - val_acc: 0.9576\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9716\n",
      "Epoch 00054: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0913 - acc: 0.9716 - val_loss: 0.1394 - val_acc: 0.9585\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9734\n",
      "Epoch 00055: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0864 - acc: 0.9733 - val_loss: 0.1546 - val_acc: 0.9592\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9668\n",
      "Epoch 00056: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1065 - acc: 0.9668 - val_loss: 0.1492 - val_acc: 0.9595\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9729\n",
      "Epoch 00057: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0888 - acc: 0.9729 - val_loss: 0.1410 - val_acc: 0.9599\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9756\n",
      "Epoch 00058: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0815 - acc: 0.9756 - val_loss: 0.1262 - val_acc: 0.9660\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9689\n",
      "Epoch 00059: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0988 - acc: 0.9689 - val_loss: 0.1500 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9736\n",
      "Epoch 00060: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0850 - acc: 0.9735 - val_loss: 0.1336 - val_acc: 0.9644\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9731\n",
      "Epoch 00061: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0874 - acc: 0.9731 - val_loss: 0.1284 - val_acc: 0.9637\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9762\n",
      "Epoch 00062: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0749 - acc: 0.9761 - val_loss: 0.1673 - val_acc: 0.9576\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9745\n",
      "Epoch 00063: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0816 - acc: 0.9744 - val_loss: 0.1371 - val_acc: 0.9625\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9755\n",
      "Epoch 00064: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0798 - acc: 0.9755 - val_loss: 0.1816 - val_acc: 0.9534\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9777\n",
      "Epoch 00065: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0730 - acc: 0.9777 - val_loss: 0.1555 - val_acc: 0.9562\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9771\n",
      "Epoch 00066: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0726 - acc: 0.9771 - val_loss: 0.1294 - val_acc: 0.9651\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9792\n",
      "Epoch 00067: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0690 - acc: 0.9792 - val_loss: 0.1565 - val_acc: 0.9569\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9782\n",
      "Epoch 00068: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0696 - acc: 0.9782 - val_loss: 0.1529 - val_acc: 0.9595\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9736\n",
      "Epoch 00069: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0817 - acc: 0.9736 - val_loss: 0.1625 - val_acc: 0.9562\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9802\n",
      "Epoch 00070: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0636 - acc: 0.9802 - val_loss: 0.1446 - val_acc: 0.9634\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9807\n",
      "Epoch 00071: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0639 - acc: 0.9807 - val_loss: 0.1501 - val_acc: 0.9595\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9773\n",
      "Epoch 00072: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0724 - acc: 0.9773 - val_loss: 0.1530 - val_acc: 0.9602\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9779\n",
      "Epoch 00073: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0704 - acc: 0.9779 - val_loss: 0.1365 - val_acc: 0.9648\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9827\n",
      "Epoch 00074: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0576 - acc: 0.9827 - val_loss: 0.1453 - val_acc: 0.9613\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9795\n",
      "Epoch 00075: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0648 - acc: 0.9795 - val_loss: 0.1631 - val_acc: 0.9574\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9795\n",
      "Epoch 00076: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0661 - acc: 0.9795 - val_loss: 0.1372 - val_acc: 0.9658\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9835\n",
      "Epoch 00077: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0545 - acc: 0.9835 - val_loss: 0.1441 - val_acc: 0.9655\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9823\n",
      "Epoch 00078: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0576 - acc: 0.9822 - val_loss: 0.1794 - val_acc: 0.9571\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9794\n",
      "Epoch 00079: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0670 - acc: 0.9794 - val_loss: 0.1574 - val_acc: 0.9620\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9822\n",
      "Epoch 00080: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0579 - acc: 0.9822 - val_loss: 0.1524 - val_acc: 0.9611\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9821\n",
      "Epoch 00081: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0581 - acc: 0.9821 - val_loss: 0.1626 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9810\n",
      "Epoch 00082: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0611 - acc: 0.9810 - val_loss: 0.1500 - val_acc: 0.9618\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9843\n",
      "Epoch 00083: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0522 - acc: 0.9843 - val_loss: 0.1557 - val_acc: 0.9623\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9804\n",
      "Epoch 00084: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0627 - acc: 0.9804 - val_loss: 0.1551 - val_acc: 0.9625\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9801\n",
      "Epoch 00085: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0636 - acc: 0.9801 - val_loss: 0.1768 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9854\n",
      "Epoch 00086: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0471 - acc: 0.9854 - val_loss: 0.1692 - val_acc: 0.9611\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9817\n",
      "Epoch 00087: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0605 - acc: 0.9817 - val_loss: 0.1945 - val_acc: 0.9557\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9798\n",
      "Epoch 00088: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0658 - acc: 0.9798 - val_loss: 0.1476 - val_acc: 0.9646\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9851\n",
      "Epoch 00089: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0487 - acc: 0.9851 - val_loss: 0.1631 - val_acc: 0.9613\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9842\n",
      "Epoch 00090: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0529 - acc: 0.9842 - val_loss: 0.1655 - val_acc: 0.9597\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9818\n",
      "Epoch 00091: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0610 - acc: 0.9818 - val_loss: 0.1622 - val_acc: 0.9609\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9861\n",
      "Epoch 00092: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0434 - acc: 0.9861 - val_loss: 0.1666 - val_acc: 0.9604\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9880\n",
      "Epoch 00093: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0418 - acc: 0.9880 - val_loss: 0.1487 - val_acc: 0.9641\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9878\n",
      "Epoch 00094: val_loss did not improve from 0.12221\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0410 - acc: 0.9878 - val_loss: 0.1665 - val_acc: 0.9623\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1758 - acc: 0.9458\n",
      "Loss: 0.17580016524466152 Accuracy: 0.9457944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_ch_128_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 377,616\n",
      "Trainable params: 376,336\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4336 - acc: 0.8829\n",
      "Loss: 0.4336039381389925 Accuracy: 0.882866\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 351,952\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2752 - acc: 0.9202\n",
      "Loss: 0.27517747267508186 Accuracy: 0.9202492\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 359,824\n",
      "Trainable params: 358,288\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1976 - acc: 0.9450\n",
      "Loss: 0.19761540082632442 Accuracy: 0.94496363\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,080\n",
      "Trainable params: 362,480\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1758 - acc: 0.9458\n",
      "Loss: 0.17580016524466152 Accuracy: 0.9457944\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 377,616\n",
      "Trainable params: 376,336\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.5388 - acc: 0.8802\n",
      "Loss: 0.5388472023411331 Accuracy: 0.8801662\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 351,952\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.3732 - acc: 0.9157\n",
      "Loss: 0.3731967528815829 Accuracy: 0.91568017\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 359,824\n",
      "Trainable params: 358,288\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.3223 - acc: 0.9246\n",
      "Loss: 0.3222704706100412 Accuracy: 0.9246106\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,080\n",
      "Trainable params: 362,480\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2225 - acc: 0.9481\n",
      "Loss: 0.22254017482150001 Accuracy: 0.94807893\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
