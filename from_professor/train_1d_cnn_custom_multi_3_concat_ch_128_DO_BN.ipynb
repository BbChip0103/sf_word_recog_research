{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 682624)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 227456)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 75776)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 985856)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 985856)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           15773712    dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,940,112\n",
      "Trainable params: 15,939,344\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 227456)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 75776)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 25216)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 328448)       0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 328448)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           5255184     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,504,144\n",
      "Trainable params: 5,503,120\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 75776)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 25216)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 16640)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 117632)       0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 117632)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1882128     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,296,208\n",
      "Trainable params: 2,294,672\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 25216)        0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 16640)        0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 5376)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 47232)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 47232)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           755728      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,498,768\n",
      "Trainable params: 1,496,720\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 16640)        0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 5376)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 1792)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 23808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 23808)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           380944      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,452,944\n",
      "Trainable params: 1,450,384\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 5376)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 1792)         0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 512)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 7680)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 7680)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           122896      dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,523,856\n",
      "Trainable params: 1,520,784\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9216 - acc: 0.3848\n",
      "Epoch 00001: val_loss improved from inf to 3.54219, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv_checkpoint/001-3.5422.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 3.9214 - acc: 0.3848 - val_loss: 3.5422 - val_acc: 0.3701\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6375 - acc: 0.5771\n",
      "Epoch 00002: val_loss improved from 3.54219 to 3.10211, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv_checkpoint/002-3.1021.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 2.6373 - acc: 0.5771 - val_loss: 3.1021 - val_acc: 0.5043\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3054 - acc: 0.6499\n",
      "Epoch 00003: val_loss did not improve from 3.10211\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 2.3055 - acc: 0.6500 - val_loss: 3.6283 - val_acc: 0.4782\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0277 - acc: 0.7107\n",
      "Epoch 00004: val_loss did not improve from 3.10211\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 2.0280 - acc: 0.7107 - val_loss: 3.1666 - val_acc: 0.5502\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9273 - acc: 0.7394\n",
      "Epoch 00005: val_loss did not improve from 3.10211\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.9271 - acc: 0.7394 - val_loss: 3.1392 - val_acc: 0.5628\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8379 - acc: 0.7609\n",
      "Epoch 00006: val_loss did not improve from 3.10211\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.8376 - acc: 0.7609 - val_loss: 3.4026 - val_acc: 0.5740\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6783 - acc: 0.7977\n",
      "Epoch 00007: val_loss improved from 3.10211 to 2.74313, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv_checkpoint/007-2.7431.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.6786 - acc: 0.7976 - val_loss: 2.7431 - val_acc: 0.6562\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6531 - acc: 0.8054\n",
      "Epoch 00008: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.6533 - acc: 0.8054 - val_loss: 3.0297 - val_acc: 0.6210\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6243 - acc: 0.8147\n",
      "Epoch 00009: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.6244 - acc: 0.8147 - val_loss: 4.1770 - val_acc: 0.4973\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5643 - acc: 0.8335\n",
      "Epoch 00010: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.5641 - acc: 0.8335 - val_loss: 3.0761 - val_acc: 0.6399\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5368 - acc: 0.8401\n",
      "Epoch 00011: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.5375 - acc: 0.8401 - val_loss: 3.1548 - val_acc: 0.6371\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4961 - acc: 0.8499\n",
      "Epoch 00012: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4961 - acc: 0.8499 - val_loss: 3.4750 - val_acc: 0.5963\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5383 - acc: 0.8436\n",
      "Epoch 00013: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.5385 - acc: 0.8436 - val_loss: 3.8026 - val_acc: 0.5698\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4724 - acc: 0.8571\n",
      "Epoch 00014: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4731 - acc: 0.8571 - val_loss: 3.1027 - val_acc: 0.6536\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4395 - acc: 0.8680\n",
      "Epoch 00015: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4393 - acc: 0.8680 - val_loss: 3.1663 - val_acc: 0.6543\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4625 - acc: 0.8637\n",
      "Epoch 00016: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4627 - acc: 0.8637 - val_loss: 3.1398 - val_acc: 0.6543\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4291 - acc: 0.8716\n",
      "Epoch 00017: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4293 - acc: 0.8716 - val_loss: 3.8332 - val_acc: 0.5952\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4422 - acc: 0.8694\n",
      "Epoch 00018: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4420 - acc: 0.8694 - val_loss: 3.6134 - val_acc: 0.6063\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4083 - acc: 0.8779\n",
      "Epoch 00019: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4086 - acc: 0.8779 - val_loss: 3.0938 - val_acc: 0.6692\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4074 - acc: 0.8791\n",
      "Epoch 00020: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4074 - acc: 0.8791 - val_loss: 3.7112 - val_acc: 0.6010\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4180 - acc: 0.8771\n",
      "Epoch 00021: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4178 - acc: 0.8771 - val_loss: 3.6144 - val_acc: 0.6445\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3647 - acc: 0.8886\n",
      "Epoch 00022: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3650 - acc: 0.8886 - val_loss: 3.2348 - val_acc: 0.6585\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3651 - acc: 0.8904\n",
      "Epoch 00023: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3654 - acc: 0.8904 - val_loss: 2.9755 - val_acc: 0.6935\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3905 - acc: 0.8837\n",
      "Epoch 00024: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3903 - acc: 0.8837 - val_loss: 3.0200 - val_acc: 0.6921\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3479 - acc: 0.8929\n",
      "Epoch 00025: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3480 - acc: 0.8929 - val_loss: 3.1105 - val_acc: 0.6704\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3768 - acc: 0.8878\n",
      "Epoch 00026: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3766 - acc: 0.8878 - val_loss: 3.6908 - val_acc: 0.6289\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3533 - acc: 0.8935\n",
      "Epoch 00027: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3531 - acc: 0.8935 - val_loss: 3.1706 - val_acc: 0.6816\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3393 - acc: 0.8971\n",
      "Epoch 00028: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3394 - acc: 0.8971 - val_loss: 3.3506 - val_acc: 0.6594\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3567 - acc: 0.8923\n",
      "Epoch 00029: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3566 - acc: 0.8923 - val_loss: 2.8071 - val_acc: 0.7126\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3504 - acc: 0.8949\n",
      "Epoch 00030: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3508 - acc: 0.8948 - val_loss: 3.0353 - val_acc: 0.6865\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3452 - acc: 0.8976\n",
      "Epoch 00031: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3451 - acc: 0.8976 - val_loss: 3.0959 - val_acc: 0.6949\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3412 - acc: 0.8984\n",
      "Epoch 00032: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3410 - acc: 0.8984 - val_loss: 3.1180 - val_acc: 0.6862\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3365 - acc: 0.8982\n",
      "Epoch 00033: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3363 - acc: 0.8982 - val_loss: 3.1608 - val_acc: 0.6907\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3284 - acc: 0.9010\n",
      "Epoch 00034: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3282 - acc: 0.9010 - val_loss: 3.7154 - val_acc: 0.6438\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3380 - acc: 0.8983\n",
      "Epoch 00035: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3378 - acc: 0.8983 - val_loss: 3.6539 - val_acc: 0.6569\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3164 - acc: 0.9030\n",
      "Epoch 00036: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3162 - acc: 0.9031 - val_loss: 3.0588 - val_acc: 0.6900\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3285 - acc: 0.9009\n",
      "Epoch 00037: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3283 - acc: 0.9009 - val_loss: 3.1302 - val_acc: 0.7028\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3164 - acc: 0.9048\n",
      "Epoch 00038: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3162 - acc: 0.9048 - val_loss: 3.4768 - val_acc: 0.6601\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9954 - acc: 0.9104\n",
      "Epoch 00039: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.9953 - acc: 0.9104 - val_loss: 3.2089 - val_acc: 0.6699\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7694 - acc: 0.9292\n",
      "Epoch 00040: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7693 - acc: 0.9292 - val_loss: 2.9111 - val_acc: 0.7044\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7105 - acc: 0.9392\n",
      "Epoch 00041: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7104 - acc: 0.9392 - val_loss: 3.2636 - val_acc: 0.6627\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7058 - acc: 0.9392\n",
      "Epoch 00042: val_loss did not improve from 2.74313\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7057 - acc: 0.9392 - val_loss: 3.1758 - val_acc: 0.6716\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.9415\n",
      "Epoch 00043: val_loss improved from 2.74313 to 2.70587, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv_checkpoint/043-2.7059.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6919 - acc: 0.9415 - val_loss: 2.7059 - val_acc: 0.7154\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.9455\n",
      "Epoch 00044: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6768 - acc: 0.9455 - val_loss: 2.9817 - val_acc: 0.7016\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6774 - acc: 0.9448\n",
      "Epoch 00045: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6778 - acc: 0.9448 - val_loss: 2.9510 - val_acc: 0.7032\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.9410\n",
      "Epoch 00046: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6938 - acc: 0.9410 - val_loss: 2.9938 - val_acc: 0.6997\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.9436\n",
      "Epoch 00047: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6831 - acc: 0.9436 - val_loss: 2.7372 - val_acc: 0.7184\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.9466\n",
      "Epoch 00048: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6756 - acc: 0.9466 - val_loss: 3.3298 - val_acc: 0.6583\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9446\n",
      "Epoch 00049: val_loss did not improve from 2.70587\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6801 - acc: 0.9446 - val_loss: 2.7481 - val_acc: 0.7265\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6762 - acc: 0.9465\n",
      "Epoch 00050: val_loss improved from 2.70587 to 2.52054, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv_checkpoint/050-2.5205.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6763 - acc: 0.9465 - val_loss: 2.5205 - val_acc: 0.7396\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.9467\n",
      "Epoch 00051: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6760 - acc: 0.9467 - val_loss: 2.5366 - val_acc: 0.7328\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.9479\n",
      "Epoch 00052: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6708 - acc: 0.9479 - val_loss: 3.0152 - val_acc: 0.7000\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.9504\n",
      "Epoch 00053: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6592 - acc: 0.9503 - val_loss: 2.6228 - val_acc: 0.7405\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.9465\n",
      "Epoch 00054: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6792 - acc: 0.9465 - val_loss: 2.6120 - val_acc: 0.7368\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.9489\n",
      "Epoch 00055: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6644 - acc: 0.9489 - val_loss: 2.7271 - val_acc: 0.7219\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.9502\n",
      "Epoch 00056: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6572 - acc: 0.9502 - val_loss: 2.6483 - val_acc: 0.7298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6713 - acc: 0.9478\n",
      "Epoch 00057: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6712 - acc: 0.9478 - val_loss: 2.6923 - val_acc: 0.7219\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.9477\n",
      "Epoch 00058: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6647 - acc: 0.9477 - val_loss: 2.7271 - val_acc: 0.7270\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.9501\n",
      "Epoch 00059: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6580 - acc: 0.9501 - val_loss: 2.7188 - val_acc: 0.7223\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.9491\n",
      "Epoch 00060: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6625 - acc: 0.9491 - val_loss: 2.6835 - val_acc: 0.7268\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6528 - acc: 0.9507\n",
      "Epoch 00061: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6535 - acc: 0.9507 - val_loss: 2.7067 - val_acc: 0.7307\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.9515\n",
      "Epoch 00062: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6538 - acc: 0.9515 - val_loss: 2.6045 - val_acc: 0.7412\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6519 - acc: 0.9517\n",
      "Epoch 00063: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6521 - acc: 0.9516 - val_loss: 3.0733 - val_acc: 0.7007\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6717 - acc: 0.9482\n",
      "Epoch 00064: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6720 - acc: 0.9481 - val_loss: 2.9388 - val_acc: 0.7042\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6566 - acc: 0.9518\n",
      "Epoch 00065: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6569 - acc: 0.9517 - val_loss: 2.6908 - val_acc: 0.7293\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6595 - acc: 0.9504\n",
      "Epoch 00066: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6594 - acc: 0.9504 - val_loss: 2.7292 - val_acc: 0.7356\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.9518\n",
      "Epoch 00067: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6512 - acc: 0.9518 - val_loss: 2.9409 - val_acc: 0.7042\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.9523\n",
      "Epoch 00068: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6493 - acc: 0.9523 - val_loss: 2.9435 - val_acc: 0.7091\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.9518\n",
      "Epoch 00069: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6512 - acc: 0.9518 - val_loss: 2.5843 - val_acc: 0.7468\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6484 - acc: 0.9525\n",
      "Epoch 00070: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6484 - acc: 0.9525 - val_loss: 2.7504 - val_acc: 0.7282\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.9514\n",
      "Epoch 00071: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6525 - acc: 0.9514 - val_loss: 2.7221 - val_acc: 0.7342\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6454 - acc: 0.9533\n",
      "Epoch 00072: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6453 - acc: 0.9533 - val_loss: 2.5740 - val_acc: 0.7470\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6509 - acc: 0.9523\n",
      "Epoch 00073: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6508 - acc: 0.9523 - val_loss: 2.5710 - val_acc: 0.7468\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6402 - acc: 0.9547\n",
      "Epoch 00074: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6401 - acc: 0.9547 - val_loss: 2.7457 - val_acc: 0.7261\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6499 - acc: 0.9531\n",
      "Epoch 00075: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6498 - acc: 0.9531 - val_loss: 2.8125 - val_acc: 0.7275\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6417 - acc: 0.9543\n",
      "Epoch 00076: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6416 - acc: 0.9543 - val_loss: 2.8361 - val_acc: 0.7261\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6472 - acc: 0.9525\n",
      "Epoch 00077: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6471 - acc: 0.9525 - val_loss: 2.7630 - val_acc: 0.7338\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.9541\n",
      "Epoch 00078: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6467 - acc: 0.9541 - val_loss: 2.9940 - val_acc: 0.7060\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.9537\n",
      "Epoch 00079: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6449 - acc: 0.9536 - val_loss: 2.7912 - val_acc: 0.7214\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.9522\n",
      "Epoch 00080: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6532 - acc: 0.9522 - val_loss: 2.7368 - val_acc: 0.7354\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.9538\n",
      "Epoch 00081: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6451 - acc: 0.9538 - val_loss: 2.9076 - val_acc: 0.7177\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6412 - acc: 0.9545\n",
      "Epoch 00082: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6411 - acc: 0.9545 - val_loss: 2.8920 - val_acc: 0.7207\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.9552\n",
      "Epoch 00083: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6365 - acc: 0.9551 - val_loss: 2.6797 - val_acc: 0.7452\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6457 - acc: 0.9541\n",
      "Epoch 00084: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6456 - acc: 0.9541 - val_loss: 2.6124 - val_acc: 0.7461\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6398 - acc: 0.9546\n",
      "Epoch 00085: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6397 - acc: 0.9546 - val_loss: 2.6394 - val_acc: 0.7345\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.9539\n",
      "Epoch 00086: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6455 - acc: 0.9539 - val_loss: 2.6026 - val_acc: 0.7398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.9541\n",
      "Epoch 00087: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6437 - acc: 0.9541 - val_loss: 2.7033 - val_acc: 0.7342\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6316 - acc: 0.9568\n",
      "Epoch 00088: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6315 - acc: 0.9568 - val_loss: 2.7702 - val_acc: 0.7282\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6445 - acc: 0.9546\n",
      "Epoch 00089: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6444 - acc: 0.9546 - val_loss: 2.5221 - val_acc: 0.7547\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6341 - acc: 0.9560\n",
      "Epoch 00090: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6340 - acc: 0.9560 - val_loss: 2.5565 - val_acc: 0.7552\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6372 - acc: 0.9557\n",
      "Epoch 00091: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6376 - acc: 0.9556 - val_loss: 2.7365 - val_acc: 0.7410\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6439 - acc: 0.9538\n",
      "Epoch 00092: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6443 - acc: 0.9538 - val_loss: 2.6962 - val_acc: 0.7442\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6451 - acc: 0.9541\n",
      "Epoch 00093: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6450 - acc: 0.9541 - val_loss: 2.5937 - val_acc: 0.7515\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.9574\n",
      "Epoch 00094: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6293 - acc: 0.9574 - val_loss: 2.7113 - val_acc: 0.7363\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.9553\n",
      "Epoch 00095: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6391 - acc: 0.9553 - val_loss: 2.5537 - val_acc: 0.7594\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.9558\n",
      "Epoch 00096: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6375 - acc: 0.9558 - val_loss: 2.8550 - val_acc: 0.7205\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.9545\n",
      "Epoch 00097: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6423 - acc: 0.9545 - val_loss: 2.6725 - val_acc: 0.7405\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.9569\n",
      "Epoch 00098: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6328 - acc: 0.9569 - val_loss: 2.5597 - val_acc: 0.7622\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6375 - acc: 0.9554\n",
      "Epoch 00099: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6374 - acc: 0.9554 - val_loss: 2.6918 - val_acc: 0.7375\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.9558\n",
      "Epoch 00100: val_loss did not improve from 2.52054\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6365 - acc: 0.9558 - val_loss: 3.0144 - val_acc: 0.7081\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.8364 - acc: 0.6997\n",
      "Loss: 2.836447862672657 Accuracy: 0.6996885\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9832 - acc: 0.4639\n",
      "Epoch 00001: val_loss improved from inf to 1.66729, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/001-1.6673.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 1.9831 - acc: 0.4639 - val_loss: 1.6673 - val_acc: 0.4859\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6599\n",
      "Epoch 00002: val_loss improved from 1.66729 to 0.96082, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/002-0.9608.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1739 - acc: 0.6599 - val_loss: 0.9608 - val_acc: 0.7268\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9253 - acc: 0.7312\n",
      "Epoch 00003: val_loss did not improve from 0.96082\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.9252 - acc: 0.7313 - val_loss: 0.9668 - val_acc: 0.7377\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7718\n",
      "Epoch 00004: val_loss improved from 0.96082 to 0.90159, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/004-0.9016.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7860 - acc: 0.7717 - val_loss: 0.9016 - val_acc: 0.7515\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.8033\n",
      "Epoch 00005: val_loss did not improve from 0.90159\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6712 - acc: 0.8033 - val_loss: 1.1886 - val_acc: 0.6881\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8320\n",
      "Epoch 00006: val_loss did not improve from 0.90159\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5629 - acc: 0.8320 - val_loss: 0.9099 - val_acc: 0.7563\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8553\n",
      "Epoch 00007: val_loss did not improve from 0.90159\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4745 - acc: 0.8553 - val_loss: 1.4043 - val_acc: 0.6925\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8682\n",
      "Epoch 00008: val_loss did not improve from 0.90159\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4328 - acc: 0.8681 - val_loss: 0.9791 - val_acc: 0.7636\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8896\n",
      "Epoch 00009: val_loss improved from 0.90159 to 0.86676, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/009-0.8668.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3519 - acc: 0.8897 - val_loss: 0.8668 - val_acc: 0.7736\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9013\n",
      "Epoch 00010: val_loss did not improve from 0.86676\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3139 - acc: 0.9012 - val_loss: 1.2252 - val_acc: 0.7331\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9126\n",
      "Epoch 00011: val_loss did not improve from 0.86676\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2770 - acc: 0.9125 - val_loss: 0.9394 - val_acc: 0.7773\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9194\n",
      "Epoch 00012: val_loss did not improve from 0.86676\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2573 - acc: 0.9194 - val_loss: 0.9342 - val_acc: 0.7743\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9240\n",
      "Epoch 00013: val_loss improved from 0.86676 to 0.85722, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/013-0.8572.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2377 - acc: 0.9240 - val_loss: 0.8572 - val_acc: 0.8013\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9330\n",
      "Epoch 00014: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2077 - acc: 0.9330 - val_loss: 1.0728 - val_acc: 0.7666\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9389\n",
      "Epoch 00015: val_loss did not improve from 0.85722\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1919 - acc: 0.9389 - val_loss: 0.9554 - val_acc: 0.7913\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9446\n",
      "Epoch 00016: val_loss improved from 0.85722 to 0.84596, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/016-0.8460.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1777 - acc: 0.9446 - val_loss: 0.8460 - val_acc: 0.8148\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9474\n",
      "Epoch 00017: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1661 - acc: 0.9475 - val_loss: 0.9201 - val_acc: 0.8088\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9499\n",
      "Epoch 00018: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1606 - acc: 0.9498 - val_loss: 1.1047 - val_acc: 0.7890\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9541\n",
      "Epoch 00019: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1468 - acc: 0.9541 - val_loss: 0.9361 - val_acc: 0.8076\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9605\n",
      "Epoch 00020: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1286 - acc: 0.9605 - val_loss: 0.9627 - val_acc: 0.8199\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9610\n",
      "Epoch 00021: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1240 - acc: 0.9609 - val_loss: 0.9912 - val_acc: 0.8078\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9592\n",
      "Epoch 00022: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1324 - acc: 0.9591 - val_loss: 1.0242 - val_acc: 0.7957\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9635\n",
      "Epoch 00023: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1196 - acc: 0.9635 - val_loss: 0.8547 - val_acc: 0.8274\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9664\n",
      "Epoch 00024: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1075 - acc: 0.9664 - val_loss: 1.0121 - val_acc: 0.8183\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9653\n",
      "Epoch 00025: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1146 - acc: 0.9653 - val_loss: 0.9510 - val_acc: 0.8185\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9693\n",
      "Epoch 00026: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0986 - acc: 0.9693 - val_loss: 1.0802 - val_acc: 0.7904\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9705\n",
      "Epoch 00027: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0982 - acc: 0.9705 - val_loss: 0.9256 - val_acc: 0.8348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9700\n",
      "Epoch 00028: val_loss did not improve from 0.84596\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0993 - acc: 0.9700 - val_loss: 0.9730 - val_acc: 0.8262\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9685\n",
      "Epoch 00029: val_loss improved from 0.84596 to 0.79336, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv_checkpoint/029-0.7934.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1068 - acc: 0.9684 - val_loss: 0.7934 - val_acc: 0.8586\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9754\n",
      "Epoch 00030: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0819 - acc: 0.9754 - val_loss: 1.1368 - val_acc: 0.8127\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9730\n",
      "Epoch 00031: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0946 - acc: 0.9730 - val_loss: 1.1600 - val_acc: 0.8125\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9752\n",
      "Epoch 00032: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0870 - acc: 0.9752 - val_loss: 0.9546 - val_acc: 0.8300\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9762\n",
      "Epoch 00033: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0822 - acc: 0.9761 - val_loss: 1.0747 - val_acc: 0.8153\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9769\n",
      "Epoch 00034: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0810 - acc: 0.9769 - val_loss: 0.9679 - val_acc: 0.8369\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9768\n",
      "Epoch 00035: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0791 - acc: 0.9769 - val_loss: 0.9876 - val_acc: 0.8346\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9773\n",
      "Epoch 00036: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0782 - acc: 0.9773 - val_loss: 0.9969 - val_acc: 0.8281\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9789\n",
      "Epoch 00037: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0734 - acc: 0.9789 - val_loss: 0.8819 - val_acc: 0.8493\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9790\n",
      "Epoch 00038: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0714 - acc: 0.9790 - val_loss: 1.0077 - val_acc: 0.8400\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9813\n",
      "Epoch 00039: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0657 - acc: 0.9813 - val_loss: 0.9949 - val_acc: 0.8341\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9809\n",
      "Epoch 00040: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0670 - acc: 0.9809 - val_loss: 0.8386 - val_acc: 0.8581\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9823\n",
      "Epoch 00041: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0636 - acc: 0.9823 - val_loss: 1.0088 - val_acc: 0.8372\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9793\n",
      "Epoch 00042: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0711 - acc: 0.9794 - val_loss: 0.8675 - val_acc: 0.8535\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9843\n",
      "Epoch 00043: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0592 - acc: 0.9843 - val_loss: 0.9075 - val_acc: 0.8567\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9815\n",
      "Epoch 00044: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0643 - acc: 0.9815 - val_loss: 1.1604 - val_acc: 0.8209\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9826\n",
      "Epoch 00045: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0634 - acc: 0.9826 - val_loss: 0.9341 - val_acc: 0.8574\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9824\n",
      "Epoch 00046: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0620 - acc: 0.9824 - val_loss: 1.0220 - val_acc: 0.8437\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9791\n",
      "Epoch 00047: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0741 - acc: 0.9791 - val_loss: 1.0447 - val_acc: 0.8404\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9870\n",
      "Epoch 00048: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0485 - acc: 0.9870 - val_loss: 0.9341 - val_acc: 0.8512\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9846\n",
      "Epoch 00049: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0572 - acc: 0.9846 - val_loss: 0.9369 - val_acc: 0.8572\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9854\n",
      "Epoch 00050: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0546 - acc: 0.9854 - val_loss: 0.9794 - val_acc: 0.8381\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9827\n",
      "Epoch 00051: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0649 - acc: 0.9827 - val_loss: 0.9230 - val_acc: 0.8505\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9853\n",
      "Epoch 00052: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0552 - acc: 0.9853 - val_loss: 0.9425 - val_acc: 0.8535\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9899\n",
      "Epoch 00053: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0384 - acc: 0.9899 - val_loss: 0.9164 - val_acc: 0.8542\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9873\n",
      "Epoch 00054: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0460 - acc: 0.9873 - val_loss: 0.9803 - val_acc: 0.8505\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9865\n",
      "Epoch 00055: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0500 - acc: 0.9866 - val_loss: 0.9307 - val_acc: 0.8591\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9865\n",
      "Epoch 00056: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0514 - acc: 0.9865 - val_loss: 1.0536 - val_acc: 0.8425\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9879\n",
      "Epoch 00057: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0463 - acc: 0.9879 - val_loss: 1.0090 - val_acc: 0.8465\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9876\n",
      "Epoch 00058: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0453 - acc: 0.9876 - val_loss: 0.9918 - val_acc: 0.8432\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9880\n",
      "Epoch 00059: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0450 - acc: 0.9880 - val_loss: 0.8436 - val_acc: 0.8656\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9877\n",
      "Epoch 00060: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0436 - acc: 0.9877 - val_loss: 1.0173 - val_acc: 0.8523\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9884\n",
      "Epoch 00061: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0449 - acc: 0.9884 - val_loss: 0.9923 - val_acc: 0.8549\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9878\n",
      "Epoch 00062: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0472 - acc: 0.9878 - val_loss: 0.8955 - val_acc: 0.8581\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9866\n",
      "Epoch 00063: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0512 - acc: 0.9866 - val_loss: 0.8863 - val_acc: 0.8710\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9898\n",
      "Epoch 00064: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0375 - acc: 0.9898 - val_loss: 0.9326 - val_acc: 0.8598\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9895\n",
      "Epoch 00065: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0402 - acc: 0.9895 - val_loss: 0.9940 - val_acc: 0.8581\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9874\n",
      "Epoch 00066: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0467 - acc: 0.9874 - val_loss: 0.9804 - val_acc: 0.8649\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9907\n",
      "Epoch 00067: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0383 - acc: 0.9907 - val_loss: 0.9080 - val_acc: 0.8677\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9890\n",
      "Epoch 00068: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0437 - acc: 0.9890 - val_loss: 0.9233 - val_acc: 0.8658\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9863\n",
      "Epoch 00069: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0553 - acc: 0.9863 - val_loss: 0.9096 - val_acc: 0.8663\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9893\n",
      "Epoch 00070: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0408 - acc: 0.9893 - val_loss: 0.9178 - val_acc: 0.8658\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9918\n",
      "Epoch 00071: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0323 - acc: 0.9918 - val_loss: 1.0385 - val_acc: 0.8444\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9896\n",
      "Epoch 00072: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0390 - acc: 0.9896 - val_loss: 0.9240 - val_acc: 0.8689\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9909\n",
      "Epoch 00073: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0365 - acc: 0.9909 - val_loss: 0.8826 - val_acc: 0.8700\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9899\n",
      "Epoch 00074: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0409 - acc: 0.9898 - val_loss: 1.0004 - val_acc: 0.8572\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9874\n",
      "Epoch 00075: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0489 - acc: 0.9874 - val_loss: 0.9947 - val_acc: 0.8600\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9929\n",
      "Epoch 00076: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0278 - acc: 0.9929 - val_loss: 1.0095 - val_acc: 0.8558\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9879\n",
      "Epoch 00077: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0460 - acc: 0.9879 - val_loss: 0.9060 - val_acc: 0.8668\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9916\n",
      "Epoch 00078: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0328 - acc: 0.9916 - val_loss: 0.9851 - val_acc: 0.8605\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9917\n",
      "Epoch 00079: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0324 - acc: 0.9917 - val_loss: 0.9280 - val_acc: 0.8675\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9248 - acc: 0.8280\n",
      "Loss: 0.9247710018514472 Accuracy: 0.8280374\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8042 - acc: 0.4764\n",
      "Epoch 00001: val_loss improved from inf to 1.24883, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/001-1.2488.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 1.8040 - acc: 0.4764 - val_loss: 1.2488 - val_acc: 0.6096\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.6995\n",
      "Epoch 00002: val_loss improved from 1.24883 to 0.92921, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/002-0.9292.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.0069 - acc: 0.6996 - val_loss: 0.9292 - val_acc: 0.7354\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7384 - acc: 0.7828\n",
      "Epoch 00003: val_loss improved from 0.92921 to 0.72316, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/003-0.7232.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7383 - acc: 0.7829 - val_loss: 0.7232 - val_acc: 0.7941\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5864 - acc: 0.8274\n",
      "Epoch 00004: val_loss improved from 0.72316 to 0.62356, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/004-0.6236.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5866 - acc: 0.8273 - val_loss: 0.6236 - val_acc: 0.8206\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8517\n",
      "Epoch 00005: val_loss improved from 0.62356 to 0.53737, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/005-0.5374.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4895 - acc: 0.8517 - val_loss: 0.5374 - val_acc: 0.8414\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8741\n",
      "Epoch 00006: val_loss improved from 0.53737 to 0.44194, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/006-0.4419.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4140 - acc: 0.8741 - val_loss: 0.4419 - val_acc: 0.8789\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8863\n",
      "Epoch 00007: val_loss improved from 0.44194 to 0.36746, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/007-0.3675.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3699 - acc: 0.8863 - val_loss: 0.3675 - val_acc: 0.9024\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8973\n",
      "Epoch 00008: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3361 - acc: 0.8972 - val_loss: 0.4562 - val_acc: 0.8877\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9110\n",
      "Epoch 00009: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2881 - acc: 0.9110 - val_loss: 0.4283 - val_acc: 0.8889\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9177\n",
      "Epoch 00010: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2616 - acc: 0.9177 - val_loss: 0.3967 - val_acc: 0.8931\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9266\n",
      "Epoch 00011: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2349 - acc: 0.9266 - val_loss: 0.4661 - val_acc: 0.8719\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9335\n",
      "Epoch 00012: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2075 - acc: 0.9335 - val_loss: 0.6787 - val_acc: 0.8286\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9382\n",
      "Epoch 00013: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1933 - acc: 0.9382 - val_loss: 0.5042 - val_acc: 0.8679\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9430\n",
      "Epoch 00014: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1781 - acc: 0.9430 - val_loss: 0.3793 - val_acc: 0.9033\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9458\n",
      "Epoch 00015: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1673 - acc: 0.9458 - val_loss: 0.4792 - val_acc: 0.8735\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9508\n",
      "Epoch 00016: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1533 - acc: 0.9508 - val_loss: 0.4450 - val_acc: 0.8928\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9536\n",
      "Epoch 00017: val_loss did not improve from 0.36746\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1429 - acc: 0.9536 - val_loss: 0.5935 - val_acc: 0.8647\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9527\n",
      "Epoch 00018: val_loss improved from 0.36746 to 0.33881, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/018-0.3388.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1470 - acc: 0.9527 - val_loss: 0.3388 - val_acc: 0.9117\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9640\n",
      "Epoch 00019: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1142 - acc: 0.9640 - val_loss: 0.4796 - val_acc: 0.8933\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9601\n",
      "Epoch 00020: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1232 - acc: 0.9600 - val_loss: 0.3768 - val_acc: 0.9068\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9646\n",
      "Epoch 00021: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1089 - acc: 0.9645 - val_loss: 0.3767 - val_acc: 0.9145\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9664\n",
      "Epoch 00022: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1017 - acc: 0.9664 - val_loss: 0.3603 - val_acc: 0.9161\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9696\n",
      "Epoch 00023: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0948 - acc: 0.9696 - val_loss: 0.3398 - val_acc: 0.9245\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9702\n",
      "Epoch 00024: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0911 - acc: 0.9701 - val_loss: 0.4170 - val_acc: 0.9113\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9718\n",
      "Epoch 00025: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0889 - acc: 0.9718 - val_loss: 0.3489 - val_acc: 0.9222\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9740\n",
      "Epoch 00026: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0774 - acc: 0.9740 - val_loss: 0.3484 - val_acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9740\n",
      "Epoch 00027: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0786 - acc: 0.9740 - val_loss: 0.4565 - val_acc: 0.8961\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9755\n",
      "Epoch 00028: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0751 - acc: 0.9754 - val_loss: 0.4110 - val_acc: 0.9159\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9752\n",
      "Epoch 00029: val_loss did not improve from 0.33881\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0769 - acc: 0.9752 - val_loss: 0.4476 - val_acc: 0.9087\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9746\n",
      "Epoch 00030: val_loss improved from 0.33881 to 0.31602, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv_checkpoint/030-0.3160.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0773 - acc: 0.9746 - val_loss: 0.3160 - val_acc: 0.9317\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9814\n",
      "Epoch 00031: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0575 - acc: 0.9814 - val_loss: 0.3441 - val_acc: 0.9262\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9772\n",
      "Epoch 00032: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0681 - acc: 0.9772 - val_loss: 0.3721 - val_acc: 0.9208\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9781\n",
      "Epoch 00033: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0666 - acc: 0.9781 - val_loss: 0.3866 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 00034: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0516 - acc: 0.9827 - val_loss: 0.3885 - val_acc: 0.9227\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9772\n",
      "Epoch 00035: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0689 - acc: 0.9772 - val_loss: 0.3328 - val_acc: 0.9320\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9853\n",
      "Epoch 00036: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0459 - acc: 0.9853 - val_loss: 0.3805 - val_acc: 0.9292\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9809\n",
      "Epoch 00037: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0573 - acc: 0.9809 - val_loss: 0.3754 - val_acc: 0.9283\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9830\n",
      "Epoch 00038: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0536 - acc: 0.9830 - val_loss: 0.4118 - val_acc: 0.9201\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9818\n",
      "Epoch 00039: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0565 - acc: 0.9817 - val_loss: 0.4667 - val_acc: 0.9075\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9823\n",
      "Epoch 00040: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0552 - acc: 0.9823 - val_loss: 0.4216 - val_acc: 0.9189\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9850\n",
      "Epoch 00041: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0448 - acc: 0.9849 - val_loss: 0.3582 - val_acc: 0.9324\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9808\n",
      "Epoch 00042: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0613 - acc: 0.9808 - val_loss: 0.4088 - val_acc: 0.9238\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9885\n",
      "Epoch 00043: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0363 - acc: 0.9885 - val_loss: 0.3187 - val_acc: 0.9394\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9884\n",
      "Epoch 00044: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0380 - acc: 0.9884 - val_loss: 0.3714 - val_acc: 0.9292\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9868\n",
      "Epoch 00045: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0413 - acc: 0.9868 - val_loss: 0.3883 - val_acc: 0.9238\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9862\n",
      "Epoch 00046: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0431 - acc: 0.9862 - val_loss: 0.3459 - val_acc: 0.9322\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9862\n",
      "Epoch 00047: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0421 - acc: 0.9862 - val_loss: 0.4451 - val_acc: 0.9206\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9858\n",
      "Epoch 00048: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0439 - acc: 0.9858 - val_loss: 0.3756 - val_acc: 0.9327\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9876\n",
      "Epoch 00049: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0400 - acc: 0.9876 - val_loss: 0.3892 - val_acc: 0.9250\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9888\n",
      "Epoch 00050: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0362 - acc: 0.9888 - val_loss: 0.4143 - val_acc: 0.9217\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9890\n",
      "Epoch 00051: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0357 - acc: 0.9890 - val_loss: 0.4445 - val_acc: 0.9185\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9881\n",
      "Epoch 00052: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0372 - acc: 0.9881 - val_loss: 0.3389 - val_acc: 0.9406\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9901\n",
      "Epoch 00053: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0323 - acc: 0.9901 - val_loss: 0.4782 - val_acc: 0.9161\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9900\n",
      "Epoch 00054: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0311 - acc: 0.9900 - val_loss: 0.4273 - val_acc: 0.9241\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9885\n",
      "Epoch 00055: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0366 - acc: 0.9885 - val_loss: 0.4125 - val_acc: 0.9278\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9885\n",
      "Epoch 00056: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0377 - acc: 0.9885 - val_loss: 0.3994 - val_acc: 0.9273\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9879\n",
      "Epoch 00057: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0398 - acc: 0.9879 - val_loss: 0.3199 - val_acc: 0.9420\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9910\n",
      "Epoch 00058: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0276 - acc: 0.9909 - val_loss: 0.3689 - val_acc: 0.9313\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9873\n",
      "Epoch 00059: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0418 - acc: 0.9872 - val_loss: 0.3871 - val_acc: 0.9306\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9884\n",
      "Epoch 00060: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0384 - acc: 0.9884 - val_loss: 0.3545 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9925\n",
      "Epoch 00061: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.4005 - val_acc: 0.9299\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9884\n",
      "Epoch 00062: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0386 - acc: 0.9884 - val_loss: 0.3500 - val_acc: 0.9350\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9913\n",
      "Epoch 00063: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0282 - acc: 0.9913 - val_loss: 0.3574 - val_acc: 0.9366\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9919\n",
      "Epoch 00064: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0262 - acc: 0.9919 - val_loss: 0.4333 - val_acc: 0.9243\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9899\n",
      "Epoch 00065: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0359 - acc: 0.9899 - val_loss: 0.4515 - val_acc: 0.9220\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00066: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.3785 - val_acc: 0.9311\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9932\n",
      "Epoch 00067: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0235 - acc: 0.9932 - val_loss: 0.3759 - val_acc: 0.9357\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0323 - acc: 0.9895 - val_loss: 0.3840 - val_acc: 0.9336\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00069: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.3962 - val_acc: 0.9290\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9917\n",
      "Epoch 00070: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0284 - acc: 0.9917 - val_loss: 0.4404 - val_acc: 0.9171\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9912\n",
      "Epoch 00071: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0279 - acc: 0.9912 - val_loss: 0.4054 - val_acc: 0.9308\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 00072: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.4101 - val_acc: 0.9304\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9909\n",
      "Epoch 00073: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0299 - acc: 0.9909 - val_loss: 0.3575 - val_acc: 0.9362\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9906\n",
      "Epoch 00074: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0303 - acc: 0.9906 - val_loss: 0.3954 - val_acc: 0.9331\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9940\n",
      "Epoch 00075: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0199 - acc: 0.9940 - val_loss: 0.3673 - val_acc: 0.9390\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9934\n",
      "Epoch 00076: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0231 - acc: 0.9934 - val_loss: 0.4314 - val_acc: 0.9241\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9893\n",
      "Epoch 00077: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0375 - acc: 0.9893 - val_loss: 0.3384 - val_acc: 0.9413\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9932\n",
      "Epoch 00078: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0225 - acc: 0.9932 - val_loss: 0.3508 - val_acc: 0.9443\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9937\n",
      "Epoch 00079: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0226 - acc: 0.9937 - val_loss: 0.4014 - val_acc: 0.9304\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9944\n",
      "Epoch 00080: val_loss did not improve from 0.31602\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0197 - acc: 0.9944 - val_loss: 0.4507 - val_acc: 0.9269\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3817 - acc: 0.9140\n",
      "Loss: 0.3817043314717888 Accuracy: 0.9140187\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6833 - acc: 0.4951\n",
      "Epoch 00001: val_loss improved from inf to 0.98153, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/001-0.9815.hdf5\n",
      "36805/36805 [==============================] - 120s 3ms/sample - loss: 1.6834 - acc: 0.4951 - val_loss: 0.9815 - val_acc: 0.6967\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7710\n",
      "Epoch 00002: val_loss improved from 0.98153 to 0.53793, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/002-0.5379.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.7608 - acc: 0.7710 - val_loss: 0.5379 - val_acc: 0.8367\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8472\n",
      "Epoch 00003: val_loss improved from 0.53793 to 0.37646, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/003-0.3765.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.5096 - acc: 0.8472 - val_loss: 0.3765 - val_acc: 0.8901\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8822\n",
      "Epoch 00004: val_loss improved from 0.37646 to 0.29203, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/004-0.2920.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3914 - acc: 0.8822 - val_loss: 0.2920 - val_acc: 0.9147\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9000\n",
      "Epoch 00005: val_loss did not improve from 0.29203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3226 - acc: 0.8999 - val_loss: 0.3380 - val_acc: 0.9085\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9136\n",
      "Epoch 00006: val_loss improved from 0.29203 to 0.25678, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/006-0.2568.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2766 - acc: 0.9136 - val_loss: 0.2568 - val_acc: 0.9245\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9258\n",
      "Epoch 00007: val_loss did not improve from 0.25678\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2394 - acc: 0.9258 - val_loss: 0.3020 - val_acc: 0.9101\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9331\n",
      "Epoch 00008: val_loss improved from 0.25678 to 0.22810, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/008-0.2281.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2127 - acc: 0.9331 - val_loss: 0.2281 - val_acc: 0.9306\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9415\n",
      "Epoch 00009: val_loss did not improve from 0.22810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1866 - acc: 0.9414 - val_loss: 0.3966 - val_acc: 0.8887\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9438\n",
      "Epoch 00010: val_loss did not improve from 0.22810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1786 - acc: 0.9438 - val_loss: 0.2657 - val_acc: 0.9301\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9521\n",
      "Epoch 00011: val_loss did not improve from 0.22810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1492 - acc: 0.9522 - val_loss: 0.2848 - val_acc: 0.9187\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9540\n",
      "Epoch 00012: val_loss did not improve from 0.22810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1425 - acc: 0.9540 - val_loss: 0.2553 - val_acc: 0.9324\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9579\n",
      "Epoch 00013: val_loss improved from 0.22810 to 0.20548, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/013-0.2055.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1345 - acc: 0.9579 - val_loss: 0.2055 - val_acc: 0.9443\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9632\n",
      "Epoch 00014: val_loss did not improve from 0.20548\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1120 - acc: 0.9632 - val_loss: 0.2512 - val_acc: 0.9338\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9642\n",
      "Epoch 00015: val_loss improved from 0.20548 to 0.18947, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/015-0.1895.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1103 - acc: 0.9642 - val_loss: 0.1895 - val_acc: 0.9506\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9695\n",
      "Epoch 00016: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0952 - acc: 0.9695 - val_loss: 0.2109 - val_acc: 0.9441\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9720\n",
      "Epoch 00017: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0864 - acc: 0.9720 - val_loss: 0.2186 - val_acc: 0.9422\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9719\n",
      "Epoch 00018: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0834 - acc: 0.9719 - val_loss: 0.2018 - val_acc: 0.9462\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9747\n",
      "Epoch 00019: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0760 - acc: 0.9747 - val_loss: 0.2036 - val_acc: 0.9469\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9756\n",
      "Epoch 00020: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0747 - acc: 0.9755 - val_loss: 0.2598 - val_acc: 0.9336\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9772\n",
      "Epoch 00021: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0702 - acc: 0.9772 - val_loss: 0.2004 - val_acc: 0.9492\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9795\n",
      "Epoch 00022: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0632 - acc: 0.9795 - val_loss: 0.1972 - val_acc: 0.9511\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9807\n",
      "Epoch 00023: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0573 - acc: 0.9807 - val_loss: 0.1955 - val_acc: 0.9518\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9802\n",
      "Epoch 00024: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0595 - acc: 0.9802 - val_loss: 0.1993 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9850\n",
      "Epoch 00025: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0467 - acc: 0.9849 - val_loss: 0.2375 - val_acc: 0.9434\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9838\n",
      "Epoch 00026: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0491 - acc: 0.9838 - val_loss: 0.1968 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9848\n",
      "Epoch 00027: val_loss did not improve from 0.18947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0447 - acc: 0.9848 - val_loss: 0.2102 - val_acc: 0.9485\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9832\n",
      "Epoch 00028: val_loss improved from 0.18947 to 0.18351, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/028-0.1835.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0479 - acc: 0.9832 - val_loss: 0.1835 - val_acc: 0.9536\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9879\n",
      "Epoch 00029: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0372 - acc: 0.9879 - val_loss: 0.2183 - val_acc: 0.9469\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9864\n",
      "Epoch 00030: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0420 - acc: 0.9864 - val_loss: 0.2688 - val_acc: 0.9392\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9884\n",
      "Epoch 00031: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0343 - acc: 0.9884 - val_loss: 0.2276 - val_acc: 0.9467\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9865\n",
      "Epoch 00032: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0385 - acc: 0.9866 - val_loss: 0.2328 - val_acc: 0.9467\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9874\n",
      "Epoch 00033: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0393 - acc: 0.9874 - val_loss: 0.2296 - val_acc: 0.9460\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9872\n",
      "Epoch 00034: val_loss did not improve from 0.18351\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0391 - acc: 0.9871 - val_loss: 0.2102 - val_acc: 0.9506\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9894\n",
      "Epoch 00035: val_loss improved from 0.18351 to 0.18135, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/035-0.1814.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0320 - acc: 0.9894 - val_loss: 0.1814 - val_acc: 0.9581\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9917\n",
      "Epoch 00036: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0263 - acc: 0.9917 - val_loss: 0.2031 - val_acc: 0.9590\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9910\n",
      "Epoch 00037: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0264 - acc: 0.9910 - val_loss: 0.2140 - val_acc: 0.9546\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 00038: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0319 - acc: 0.9893 - val_loss: 0.2371 - val_acc: 0.9471\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9905\n",
      "Epoch 00039: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.2614 - val_acc: 0.9399\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9918\n",
      "Epoch 00040: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0252 - acc: 0.9918 - val_loss: 0.2253 - val_acc: 0.9499\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9894\n",
      "Epoch 00041: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0313 - acc: 0.9894 - val_loss: 0.2090 - val_acc: 0.9548\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9944\n",
      "Epoch 00042: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0186 - acc: 0.9944 - val_loss: 0.3211 - val_acc: 0.9352\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9876\n",
      "Epoch 00043: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0357 - acc: 0.9876 - val_loss: 0.2475 - val_acc: 0.9509\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 00044: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0302 - acc: 0.9907 - val_loss: 0.2528 - val_acc: 0.9478\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9912\n",
      "Epoch 00045: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0266 - acc: 0.9912 - val_loss: 0.1964 - val_acc: 0.9592\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9942\n",
      "Epoch 00046: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0185 - acc: 0.9942 - val_loss: 0.2450 - val_acc: 0.9460\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9902\n",
      "Epoch 00047: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0321 - acc: 0.9902 - val_loss: 0.1878 - val_acc: 0.9595\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9943\n",
      "Epoch 00048: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.1901 - val_acc: 0.9597\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9947\n",
      "Epoch 00049: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0169 - acc: 0.9947 - val_loss: 0.2023 - val_acc: 0.9555\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9913\n",
      "Epoch 00050: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0261 - acc: 0.9913 - val_loss: 0.2219 - val_acc: 0.9499\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9938\n",
      "Epoch 00051: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0189 - acc: 0.9938 - val_loss: 0.2278 - val_acc: 0.9534\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9886\n",
      "Epoch 00052: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0383 - acc: 0.9886 - val_loss: 0.1866 - val_acc: 0.9597\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9954\n",
      "Epoch 00053: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0154 - acc: 0.9954 - val_loss: 0.2088 - val_acc: 0.9553\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9949\n",
      "Epoch 00054: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0158 - acc: 0.9948 - val_loss: 0.2273 - val_acc: 0.9569\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9893\n",
      "Epoch 00055: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0329 - acc: 0.9893 - val_loss: 0.2548 - val_acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9949\n",
      "Epoch 00056: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0143 - acc: 0.9949 - val_loss: 0.2612 - val_acc: 0.9495\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9957\n",
      "Epoch 00057: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.2845 - val_acc: 0.9457\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9924\n",
      "Epoch 00058: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0227 - acc: 0.9924 - val_loss: 0.2161 - val_acc: 0.9620\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9956\n",
      "Epoch 00059: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0143 - acc: 0.9956 - val_loss: 0.2154 - val_acc: 0.9581\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9935\n",
      "Epoch 00060: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0207 - acc: 0.9935 - val_loss: 0.1909 - val_acc: 0.9599\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0156 - acc: 0.9951 - val_loss: 0.2026 - val_acc: 0.9583\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9936\n",
      "Epoch 00062: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0194 - acc: 0.9936 - val_loss: 0.2097 - val_acc: 0.9567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9917\n",
      "Epoch 00063: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0251 - acc: 0.9917 - val_loss: 0.2297 - val_acc: 0.9583\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9955\n",
      "Epoch 00064: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0140 - acc: 0.9955 - val_loss: 0.1894 - val_acc: 0.9623\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9967\n",
      "Epoch 00065: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.2320 - val_acc: 0.9578\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9929\n",
      "Epoch 00066: val_loss did not improve from 0.18135\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.2021 - val_acc: 0.9597\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9967\n",
      "Epoch 00067: val_loss improved from 0.18135 to 0.17947, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv_checkpoint/067-0.1795.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0122 - acc: 0.9967 - val_loss: 0.1795 - val_acc: 0.9639\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9949\n",
      "Epoch 00068: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0150 - acc: 0.9949 - val_loss: 0.2212 - val_acc: 0.9599\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9960\n",
      "Epoch 00069: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.2251 - val_acc: 0.9548\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9936\n",
      "Epoch 00070: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0196 - acc: 0.9936 - val_loss: 0.2281 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 00071: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0144 - acc: 0.9954 - val_loss: 0.2351 - val_acc: 0.9546\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 00072: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0146 - acc: 0.9954 - val_loss: 0.2036 - val_acc: 0.9646\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9955\n",
      "Epoch 00073: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0158 - acc: 0.9955 - val_loss: 0.2595 - val_acc: 0.9541\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9971\n",
      "Epoch 00074: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0101 - acc: 0.9971 - val_loss: 0.2075 - val_acc: 0.9630\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9943\n",
      "Epoch 00075: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.1967 - val_acc: 0.9632\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9963\n",
      "Epoch 00076: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0114 - acc: 0.9962 - val_loss: 0.2119 - val_acc: 0.9597\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9921\n",
      "Epoch 00077: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0249 - acc: 0.9921 - val_loss: 0.2752 - val_acc: 0.9504\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 00078: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.1809 - val_acc: 0.9611\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9958\n",
      "Epoch 00079: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0136 - acc: 0.9958 - val_loss: 0.2216 - val_acc: 0.9595\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9957\n",
      "Epoch 00080: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.1965 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9952\n",
      "Epoch 00081: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0150 - acc: 0.9952 - val_loss: 0.2242 - val_acc: 0.9592\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0095 - acc: 0.9966 - val_loss: 0.2272 - val_acc: 0.9604\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 00083: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.1983 - val_acc: 0.9618\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9970\n",
      "Epoch 00084: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.2009 - val_acc: 0.9613\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9951\n",
      "Epoch 00085: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0152 - acc: 0.9950 - val_loss: 0.2884 - val_acc: 0.9481\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9943\n",
      "Epoch 00086: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0190 - acc: 0.9943 - val_loss: 0.1805 - val_acc: 0.9653\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9960\n",
      "Epoch 00087: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0108 - acc: 0.9960 - val_loss: 0.2084 - val_acc: 0.9599\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9959\n",
      "Epoch 00088: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.1949 - val_acc: 0.9662\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 00089: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0093 - acc: 0.9971 - val_loss: 0.2357 - val_acc: 0.9546\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9950\n",
      "Epoch 00090: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0162 - acc: 0.9950 - val_loss: 0.2824 - val_acc: 0.9499\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9958\n",
      "Epoch 00091: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0138 - acc: 0.9958 - val_loss: 0.2801 - val_acc: 0.9522\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9970\n",
      "Epoch 00092: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0093 - acc: 0.9970 - val_loss: 0.2313 - val_acc: 0.9585\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9977\n",
      "Epoch 00093: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 0.2232 - val_acc: 0.9599\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9967\n",
      "Epoch 00094: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0100 - acc: 0.9967 - val_loss: 0.2438 - val_acc: 0.9578\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9971\n",
      "Epoch 00095: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 0.2150 - val_acc: 0.9592\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00096: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.2052 - val_acc: 0.9639\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9970\n",
      "Epoch 00097: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 0.2116 - val_acc: 0.9618\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9968\n",
      "Epoch 00098: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.2228 - val_acc: 0.9581\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9973\n",
      "Epoch 00099: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0082 - acc: 0.9973 - val_loss: 0.1941 - val_acc: 0.9665\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00100: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.2659 - val_acc: 0.9532\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9965\n",
      "Epoch 00101: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0102 - acc: 0.9965 - val_loss: 0.2347 - val_acc: 0.9574\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 00102: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0137 - acc: 0.9954 - val_loss: 0.1901 - val_acc: 0.9653\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 00103: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0063 - acc: 0.9980 - val_loss: 0.2319 - val_acc: 0.9595\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00104: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.2316 - val_acc: 0.9611\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9965\n",
      "Epoch 00105: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0105 - acc: 0.9965 - val_loss: 0.2445 - val_acc: 0.9576\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9964\n",
      "Epoch 00106: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0122 - acc: 0.9964 - val_loss: 0.2502 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
      "Epoch 00107: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2183 - val_acc: 0.9599\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9968\n",
      "Epoch 00108: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0098 - acc: 0.9968 - val_loss: 0.2252 - val_acc: 0.9606\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 00109: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.2175 - val_acc: 0.9623\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00110: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.3278 - val_acc: 0.9497\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9961\n",
      "Epoch 00111: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0117 - acc: 0.9961 - val_loss: 0.2512 - val_acc: 0.9590\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9950\n",
      "Epoch 00112: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0170 - acc: 0.9950 - val_loss: 0.2529 - val_acc: 0.9543\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00113: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2254 - val_acc: 0.9609\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9969\n",
      "Epoch 00114: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0097 - acc: 0.9969 - val_loss: 0.2615 - val_acc: 0.9555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
      "Epoch 00115: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.1908 - val_acc: 0.9669\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9961\n",
      "Epoch 00116: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0130 - acc: 0.9961 - val_loss: 0.2179 - val_acc: 0.9630\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 00117: val_loss did not improve from 0.17947\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0068 - acc: 0.9977 - val_loss: 0.2269 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2447 - acc: 0.9483\n",
      "Loss: 0.24473613623755974 Accuracy: 0.9482866\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5685 - acc: 0.5268\n",
      "Epoch 00001: val_loss improved from inf to 0.90304, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/001-0.9030.hdf5\n",
      "36805/36805 [==============================] - 125s 3ms/sample - loss: 1.5683 - acc: 0.5268 - val_loss: 0.9030 - val_acc: 0.7133\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.8107\n",
      "Epoch 00002: val_loss improved from 0.90304 to 0.35476, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/002-0.3548.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.6147 - acc: 0.8107 - val_loss: 0.3548 - val_acc: 0.8980\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8739\n",
      "Epoch 00003: val_loss improved from 0.35476 to 0.33312, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/003-0.3331.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4102 - acc: 0.8739 - val_loss: 0.3331 - val_acc: 0.8989\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9011\n",
      "Epoch 00004: val_loss did not improve from 0.33312\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3153 - acc: 0.9010 - val_loss: 0.3577 - val_acc: 0.8952\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9193\n",
      "Epoch 00005: val_loss improved from 0.33312 to 0.27643, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/005-0.2764.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2598 - acc: 0.9193 - val_loss: 0.2764 - val_acc: 0.9119\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9315\n",
      "Epoch 00006: val_loss improved from 0.27643 to 0.24864, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/006-0.2486.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2153 - acc: 0.9315 - val_loss: 0.2486 - val_acc: 0.9208\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9397\n",
      "Epoch 00007: val_loss improved from 0.24864 to 0.18954, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/007-0.1895.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1913 - acc: 0.9397 - val_loss: 0.1895 - val_acc: 0.9418\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9468\n",
      "Epoch 00008: val_loss did not improve from 0.18954\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1666 - acc: 0.9468 - val_loss: 0.2089 - val_acc: 0.9343\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9534\n",
      "Epoch 00009: val_loss improved from 0.18954 to 0.16505, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/009-0.1650.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1447 - acc: 0.9534 - val_loss: 0.1650 - val_acc: 0.9513\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9563\n",
      "Epoch 00010: val_loss improved from 0.16505 to 0.15757, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/010-0.1576.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1336 - acc: 0.9563 - val_loss: 0.1576 - val_acc: 0.9578\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9623\n",
      "Epoch 00011: val_loss did not improve from 0.15757\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1175 - acc: 0.9623 - val_loss: 0.1633 - val_acc: 0.9525\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9633\n",
      "Epoch 00012: val_loss improved from 0.15757 to 0.15667, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/012-0.1567.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1146 - acc: 0.9633 - val_loss: 0.1567 - val_acc: 0.9555\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9680\n",
      "Epoch 00013: val_loss did not improve from 0.15667\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0974 - acc: 0.9679 - val_loss: 0.1686 - val_acc: 0.9490\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9707\n",
      "Epoch 00014: val_loss improved from 0.15667 to 0.14654, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/014-0.1465.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0890 - acc: 0.9707 - val_loss: 0.1465 - val_acc: 0.9560\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9744\n",
      "Epoch 00015: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0782 - acc: 0.9744 - val_loss: 0.1704 - val_acc: 0.9506\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9768\n",
      "Epoch 00016: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0722 - acc: 0.9768 - val_loss: 0.1799 - val_acc: 0.9522\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9781\n",
      "Epoch 00017: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0674 - acc: 0.9781 - val_loss: 0.1805 - val_acc: 0.9502\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9760\n",
      "Epoch 00018: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0760 - acc: 0.9760 - val_loss: 0.1653 - val_acc: 0.9555\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9810\n",
      "Epoch 00019: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0581 - acc: 0.9810 - val_loss: 0.1768 - val_acc: 0.9522\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9794\n",
      "Epoch 00020: val_loss did not improve from 0.14654\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0640 - acc: 0.9794 - val_loss: 0.1695 - val_acc: 0.9527\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9842\n",
      "Epoch 00021: val_loss improved from 0.14654 to 0.14453, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/021-0.1445.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0497 - acc: 0.9842 - val_loss: 0.1445 - val_acc: 0.9588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9868\n",
      "Epoch 00022: val_loss improved from 0.14453 to 0.13612, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv_checkpoint/022-0.1361.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0425 - acc: 0.9868 - val_loss: 0.1361 - val_acc: 0.9613\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9869\n",
      "Epoch 00023: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0408 - acc: 0.9869 - val_loss: 0.1631 - val_acc: 0.9509\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9873\n",
      "Epoch 00024: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0397 - acc: 0.9873 - val_loss: 0.1672 - val_acc: 0.9562\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9866\n",
      "Epoch 00025: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0409 - acc: 0.9866 - val_loss: 0.1408 - val_acc: 0.9637\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9887\n",
      "Epoch 00026: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0352 - acc: 0.9887 - val_loss: 0.1811 - val_acc: 0.9527\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 00027: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0357 - acc: 0.9885 - val_loss: 0.1514 - val_acc: 0.9599\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9892\n",
      "Epoch 00028: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0340 - acc: 0.9892 - val_loss: 0.1471 - val_acc: 0.9616\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9910\n",
      "Epoch 00029: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0284 - acc: 0.9910 - val_loss: 0.2048 - val_acc: 0.9513\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9906\n",
      "Epoch 00030: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0284 - acc: 0.9906 - val_loss: 0.1687 - val_acc: 0.9578\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9911\n",
      "Epoch 00031: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0269 - acc: 0.9911 - val_loss: 0.2349 - val_acc: 0.9406\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9899\n",
      "Epoch 00032: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0309 - acc: 0.9898 - val_loss: 0.1632 - val_acc: 0.9550\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9892\n",
      "Epoch 00033: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0360 - acc: 0.9892 - val_loss: 0.1434 - val_acc: 0.9627\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9929\n",
      "Epoch 00034: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0221 - acc: 0.9929 - val_loss: 0.1613 - val_acc: 0.9616\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9931\n",
      "Epoch 00035: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0211 - acc: 0.9931 - val_loss: 0.1535 - val_acc: 0.9651\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9932\n",
      "Epoch 00036: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.1632 - val_acc: 0.9602\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9903\n",
      "Epoch 00037: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0294 - acc: 0.9902 - val_loss: 0.1575 - val_acc: 0.9595\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9917\n",
      "Epoch 00038: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0281 - acc: 0.9917 - val_loss: 0.1501 - val_acc: 0.9665\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9954\n",
      "Epoch 00039: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0159 - acc: 0.9954 - val_loss: 0.1373 - val_acc: 0.9686\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9934\n",
      "Epoch 00040: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0199 - acc: 0.9934 - val_loss: 0.1973 - val_acc: 0.9541\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9953\n",
      "Epoch 00041: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0149 - acc: 0.9953 - val_loss: 0.1955 - val_acc: 0.9534\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9928\n",
      "Epoch 00042: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0209 - acc: 0.9928 - val_loss: 0.1816 - val_acc: 0.9620\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9935\n",
      "Epoch 00043: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.1537 - val_acc: 0.9658\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9955\n",
      "Epoch 00044: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.2876 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9899\n",
      "Epoch 00045: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0307 - acc: 0.9899 - val_loss: 0.2543 - val_acc: 0.9467\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9961\n",
      "Epoch 00046: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0130 - acc: 0.9961 - val_loss: 0.1562 - val_acc: 0.9665\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00047: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9966 - val_loss: 0.1477 - val_acc: 0.9648\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9922\n",
      "Epoch 00048: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0243 - acc: 0.9922 - val_loss: 0.1837 - val_acc: 0.9618\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9964\n",
      "Epoch 00049: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.1586 - val_acc: 0.9637\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9956\n",
      "Epoch 00050: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0136 - acc: 0.9956 - val_loss: 0.2163 - val_acc: 0.9539\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9927\n",
      "Epoch 00051: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0242 - acc: 0.9927 - val_loss: 0.1755 - val_acc: 0.9578\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 00052: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.1615 - val_acc: 0.9658\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9952\n",
      "Epoch 00053: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0147 - acc: 0.9952 - val_loss: 0.1680 - val_acc: 0.9630\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 00054: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9974 - val_loss: 0.1667 - val_acc: 0.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9921\n",
      "Epoch 00055: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0234 - acc: 0.9921 - val_loss: 0.1626 - val_acc: 0.9644\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
      "Epoch 00056: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.1654 - val_acc: 0.9644\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9923\n",
      "Epoch 00057: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0256 - acc: 0.9923 - val_loss: 0.1680 - val_acc: 0.9639\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9964\n",
      "Epoch 00058: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9964 - val_loss: 0.2333 - val_acc: 0.9495\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9967\n",
      "Epoch 00059: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.1609 - val_acc: 0.9672\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 00060: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0137 - acc: 0.9954 - val_loss: 0.2682 - val_acc: 0.9471\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968\n",
      "Epoch 00061: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0110 - acc: 0.9968 - val_loss: 0.1512 - val_acc: 0.9672\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9932\n",
      "Epoch 00062: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0211 - acc: 0.9932 - val_loss: 0.1912 - val_acc: 0.9634\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0117 - acc: 0.9963 - val_loss: 0.2083 - val_acc: 0.9574\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00064: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.1401 - val_acc: 0.9676\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9975\n",
      "Epoch 00065: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1993 - val_acc: 0.9536\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9959\n",
      "Epoch 00066: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0130 - acc: 0.9959 - val_loss: 0.2312 - val_acc: 0.9562\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9966\n",
      "Epoch 00067: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0113 - acc: 0.9966 - val_loss: 0.1595 - val_acc: 0.9674\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9962\n",
      "Epoch 00068: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0114 - acc: 0.9962 - val_loss: 0.1551 - val_acc: 0.9651\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.1797 - val_acc: 0.9662\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00070: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.2278 - val_acc: 0.9548\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9962\n",
      "Epoch 00071: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0116 - acc: 0.9962 - val_loss: 0.1561 - val_acc: 0.9690\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00072: val_loss did not improve from 0.13612\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1643 - val_acc: 0.9637\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1794 - acc: 0.9518\n",
      "Loss: 0.17936863774513034 Accuracy: 0.9518172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_ch_128_DO_BN'\n",
    "\n",
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 227456)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 328448)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 328448)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           5255184     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,504,144\n",
      "Trainable params: 5,503,120\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 2.8364 - acc: 0.6997\n",
      "Loss: 2.836447862672657 Accuracy: 0.6996885\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 75776)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 117632)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 117632)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1882128     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,296,208\n",
      "Trainable params: 2,294,672\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9248 - acc: 0.8280\n",
      "Loss: 0.9247710018514472 Accuracy: 0.8280374\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 25216)        0           max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 47232)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 47232)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           755728      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,498,768\n",
      "Trainable params: 1,496,720\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3817 - acc: 0.9140\n",
      "Loss: 0.3817043314717888 Accuracy: 0.9140187\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 16640)        0           max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           380944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,452,944\n",
      "Trainable params: 1,450,384\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2447 - acc: 0.9483\n",
      "Loss: 0.24473613623755974 Accuracy: 0.9482866\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 5376)         0           max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 7680)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 7680)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           122896      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,523,856\n",
      "Trainable params: 1,520,784\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1794 - acc: 0.9518\n",
      "Loss: 0.17936863774513034 Accuracy: 0.9518172\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 227456)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 328448)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 328448)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           5255184     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,504,144\n",
      "Trainable params: 5,503,120\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 3.3985 - acc: 0.6690\n",
      "Loss: 3.398548726698815 Accuracy: 0.6689512\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 75776)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 117632)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 117632)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1882128     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,296,208\n",
      "Trainable params: 2,294,672\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.1233 - acc: 0.8370\n",
      "Loss: 1.1233408542997896 Accuracy: 0.8369678\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 25216)        0           max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 47232)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 47232)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           755728      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,498,768\n",
      "Trainable params: 1,496,720\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5819 - acc: 0.9057\n",
      "Loss: 0.5818835045568589 Accuracy: 0.9057113\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 16640)        0           max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           380944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,452,944\n",
      "Trainable params: 1,450,384\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2870 - acc: 0.9472\n",
      "Loss: 0.2870340168372776 Accuracy: 0.94724816\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 5376)         0           max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 7680)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 7680)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           122896      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,523,856\n",
      "Trainable params: 1,520,784\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2561 - acc: 0.9472\n",
      "Loss: 0.2560636434481148 Accuracy: 0.94724816\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
