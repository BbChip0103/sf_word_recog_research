{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    channel_size = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 2048000)           8192000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 41,010,832\n",
      "Trainable params: 36,914,320\n",
      "Non-trainable params: 4,096,512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 682624)            2730496   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 13,802,896\n",
      "Trainable params: 12,436,624\n",
      "Non-trainable params: 1,366,272\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 227456)            909824    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 4,799,120\n",
      "Trainable params: 4,342,672\n",
      "Non-trainable params: 456,448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 75776)             303104    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,865,104\n",
      "Trainable params: 1,711,504\n",
      "Non-trainable params: 153,600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 50432)             201728    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,655,696\n",
      "Trainable params: 1,551,760\n",
      "Non-trainable params: 103,936\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 16640)             66560     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,375,632\n",
      "Trainable params: 1,338,256\n",
      "Non-trainable params: 37,376\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5376)              21504     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,546,128\n",
      "Trainable params: 1,530,256\n",
      "Non-trainable params: 15,872\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1792)              7168      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,870,224\n",
      "Trainable params: 1,860,496\n",
      "Non-trainable params: 9,728\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,039,632\n",
      "Trainable params: 3,029,392\n",
      "Non-trainable params: 10,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0134 - acc: 0.4604\n",
      "Epoch 00001: val_loss improved from inf to 1.57889, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv_checkpoint/001-1.5789.hdf5\n",
      "36805/36805 [==============================] - 293s 8ms/sample - loss: 2.0136 - acc: 0.4603 - val_loss: 1.5789 - val_acc: 0.5660\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1578 - acc: 0.6737\n",
      "Epoch 00002: val_loss improved from 1.57889 to 1.44796, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv_checkpoint/002-1.4480.hdf5\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 1.1581 - acc: 0.6736 - val_loss: 1.4480 - val_acc: 0.6131\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6990 - acc: 0.7919\n",
      "Epoch 00003: val_loss improved from 1.44796 to 1.30666, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv_checkpoint/003-1.3067.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.6992 - acc: 0.7919 - val_loss: 1.3067 - val_acc: 0.6671\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8758\n",
      "Epoch 00004: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.4159 - acc: 0.8757 - val_loss: 1.6083 - val_acc: 0.6157\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9259\n",
      "Epoch 00005: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.2634 - acc: 0.9259 - val_loss: 1.3807 - val_acc: 0.6765\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9530\n",
      "Epoch 00006: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1794 - acc: 0.9529 - val_loss: 1.3167 - val_acc: 0.6883\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9608\n",
      "Epoch 00007: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1527 - acc: 0.9608 - val_loss: 1.3250 - val_acc: 0.6960\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9711\n",
      "Epoch 00008: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1162 - acc: 0.9711 - val_loss: 1.4087 - val_acc: 0.6902\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9751\n",
      "Epoch 00009: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 287s 8ms/sample - loss: 0.1039 - acc: 0.9751 - val_loss: 1.6067 - val_acc: 0.6543\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9622\n",
      "Epoch 00010: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1386 - acc: 0.9621 - val_loss: 1.6136 - val_acc: 0.6739\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9595\n",
      "Epoch 00011: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.1400 - acc: 0.9595 - val_loss: 1.5598 - val_acc: 0.6907\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9755\n",
      "Epoch 00012: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0965 - acc: 0.9755 - val_loss: 1.5514 - val_acc: 0.6932\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9805\n",
      "Epoch 00013: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0795 - acc: 0.9805 - val_loss: 1.6566 - val_acc: 0.6865\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9799\n",
      "Epoch 00014: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0791 - acc: 0.9799 - val_loss: 1.7455 - val_acc: 0.6639\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9845\n",
      "Epoch 00015: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0665 - acc: 0.9843 - val_loss: 1.8703 - val_acc: 0.6660\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9703\n",
      "Epoch 00016: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.1043 - acc: 0.9702 - val_loss: 1.7614 - val_acc: 0.6818\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9785\n",
      "Epoch 00017: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0806 - acc: 0.9785 - val_loss: 1.7804 - val_acc: 0.6741\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9842\n",
      "Epoch 00018: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0604 - acc: 0.9841 - val_loss: 1.9275 - val_acc: 0.6776\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9819\n",
      "Epoch 00019: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0682 - acc: 0.9819 - val_loss: 1.8478 - val_acc: 0.6776\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9844\n",
      "Epoch 00020: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0610 - acc: 0.9843 - val_loss: 2.0064 - val_acc: 0.6664\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9759\n",
      "Epoch 00021: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0865 - acc: 0.9758 - val_loss: 1.9171 - val_acc: 0.6702\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9851\n",
      "Epoch 00022: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0602 - acc: 0.9851 - val_loss: 1.9393 - val_acc: 0.6744\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9887\n",
      "Epoch 00023: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0482 - acc: 0.9887 - val_loss: 2.0532 - val_acc: 0.6725\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9818\n",
      "Epoch 00024: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0731 - acc: 0.9817 - val_loss: 2.0007 - val_acc: 0.6706\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9859\n",
      "Epoch 00025: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0560 - acc: 0.9859 - val_loss: 1.9394 - val_acc: 0.6876\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9842\n",
      "Epoch 00026: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0622 - acc: 0.9841 - val_loss: 2.0013 - val_acc: 0.6790\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9821\n",
      "Epoch 00027: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0682 - acc: 0.9820 - val_loss: 2.0528 - val_acc: 0.6765\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9847\n",
      "Epoch 00028: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0575 - acc: 0.9846 - val_loss: 2.0845 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9861\n",
      "Epoch 00029: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0525 - acc: 0.9860 - val_loss: 2.0503 - val_acc: 0.6753\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9879\n",
      "Epoch 00030: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0497 - acc: 0.9878 - val_loss: 2.2438 - val_acc: 0.6646\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9840\n",
      "Epoch 00031: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0643 - acc: 0.9840 - val_loss: 2.0527 - val_acc: 0.6823\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9919\n",
      "Epoch 00032: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0364 - acc: 0.9919 - val_loss: 2.1285 - val_acc: 0.6783\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9924\n",
      "Epoch 00033: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0386 - acc: 0.9923 - val_loss: 2.3108 - val_acc: 0.6690\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9854\n",
      "Epoch 00034: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0599 - acc: 0.9853 - val_loss: 2.2298 - val_acc: 0.6604\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9858\n",
      "Epoch 00035: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0531 - acc: 0.9858 - val_loss: 2.2183 - val_acc: 0.6816\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9880\n",
      "Epoch 00036: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0487 - acc: 0.9880 - val_loss: 2.3227 - val_acc: 0.6713\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9850\n",
      "Epoch 00037: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0599 - acc: 0.9850 - val_loss: 2.3423 - val_acc: 0.6681\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9905\n",
      "Epoch 00038: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0410 - acc: 0.9904 - val_loss: 2.4401 - val_acc: 0.6692\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9834\n",
      "Epoch 00039: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0628 - acc: 0.9834 - val_loss: 2.1952 - val_acc: 0.6867\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9893\n",
      "Epoch 00040: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 0.0460 - acc: 0.9893 - val_loss: 2.2719 - val_acc: 0.6776\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9872\n",
      "Epoch 00041: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0489 - acc: 0.9872 - val_loss: 2.2072 - val_acc: 0.6876\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9940\n",
      "Epoch 00042: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0290 - acc: 0.9940 - val_loss: 2.3067 - val_acc: 0.6851\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9936\n",
      "Epoch 00043: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0298 - acc: 0.9935 - val_loss: 2.2776 - val_acc: 0.6816\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9867\n",
      "Epoch 00044: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0531 - acc: 0.9867 - val_loss: 2.3222 - val_acc: 0.6667\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9913\n",
      "Epoch 00045: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0382 - acc: 0.9912 - val_loss: 2.3699 - val_acc: 0.6818\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9861\n",
      "Epoch 00046: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0556 - acc: 0.9860 - val_loss: 2.4011 - val_acc: 0.6730\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9879\n",
      "Epoch 00047: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0523 - acc: 0.9878 - val_loss: 2.2384 - val_acc: 0.6890\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9897\n",
      "Epoch 00048: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 291s 8ms/sample - loss: 0.0428 - acc: 0.9897 - val_loss: 2.3267 - val_acc: 0.6846\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9937\n",
      "Epoch 00049: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0321 - acc: 0.9936 - val_loss: 2.3319 - val_acc: 0.6841\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9880\n",
      "Epoch 00050: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0482 - acc: 0.9880 - val_loss: 2.4050 - val_acc: 0.6834\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9912\n",
      "Epoch 00051: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 289s 8ms/sample - loss: 0.0380 - acc: 0.9912 - val_loss: 2.4385 - val_acc: 0.6844\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9908\n",
      "Epoch 00052: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0396 - acc: 0.9908 - val_loss: 2.3772 - val_acc: 0.6846\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9941\n",
      "Epoch 00053: val_loss did not improve from 1.30666\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 0.0287 - acc: 0.9940 - val_loss: 2.4391 - val_acc: 0.6769\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 1.4796 - acc: 0.6258\n",
      "Loss: 1.4795852784427155 Accuracy: 0.62575287\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8160 - acc: 0.4939\n",
      "Epoch 00001: val_loss improved from inf to 1.58095, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv_checkpoint/001-1.5810.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 1.8164 - acc: 0.4938 - val_loss: 1.5810 - val_acc: 0.5821\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1116 - acc: 0.6862\n",
      "Epoch 00002: val_loss improved from 1.58095 to 1.17622, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv_checkpoint/002-1.1762.hdf5\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 1.1116 - acc: 0.6862 - val_loss: 1.1762 - val_acc: 0.6967\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8275 - acc: 0.7586\n",
      "Epoch 00003: val_loss improved from 1.17622 to 1.06622, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv_checkpoint/003-1.0662.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.8277 - acc: 0.7585 - val_loss: 1.0662 - val_acc: 0.7130\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.8215\n",
      "Epoch 00004: val_loss did not improve from 1.06622\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.5961 - acc: 0.8215 - val_loss: 1.0944 - val_acc: 0.7256\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8661\n",
      "Epoch 00005: val_loss did not improve from 1.06622\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.4445 - acc: 0.8661 - val_loss: 1.0980 - val_acc: 0.7186\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9026\n",
      "Epoch 00006: val_loss did not improve from 1.06622\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.3204 - acc: 0.9026 - val_loss: 1.1876 - val_acc: 0.7070\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9349\n",
      "Epoch 00007: val_loss improved from 1.06622 to 1.05238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv_checkpoint/007-1.0524.hdf5\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.2272 - acc: 0.9348 - val_loss: 1.0524 - val_acc: 0.7289\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9486\n",
      "Epoch 00008: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.1830 - acc: 0.9485 - val_loss: 1.3184 - val_acc: 0.6988\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9427\n",
      "Epoch 00009: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1946 - acc: 0.9426 - val_loss: 1.1012 - val_acc: 0.7396\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9677\n",
      "Epoch 00010: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.1195 - acc: 0.9677 - val_loss: 1.1515 - val_acc: 0.7340\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9673\n",
      "Epoch 00011: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1181 - acc: 0.9672 - val_loss: 1.0762 - val_acc: 0.7573\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9584\n",
      "Epoch 00012: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.1426 - acc: 0.9583 - val_loss: 1.2978 - val_acc: 0.7328\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9621\n",
      "Epoch 00013: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1316 - acc: 0.9621 - val_loss: 1.1400 - val_acc: 0.7505\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9799\n",
      "Epoch 00014: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0783 - acc: 0.9798 - val_loss: 1.3927 - val_acc: 0.7177\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9701\n",
      "Epoch 00015: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1032 - acc: 0.9701 - val_loss: 1.2923 - val_acc: 0.7275\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9747\n",
      "Epoch 00016: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0919 - acc: 0.9747 - val_loss: 1.4216 - val_acc: 0.7167\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9694\n",
      "Epoch 00017: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.1047 - acc: 0.9694 - val_loss: 1.2275 - val_acc: 0.7477\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9795\n",
      "Epoch 00018: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0714 - acc: 0.9795 - val_loss: 1.3023 - val_acc: 0.7410\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9814\n",
      "Epoch 00019: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0678 - acc: 0.9813 - val_loss: 1.3536 - val_acc: 0.7536\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9714\n",
      "Epoch 00020: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.1056 - acc: 0.9713 - val_loss: 1.4047 - val_acc: 0.7275\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9781\n",
      "Epoch 00021: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0772 - acc: 0.9780 - val_loss: 1.3345 - val_acc: 0.7452\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9794\n",
      "Epoch 00022: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0738 - acc: 0.9794 - val_loss: 1.3385 - val_acc: 0.7412\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9799\n",
      "Epoch 00023: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0705 - acc: 0.9798 - val_loss: 1.3288 - val_acc: 0.7442\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9798\n",
      "Epoch 00024: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0702 - acc: 0.9798 - val_loss: 1.4037 - val_acc: 0.7435\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9849\n",
      "Epoch 00025: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0554 - acc: 0.9849 - val_loss: 1.4751 - val_acc: 0.7433\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9821\n",
      "Epoch 00026: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0663 - acc: 0.9820 - val_loss: 1.3231 - val_acc: 0.7561\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9763\n",
      "Epoch 00027: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0834 - acc: 0.9763 - val_loss: 1.4201 - val_acc: 0.7412\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9849\n",
      "Epoch 00028: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0570 - acc: 0.9848 - val_loss: 1.3941 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9774\n",
      "Epoch 00029: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0813 - acc: 0.9774 - val_loss: 1.3715 - val_acc: 0.7568\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9833\n",
      "Epoch 00030: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0634 - acc: 0.9832 - val_loss: 1.2833 - val_acc: 0.7678\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9814\n",
      "Epoch 00031: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0682 - acc: 0.9814 - val_loss: 1.4749 - val_acc: 0.7510\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9884\n",
      "Epoch 00032: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0440 - acc: 0.9884 - val_loss: 1.3964 - val_acc: 0.7582\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9831\n",
      "Epoch 00033: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0623 - acc: 0.9831 - val_loss: 1.5026 - val_acc: 0.7480\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9838\n",
      "Epoch 00034: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0581 - acc: 0.9838 - val_loss: 1.3176 - val_acc: 0.7734\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9863\n",
      "Epoch 00035: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0507 - acc: 0.9862 - val_loss: 1.5846 - val_acc: 0.7454\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9818\n",
      "Epoch 00036: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0667 - acc: 0.9817 - val_loss: 1.4456 - val_acc: 0.7608\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9827\n",
      "Epoch 00037: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0633 - acc: 0.9827 - val_loss: 1.5667 - val_acc: 0.7426\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9924\n",
      "Epoch 00038: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0319 - acc: 0.9924 - val_loss: 1.4806 - val_acc: 0.7552\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9818\n",
      "Epoch 00039: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0620 - acc: 0.9818 - val_loss: 1.5010 - val_acc: 0.7556\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9904\n",
      "Epoch 00040: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0395 - acc: 0.9903 - val_loss: 1.4365 - val_acc: 0.7708\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9852\n",
      "Epoch 00041: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0556 - acc: 0.9852 - val_loss: 1.5364 - val_acc: 0.7545\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9835\n",
      "Epoch 00042: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0591 - acc: 0.9835 - val_loss: 1.5128 - val_acc: 0.7619\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9856\n",
      "Epoch 00043: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0522 - acc: 0.9856 - val_loss: 1.6026 - val_acc: 0.7524\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9918\n",
      "Epoch 00044: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0330 - acc: 0.9917 - val_loss: 1.6966 - val_acc: 0.7433\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9836\n",
      "Epoch 00045: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0615 - acc: 0.9836 - val_loss: 1.5431 - val_acc: 0.7524\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9915\n",
      "Epoch 00046: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0335 - acc: 0.9915 - val_loss: 1.5218 - val_acc: 0.7575\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9873\n",
      "Epoch 00047: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0489 - acc: 0.9873 - val_loss: 1.5202 - val_acc: 0.7608\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9927\n",
      "Epoch 00048: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0308 - acc: 0.9925 - val_loss: 1.5705 - val_acc: 0.7531\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9804\n",
      "Epoch 00049: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0748 - acc: 0.9803 - val_loss: 1.8012 - val_acc: 0.7272\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9853\n",
      "Epoch 00050: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0544 - acc: 0.9853 - val_loss: 1.5806 - val_acc: 0.7570\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9851\n",
      "Epoch 00051: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0557 - acc: 0.9850 - val_loss: 1.5803 - val_acc: 0.7591\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9901\n",
      "Epoch 00052: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0384 - acc: 0.9901 - val_loss: 1.6172 - val_acc: 0.7524\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9899\n",
      "Epoch 00053: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0401 - acc: 0.9899 - val_loss: 1.7105 - val_acc: 0.7352\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9874\n",
      "Epoch 00054: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 299s 8ms/sample - loss: 0.0475 - acc: 0.9874 - val_loss: 1.6241 - val_acc: 0.7575\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9871\n",
      "Epoch 00055: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0507 - acc: 0.9871 - val_loss: 1.6250 - val_acc: 0.7589\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9903\n",
      "Epoch 00056: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0383 - acc: 0.9902 - val_loss: 1.7273 - val_acc: 0.7468\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9877\n",
      "Epoch 00057: val_loss did not improve from 1.05238\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0508 - acc: 0.9876 - val_loss: 1.6431 - val_acc: 0.7519\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd8VMX2wL+TTiohCS30KhAIXYqAICoWRFHE3p7y1J9PsT3BysNnec9G0adiQeyiWEARFAig9FACSO8lBBJI75ud3x8nm7qbbJJdEsJ8P5/L3b137szsspkzc86cc5TWGoPBYDAYKsOjtjtgMBgMhnMDIzAMBoPB4BRGYBgMBoPBKYzAMBgMBoNTGIFhMBgMBqcwAsNgMBgMTmEEhsFgMBicwggMg8FgMDiFERgGg8FgcAqv2u6AKwkPD9dt2rSp7W4YDAbDOcPGjRuTtNYRzpStVwKjTZs2xMbG1nY3DAaD4ZxBKXXY2bJGJWUwGAwGpzACw2AwGAxOYQSGwWAwGJyiXtkw7JGfn8+xY8fIycmp7a6ck/j5+dGiRQu8vb1ruysGg6GWqfcC49ixYwQFBdGmTRuUUrXdnXMKrTWnT5/m2LFjtG3btra7YzAYapl6r5LKyckhLCzMCItqoJQiLCzMrM4MBgNwHggMwAiLGmC+O4PBYOO8EBgVobUmNzceiyW1trtiMBgMdZrzXmAopcjLS8BiSXNL/SkpKfzvf/+r1rNXXnklKSkpTpefMmUKr7/+erXaMhgMhso47wUGgFJeaG1xS90VCQyLpeI2Fy5cSMOGDd3RLYPB4Ii8PLj+eli/vrZ7UucwAgP3CoxJkyaxf/9+evbsyZNPPsny5csZMmQI11xzDV27dgXg2muvpU+fPnTr1o1Zs2YVPdumTRuSkpI4dOgQXbp04b777qNbt25cdtllZGdnV9juli1bGDBgAD169OC6664jOTkZgBkzZtC1a1d69OjBTTfdBMCKFSvo2bMnPXv2pFevXqSnp7vluzAYzgk2bYLvv4fPPqvtntQ56v222pLs3TuRjIwt5a5brVkAeHj4V7nOwMCedOw4zeH9V199le3bt7Nli7S7fPlyNm3axPbt24u2qn788cc0atSI7Oxs+vXrx/XXX09YWFiZvu/lq6++4oMPPuDGG29k3rx53HbbbQ7bveOOO5g5cybDhg3j+eef51//+hfTpk3j1Vdf5eDBg/j6+hapu15//XXeeecdBg8eTEZGBn5+flX+HgyGeoNtZbFmTe32ow7ithWGUupjpdQppdR2B/efVEptKTy2K6UKlFKNCu8dUkptK7x3FqIJKkC7v5lC+vfvX8qvYcaMGURHRzNgwACOHj3K3r17yz3Ttm1bevbsCUCfPn04dOiQw/pTU1NJSUlh2LBhANx5552sXLkSgB49enDrrbfy+eef4+Ul84XBgwfz2GOPMWPGDFJSUoquGwznJTaBERcHWVm125c6hjtHhk+At4FP7d3UWr8GvAaglBoNPKq1PlOiyHCtdZIrO+RoJZCTcxiLJZnAwJ6ubM4hAQEBRa+XL1/OkiVLWLNmDf7+/lx88cV2/R58fX2LXnt6elaqknLEL7/8wsqVK1mwYAEvvfQS27ZtY9KkSVx11VUsXLiQwYMHs3jxYi644IJq1W8wnPOsWwchIZCaCrGxMHRobfeozuC2FYbWeiVwptKCws3AV+7qS2Uo5YnWBWjt+lVGUFBQhTaB1NRUQkND8ff3Z9euXaxdu7bGbYaEhBAaGsoff/wBwGeffcawYcOwWq0cPXqU4cOH85///IfU1FQyMjLYv38/3bt356mnnqJfv37s2rWrxn0wGM5JzpyBffvg3nvlvVFLlaLWdQ9KKX9gFPBQicsa+E0ppYH3tdaz7D7sMrwKm7QCni6tOSwsjMGDBxMVFcUVV1zBVVddVer+qFGjeO+99+jSpQudO3dmwIABLml3zpw53H///WRlZdGuXTtmz55NQUEBt912G6mpqWitefjhh2nYsCHPPfccMTExeHh40K1bN6644gqX9MFgOOewqaOuugp++skIjDIod8yqiypXqg3ws9Y6qoIy44HbtNajS1yL1FofV0o1Bn4H/lG4YrH3/ARgAkCrVq36HD5cOhfIzp076dKlS4X9zMtLJDf3MAEB3fHw8K2w7PmIM9+hwVAvmDoVpkyBlBR46CFYvBgSEqAeRzxQSm3UWvd1pmxd2FZ7E2XUUVrr44XnU8APQH9HD2utZ2mt+2qt+0ZEOJVlsBxKeRXWVVCt5w0GQz1h3Tro2hWCg2HgQDh1Cg4erO1eCVqLMNu6tda6UKsCQykVAgwDfipxLUApFWR7DVwG2N1p5bp+2ASGe3wxDAbDOYDWopLqXzg/HThQznVFLfXzz/Cvf8Edd0BB7Uxu3bmt9itgDdBZKXVMKfU3pdT9Sqn7SxS7DvhNa51Z4loT4E+lVBywHvhFa73IXf2UvordwggMg+E85uBBSEqCCy+U91FREBDgOoGxaxfcfru0UVW0hpdeAn9/2e774Yeu6VMVcZvRW2t9sxNlPkG235a8dgCIdk+v7GNUUgbDOcCePdCpk/vqtxm8bSsMLy957SqB8eijsGgRBAVBVePLLVsm6rJ334WvvoJnnoEbb4TQUNf0zUnqgg2j1jErDIOhjvPNN9C5M/z+u/vaWL8e/PxkZWFj4ECZ0WdmOn7OGVauFGHRqhW8/z789VfVnn/pJWjWDO66C2bMgORkeOGFmvWpGhiBAcjXoMwKw2Coi+Tnw7PPyuu5c93Xzrp10KcPlExHPHCg2AtiaxBwQmuYPBmaN4dVq8Sg/thjct0Z1qyBmBh44gkRaNHRMGGCrFK2u9W8Ww4jMJAQ56KWqhsrjMDAwCpdNxjqNXPmiDNdmzbw44/uMfjm50vQwf5lNmTa/KJqopb65RdYvRqefx5atJDzb7/Br7869/xLL0FYGPz978XXXnxRBM8jjzgveFyAERiFuDNircFgqCY5OeIbceGF8N//isH4zz9d3862bdKWzeBtIzwcOnasvsCwWsXe0L493HOPXPu//5M6H39cBFVFbNkiAmfiRDHAl+zX1Kli2/jhh+r1rRoYgVGILTyIq5k0aRLvvPNO0XtbkqOMjAwuueQSevfuTffu3fnpp58qqKU0WmuefPJJoqKi6N69O9988w0AJ06cYOjQofTs2ZOoqCj++OMPCgoKuOuuu4rKvvXWWy7/jAaD23j/fTh6VGbZV1whKpnvv3d9O2UN3iUZOFAERnVm8t98I34TU6cWq7p8fOD112XX1PvvV/z8yy/LSuKhh8rfu/9+sbc8/jhUM7ZcVXGrp/fZpm/fvjq2jK6xlJfyxIkise1QYM0GbcXTM8DufYf07AnTHIc337x5MxMnTmTFihUAdO3alcWLF9OsWTOysrIIDg4mKSmJAQMGsHfvXpRSBAYGkpGRUa4u2/V58+bx3nvvsWjRIpKSkujXrx/r1q3jyy+/JCcnh2eeeYaCggKysrLYs2cPkyZN4vdCY2FKSkqVkzIZT29DrZCRITPzqChYulSuXXutqI4OH3at9/Xdd8tM/uTJ8vW+9x488ICoxdq3d77O/Hzo0kVWBps3g0eJ+bnWMHKkjEf79tnf7bRrlzgRTpokgsMeMTEwYoQIpOeec75vJTjXPL3rCO5x/e/VqxenTp0iPj6euLg4QkNDadmyJVprnn76aXr06MHIkSM5fvw4J0+edKrOP//8k5tvvhlPT0+aNGnCsGHD2LBhA/369WP27NlMmTKFbdu2ERQURLt27Thw4AD/+Mc/WLRoEcHBwW75nAaDy5kxQzytX3qp+NrYsbLi2LjRtW3ZHPbsCaHqOvB9/DHs3y/99ygz1CoFb74pu52mTrX//Kuvyorq0UcdtzF8uGQHnDbtrIRir/Xgg2eVClYC+TlHyc9PJCiot8ubHTduHN999x0JCQmMHz8egC+++ILExEQ2btyIt7c3bdq0sRvWvCoMHTqUlStX8ssvv3DXXXfx2GOPcccddxAXF8fixYt57733mDt3Lh9//LErPpbB4D6Sk8VmMXp0seEZ4OqrxT/i+++hr1OT4spJS4OdO6EwA2U5oqIgMFAERgVJy0qRnS2CYNAgCWRoj+hoiYr79tuySggKklVJfr6EVv/8c1FFVRbyaNo0sFjEqc/daK3rzdGnTx9dlh07dpS7Zo+cnHidlrZBW60FTpWvCtu3b9cDBw7UHTt21PHx8VprradNm6YfeughrbXWy5Yt04A+ePCg1lrrgIAAu/XYrs+bN09fdtll2mKx6FOnTulWrVrpEydO6EOHDmmLxaK11nrmzJn6kUce0YmJiTo1NVVrrfW2bdt0dHR0lfvv7HdoMLiMp5/WGrSOiyt/b+RIrTt10tpqdU1bS5dKW4sWOS4zYoTWvXs7X+fLL0udy5dXXC4hQevgYClb9vDz0/roUefbrCZArHZyjD2/VhgVUOy8V4BSrtXUdevWjfT0dCIjI2nWrBkAt956K6NHj6Z79+707du3SgmLrrvuOtasWUN0dDRKKf773//StGlT5syZw2uvvYa3tzeBgYF8+umnHD9+nLvvvhur1QrAK6+84tLPZjC4nJMnZdZ8883Qo0f5+2PHwoMPyqqga9eat7dunZz79XNcZuBAURFlZpberVSW/HzZFfXaa7I6Ksx66ZAmTcSXYudOMYr7+MjZ21vuNW9e9c/jTpyVLOfCUZMVRl7eaZ2WtkFbLFlOlT+fMCsMw1nl4Ye19vTUes8e+/fj47VWSusXX3RNe9deq3XHjhWX+fnnylcMR45oPWiQlLv/fq2zs13TPzdDFVYYxuhdiIknZTDUAfbvl3hJ99wjvgr2aNZMZvyu2l5bMkKtIypz4Fu4UHZMbt0qsZ7efVcM1vUMo5LSGhITUb4KPEw8KUM9JDUVxo2Dt96Cbt1quzcV88wzoo75178qLjd2rITKOHgQ2ratfnvHjkF8fHmHvbKEhUngw++/ly2wHh5yKCVCYvp0MWLPneveAIm1jBEYAMeO4REWCo2groQHMRhcxqpVErTv7bdl5ltXWb9eHN2ef15WERVx3XUiMH74QeIy1aRNqHyFAXD55TBzJmzYUP7ehAlid2nQoPp9OQcwKimlwNcX8sRF36ikDPWOuDg5z50LeXkVl925Ey66CA4dcnu3SqE1/POf0LixCILKaNdOVEA1UUulpsIrr4gRu2fPystPmwYnTsDx4+ILcuSIfE8nTojHdj0XFmAEhuDjU0JgmBWGoZ6xZYtMjM6ckaB3FfHmm7IiefLJs9M3G7/8AitWSArSoCDnnrnuOgnql5BQ9fbS0yXUyJYtYnPw9a38GQ8PaNpUdi61aAEtW0Lr1nLtPMEIDABfX1ReHmACEBrqIXFxMjiGhcGXXzoul5oq98PC4LvvJIfD2cBigaeeEt3/vfc6/9zYsbIyqWrwvcxMcaazqcBGj67a8+cxRmCArDAKCvDQrg9AmJKSwv+qml2rkCuvvJKUlBSX9sdwnpGVBXv3ilf0jTfCTz9JjCZ7fPGFlP/+e5k9T5zoulDie/bABx/Yb/uTT2DHDvFzKJmLojK6dZPj4YfhlluK/SkqIitLBMSqVSIcx451vj2DERiACAzAw+Lh8hVGRQLDYqm4rYULF1Y5UKDBUIrt2yXEdnS0DKpZWSI0yqK1BNnr0weGDoX//EcC5n36ac37oLVkipswQVQ4U6dK6A+Q2f7zz0sIjWuvrVq9SklOiYceEpXWgAFyfPWV/bDhOTmixlq+XD7XjTfW9JOdfzjrsFHVA/gYOAVsd3D/YiAV2FJ4PF/i3ihgN7APmORsm9V23EtP13rDBp2T8JfOyHCtk9r48eO1n5+fjo6O1k888YSOiYnRF110kR49erTuWOgsNGbMGN27d2/dtWtX/f777xc927p1a52YmKgPHjyoL7jgAn3vvffqrl276ksvvVRnZZV3MJw/f77u37+/7tmzp77kkkt0QkJC4cdL13fddZeOiorS3bt31999953WWutff/1V9+rVS/fo0UOPGDHC4WcwjnvnMLNmiSPZvn1aFxRo3aqV1ldcUb7cqlVS7oMP5L3VqvWAAVo3bap1WlrN+rBkidT96KNajx4tr4OCtJ40SevHH5f3q1bVrI20NK1nzBAHPNA6JETrtm217tpVQnoMHqx1ly5yb/bsmrVVz6AKjntuC2+ulBoKZACfaq2j7Ny/GHhCa311meuewB7gUuAYsAG4WWu9o7I2Kwtv7jC6ubZCRibaxwOrl8bT0/nMdpVEN+fQoUNcffXVbC9Mpbh8+XKuuuoqtm/fTtvC/eNnzpyhUaNGZGdn069fP1asWEFYWBht2rQhNjaWjIwMOnToQGxsLD179uTGG2/kmmuu4bYygdCSk5Np2LAhSik+/PBDdu7cyRtvvMFTTz1Fbm4u0wo7mpycjMVioXfv3qxcuZK2bdsW9cEeJrz5OcxDD0nGutRUMdpOnixhK06cKB3U7o47JJtdfLwE2gNR8QwYAE8/XTpibFUZNkwc8vbvF+NyXJyE6/72W1l9jB0L8+bV7HPasFold/b8+bJ6ycmRQIA5ObJD7L774PbbXdNWPaEq4c3d5oehtV6plGpTjUf7A/u01gcAlFJfA2OASgVGtbGFNNZF/7iV/v37FwkLgBkzZvBDoeHu6NGj7N27l7CwsFLPtG3blp6FW//69OnDITvbHo8dO8b48eM5ceIEeXl5RW0sWbKEr7/+uqhcaGgoCxYsYOjQoUVlHAkLQx3FYhEVz333FYfftkdcnMRjsoXXvuUWsRXMnSuZ30B2T82dC3/7W7GwAHFmu+02eOMNaadNm9LtL1gAf/0l+Rq8HAwlK1aI8XzGjOKdSNHRYmyeOhU++0wSAbkKDw+48ko5DC6nth33Biql4oB4ZLXxFxAJHC1R5hjg0A1TKTUBmADQqlWrChtzvBJQsO0QBX6KrKY5BAb2QbkyOUsZAkoEL1u+fDlLlixhzZo1+Pv7c/HFF9sNc+5bYtufp6cn2XYybP3jH//gscce45prrmH58uVMmTLFLf031AHmzoXZs8Uo7UhgaC1eyLfeWnyte3cJ1/3ll8UCY84cyM0tnTPaxiuvyOz/qadkkE9IgA8/FL+DY8ekTEaGCCF7vPiibDu1t/upc2f497+d/8yGWqc2jd6bgNZa62hgJvBjdSrRWs/SWvfVWveNqCxufEX4+qLyrYV1um6nVFBQEOnp6Q7vp6amEhoair+/P7t27WLt2rXVbis1NZXIyEgA5syZU3T90ksvLZUmNjk5mQEDBrBy5UoOHjwIiFrMcI6gteSKAFiyxHHq0EOHJNdDWae0W28V/4WDB4uN3YMG2Y8M26KFONTNnSu5KFq1ksxuXbqICuu++8RA/vPP5Z9dtUoy5f3zn+eFU9v5QK0JDK11mtY6o/D1QsBbKRUOHAdalijaovCae/HxQeXZBIXrdkqFhYUxePBgoqKieNKOM9SoUaOwWCx06dKFSZMmMaBkspgqMmXKFMaNG0efPn0IDw8vuv7ss8+SnJxMVFQU0dHRxMTEEBERwaxZsxg7dizR0dFFiZ3Oa5YsEUOXm+x6TvHqqxLIriJ++01UTUOGiM1h1y775Wwe3tHRpa/ffLOcv/pKdgzt2VOxWujJJ2V3059/yqpk927pw5gxomrq2VNsIGXVpC++KHYSeysXw7mJs9bx6hxAGxzvkmpKcU7x/sARJE+qF3AAaAv4AHFAN2faq0l4c338uNYbNui0lA3aYslw7pnzhPNil9SJE1qHhWmHSXvOBsnJWnt4aB0aqvXJk47LjRihdfPmWu/aJf2dMcN+uRdekDDgGXZ+zxddJDuIbrxR2rOz664UqamOy+zbJ0mA+vXTOjdXrq1dK337z38qrtdQ61AXwpsrpb4C1gCdlVLHlFJ/U0rdr5SyTWVuALYX2jBmADcV9t8CPAQsBnYCc7XYNtxLoS+GyjfhQc47tBYDckaGbIBwVdjsqrJypezySU52HE8pNhaWLZM8z507S0ylpUvtl42LkxDh9hL+3HKLOMt9+634SFSmMgoOdlymfXuxp2zYUNzvF18Uj/EHH6y4XsO5hbOS5Vw4arTCSEvTesMGnXlig87LS3LumfOEer/C+OgjmQ2/9ZbWQ4ZoHRVVO/2YOFHScj75pPRnyZLyZcaNEx+DwrS7esIEmd3n55cv27atlLdHYqLWXl7Szq5drun/o49Kfbb0qv/+t2vqNbgV6sIK45zD5u2dbyLWnlccOiR2i4svlhAT118v3tF79pz9vixbBoMHSy6IDh3ErlByx9y+fbJj6YEHZMYPMHKkGLbL+B+RliZG7bL2Cxvh4XDTTWKH6NzZNf1/9VXx23j5ZWjYUHxADPUKIzBseHujMSqp8wqrVdQxIPGMPDyKYwudbbVUYqJsgR0xQlQ/774rAuLll4vLvPGG+Ds8/HDxteHDRY22ZEnp+rZulbMjgQHiA/FjtTYn2sfHR3ZTtW4NL7wAISGuq9tQJzACw4aHB8rHBw+LMiuM6rBuHYwfbz+GT11l+nRxLJs+XQY5kKB7/ftX7nn8++/iQ2CLiVRTVqyQ8/Dhch45UpzmXn1VbA0nT4qd4M47SycXCg+HXr3KCwxHO6TcTcuWcOCArNoM9Q4jMEri41OokjIrjCrz0ksyu7RlMKvr7NghYTKuuaZ4lWFj7FhR8Rw+bP/Z7GzZRvrcc2J0fvVVCepXE5YtEy/rviUiNLz5puSG+PvfZftqXh48/nj5Z0eOFL+KzMzia3Fxkkq0RYua9as6eJhhpb5i/mdL4uODstS+wAgMdD6WVZ0gIaHYdyAmxnX1WiziGDZ+vKiPXEV+vgz4QUEwa1ZxaBgb118vZ0d5Ft5/Xz7ze+9JdrrJk2Wn0P/+V3lGO0fExEiU2JLhvSMiJO7Tn3+KULr2Wvv2hpEj5TP98Ufxtbg48Y9wY8QCw/mHERgl8fVF5WujkrJhsYhjWGWD9WefSYiKpk1dJzCsVolt9OGHsnJ5803X1Avw+uuwcaPYCZo0KX+/QwfxeranlsrMlHAZI0bIzH/BAhnQO3YUp7YuXSTvg53wLg6xOd/Z1FEluftuESRWq4TnsMdFF0mcJptaqqAAtm07++ooQ73HCIyS+PigAOVCPfykSZNKheWYMmUKr7/+OhkZGVxyySX07t2b7t2785O9HAVluPbaa+nTpw/dunVj1qxZRdcXLVpE7969iY6O5pJLLgEgIyODu+++m+7du9OjRw/mVScaaHKyDGaOEu6A+DDMni3xjG66SVQjublVb6tsnQ8+KDkLpk6VmfUzzxQbcmvCzp2SBvSGG+RwxNixEtqibPrPd9+FU6dkJ5ONwYPFBrFwoaiBJkyQQH2vvOKcjWP5cjmPGFH+nlIiMH/8UYIB2qNBA+mDTWDs2ydqMyMwDC7GbeHNa4NKw5svmsiWBHvxzQuxWCA7G6ufwsPbObVQz6Y9mTbKcXzzzZs3M3HiRFYUGjW7du3K4sWLadasGVlZWQQHB5OUlMSAAQPYu3cvSikCAwPJsDNI2wuDbrVa7YYptxfSPDQ01KnPVMThw5CYyM4zZ+gyZIh9xy1bCOxZs2S2PmaMDJ5Dh1atLRtaw2OPSaTIyZPFNpKUJEHzIiLEOczPr3p1FxRIOI3du8WGYW91YWP7dmnz3XeLw2ZkZIjNolcvWLzYcf9jYiTW0+LFYpe47z6Jp+Qo9/O998pqJikJPD2r99leflmE6smTIoDGj4dNm6SvBkMFVCW8uVlhlMRmrLO6Toj26tWLU6dOER8fT1xcHKGhobRs2RKtNU8//TQ9evRg5MiRHD9+nJMnT1ZY14wZM4iOjmbAgAFFYdDXrl1rN0z5kiVL+D9bNFKourAAMeR6eYk65IMP7JeZPVsEyfjxMhgrVTO11HPPibB45BERFkqJoPj4YxnEn3mm+nXPnAlr1ogBuSJhAZL6s1On0mqpd96R7a8lVxdlUUpWCosWSfIVW7ylyy6TCYk9li0TP5DqCgsQOwbId79li/y/de1a/foMBns46+F3Lhw18vTWWjKSbdigcw5u0FarxfnnKuG5557T06dP15MnT9bTp0/XWms9e/ZsfeONN+q8vDyttWTXO3jwoNZa64CAgHJ1xMTE6MGDB+vMzEyttdbDhg3TMTExev78+fqWW24pV7537956z5491e+01ar1xo1aHzmidyxbJrGLsrNLl8nMFC/j228v2bDWw4ZVr82XXhIP4QkTpP2yPPCA3F+6tOp179undYMGWl99tf267TFpktaenlonJUkkgLAw+9nqKmPuXF0qm11JDh7UFcaDchaLRTzA771X6yuvrD1vdcM5B8bTu5p4eKC9PAqd91xn+B4/fjxff/013333HePGjQMkFHnjxo3x9vYmJiaGw462cBbiKAy6ozDl9kKaV4mcHFlZNGggXrvx8bKaKMkPP4hH8d13F18bPlxm8XbydVTItGmyerj9dlED2dvd8/rrMuu/886q+T9YraL28faWnU3O7hy6/npRYy1YIKuT06fF/lFVbrhBwoc/91x5e5BtNWbP4F0VPD1lZfP777JDytgvDG7ACIwyaB9vl/tidOvWjfT0dCIjI2lW6HR16623EhsbS/fu3fn000+54IILKqzDURh0R2HK7YU0rxI2v4KAALEZDBwoWztLbhudPVuMu8OGFV8bPlzKVCWvx6xZEkzvhhtE9eRoH7+/P3z+uRii//53sZV8950ImKlT4R//gOefFwPx0aPFYcpnzRK9/htvQGG+EKfo00cc+j75RITV1VeLU19VUUraTkgozmNhIyZGVG7dulW93rKMHCl2p+PHjcAwuAdnlyLnwlFjlZTWumDvLm2J26Dz81Or9Fy948gRrWNjtbZa5TtcuFBUJx9+KPcPHZLQ2VOmlH4uJUVCdD/3nHPtfPaZ1HPVVcWhsStj6lTpS9kjJETatr2PiND68su1DgzU+pJLnFdFlcQWUA9ERVcTxo8XtdixY/LeatU6MlJCjLuC3buL+7p4sWvqNNR7MCqpGlDk7V0HfDGysmov1EZWlszobeqbUaPEC/nll8WIR7VHAAAgAElEQVR4O2eODE133ln6uZAQmZk7s6KZN0+eHz5cVgqFASAr5ZlnJKT30qWy1TY+XlY1KSmQni5be99+W1YECQnQqJEY7avjxGaLLTVmDPTuXfXnS/LKK6LievZZeb93r6wG7G2nrQ4dO0poDjArDINbqO2c3nUPH1+UBvJzwbvS0u5Da9n+2bAhFO6AOqttZ2XJQGtDKdHBjxkDX3whappLLhGVVFmGD4e33ioWOvZYuFAyvw0YAD/9VLWtsh4ejgdZf39RnznKc11VBg2Sgf6mm2peV9u2EjjwjTdkF9i6dXLdVQJDKRg9WrbzVrYLzGCoBufFCkPbdNlOoHwLB67qhnhwFbm5MhutyGmuulTmuW1r29+/9Hc3erTMXB9+WEJnlzR2l2T4cFkZrV5t//7KlTJz79FDBEddDoXi4QGTJtkXjNXh6afFue/xx2U7bWSkeJa7ijfeOHfieRnOOeq9wPDz8+P06dPOCw2bwMitZYFh22WUm+tatVRuLmzeLLubHFFo8NYNGnD69Gn8bLN/pUSdkpYm+Riuu87+84MHy64de2qptDSJwtq6tcyEz7cQ2KGhEvp72TLZZTZihGvjPfn5lV4ZGgwupN6rpFq0aMGxY8dITEx07gGrFZKSKMhLw/NMLYbqTkmB1FR5vXWrY9VOVcnIkO2hWVkSGtseyckysB85gl+DBrQoGfF07FgRCBdd5LhPQUHQr599gfHUU6K3X71aUniej9x/v2zT3bfPdeoog+Fs4Kx1vKoH8DFwCtju4P6twFZgG7AaiC5x71Dh9S1UwYJvb5dUdcgP8NBJN7VzSV3V5pprtG7XTmsfH0nZ6Sruu0920TRqZD+tp9aysyg6umbtTJ4sKUDT04uvLV0qbT/+eM3qrg8sXCjOkPHxtd0Tw3lOVcZYd6qkPgFGVXD/IDBMa90deBGYVeb+cK11T+1kjBNXkt/cD6/41LPdbGni4mSW3revY1tAdVizRnwrzpyRKKtl0VpiENV0R9Dw4bKbatUqeZ+ZKc5zHTqIz8T5zhVXyEqrZDIkg6GO4zaBobVeCZyp4P5qrbXNXXctUAuZXuyT3zwI7/jMygu6i5QUccDq2VN2+8TG1jwCLIia6a+/JBKsr6/sTirL8eMSL6mmAmPQIPGstqmlnnlGDOUffeQ69ZrBYDir1BWj99+AX0u818BvSqmNSqkJZ7szlshQfE64YICuLiXzMQ8aVGyorinr18sK4tJLxSv4xx+LvaFt2NqpaZTTgADxio6JkVXGjBmSL6K6UWwNBkOtU+sCQyk1HBEYJbPDXKS17g1cAfyfUsrhKKOUmqCUilVKxTpt2K4Ea8sIvDK1zPRrg5L5mG3+BGvW1LzeNWtkR07//pJj4tCh8jkmNm2SMq5w/Bo+XBIV3XUXtGol/gwGg+GcpVYFhlKqB/AhMEZrfdp2XWt9vPB8CvgBcBjAR2s9S2vdV2vdNyIiwiX9srZsLueD+1xSX5XZskV2MDVrJkfbtq6xY6xdKyGvQ0LEp0Kp8mqpTZskDagrfCOGDxd/jn37xNM6KKjmdRoMhlqj1gSGUqoV8D1wu9Z6T4nrAUqpINtr4DJg+1ntXOtWABQc2HFWmy2ibD7mQYNEYFTBAbEcWovAsK1YmjSR1z/+WLrc5s01t1/YGDhQhNN994kazGAwnNO4TWAopb4C1gCdlVLHlFJ/U0rdr5QqTF/G80AY8D+l1BallC1VXhPgT6VUHLAe+EVrvchd/bTb9zbtALAe3FNxQXdgsUiioJIqoUGDJF7SkSPVr3fPHtkZVRjlFhC11ObNxfUmJkqUV1dlaWvQQOIlvfuua+ozGAy1itsc97TWN1dy/17gXjvXDwC1GjnNs1kbCnyAwwfOfuN79oiRu6zAAFlltG5dvXptNpCSMZbGjJHUoT/9JKHBbQZvV60wQEJ3GwyGekGtG73rIl7eYeQ2AQ7XYEZfXbYU5hwvKTCiomTXUU3sGGvXSiDDknk3OnWCLl2K7RibNsnZ5IE2GAx2MALDDt7ejchpAupovOsrf+ABSfLjiLg4CfNdcmD38oILL6yZwFizRuoom5zo2msluVBysqww2rSReEcGg8FQBiMw7ODlJQLD86hrtukWsWKFpAh94w3HUWjj4mQnU9ncEIMGyb3MajgUpqeLXaSk/cLGmDGyk+mXX1zj4W0wGOotRmDYwcsrhIxO4Hk6wzX+DyBBDZ98UlRLWVkSqdQejvIxDxokA/uGDVVve/16ad9ejoh+/WTr7qefyvZXIzAMBoMDjMCwg1IeJI0KpSDYF9580zWVfvutDPYzZ4ra57PPypc5eVIyxNkTGLbVQXXUUrb82hdeWP6eh4esMn7/Xd4bgWEwGBxgBIYDPEPCSB7XHr7/XmIg1YTcXJg8WRIG3XGH5INYulS2ypbE5uHds2f5OkJDRVVlT2AcOiQhx7/91n77a9aIcbthQ/v3x4wpfm0M3gaDwQFGYDjAyyuUxPGNZQY+fXrNKnv3XRE6r70miYVuv11URF9+WbpcyZAg9hg0SAb/khnzjh4Vj+rVq2HCBDhxovQzZR327DF8uHhhN2sGTZtW/fMZDIbzAiMwHODl1YisRlmSy/mjj6ofVyolBV58UTydL7tMrnXqJPGcyqql4uKgRQvHGdMGDhTnuz2FDoXHj8tgn5wMX30FOTkSibakR/jevZIwqSKB4esLTzwhHtkGg8HgACMwHODt3QiL5Qw8+qjsaPrww+pV9MorMqD/97+lr99+uwT+Kxn8z5HB24bNgW/NGrF1XHIJnDolqU5vugn+9S8J9fHdd8XP2OwXFQkMkK2+//qX85/LYDCcdxiB4QAvr0bk558RI/DFF4taqqq5tY8ckeduv728XeKmm8S/wrbKyMmBnTvt2y9sdOokq4/580VYHDsGCxcWG7Mfewz69IGHHpJVBYhwCQ4WG4bBYDDUACMwHODj0wSLJZmCgiwZiI8dKz1zd4bnnpPziy+WvxceLlnXvvxStsvu2CHnilYYHh7FAQMPHoSff5bc2ja8vODjj0VtNXGiXHPksGcwGAxVxIwiDggI6AposrJ2wlVXyez+zTfLR4zVGr7+Gu65R7ymhwyR3UxNmohvwyOPSC4Ie9x+u+yUWrascoO3jSuvBD8/Cedx8cXl7/foAU8/DZ9/DnPnwrZtlaujDAaDwQncFnzwXCcgoDsAGRnbCArqI7aMBx6QPNhDhkih9etFIKxdK0H2mjaFsDARGGFhIihsM317jB4t4b8/+0xUTQEB0L59xR174AG4804p64hnnoF582QLr9Vq38PbYDAYqogRGA5o0KA9SvmSmVmYiuOOO2QgfuMNaNdO/Co++0yExMcfyyBeVbWPnx+MGyc7nLp0ge7dZdttRShVsbAACSvy8cfFKwsjMAwGgwswKikHKOVJQEDXYoHh7y+z+/nzJSPd3LkiNPbsgbvvrr6N4PbbJT5UbKxr0qLa6N9fdj1dd50JJmgwGFyCERgVEBAQVSwwQHYfNW4Ml18uO5pefrnmaUcvuqg4x4UrBQbAs8+Kp7rBYDC4ACMwKiAgoDt5ecfJz0+WC02biif1vHmSZ9sVeHhIqBBwvcAwGAwGF2JsGBUQEBAFQGbmdho2LDR02/Jsu5LHHpM4T/aCAxoMBkMdwawwKqCkwHArjRpJaI7KDN4Gg8FQi7hVYCilPlZKnVJK2R1xlTBDKbVPKbVVKdW7xL07lVJ7C4873dlPR/j6tsDTM8T9AsNgMBjOAdy9wvgEGFXB/SuAjoXHBOBdAKVUI+AF4EKgP/CCUuqsb/VRShUavred7aYNBoOhzuFWgaG1XgmcqaDIGOBTLawFGiqlmgGXA79rrc9orZOB36lY8LgN204pXdbD22AwGM4zatuGEQkcLfH+WOE1R9fPOgEBUVgsyeTlnai8sMFQD8nMlJBk27ZJuLPKyM0tnbKlpmgtwZnj4pzLMpCSAn/9JRsaHcULzcuTvGOrVsGKFZI12ZVYrRJPNDdX2srPB4ulZt+L1lKnM/8H7uKc3yWllJqAqLNo5ShmUw0IDJQQIZmZ2/H1be7y+g3yh5CfL39YgYFnt93TpyWtSHy8nNPSJLhvSEjx0bAhNG/u2OVGa3l++3bYv1/Kdu4sAQF8fcuXT0uDw4clMr2vLzRoIH6h/v7y+tQpSWOyb1/x+fRpyW8VGSkpU1q0kNfe3jIQFRTI2WKRQSo9vfjIyJABMTRUdobbjiZNJGiAbUDLy5MjLU0G540bYdMm2LWrOIRaYKCkgb/wQgkg0LYt7N4tn33bNjnv2yf7N2z9tR2NG8vnK3n4+ckgmJlZ+khMlAH90CH5rnJyir+/5s2hWzeJwNOtm/R9xw5xjdq5s3wOsdBQaTsiQr6L48el/pJ4e4uv68UXy2HLJBAfX/pIS5ONkkrJjnilRAicPFlc5vhxEXAWi/3fi7+//JZKHj4+xfXZzgUFIvySk+WckiL/PyXrCA6WIzJSwsu5G+WMqkUp9QgwG0gHPgR6AZO01r858Wwb4GetdZSde+8Dy7XWXxW+3w1cbDu01n+3V84Rffv21bGxsZV+nqqQl5fE6tURtG//Oi1bPu7SuusqVquEzPr8c1i+HDp2lKjptiMyUn7QWsuPOCFBjoyM4kGicePiTV9aS6T32FhJax4bK4NKVhZkZ8vZNvNq1gz69i19BAfDgQMyGNuOI0fkOdsMLjdXDtssrqCg+GzvJ661BPXNzXX+e2nYUMKD2Q6tiwdJezNfDw8ZUDt1kkDChw/LkZrqfJvh4fL9h4XJd2wbjKqiIQ0IkME5JcXxIGaPyEiJ7t+nj2TuTUuDdeskdNqWLaXr8vCADh0kuk3XrnLv+PHSR3q6c+0qJZ+3TZvSR0SEBGnesUNWEDt3Fq8MgoIkuk7XrnJu2VI+b2KiCOBTp+R1QEBpIRYZKb+RP/6Q33psrLy3/b6dpWFDqat5czkiI0W4al36KCiQv5P0dPk+bUI9P1/uW63FZ09PqbdhQxF6DRvKBCY/v/hZ29nPr+rBtIu/b7VRa93XmbLOrjDu0VpPV0pdDoQCtwOfAZUKjEqYDzyklPoaMXCnaq1PKKUWAy+XMHRfBkyuYVvVwscnHB+fpnV6p1RiIsyaJQOgp6f88Xp6Fv/guncXn8DGjSuuZ+dOERJffCEDW0CAJPQ7dAgWLSoe1Bs3lh9oQkLxjKcsnp7yh9O0qTxvm9F5e0tA3SFD5A/KNrtu0ECe2blT/mh//tnxH2xQkAwggYEyMwsJkZm6r6/U7+FR/B3YXpdFa9nNbPsjtw0eISHyR5iaWnwkJ8vM8ciR4mPVKqkjKkpSm0RFyffcvr2U3bNHZt62s9UqDv1Dh4qwad1aZvj5+aUFZ1aWDJYdO0pd9tKw5+fLLDo+XgZmLy85PD3l7ONTPHMNCCj+/FarfJaEBJkRnzgh7fr4yOHtLWd/f/k8TZqUb9vmY5qdDZs3y++kc2cZpBs0cPDDKiQvT54reeTkyG8pIKD48PNzzt3JapX/Cy+v4klMdbnqKjmnp0u249Wr5fPYBIDtCAkpLwRAvrvzAWdXGFu11j2UUtORmf4PSqnNWutelTz3FbJaCAdOIjufvAG01u8ppRTwNmLQzgLu1lrHFj57D/B0YVUvaa1nV9ZPd6wwAOLiLsViSaFPnw0ur7smnDkDr78OM2bIQOPvXzyrts2wS/73Nm0qg3XXrjKzTkoqPhITZSDx8JBMsrfdJtHabXEOs7JKqyms1tLqjaZNpWxCgqQOOXasWNXTvLmoMfr1k/b9/Cr/bOnpMovdsEFUFO3bFx9hYe7xnzQYzkeqssJwVmDMRozObYFowBMRHH1q0lFX4y6BsW/fY8THv8eQIRkodXb2CZw8KWkyrNbiGWnz5jKbSkmBt96SIyMDxo+HF16ACy4oX09SkmSBjYsrPu/aJYN7eHjpIypK6mra9Kx8RIPBUAdwh0rqb0BP4IDWOqvQT+Lu6nbwXCMgIAqrNZvs7AP4+3dwSxt5ebIMXrRIUnRv2VK+jKenLL1tqpKxYyUgbVQ561Ax4eEwYoQcBoPBUBOcFRgDgS1a60yl1G1Ab2C6+7pVtygZIsTVAsNiEbXSyy+LGsbLS3ZovPyyqIYCAoqNpUeOyFkpyefUq0KFoMFgMLgWZwXGu0C0UioaeBzZKfUpMMxdHatL+Pt3BURgRERc67J6t26VzK4bN8KYMZJWY/hw2RVUEnuqJoPBYDjbOKuQt2gxdowB3tZavwPUMBHEuYOXVyB+fu1cFiIkLw+mTJHtikePwrffwo8/itAoKywMBoOhruDsCiNdKTUZ2U47RInl9zzZSCaUS6ZUTTZuhLvukr37t90G06bJrh+DwWCo6zi7whgP5CL+GAlAC+A1t/WqDhIQEEV29h6s1ip4epVAa9nVNHCgbIddsEBSghthYTAYzhWcEhiFQuILIEQpdTWQo7X+1K09q2MEBEShtYWsrD1VfvbMGVE3PfaYOAht2wZXX+2GThoMBoMbcUpgKKVuBNYD44AbgXVKqRvc2bG6RkCALaZU1ewYq1dDz56yXXb6dEmx3aiRO3poMBgM7sVZG8YzQD+t9SkApVQEsASoZvSScw9//04o5eW0HUNr+O9/4ZlnxOlu9WqJi2QwGAznKs4KDA+bsCjkNLUfGv2s4uHhQ4MGnZ0SGPn5cN99MGcOjBsHH3wgMWgMBoPhXMZZgbGoMCCgLVrseGChe7pUdwkM7E5a2roKy2Rmwo03wsKFMHUqPPusiXtkMBjqB04JDK31k0qp64HBhZdmaa1/cF+36iYBAVGcOvU1Fks6Xl7l3VBOnxZj9vr18P77MGFCLXTSYDAY3ITTCZS01vOAeW7sS53HFiIkK2sHwcEXlrp39Chcfrnkbfj2W4nzZDAYDPWJCgWGUiodsBfOVgFaa31e+SXbBEZGxtZSAmPnTon7lJYmu6EuvriWOmgwGAxupEKBobU+b8J/OIOfX1u8vBqSnh4L3AeIGuqKK8TQvWKFbKE1GAyG+sg5n9P7bKKUB0FB/UhPXw9IkqJbb5XMZX/+aYSFwWCo35xXW2NdQXDwhWRkbKOgIIupUyV3xcyZkk3OYDAY6jNGYFSRoKD+QAHffnuQqVMlJPl999V2rwwGg8H9GIFRRYKD+xMf35a//709vXrBO+8YPwuDwXB+4FaBoZQapZTarZTap5SaZOf+W0qpLYXHHqVUSol7BSXuzXdnP6uCxdKEKVN+BizMmwcNGtR2jwznGtn52fxx+A8+i/uM+PR4l9SptWbRvkX0/6A/rae15qNNH1FgLXBJ3c4Snx7Pe7HvMfabsTy++HHWHluLpNEx1BeUu/5DlVKewB7gUuAYsAG4WWu9w0H5fwC9tNb3FL7P0FoHVqXNvn376tjY2Jp1vAK0FhXUp59aeeONe3j00U/c1pYrOJN9hvTcdFo3bF3bXTmnOZ11mtAGoXioqs+vrNrKweSDbIjfwOqjq1lzbA1bErZgsVoAUCiGth7K+G7juaHrDUQERJR69njacfae2UtqTio9m/akTcM2qDJL2hWHVvBszLP8eeRP2jRsQ+OAxqw/vp6eTXsy7fJpDGszrFyfYuNjmb97Pp7Kk1u630Ln8M6Vfo5cSy4FugCrtlJglXN8ejzzd8/np90/sSF+AwCtQlqRkJFAXkEeLYNbckPXGxjXdRy9m/XmQPIBdp/eze6k3ew5vYeDKQdp3bA1vZv2pnez3kQ3jSbQp0p/9kUUWAs4lXmK+PR4EjISOJN9huScZJKzk+Wck0yEfwQXhF9QdIT7hztdf0pOCptPbCYzP5PMvMyic15BHpHBkbQPbU/7Ru1p1KBq0UW3ndzGgeQD5BbkkmvJLTr7evkysMVAukZ0Lfd/7kqUUhu11k5FunOnwBgITNFaX174fjKA1voVB+VXAy9orX8vfF/nBMbXX8PNN8PEiasZM2YwgwadxMensdvaqy5HU4/yxpo3mLVxFtmWbK7vcj3PD3ueHk162C2flpvGj7t+JNw/nCs7XunSvhxPO45G0zyoud0BNzEzkeWHlhNzKIZNJzYxutNoHhnwSLUHDVeQlJVEzMEYlh5cytKDS9l3Zh9NAppwdaeruabzNYxsNxJ/b/9Sz2itSchIYN+ZfWw7tY2tJ7ey9eRWtp3aRkZeBgD+3v70j+zPwBYDGdhiIJHBkSzYvYCv//qaXUm78FSejGg7gmDfYPae2cve03vJtmSXaifcP5x+zfvRP7I/ncM68/GWj1lyYAnNg5rz3NDnuKfXPXh7ePPNX9/w1JKnOJJ6hLFdxvLSiJc4mnqUH3f9yE+7f+J4+nE8lScajVVbuTDyQu6MvpPxUeNp1KARVm1l28ltLDu4jGWHlrHy8ErSctMcfmf9I/szpvMYxnQeQ9eIrqTlpjF/93y+3fEti/cvJq8gr9wzTQOb0qZhGw4kH+BUpoSqUyg6hXWiV7NeRDeJpmfTnkQ3iaZpYFOUUmitOZFxgriEOOJOynEg+QDx6fGcSD9Bgba/qgr2DSbYN5ikrCRyLDlF18MahNE/sj/PDHmGwa0G233Wqq3M3jybp5Y8xens0w6/AxsN/RrSoVEHLoy8kJujbmZgy4HlfvtWbeXXvb/y2urXWHF4RYX1hfuHM6z1MIa1HsaQ1kMA+buKT4/nePpxjqcdB+CDaz6otG/2qCsC4wZglNb63sL3twMXaq0fslO2NbAWaKG1/I8rpSzAFsACvKq1/tFBOxOACQCtWrXqc/jwYXd8HE6cgG7doHNnWLBgJdu3D6N7958JC7vKLe05IjMvk/XH19M4oDEtQ1oS7FvsO7nn9B7+8+d/+GzrZ2g0t3a/lRbBLZi5fiZpuWmM7TKW54c+T3TTaPIL8lm8fzGfb/2cn3b/VPRHdEPXG3jnyndoHFAzQZiRl8ELMS8wbd00rNqKn5cfbRu2pX2j9rQPbY/WmphDMWw7JeHiA30C6RTWiU0nNtEkoAnPDX2O+/rch4+nT1GdVm1l5eGVfL71c37b/xsBPgGE+4cT4R9BhH8E4f7hKKVIyUkpdWTkZeDp4Ymn8sTLwwsvDy88PTzxUB4olJyVQqE4mXmSLQlbAAjyCWJYm2EMajGIrae2snDvQtJy0/Dz8mNku5F0atSJAykH2H9mPweSD5CZn1nU11C/UKKbRtOjcQ96NOlB72a96d6kO14e5Xeya63Zdmob32z/hnk756HRdArrRMdGHekU1olOYZ0I9Alk04lNrD++ng3xG/jr1F9oNBH+EUy+aDL3972fBt6l9aPZ+dm8ueZNXvnzlaK++Xv7M6rDKK7tfC1XdbqKXEsuX2z7gjlxc9h+ajs+nj4MbDGQ7ae2Fw2OHRt1ZHib4bQLbYeH8sBDeRR9fyG+IVza/lKaBzV3+FtIzUllwZ4F7D29l45hHekc1plOYZ0I8Qsp+vwnMk6w6cSmomNLwhYOpxb/LUf4R9ChUQf2ndlHYlZi0fXWIa3pFNaJyOBIIoMiaR7UnMigSJoFNaNRg0aE+oUS4hdS9L0XWAs4knqEXUm72H16N7uSdvHT7p9IyEjgqo5X8dKIl4huGl1U/5aELTz4y4OsObaGi1pdxNMXPU24fzgBPgEEeAcQ4BOAt4c3R9OOsv/MfvYn72ffmX3sO7OPP4/8SbYlm9Yhrbk56mZu6X4LncI68cW2L3hjzRvsSNxBi+AWTLxwIhe3uRg/Lz98vXzl7OlLam4qKw+vZMXhFaw4tKLU92FDoWgc0JiOYR354+4/HP4fVMS5KDCeQoTFP0pci9RaH1dKtQOWAZdorfdX1Ka7Vhhaw+jRsGwZbNkC7dtn8scfwbRu/Qxt2051Qf2afGt+qcHRHgeSDzD6q9HsSCzW6oX4htAypCUN/Rqy6sgqfL18ubfXvTwx6IkiVVRydjLT103nrbVvkZabxvA2w9l2ahtJWUmENQjjpqibuKX7Law8vJIXlr9AsG8w/7vyf4zrNq5an2fh3oU8+MuDHE49zITeE+jVrBf7zuxjf/L+oj8qq7YyuOVgRrQdwfA2w+nbvC/ent6sObqGSUsnsfLwSto2bMuLw1+kR5MefLntS77Y9gVH044S6BPIqA6jUCgSsxJJzEwkMSuR01kywDX0a1h0hPiFEOgTWKRGsVgtRYdVW9FotNZFr4N9gxnWehiXtL2kqE828gry+OPwH8zfPZ/5e+aTkJFAu9B2ooooVEd0aNSBqMZRRAZFulWNkJGXwY7EHXSN6Frpaiw+PZ45W+bQrXE3u6sjkN/gloQtzImbw/JDy+nVrBfD2wxneJvhtAxp6a6PUSHJ2clsPbmVuJNxbEnYwv7k/bQPbV+06ohuGk1Dv4Y1bicrP4uZ62byn1X/ITknmZujbuaJQU/wadynzFw/k7AGYbx26WvcEX1Hlf5P03PT+Wn3T3y57Ut+2/8bBboAf29/svKz6NGkB08OepLx3caX+o1VxOGUw6w+uhofTx8ig0U4Ngts5vTzjqgrAsNplZRSajPwf1rr1Q7q+gT4WWtdYf4NdwmM2bPhnnskAdLDD8u1DRuiOWNpSEKDe4g5FENSVhIdGnUoOjo26kjrhq3tzihtWKwWvt/5PW+ueZMN8Rt4sO+D/HvEv4tmXiVZfmg5N8y9Aau28vaVb+OhPDiaepQjqUc4mnaUhIwERrQdwcQBEx2uDmyC49O4T+kX2Y/but/G5R0uLyWo/jr1F3f9dBex8bGM6zqOd658h3xrPrHxsWyM38jGExvZnLCZIJ8gBrQYUHRENY4iKSuJRxY9wty/5tIlvAuzRs/iolYXleuH1poCXeDwu9Fas3j/YiYvnVw02/dUnlze4XJu634b13S+hgCfgHLPWbUVhXLrQF22n2erLYP7SclJ4bVVrzFt3TSy8rNQKO7vez8vjXiJ0AahNao7MTORb3d8y8b4jYyPGs+l7aIS6gMAAB1vSURBVC6tM7+duiIwvBCj9yXAccTofYvW+q8y5S4AFgFtdWFnlFKhQJbWOlcpFQ6sAcY4MpjbcIfAOHwYuneHPn3gx4XpLD7wK0sPLGXxnq85nCE63bAGYUQGR7L/zP5SaglvD2/6Nu/LkFZDGNJ6CINbDia0QSipOal8tPkjpq+bzpHUI3Ro1IH+kf35attXNA5ozJuXv8nNUTcX/aA+2PgBDy58kA6NOrDg5gV0aNTBpZ+xLBarhf+u+i9Tlk9Bo4sMtB7KgwvCL6B3s96k5aax5uiaIvWAv7c/HsqDvII8nh3yLP8c/E98vXxr1A+rtvLDzh84lXmK67teX2M1mcHgDCczTvLFti8Y2noofZvX/6xndUJgFHbkSmAa4Al8rLV+SSk1FYjVWs8vLDMF8NNaTyrx3CDgfcCKbP2dprX+qLL2XC0wrFYYPiqddckLGPbAt6yI/5XcglxCfEO4sGkbOnrFcftFC+jX+ko8lEcpw+e+M/vYmbSTVUdXseH4BvKt+QB0i+jGkdQjpOelM6z1MB4b+BhXd7oaD+VBbHwsD/zyALHxsVzS9hKmj5rOrI2zmLF+BqM6jOLr67+2u/pwF9tPbeejTR/RNrQtfZr1oWfTnqVm9lprDqUcYu2xtaw9tpaU3BSevujpSnfcGAyGukOdERhnG1cKjD2n9zBu1j/ZmrUIvHJpHtScG7rcwLhu4xjYYiDZWX8RGxtNly6f06TJrRXWlZ2fzYb4Dfxx+A9WHV1FuH84j1z4CH2a9ylXtsBawKyNs5i8dDKpuakAPDrgUV679DU8PTxd8tkMBoPBhhEYLmDIrMv589BaWp+5m88nj2NQq9Jb46xWC3/+GUKzZvfSseN0l7RZkpMZJ3lx5Yv0j+zPHdF3uLx+g8FggKoJDBOt1g67knbx54nf8F7/IqvmPEtkZPkyHh5eBAX1KYpc62qaBDbh7SvfdkvdBoPBUB1MLCk7vL3+bVSBD0P8J9gVFjaCgvqTnr4Zq7W8U5LBYDDUN4zAKENqTiqzN3+C3nYTVwyteFdOcPCFaJ1LRsbWs9Q7g8FgqD2MwCjDJ1s+IcuSCeseZvjwissGB/cHcJtaymAwGOoSRmCUwKqtzFw/k8a5g2iY3afSDHq+vq3w9m5MWpoRGAaDof5jBEYJft37K/uT98O6fzB0KHhWsotVKUVwcH/S09ednQ4aDAZDLWIERglmrp9JE//mnFp+PSNGOPdMUNCFZGXtwmJJdW/nDAaDoZYxAqOQ3Um7Wbx/MYN9HgCrd6X2CxvFdgz3hVU3GAyGuoARGIW8vf5tfDx98NwygbAwiIpy7rmgoH4ApKUZtZTBYKjfGIGBbKX9JO4Tbup2E2uXNmb4cPBw8pvx9g6lQYNOxvBtMBjqPUZgIFtpM/IyGBv5MEeP4rQ6ykZw8EBSU//AWhhg0GAwGOoj573AsGorb294m4EtBnIqToIBVlVghIePwWI5Q0pKxakWDQaD4VzmvBcYWflZXNHhCp4c9CQxMdC0KVxwQdXqaNTocjw8/ElK+t49nTQYDIY6wHkvMAJ9AplxxQyuveA6YmJkdVHVRFienv6EhV1JUtIPaG11T0cNBoOhljnvBYaN3bshIaHq6igb4eHXk5eXQGqq3SyzBoPBcM5jBEYhy5bJ2VmHvbKEhV2FUr4kJc1zXacMBoOhDmEERiExMdCyJbRrV73nvbyCaNToMhITv6c+JaUyGAwGG24VGEqpUUqp3UqpfUqpSXbu36WUSlRKbSk87i1x706l1N7C40539tNqheXLq2e/KElExPXk5h4xXt8Gg6Fe4raMe0opT+Ad4FLgGLBBKTVfa72jTNFvtNYPlXm2EfAC0BfQwMbCZ5Pd0dft2yEpqfrqKBthYdeglBeJifMIDu7nms4ZDAZDHcGdK4z+wD6t9QGtdR7wNTDGyWcvB37XWp8pFBK/A6Pc1E9iYuRcXYO3DW/vUBo2HEFS0jyjljIYDPUOdwqMSOBoiffHCq+V5Xql1Fal1HdKqZZVfNYlxMSI7aJVq5rXFRExluzsfWRmbqt5ZQaDwVCHqG2j9wKgjda6B7KKmFPVCpRSE5RSsUqp2MTExCp3oKAAVqyouTrKRnj4tYAiMdE48RkMhvqFOwXGcaBlifctCq8VobU+rbXOLXz7IdDH2WdL1DFLa91Xa903IiKiyp0sKIB33oG//a3Kj9rFx6cJISFDzPZag8FQ73CnwNgAdFRKtVVK+QA3AfNLFlBKNSvx9hpgZ+HrxcBlSqlQpVQocFnhNZfj4wO33AIDBriuzoiI68nM3E5W1h7XVWowGAy1jNsEhtbaAjyEDPQ7gbla67+UUlOVUtcUFntYKfWXUioOeBi4q/DZM8CLiNDZAEwtvHZOEB4+FoDERLPKMBgM9QdVn3bz9O3bV8fG1g0fiI0bB6C1hb5960Z/DAaDwR5KqY1a677OlK1to3e9JSLiejIyNpKdfai2u2IwGAwuwQgMNxERMQ6Akyc/q+WeGAwGg2swAsNNNGjQhtDQkZw48SFaF9R2dwwGg6HGGIHhRpo1m0Bu7hHOnPm9trtiMBgMNcYIDDcSHj4Gb+8ITpz4oLa7YjAYDDXGCAw34uHhQ9Omd3L69HxycxNquzsGg8FQI4zAcDPNmt2L1hYSEj6p7a4YDAZDjTACw834+3cmJGRoofHb5Ps2GAznLkZgnAWaN59ATs5+UlJiarsrBoPBUG2MwDgLhIdfj5dXKPHxxvhtMBjOXYzAOAt4evrRpMntJCX9QF5e1UOwGwwGQ13ACIyzRLNm96F1HidPflrbXTEYDIZqYQTGWSIwMIrg4IHEx39g0rcaDIZzEiMwziLNmt1HdvZuUlP/rO2uGAwGQ5UxAuMs0rjxjXh6BhMf/25td8VgMBiqjBEYZxFPzwCaN/87p059TWbmX7XdHYPBYKgSRmCcZVq1egpPzyAOHny2trtiMBgMVcIIjLOMt3cYLVs+QVLSj6Slravt7hgMBoPTGIFRC7RoMRFv7wgOHHimtrtiMBgMTuNWgaGUGqWU2q2U2qeUmmTn/mNKqR1Kqa1KqaVKqdYl7hUopbYUHvPd2c+zjZdXEK1aPU1KylKSk5fWdncMBoPBKdwmMJRSnsA7/H97dx4nV1UlcPx3au2lunrfkk7SWdlDiDGyqRB1jKCAIygii4oL44KMOiq4oBkXHFzg81FnYBAFFRVUMCrKYIyoIEsLISGQhaTT2Xqt7urqTnWt78wf9brthCxFQlNdnfP9fOpT9V69enVu96s6dd+97154E3A88E4ROX6fzZ4ClqjqQuAXwH+Ne25EVRe5t/MmKs5CmTbtKoLBGWzdep1dl2GMKQoTWcNYCjyvqltVNQX8DDh//AaqulpV4+7io0DLBMYzqXi9JbS2fpGhocfp67uv0OEYY8whTWTCmA7sGLe80113IFcCvx+3XCIibSLyqIhcMBEBFlpj4+WUlR1Le/vnbN5vY8ykNykavUXkUmAJcOO41bNUdQlwCXCTiMw9wGs/4CaWtt7e4hrYz+Px0dr6n8Tjz9Ld/eNCh2OMMQc1kQljFzBj3HKLu24vIvJ64LPAeaqaHF2vqrvc+63An4FT9vcmqnqrqi5R1SX19fUvXfQvk/r6txEKvYL29uvJZIYKHY4xxhzQRCaMJ4D5IjJbRALAxcBevZ1E5BTgFnLJomfc+moRCbqP64AzgGcnMNaCERHmzr2RZHIna9acRSrVXeiQjDFmvyYsYahqBvgI8ADwHHC3qq4XkRUiMtrr6UYgBNyzT/fZ44A2EXkaWA3coKpTMmEAVFefzUknrSQe38CTT55OPP58oUMyxpgXkKnUpXPJkiXa1tZW6DAOWyz2GGvXnouIl4UL76ei4hWFDskYM8WJyD/c9uJDmhSN3iYnHH4Vixc/jMdTypo1Z9Hf/yAAyeRuenruZvPmq2lrW8zjjx9POh0pcLTGmKONr9ABmL2VlR3D4sWPsHbtOaxbdw7B4EwSia0AeDxlhMNLGRxcx/PPX8Nxx/2owNEaY44mljAmoWBwGqec8hCbN19NNjvE9OkfobLyTEKhRXg8ftrbr6ejYwUNDRdTW3tuocM1xhwlrA2jCDlOira2xWQyUZYuXY/PV1nokIwxRcraMKY4jyfAscfeTirVyZYtnyp0OMaYo4QljCIVDi9lxoyP09l5KwMDf8r7dapKPL6ZTGZ4AqMzxkxF1oZRxFpbV9DX92s2bnwfr3zlOrze8gNuq6oMDKxi27YvEos9DHgJhRYSDp9GOHwalZWnU1IyGxF5+QpgjCkqVsMoYl5vKccccxuJRPsBp3xVVfr7/8hTT72atWvfQCKxjTlzbmTWrGvx+Wro7r6TDRsu47HH5vL0068jm43vdz/GGGM1jCJXVfUapk37EDt33kwq1YPXW4ZIEI8niMdTQjT6ELHYwwSDLcyf/z2am9+LxxMce71qlj17niES+T3t7dexfv1FnHjifXg8/gKWyhgzGVnCmALmzLmBRGIrsdgjOE5y7KaaJBCYtt9EMUrESyh0MqHQyfj91WzadBUbNryH4467E5HCV0AdJ41qFq+3pNChGHPUs4QxBfh8FSxc+PtDb3gI06Z9kHQ6Qnv7Z/H7a5k376YXtGlEow+xffvXUU0TDp9BZeUZhMOn4vNVAKDqMDz8NAMDq4hGVxGLPUp19RuZP/87BAJ1LyqePXueZf36C0mne5k9+ys0N19JbiJHY0whWMIwe5k581rS6V527rwJv7+e1tZc28jg4MO0t19PNLqKQKAJv7+Rjo4VgAIeQqGFBIMzGBx8mEymH4CysmOpqVlOb+8viUZXc8wxt1JXd/6B33yc7u6fsnHj+/F6yyktPYZNmz7Irl3fY/78m6mqeu0Eld4YczCWMMxecsOtf5N0OsK2bZ/HcRIMDbUxMPAAfn8Dc+d+i2nTrsLrLSWTiRGLPcrg4MMMDv6NeHwjdXXnUVX1OqqrlxEMTgNgeHgtGzZcwTPPXEBj4+XMm3czfn/Vft/fcVJs2fIJdu36DuHwGZxwwt0EAs309t7Dli2fZM2as6ivv4i5c2+kpGTWy/mnGROPb6Sn52eoOrS0XI3fX3vAbVWVaPQhVFNUVS3D47GPnCledqW32S/HSbN+/b8SifwWv7+OGTM+zfTp/3bQrrsH31+Kjo4v09HxVQKBJubM+SolJXPw+2vw+2vx+WpIpTpZv/7tDA09RkvLx5kz54a9Gt+z2Tg7dnyD7dtvwHFS+P11+P01+HzV7v34x9Vjj73eMCJet03GAwgiHkpLF+DzhfKKP5HYSW/vz+nuvovh4SeB3Kk6r7eCmTM/Q0vLx/B6y8a2z/VOu59t21YwNPQ4AH5/I42Nl9DYeBmh0CLrwjxJpdMRtm69lvLyk2hqunzKj6TwYq70toRhDiibHaG//36qq9+Y9xfrocRiT7BhwxXE48/t51kPXm85xx77A+rr33bAfSQSO+js/F9SqW4ymX7S6X73PkImM0A2m99FiSIBqqrOpq7uLdTWvoWSkpljz6VSPcRijxKLPUo0+hdisUcApaJiCQ0Nl9DQ8HYymShbt15HJLKSQKCZ1tYv0tT0Hvr7/0BHxwqGhtoIBmcxa9Z1+P31dHf/iEjkt6imKSs7gbq683GcOMlkJ6lUJ6nUblKpbioqXsmsWZ+nuvqsA8buOBkGB/8KKH5/A4FAA35/bV5tPJnMIH19v6Gv71ckk7vHku1oog0Gp9PYeOleCXB/stk4IyObKS9fWLDkl05HEPHj84Vfkv0lEh2sXbuceHwjoHg85TQ2vovp0z9EKHTyS/Iek40lDDOpOU6KPXvWkU5H3C/7COl0hGw2TnPzeykrW3CE+0+TyUTJZAZIp/vJZmOoOuTaWxxUHVRTDA4+QiTyG0ZGNgNQXr6QsrJjGRpqGxshWMRHefnJ1NWdR0PDxfuNLRr9G1u3fppY7BG83gqy2SFKSmYza9ZnaWy8fK9aUjodoafnbrq7f0Qs9ne83hCBQPPYze+voa/vXlKpLiorX0Nr6xeoqlo29oU8PLyOrq4f0t39E9LpfWdnFPz+OoLB6ZSWLqCsbMHYfSDQxMDAKnp7f8nAwIOopgkEplFefjyZTHQs6WYyUQBKSmazYMEt1NS8Yb9/476+lWze/FGSye2Ul5/IjBn/QUPDxXg8gRdsm0r10td3H0NDj4/7n48m+EECgSZKS+dRVjaf0tJ5lJbOJxRaTDDYdMD/cSYzzPbtX2PHjm/i8QRpbf0C06d/dL/vn6/h4bWsXbucbDbOSSetxOMpY/fu/6an5y4cJ0E4fDq1teegmnV7IaZwnNys0tXVr6emZvkRvf94qopqNq9TmKpKKtV90L/XwVjCMOZFiMc3Eon8lr6+35BItFNRsYRw+FTC4dOoqFh8yF/akPvQRiIr6e7+MTU159LY+K5DXsviOMn9dnXOZkfo7LyN7dtvIJXaTTh8BrW159Lbew/Dw08h4qO29s00Nl6Gz1dDOt1DKtUzdp9MbmdkZDMjI+1Adq99B4OzqK+/kPr6txEOv+oFXadVs0Sjf2HTpqsYGdnktjl9a6ydJpHoYPPmq4lEVlJWdgLNzVfS1XU7e/Y8QyAwnRkz/p3m5vfjOCP09t5Lb+89RKN/Bhz8/noCgcZxtZlafL4KkslON97NZLOj89p7qK09l+bmK6mpOWfsb6nq0N19F1u3fppUajcNDZeQyUTp77+f0tJ5zJ37DWprz9urxhOPb6Kv714ikd9TUjKTxsbLqa4+e6/a2MDAn3nmmfPxenM9DkOhk8aeS6f76eq6g927v8fIyOhsmF48ngAeTxDHSeM4e/D5qqmvv4jGxkuorHz1AbulZ7N7iMc3MzKykXh8I/H4JtLpbtLpgbEfOplMFBEf4fCrqKo6m6qqswmHT8XrLUFVGRnZQjT6J6LR1QwMrMbjCXLqqdsOq6ZnCcOYKSCbTdDVdTvbt3+NZHInodBimpquoKHhnQQC9Yd8veOkSCTaicc3kUxuJxw+lVBocV5fKtlsgo6OL7Njx9fx+aqZN+/bJJM72bbtS4DQ2vpFWlquwePxu+01f2DHjhuJRlfj8ZTjOHFAKS1dQH39RdTXX0godPJB31tVSad7GRnZTCTyO7q6fkAq1UUg0ERT07uprDyTjo4vE4s9SkXFEubNu5nKytMBiET+wJYtHycef46qqmW0tFxDLPZ3+vruGzv9WV5+MolEO9lsjGCwhcbGS2lsvJw9e9bx3HOXUVo6j4UL/0BJyYwDxuc4CTyewF7JxnHSDAw8SHf3XfT13Yfj7CEYbCEUWoTjJMhmR3CcERwnQSYzSCq1a6/9BoMzCQan4/NVjbW9+XxVOE6CwcGHGBp6EnAQCRIOv5JEYhvJ5E4AAoFmqqqWUV19Nk1N7z6sbueWMIyZQhwnRSrVfcAvsok0PLyOjRvfN9ZwX1f3VubNu2mv9p7xYrE2OjtvJRCYRn39hZSXn3DY7RuOk6G//346O28jEvkduVpKI3PmfI2mpite8AvecTJ0dt5Ce/sX3K7dXqqqXktd3QXU1V1ASckMstkRIpGVdHXdSX//A4zWwCorz+TEE3+N319zWLGOymb30Ne3kp6en5JM7sDjKR27eb2leL0h97TbAsrKjqG0dD5eb+lB95lORxkc/CvR6GoGBx+hpGQmVVVnU129jNLSBUfcfjRpEoaILAduBrzAbap6wz7PB4E7gVcAEeAdqrrNfe5a4Epy/9GrVfWBQ72fJQxjXnqqWbq67iAQaKa29k0FiSGZ3MXg4MPU1Cw/ZAN3Oj3A4ODDVFaedtAuz8lkFz09d5FKddLauuKQX9xT1aRIGJKrG20C3gDsBJ4A3qmqz47b5kPAQlW9SkQuBt6qqu8QkeOBnwJLgWnAH4EFqprd933Gs4RhjDEvzmSZQGkp8LyqblXVFPAzYN/LfM8H7nAf/wJ4neTqV+cDP1PVpKq2A8+7+zPGGFMgE5kwpgM7xi3vdNftdxtVzQCDQG2erwVARD4gIm0i0tbb2/sShW6MMWZfhR+O9Aip6q2qukRVl9TXH7rniDHGmMMzkQljFzC+W0eLu26/24iID6gk1/idz2uNMca8jCYyYTwBzBeR2SISAC4GVu6zzUrgCvfxhcCfNNcKvxK4WESCIjIbmA88PoGxGmOMOYQJGzpTVTMi8hHgAXLdam9X1fUisgJoU9WVwPeBH4nI80A/uaSCu93dwLNABvjwoXpIGWOMmVh24Z4xxhzFJku3WmOMMVPIlKphiEgv0HGYL68D+l7CcCYLK1fxmaplm6rlguIu2yxVzauL6ZRKGEdCRNryrZYVEytX8ZmqZZuq5YKpXbbx7JSUMcaYvFjCMMYYkxdLGP90a6EDmCBWruIzVcs2VcsFU7tsY6wNwxhjTF6shmGMMSYvR33CEJHlIrJRRJ4Xkc8UOp4jISK3i0iPiDwzbl2NiDwoIpvd++pCxng4RGSGiKwWkWdFZL2IfMxdX9RlE5ESEXlcRJ52y/Uld/1sEXnMPSZ/7g6tU3RExCsiT4nIb93lqVKubSKyTkTWiEibu66oj8V8HdUJw53k6bvAm4DjgXe6kzcVqx8Cy/dZ9xlglarOB1a5y8UmA3xCVY8HTgU+7P6fir1sSWCZqp4MLAKWi8ipwNeBb6vqPGCA3MyTxehjwHPjlqdKuQDOVtVF47rSFvuxmJejOmGQ3yRPRUNV/0JuTK7xxk9SdQdwwcsa1EtAVTtV9Un38RC5L6HpFHnZNGfYXfS7NwWWkZtQDIqwXAAi0gKcC9zmLgtToFwHUdTHYr6O9oSR90RNRaxRVTvdx11AYyGDOVIi0gqcAjzGFCibe9pmDdADPAhsAaLuhGJQvMfkTcCnAMddrmVqlAtySf3/ROQfIvIBd13RH4v5mLDRas3ko6oqIkXbLU5EQsAvgWtUNZb70ZpTrGVzR2FeJCJVwL3AsQUO6YiJyJuBHlX9h4icVeh4JsCZqrpLRBqAB0Vkw/gni/VYzMfRXsM4GiZq6haRZgD3vqfA8RwWEfGTSxY/UdVfuaunRNkAVDUKrAZOA6rcCcWgOI/JM4DzRGQbudO8y4CbKf5yAaCqu9z7HnJJfilT6Fg8mKM9YeQzyVOxGz9J1RXArwsYy2Fxz39/H3hOVb817qmiLpuI1Ls1C0SkFHgDufaZ1eQmFIMiLJeqXquqLaraSu4z9SdVfRdFXi4AESkXkYrRx8C/AM9Q5Mdivo76C/dE5Bxy51tHJ3n6SoFDOmwi8lPgLHIjZ3YD1wP3AXcDM8mN5Pt2Vd23YXxSE5Ezgb8C6/jnOfHryLVjFG3ZRGQhuQZSL7kfb3er6goRmUPul3kN8BRwqaomCxfp4XNPSX1SVd88FcrlluFed9EH3KWqXxGRWor4WMzXUZ8wjDHG5OdoPyVljDEmT5YwjDHG5MUShjHGmLxYwjDGGJMXSxjGGGPyYgnDmElARM4aHdXVmMnKEoYxxpi8WMIw5kUQkUvdOSzWiMgt7uCBwyLybXdOi1UiUu9uu0hEHhWRtSJy7+gcCSIyT0T+6M6D8aSIzHV3HxKRX4jIBhH5iYwfLMuYScAShjF5EpHjgHcAZ6jqIiALvAsoB9pU9QTgIXJX2APcCXxaVReSu0p9dP1PgO+682CcDoyOcnoKcA25uVnmkBuTyZhJw0arNSZ/rwNeATzh/vgvJTfInAP83N3mx8CvRKQSqFLVh9z1dwD3uOMQTVfVewFUNQHg7u9xVd3pLq8BWoG/TXyxjMmPJQxj8ifAHap67V4rRT6/z3aHO97O+HGVstjn00wydkrKmPytAi5050EYncd5FrnP0egorJcAf1PVQWBARF7trr8MeMidMXCniFzg7iMoImUvaymMOUz2C8aYPKnqsyLyOXKzrXmANPBhYA+w1H2uh1w7B+SGuf4fNyFsBd7jrr8MuEVEVrj7uOhlLIYxh81GqzXmCInIsKqGCh2HMRPNTkkZY4zJi9UwjDHG5MVqGMYYY/JiCcMYY0xeLGEYY4zJiyUMY4wxebGEYYwxJi+WMIwxxuTl/wEpFnOe8bePAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 17s 3ms/sample - loss: 1.1398 - acc: 0.7034\n",
      "Loss: 1.1398343253110923 Accuracy: 0.7034268\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6820 - acc: 0.4935\n",
      "Epoch 00001: val_loss improved from inf to 1.13120, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/001-1.1312.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 1.6821 - acc: 0.4935 - val_loss: 1.1312 - val_acc: 0.6643\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.7043\n",
      "Epoch 00002: val_loss improved from 1.13120 to 0.88594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/002-0.8859.hdf5\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.9992 - acc: 0.7043 - val_loss: 0.8859 - val_acc: 0.7526\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7841 - acc: 0.7720\n",
      "Epoch 00003: val_loss improved from 0.88594 to 0.86142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/003-0.8614.hdf5\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.7842 - acc: 0.7719 - val_loss: 0.8614 - val_acc: 0.7540\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.8098\n",
      "Epoch 00004: val_loss improved from 0.86142 to 0.67690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/004-0.6769.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.6491 - acc: 0.8098 - val_loss: 0.6769 - val_acc: 0.8127\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8393\n",
      "Epoch 00005: val_loss improved from 0.67690 to 0.62379, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/005-0.6238.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.5483 - acc: 0.8393 - val_loss: 0.6238 - val_acc: 0.8192\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8599\n",
      "Epoch 00006: val_loss improved from 0.62379 to 0.57537, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/006-0.5754.hdf5\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.4778 - acc: 0.8598 - val_loss: 0.5754 - val_acc: 0.8418\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8786\n",
      "Epoch 00007: val_loss improved from 0.57537 to 0.55389, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/007-0.5539.hdf5\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.4133 - acc: 0.8785 - val_loss: 0.5539 - val_acc: 0.8500\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8873\n",
      "Epoch 00008: val_loss did not improve from 0.55389\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.3772 - acc: 0.8872 - val_loss: 0.5660 - val_acc: 0.8481\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9055\n",
      "Epoch 00009: val_loss improved from 0.55389 to 0.49153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/009-0.4915.hdf5\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.3142 - acc: 0.9054 - val_loss: 0.4915 - val_acc: 0.8700\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9175\n",
      "Epoch 00010: val_loss improved from 0.49153 to 0.49006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/010-0.4901.hdf5\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.2730 - acc: 0.9174 - val_loss: 0.4901 - val_acc: 0.8742\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9272\n",
      "Epoch 00011: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.2424 - acc: 0.9272 - val_loss: 0.6000 - val_acc: 0.8563\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9383\n",
      "Epoch 00012: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.2030 - acc: 0.9383 - val_loss: 0.5131 - val_acc: 0.8691\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9377\n",
      "Epoch 00013: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.2051 - acc: 0.9376 - val_loss: 0.6043 - val_acc: 0.8532\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9500\n",
      "Epoch 00014: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.1646 - acc: 0.9500 - val_loss: 0.6057 - val_acc: 0.8407\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9576\n",
      "Epoch 00015: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.1384 - acc: 0.9576 - val_loss: 0.7080 - val_acc: 0.8297\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9644\n",
      "Epoch 00016: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.1203 - acc: 0.9644 - val_loss: 0.5908 - val_acc: 0.8558\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9647\n",
      "Epoch 00017: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.1167 - acc: 0.9647 - val_loss: 0.5195 - val_acc: 0.8791\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9596\n",
      "Epoch 00018: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.1336 - acc: 0.9595 - val_loss: 0.5672 - val_acc: 0.8761\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9710\n",
      "Epoch 00019: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.1001 - acc: 0.9708 - val_loss: 0.5240 - val_acc: 0.8707\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9643\n",
      "Epoch 00020: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.1168 - acc: 0.9643 - val_loss: 0.5281 - val_acc: 0.8814\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9729\n",
      "Epoch 00021: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0934 - acc: 0.9728 - val_loss: 0.5276 - val_acc: 0.8810\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9705\n",
      "Epoch 00022: val_loss did not improve from 0.49006\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0970 - acc: 0.9705 - val_loss: 0.5604 - val_acc: 0.8761\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9819\n",
      "Epoch 00023: val_loss improved from 0.49006 to 0.48552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/023-0.4855.hdf5\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0652 - acc: 0.9819 - val_loss: 0.4855 - val_acc: 0.8870\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9762\n",
      "Epoch 00024: val_loss did not improve from 0.48552\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0820 - acc: 0.9761 - val_loss: 0.5347 - val_acc: 0.8805\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9711\n",
      "Epoch 00025: val_loss did not improve from 0.48552\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0964 - acc: 0.9711 - val_loss: 0.5692 - val_acc: 0.8758\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9768\n",
      "Epoch 00026: val_loss did not improve from 0.48552\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0791 - acc: 0.9767 - val_loss: 0.5821 - val_acc: 0.8758\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9804\n",
      "Epoch 00027: val_loss did not improve from 0.48552\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0679 - acc: 0.9803 - val_loss: 0.5595 - val_acc: 0.8807\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9783\n",
      "Epoch 00028: val_loss improved from 0.48552 to 0.47951, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv_checkpoint/028-0.4795.hdf5\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0716 - acc: 0.9783 - val_loss: 0.4795 - val_acc: 0.8968\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9829\n",
      "Epoch 00029: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0586 - acc: 0.9829 - val_loss: 0.5314 - val_acc: 0.8882\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9844\n",
      "Epoch 00030: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0556 - acc: 0.9843 - val_loss: 0.5563 - val_acc: 0.8896\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9815\n",
      "Epoch 00031: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0621 - acc: 0.9814 - val_loss: 0.6183 - val_acc: 0.8838\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9805\n",
      "Epoch 00032: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0668 - acc: 0.9805 - val_loss: 0.6336 - val_acc: 0.8735\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9838\n",
      "Epoch 00033: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0536 - acc: 0.9837 - val_loss: 0.5734 - val_acc: 0.8831\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9832\n",
      "Epoch 00034: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0578 - acc: 0.9831 - val_loss: 0.6293 - val_acc: 0.8751\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9801\n",
      "Epoch 00035: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0672 - acc: 0.9801 - val_loss: 0.7331 - val_acc: 0.8570\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9835\n",
      "Epoch 00036: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0538 - acc: 0.9834 - val_loss: 0.5275 - val_acc: 0.8938\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9855\n",
      "Epoch 00037: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0521 - acc: 0.9855 - val_loss: 0.6229 - val_acc: 0.8863\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9874\n",
      "Epoch 00038: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0434 - acc: 0.9873 - val_loss: 0.6390 - val_acc: 0.8798\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9806\n",
      "Epoch 00039: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0637 - acc: 0.9806 - val_loss: 0.6037 - val_acc: 0.8803\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9867\n",
      "Epoch 00040: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0450 - acc: 0.9866 - val_loss: 0.5846 - val_acc: 0.8868\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9900\n",
      "Epoch 00041: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0356 - acc: 0.9899 - val_loss: 0.5776 - val_acc: 0.8896\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9832\n",
      "Epoch 00042: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0553 - acc: 0.9831 - val_loss: 0.5959 - val_acc: 0.8838\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9874\n",
      "Epoch 00043: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0435 - acc: 0.9874 - val_loss: 0.5827 - val_acc: 0.8870\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9928\n",
      "Epoch 00044: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0277 - acc: 0.9928 - val_loss: 0.5838 - val_acc: 0.8898\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9835\n",
      "Epoch 00045: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0546 - acc: 0.9835 - val_loss: 0.6415 - val_acc: 0.8786\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9930\n",
      "Epoch 00046: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 0.6586 - val_acc: 0.8747\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9829\n",
      "Epoch 00047: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0564 - acc: 0.9829 - val_loss: 0.6451 - val_acc: 0.8833\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9923\n",
      "Epoch 00048: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0286 - acc: 0.9923 - val_loss: 0.6647 - val_acc: 0.8828\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9912\n",
      "Epoch 00049: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0310 - acc: 0.9912 - val_loss: 0.6087 - val_acc: 0.8882\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0229 - acc: 0.9944 - val_loss: 0.7345 - val_acc: 0.8779\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9744\n",
      "Epoch 00051: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0865 - acc: 0.9744 - val_loss: 0.5596 - val_acc: 0.8980\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9873\n",
      "Epoch 00052: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0421 - acc: 0.9873 - val_loss: 0.6423 - val_acc: 0.8803\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9915\n",
      "Epoch 00053: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0307 - acc: 0.9915 - val_loss: 0.6093 - val_acc: 0.8849\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9891\n",
      "Epoch 00054: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0360 - acc: 0.9891 - val_loss: 0.5734 - val_acc: 0.8924\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9868\n",
      "Epoch 00055: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0445 - acc: 0.9868 - val_loss: 0.5705 - val_acc: 0.8954\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9904\n",
      "Epoch 00056: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0331 - acc: 0.9903 - val_loss: 0.5465 - val_acc: 0.8963\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9880\n",
      "Epoch 00057: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0399 - acc: 0.9880 - val_loss: 0.5971 - val_acc: 0.8898\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9939\n",
      "Epoch 00058: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0215 - acc: 0.9938 - val_loss: 0.5972 - val_acc: 0.8880\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9898\n",
      "Epoch 00059: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0340 - acc: 0.9898 - val_loss: 0.6272 - val_acc: 0.8877\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9876\n",
      "Epoch 00060: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0410 - acc: 0.9876 - val_loss: 0.6715 - val_acc: 0.8826\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9885\n",
      "Epoch 00061: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0381 - acc: 0.9885 - val_loss: 0.5978 - val_acc: 0.8903\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9962\n",
      "Epoch 00062: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0159 - acc: 0.9962 - val_loss: 0.6082 - val_acc: 0.8928\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9878\n",
      "Epoch 00063: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0423 - acc: 0.9877 - val_loss: 0.6402 - val_acc: 0.8896\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9862\n",
      "Epoch 00064: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0443 - acc: 0.9862 - val_loss: 0.6754 - val_acc: 0.8796\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9881\n",
      "Epoch 00065: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0418 - acc: 0.9880 - val_loss: 0.6789 - val_acc: 0.8833\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9889\n",
      "Epoch 00066: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0373 - acc: 0.9889 - val_loss: 0.6342 - val_acc: 0.8863\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9886\n",
      "Epoch 00067: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 300s 8ms/sample - loss: 0.0382 - acc: 0.9886 - val_loss: 0.5838 - val_acc: 0.8931\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9897\n",
      "Epoch 00068: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0341 - acc: 0.9897 - val_loss: 0.5911 - val_acc: 0.8991\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0156 - acc: 0.9966 - val_loss: 0.5892 - val_acc: 0.8908\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9883\n",
      "Epoch 00070: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0418 - acc: 0.9882 - val_loss: 0.5935 - val_acc: 0.8989\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9920\n",
      "Epoch 00071: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0288 - acc: 0.9919 - val_loss: 0.5896 - val_acc: 0.8977\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9933\n",
      "Epoch 00072: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 301s 8ms/sample - loss: 0.0235 - acc: 0.9932 - val_loss: 0.6442 - val_acc: 0.8875\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9915\n",
      "Epoch 00073: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 0.6711 - val_acc: 0.8807\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9888\n",
      "Epoch 00074: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0356 - acc: 0.9888 - val_loss: 0.6049 - val_acc: 0.8938\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9935\n",
      "Epoch 00075: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0218 - acc: 0.9935 - val_loss: 0.6053 - val_acc: 0.8968\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9963\n",
      "Epoch 00076: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0149 - acc: 0.9963 - val_loss: 0.6106 - val_acc: 0.8933\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9873\n",
      "Epoch 00077: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0441 - acc: 0.9873 - val_loss: 0.6264 - val_acc: 0.8847\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9957\n",
      "Epoch 00078: val_loss did not improve from 0.47951\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.6365 - val_acc: 0.8915\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.5696 - acc: 0.8663\n",
      "Loss: 0.5696179456180996 Accuracy: 0.8662513\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6560 - acc: 0.4955\n",
      "Epoch 00001: val_loss improved from inf to 1.00879, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/001-1.0088.hdf5\n",
      "36805/36805 [==============================] - 320s 9ms/sample - loss: 1.6562 - acc: 0.4955 - val_loss: 1.0088 - val_acc: 0.7028\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8889 - acc: 0.7378\n",
      "Epoch 00002: val_loss improved from 1.00879 to 0.73769, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/002-0.7377.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.8891 - acc: 0.7378 - val_loss: 0.7377 - val_acc: 0.7873\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6232 - acc: 0.8180\n",
      "Epoch 00003: val_loss improved from 0.73769 to 0.50948, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/003-0.5095.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.6234 - acc: 0.8180 - val_loss: 0.5095 - val_acc: 0.8505\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8630\n",
      "Epoch 00004: val_loss improved from 0.50948 to 0.45981, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/004-0.4598.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.4769 - acc: 0.8629 - val_loss: 0.4598 - val_acc: 0.8710\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8833\n",
      "Epoch 00005: val_loss improved from 0.45981 to 0.42326, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/005-0.4233.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.4011 - acc: 0.8833 - val_loss: 0.4233 - val_acc: 0.8863\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.9011\n",
      "Epoch 00006: val_loss did not improve from 0.42326\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.3310 - acc: 0.9011 - val_loss: 0.4376 - val_acc: 0.8700\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9148\n",
      "Epoch 00007: val_loss improved from 0.42326 to 0.31327, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/007-0.3133.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.2875 - acc: 0.9148 - val_loss: 0.3133 - val_acc: 0.9175\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9224\n",
      "Epoch 00008: val_loss improved from 0.31327 to 0.27611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/008-0.2761.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.2603 - acc: 0.9224 - val_loss: 0.2761 - val_acc: 0.9236\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9331\n",
      "Epoch 00009: val_loss did not improve from 0.27611\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.2197 - acc: 0.9330 - val_loss: 0.3467 - val_acc: 0.9101\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9390\n",
      "Epoch 00010: val_loss did not improve from 0.27611\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.2016 - acc: 0.9390 - val_loss: 0.3322 - val_acc: 0.9071\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9484\n",
      "Epoch 00011: val_loss did not improve from 0.27611\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.1754 - acc: 0.9483 - val_loss: 0.3509 - val_acc: 0.9087\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9455\n",
      "Epoch 00012: val_loss did not improve from 0.27611\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.1761 - acc: 0.9455 - val_loss: 0.2814 - val_acc: 0.9262\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9536\n",
      "Epoch 00013: val_loss improved from 0.27611 to 0.25055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/013-0.2506.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1519 - acc: 0.9536 - val_loss: 0.2506 - val_acc: 0.9304\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9635\n",
      "Epoch 00014: val_loss did not improve from 0.25055\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.1208 - acc: 0.9635 - val_loss: 0.2682 - val_acc: 0.9287\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9649\n",
      "Epoch 00015: val_loss did not improve from 0.25055\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.1161 - acc: 0.9649 - val_loss: 0.2771 - val_acc: 0.9250\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9659\n",
      "Epoch 00016: val_loss did not improve from 0.25055\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.1141 - acc: 0.9659 - val_loss: 0.2854 - val_acc: 0.9255\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9686\n",
      "Epoch 00017: val_loss improved from 0.25055 to 0.22528, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/017-0.2253.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.1041 - acc: 0.9686 - val_loss: 0.2253 - val_acc: 0.9373\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9766\n",
      "Epoch 00018: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0813 - acc: 0.9765 - val_loss: 0.3996 - val_acc: 0.9136\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9690\n",
      "Epoch 00019: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1013 - acc: 0.9689 - val_loss: 0.2622 - val_acc: 0.9336\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9719\n",
      "Epoch 00020: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0920 - acc: 0.9719 - val_loss: 0.2318 - val_acc: 0.9371\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9785\n",
      "Epoch 00021: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0701 - acc: 0.9785 - val_loss: 0.2621 - val_acc: 0.9315\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9762\n",
      "Epoch 00022: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0802 - acc: 0.9763 - val_loss: 0.2608 - val_acc: 0.9266\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9865\n",
      "Epoch 00023: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0494 - acc: 0.9866 - val_loss: 0.2885 - val_acc: 0.9213\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9865\n",
      "Epoch 00024: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0483 - acc: 0.9865 - val_loss: 0.3053 - val_acc: 0.9248\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9766\n",
      "Epoch 00025: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0769 - acc: 0.9766 - val_loss: 0.2375 - val_acc: 0.9418\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9889\n",
      "Epoch 00026: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0414 - acc: 0.9889 - val_loss: 0.2923 - val_acc: 0.9257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9808\n",
      "Epoch 00027: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0617 - acc: 0.9807 - val_loss: 0.2557 - val_acc: 0.9380\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9871\n",
      "Epoch 00028: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0459 - acc: 0.9870 - val_loss: 0.2355 - val_acc: 0.9455\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9832\n",
      "Epoch 00029: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0563 - acc: 0.9831 - val_loss: 0.3614 - val_acc: 0.9080\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9815\n",
      "Epoch 00030: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0607 - acc: 0.9814 - val_loss: 0.2789 - val_acc: 0.9350\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9887\n",
      "Epoch 00031: val_loss did not improve from 0.22528\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0401 - acc: 0.9888 - val_loss: 0.2405 - val_acc: 0.9408\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9934\n",
      "Epoch 00032: val_loss improved from 0.22528 to 0.21129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/032-0.2113.hdf5\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0263 - acc: 0.9933 - val_loss: 0.2113 - val_acc: 0.9471\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9810\n",
      "Epoch 00033: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0616 - acc: 0.9809 - val_loss: 0.2350 - val_acc: 0.9394\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9911\n",
      "Epoch 00034: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0325 - acc: 0.9911 - val_loss: 0.2294 - val_acc: 0.9439\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9870\n",
      "Epoch 00035: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0429 - acc: 0.9870 - val_loss: 0.2216 - val_acc: 0.9455\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9924\n",
      "Epoch 00036: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0288 - acc: 0.9924 - val_loss: 0.2446 - val_acc: 0.9436\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9843\n",
      "Epoch 00037: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0506 - acc: 0.9843 - val_loss: 0.2715 - val_acc: 0.9348\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9901\n",
      "Epoch 00038: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0350 - acc: 0.9901 - val_loss: 0.2667 - val_acc: 0.9397\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9912\n",
      "Epoch 00039: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0325 - acc: 0.9911 - val_loss: 0.2386 - val_acc: 0.9397\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9893\n",
      "Epoch 00040: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0365 - acc: 0.9892 - val_loss: 0.4368 - val_acc: 0.9124\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9855\n",
      "Epoch 00041: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0486 - acc: 0.9855 - val_loss: 0.2338 - val_acc: 0.9497\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9960\n",
      "Epoch 00042: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0173 - acc: 0.9960 - val_loss: 0.2384 - val_acc: 0.9446\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9940\n",
      "Epoch 00043: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0228 - acc: 0.9940 - val_loss: 0.2146 - val_acc: 0.9499\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9959\n",
      "Epoch 00044: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0165 - acc: 0.9958 - val_loss: 0.2558 - val_acc: 0.9429\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9855\n",
      "Epoch 00045: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0459 - acc: 0.9855 - val_loss: 0.2200 - val_acc: 0.9497\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9894\n",
      "Epoch 00046: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0340 - acc: 0.9893 - val_loss: 0.2422 - val_acc: 0.9460\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 00047: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 0.2373 - val_acc: 0.9457\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9964\n",
      "Epoch 00048: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0146 - acc: 0.9963 - val_loss: 0.2482 - val_acc: 0.9434\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9922\n",
      "Epoch 00049: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0265 - acc: 0.9921 - val_loss: 0.2984 - val_acc: 0.9418\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9890\n",
      "Epoch 00050: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0347 - acc: 0.9890 - val_loss: 0.2303 - val_acc: 0.9476\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0158 - acc: 0.9957 - val_loss: 0.2264 - val_acc: 0.9506\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9921\n",
      "Epoch 00052: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0253 - acc: 0.9920 - val_loss: 0.2935 - val_acc: 0.9406\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9914\n",
      "Epoch 00053: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0289 - acc: 0.9914 - val_loss: 0.2440 - val_acc: 0.9460\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0284 - acc: 0.9916 - val_loss: 0.2496 - val_acc: 0.9469\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9936\n",
      "Epoch 00055: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0220 - acc: 0.9935 - val_loss: 0.2970 - val_acc: 0.9373\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9873\n",
      "Epoch 00056: val_loss improved from 0.21129 to 0.20910, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv_checkpoint/056-0.2091.hdf5\n",
      "36805/36805 [==============================] - 312s 8ms/sample - loss: 0.0434 - acc: 0.9873 - val_loss: 0.2091 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9971\n",
      "Epoch 00057: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0120 - acc: 0.9971 - val_loss: 0.2912 - val_acc: 0.9408\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9912\n",
      "Epoch 00058: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0277 - acc: 0.9912 - val_loss: 0.2417 - val_acc: 0.9460\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9944\n",
      "Epoch 00059: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0210 - acc: 0.9944 - val_loss: 0.2454 - val_acc: 0.9457\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9957\n",
      "Epoch 00060: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0157 - acc: 0.9957 - val_loss: 0.2316 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9942\n",
      "Epoch 00061: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0207 - acc: 0.9942 - val_loss: 0.2706 - val_acc: 0.9441\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9949\n",
      "Epoch 00062: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0176 - acc: 0.9949 - val_loss: 0.2121 - val_acc: 0.9492\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9972\n",
      "Epoch 00063: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0110 - acc: 0.9971 - val_loss: 0.2677 - val_acc: 0.9481\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9924\n",
      "Epoch 00064: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0244 - acc: 0.9924 - val_loss: 0.3113 - val_acc: 0.9432\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9940\n",
      "Epoch 00065: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0204 - acc: 0.9940 - val_loss: 0.2738 - val_acc: 0.9394\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9961\n",
      "Epoch 00066: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0139 - acc: 0.9961 - val_loss: 0.2470 - val_acc: 0.9483\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9965\n",
      "Epoch 00067: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.2712 - val_acc: 0.9439\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9912\n",
      "Epoch 00068: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0275 - acc: 0.9912 - val_loss: 0.2423 - val_acc: 0.9476\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9940\n",
      "Epoch 00069: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0197 - acc: 0.9939 - val_loss: 0.2536 - val_acc: 0.9488\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9929\n",
      "Epoch 00070: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0242 - acc: 0.9929 - val_loss: 0.2825 - val_acc: 0.9422\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9945\n",
      "Epoch 00071: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0188 - acc: 0.9945 - val_loss: 0.2596 - val_acc: 0.9450\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9938\n",
      "Epoch 00072: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 311s 8ms/sample - loss: 0.0202 - acc: 0.9938 - val_loss: 0.2426 - val_acc: 0.9483\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9960\n",
      "Epoch 00073: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0161 - acc: 0.9960 - val_loss: 0.2511 - val_acc: 0.9492\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9939\n",
      "Epoch 00074: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0202 - acc: 0.9939 - val_loss: 0.2329 - val_acc: 0.9478\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00075: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.2383 - val_acc: 0.9488\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 00076: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0080 - acc: 0.9981 - val_loss: 0.2270 - val_acc: 0.9536\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9956\n",
      "Epoch 00077: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.2597 - val_acc: 0.9492\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9920\n",
      "Epoch 00078: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0264 - acc: 0.9919 - val_loss: 0.2509 - val_acc: 0.9474\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9920\n",
      "Epoch 00079: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0264 - acc: 0.9920 - val_loss: 0.2485 - val_acc: 0.9481\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9945\n",
      "Epoch 00080: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0184 - acc: 0.9945 - val_loss: 0.2259 - val_acc: 0.9548\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9974\n",
      "Epoch 00081: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0096 - acc: 0.9973 - val_loss: 0.2260 - val_acc: 0.9548\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9955\n",
      "Epoch 00082: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.2255 - val_acc: 0.9529\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9941\n",
      "Epoch 00083: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0201 - acc: 0.9940 - val_loss: 0.2500 - val_acc: 0.9511\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9939\n",
      "Epoch 00084: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0208 - acc: 0.9938 - val_loss: 0.2856 - val_acc: 0.9427\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 00085: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0147 - acc: 0.9953 - val_loss: 0.2145 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9964\n",
      "Epoch 00086: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0139 - acc: 0.9963 - val_loss: 0.2705 - val_acc: 0.9471\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9946\n",
      "Epoch 00087: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0174 - acc: 0.9946 - val_loss: 0.2580 - val_acc: 0.9525\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 00088: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0089 - acc: 0.9976 - val_loss: 0.2350 - val_acc: 0.9511\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9924\n",
      "Epoch 00089: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0235 - acc: 0.9924 - val_loss: 0.2278 - val_acc: 0.9539\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9954\n",
      "Epoch 00090: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0161 - acc: 0.9954 - val_loss: 0.2219 - val_acc: 0.9534\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9966\n",
      "Epoch 00091: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0116 - acc: 0.9966 - val_loss: 0.2327 - val_acc: 0.9522\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00092: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2235 - val_acc: 0.9548\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 00093: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0180 - acc: 0.9943 - val_loss: 0.2364 - val_acc: 0.9553\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9969\n",
      "Epoch 00094: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.2584 - val_acc: 0.9515\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9978\n",
      "Epoch 00095: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0087 - acc: 0.9978 - val_loss: 0.3098 - val_acc: 0.9408\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9967\n",
      "Epoch 00096: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0122 - acc: 0.9967 - val_loss: 0.2899 - val_acc: 0.9394\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9935\n",
      "Epoch 00097: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0206 - acc: 0.9935 - val_loss: 0.2486 - val_acc: 0.9509\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00098: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0121 - acc: 0.9966 - val_loss: 0.2398 - val_acc: 0.9536\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9941\n",
      "Epoch 00099: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0194 - acc: 0.9941 - val_loss: 0.2299 - val_acc: 0.9555\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9985\n",
      "Epoch 00100: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0062 - acc: 0.9985 - val_loss: 0.2625 - val_acc: 0.9520\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 00101: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.2608 - val_acc: 0.9490\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9937\n",
      "Epoch 00102: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0208 - acc: 0.9937 - val_loss: 0.2385 - val_acc: 0.9550\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9939\n",
      "Epoch 00103: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0205 - acc: 0.9939 - val_loss: 0.2349 - val_acc: 0.9525\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9969\n",
      "Epoch 00104: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.2326 - val_acc: 0.9529\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9961\n",
      "Epoch 00105: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0129 - acc: 0.9961 - val_loss: 0.2198 - val_acc: 0.9560\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9957\n",
      "Epoch 00106: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0140 - acc: 0.9956 - val_loss: 0.2497 - val_acc: 0.9506\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.2327 - acc: 0.9427\n",
      "Loss: 0.23270402601837864 Accuracy: 0.9426791\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4316 - acc: 0.5655\n",
      "Epoch 00001: val_loss improved from inf to 0.67747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/001-0.6775.hdf5\n",
      "36805/36805 [==============================] - 321s 9ms/sample - loss: 1.4317 - acc: 0.5654 - val_loss: 0.6775 - val_acc: 0.8027\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8309\n",
      "Epoch 00002: val_loss improved from 0.67747 to 0.44925, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/002-0.4493.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.5784 - acc: 0.8309 - val_loss: 0.4493 - val_acc: 0.8642\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8843\n",
      "Epoch 00003: val_loss improved from 0.44925 to 0.30279, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/003-0.3028.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.3939 - acc: 0.8843 - val_loss: 0.3028 - val_acc: 0.9087\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9113\n",
      "Epoch 00004: val_loss did not improve from 0.30279\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.2990 - acc: 0.9113 - val_loss: 0.3044 - val_acc: 0.9108\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9208\n",
      "Epoch 00005: val_loss improved from 0.30279 to 0.22400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/005-0.2240.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.2620 - acc: 0.9207 - val_loss: 0.2240 - val_acc: 0.9331\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9356\n",
      "Epoch 00006: val_loss improved from 0.22400 to 0.21019, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/006-0.2102.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.2147 - acc: 0.9356 - val_loss: 0.2102 - val_acc: 0.9373\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9408\n",
      "Epoch 00007: val_loss improved from 0.21019 to 0.20261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/007-0.2026.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1909 - acc: 0.9407 - val_loss: 0.2026 - val_acc: 0.9406\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9482\n",
      "Epoch 00008: val_loss did not improve from 0.20261\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1684 - acc: 0.9481 - val_loss: 0.2040 - val_acc: 0.9383\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9535\n",
      "Epoch 00009: val_loss did not improve from 0.20261\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1542 - acc: 0.9535 - val_loss: 0.2307 - val_acc: 0.9278\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9643\n",
      "Epoch 00010: val_loss improved from 0.20261 to 0.17211, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/010-0.1721.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.1213 - acc: 0.9643 - val_loss: 0.1721 - val_acc: 0.9527\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9670\n",
      "Epoch 00011: val_loss did not improve from 0.17211\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1104 - acc: 0.9670 - val_loss: 0.3705 - val_acc: 0.8952\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9679\n",
      "Epoch 00012: val_loss improved from 0.17211 to 0.16633, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/012-0.1663.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1071 - acc: 0.9679 - val_loss: 0.1663 - val_acc: 0.9567\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9664\n",
      "Epoch 00013: val_loss improved from 0.16633 to 0.15237, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/013-0.1524.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.1070 - acc: 0.9664 - val_loss: 0.1524 - val_acc: 0.9564\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9710\n",
      "Epoch 00014: val_loss did not improve from 0.15237\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0960 - acc: 0.9709 - val_loss: 0.1966 - val_acc: 0.9436\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9731\n",
      "Epoch 00015: val_loss did not improve from 0.15237\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0883 - acc: 0.9731 - val_loss: 0.1536 - val_acc: 0.9562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9810\n",
      "Epoch 00016: val_loss did not improve from 0.15237\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0662 - acc: 0.9810 - val_loss: 0.1746 - val_acc: 0.9571\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9781\n",
      "Epoch 00017: val_loss did not improve from 0.15237\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0719 - acc: 0.9781 - val_loss: 0.1901 - val_acc: 0.9469\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9834\n",
      "Epoch 00018: val_loss did not improve from 0.15237\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0580 - acc: 0.9833 - val_loss: 0.1639 - val_acc: 0.9564\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9784\n",
      "Epoch 00019: val_loss improved from 0.15237 to 0.14173, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/019-0.1417.hdf5\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0710 - acc: 0.9784 - val_loss: 0.1417 - val_acc: 0.9611\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9875\n",
      "Epoch 00020: val_loss did not improve from 0.14173\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0437 - acc: 0.9875 - val_loss: 0.2011 - val_acc: 0.9474\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9869\n",
      "Epoch 00021: val_loss did not improve from 0.14173\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0456 - acc: 0.9869 - val_loss: 0.1473 - val_acc: 0.9592\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9874\n",
      "Epoch 00022: val_loss did not improve from 0.14173\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0445 - acc: 0.9873 - val_loss: 0.1529 - val_acc: 0.9567\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9823\n",
      "Epoch 00023: val_loss improved from 0.14173 to 0.14005, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/023-0.1400.hdf5\n",
      "36805/36805 [==============================] - 305s 8ms/sample - loss: 0.0580 - acc: 0.9823 - val_loss: 0.1400 - val_acc: 0.9658\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9870\n",
      "Epoch 00024: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0449 - acc: 0.9870 - val_loss: 0.1506 - val_acc: 0.9609\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9893\n",
      "Epoch 00025: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0355 - acc: 0.9893 - val_loss: 0.1442 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9932\n",
      "Epoch 00026: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0265 - acc: 0.9931 - val_loss: 0.1588 - val_acc: 0.9599\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9835\n",
      "Epoch 00027: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0519 - acc: 0.9835 - val_loss: 0.1443 - val_acc: 0.9597\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9926\n",
      "Epoch 00028: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0267 - acc: 0.9926 - val_loss: 0.1446 - val_acc: 0.9630\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9910\n",
      "Epoch 00029: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0315 - acc: 0.9909 - val_loss: 0.1616 - val_acc: 0.9588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9908\n",
      "Epoch 00030: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0324 - acc: 0.9908 - val_loss: 0.1532 - val_acc: 0.9637\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9944\n",
      "Epoch 00031: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0202 - acc: 0.9944 - val_loss: 0.1469 - val_acc: 0.9630\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9952\n",
      "Epoch 00032: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0185 - acc: 0.9951 - val_loss: 0.1666 - val_acc: 0.9588\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9872\n",
      "Epoch 00033: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.1856 - val_acc: 0.9492\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9923\n",
      "Epoch 00034: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0267 - acc: 0.9923 - val_loss: 0.1454 - val_acc: 0.9618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9962\n",
      "Epoch 00035: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0153 - acc: 0.9963 - val_loss: 0.1510 - val_acc: 0.9644\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 00036: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0223 - acc: 0.9930 - val_loss: 0.1639 - val_acc: 0.9613\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9937\n",
      "Epoch 00037: val_loss did not improve from 0.14005\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0219 - acc: 0.9937 - val_loss: 0.1702 - val_acc: 0.9557\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9899\n",
      "Epoch 00038: val_loss improved from 0.14005 to 0.12996, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/038-0.1300.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0328 - acc: 0.9899 - val_loss: 0.1300 - val_acc: 0.9676\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9938\n",
      "Epoch 00039: val_loss did not improve from 0.12996\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0204 - acc: 0.9938 - val_loss: 0.1630 - val_acc: 0.9632\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9974\n",
      "Epoch 00040: val_loss did not improve from 0.12996\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0113 - acc: 0.9973 - val_loss: 0.1386 - val_acc: 0.9665\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9917\n",
      "Epoch 00041: val_loss improved from 0.12996 to 0.12957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/041-0.1296.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0267 - acc: 0.9917 - val_loss: 0.1296 - val_acc: 0.9662\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9974\n",
      "Epoch 00042: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0107 - acc: 0.9974 - val_loss: 0.1435 - val_acc: 0.9644\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9973\n",
      "Epoch 00043: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0107 - acc: 0.9973 - val_loss: 0.1658 - val_acc: 0.9555\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9951\n",
      "Epoch 00044: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0183 - acc: 0.9951 - val_loss: 0.1955 - val_acc: 0.9539\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9961\n",
      "Epoch 00045: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0149 - acc: 0.9961 - val_loss: 0.1785 - val_acc: 0.9562\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9906\n",
      "Epoch 00046: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0314 - acc: 0.9905 - val_loss: 0.1963 - val_acc: 0.9583\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9927\n",
      "Epoch 00047: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0253 - acc: 0.9927 - val_loss: 0.1421 - val_acc: 0.9648\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9960\n",
      "Epoch 00048: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0144 - acc: 0.9960 - val_loss: 0.1366 - val_acc: 0.9672\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9940\n",
      "Epoch 00049: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.1691 - val_acc: 0.9639\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9949\n",
      "Epoch 00050: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0183 - acc: 0.9948 - val_loss: 0.1483 - val_acc: 0.9630\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9928\n",
      "Epoch 00051: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.1514 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00052: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.1368 - val_acc: 0.9693\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9936\n",
      "Epoch 00053: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0210 - acc: 0.9936 - val_loss: 0.1296 - val_acc: 0.9679\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0160 - acc: 0.9955 - val_loss: 0.1362 - val_acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.1574 - val_acc: 0.9632\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9942\n",
      "Epoch 00056: val_loss did not improve from 0.12957\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.1359 - val_acc: 0.9683\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9970\n",
      "Epoch 00057: val_loss improved from 0.12957 to 0.11967, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv_checkpoint/057-0.1197.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0112 - acc: 0.9970 - val_loss: 0.1197 - val_acc: 0.9706\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9985\n",
      "Epoch 00058: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0070 - acc: 0.9985 - val_loss: 0.1667 - val_acc: 0.9583\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9937\n",
      "Epoch 00059: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0214 - acc: 0.9936 - val_loss: 0.1573 - val_acc: 0.9620\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9925\n",
      "Epoch 00060: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0229 - acc: 0.9925 - val_loss: 0.1339 - val_acc: 0.9639\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 00061: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.1233 - val_acc: 0.9688\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
      "Epoch 00062: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1525 - val_acc: 0.9634\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9951\n",
      "Epoch 00063: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0165 - acc: 0.9951 - val_loss: 0.1264 - val_acc: 0.9672\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9989\n",
      "Epoch 00064: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0053 - acc: 0.9989 - val_loss: 0.1296 - val_acc: 0.9676\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 00065: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1531 - val_acc: 0.9660\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9932\n",
      "Epoch 00066: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0202 - acc: 0.9932 - val_loss: 0.1338 - val_acc: 0.9669\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 00067: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.1399 - val_acc: 0.9634\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9994\n",
      "Epoch 00068: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0041 - acc: 0.9994 - val_loss: 0.1213 - val_acc: 0.9709\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00069: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0044 - acc: 0.9989 - val_loss: 0.1685 - val_acc: 0.9595\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9945\n",
      "Epoch 00070: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0179 - acc: 0.9945 - val_loss: 0.1393 - val_acc: 0.9681\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9966\n",
      "Epoch 00071: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0107 - acc: 0.9966 - val_loss: 0.1629 - val_acc: 0.9644\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9945\n",
      "Epoch 00072: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0167 - acc: 0.9945 - val_loss: 0.1410 - val_acc: 0.9706\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.2068 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9956\n",
      "Epoch 00074: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0165 - acc: 0.9955 - val_loss: 0.1469 - val_acc: 0.9697\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9948\n",
      "Epoch 00075: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0176 - acc: 0.9948 - val_loss: 0.1343 - val_acc: 0.9674\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.1319 - val_acc: 0.9716\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00077: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0064 - acc: 0.9985 - val_loss: 0.1406 - val_acc: 0.9679\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9962\n",
      "Epoch 00078: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1377 - val_acc: 0.9711\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 00079: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0146 - acc: 0.9954 - val_loss: 0.1378 - val_acc: 0.9688\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00080: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.1424 - val_acc: 0.9686\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00081: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0044 - acc: 0.9991 - val_loss: 0.1398 - val_acc: 0.9695\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00082: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1700 - val_acc: 0.9616\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00083: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1554 - val_acc: 0.9665\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9968\n",
      "Epoch 00084: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0103 - acc: 0.9968 - val_loss: 0.1419 - val_acc: 0.9686\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00085: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1402 - val_acc: 0.9676\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00086: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.1656 - val_acc: 0.9634\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 00087: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1454 - val_acc: 0.9681\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9949\n",
      "Epoch 00088: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0185 - acc: 0.9949 - val_loss: 0.1518 - val_acc: 0.9683\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00089: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1299 - val_acc: 0.9711\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00090: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1303 - val_acc: 0.9695\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00091: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1242 - val_acc: 0.9713\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 00092: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1631 - val_acc: 0.9665\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 00093: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.1413 - val_acc: 0.9716\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9952\n",
      "Epoch 00094: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0147 - acc: 0.9951 - val_loss: 0.1353 - val_acc: 0.9704\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9971\n",
      "Epoch 00095: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0110 - acc: 0.9970 - val_loss: 0.1357 - val_acc: 0.9690\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 00096: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0127 - acc: 0.9962 - val_loss: 0.1323 - val_acc: 0.9706\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00097: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1527 - val_acc: 0.9683\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9969\n",
      "Epoch 00098: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0108 - acc: 0.9969 - val_loss: 0.1227 - val_acc: 0.9727\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00099: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1299 - val_acc: 0.9716\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00100: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1493 - val_acc: 0.9658\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 00101: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.1396 - val_acc: 0.9690\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 00102: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2436 - val_acc: 0.9546\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 00103: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0083 - acc: 0.9977 - val_loss: 0.1257 - val_acc: 0.9716\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 00104: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.1564 - val_acc: 0.9672\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 00105: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 304s 8ms/sample - loss: 0.0115 - acc: 0.9965 - val_loss: 0.1300 - val_acc: 0.9690\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00106: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0033 - acc: 0.9994 - val_loss: 0.1351 - val_acc: 0.9716\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00107: val_loss did not improve from 0.11967\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.1534 - val_acc: 0.9693\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 17s 4ms/sample - loss: 0.1532 - acc: 0.9597\n",
      "Loss: 0.1532023442385434 Accuracy: 0.9597092\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6484\n",
      "Epoch 00001: val_loss improved from inf to 0.43703, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/001-0.4370.hdf5\n",
      "36805/36805 [==============================] - 327s 9ms/sample - loss: 1.1167 - acc: 0.6484 - val_loss: 0.4370 - val_acc: 0.8591\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8667\n",
      "Epoch 00002: val_loss improved from 0.43703 to 0.31721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/002-0.3172.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.4238 - acc: 0.8666 - val_loss: 0.3172 - val_acc: 0.9038\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9029\n",
      "Epoch 00003: val_loss improved from 0.31721 to 0.27814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/003-0.2781.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.2994 - acc: 0.9029 - val_loss: 0.2781 - val_acc: 0.9147\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9207\n",
      "Epoch 00004: val_loss improved from 0.27814 to 0.19784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/004-0.1978.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.2483 - acc: 0.9206 - val_loss: 0.1978 - val_acc: 0.9406\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9425\n",
      "Epoch 00005: val_loss improved from 0.19784 to 0.18240, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/005-0.1824.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1817 - acc: 0.9425 - val_loss: 0.1824 - val_acc: 0.9467\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9496\n",
      "Epoch 00006: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1566 - acc: 0.9497 - val_loss: 0.1887 - val_acc: 0.9411\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9580\n",
      "Epoch 00007: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.1281 - acc: 0.9580 - val_loss: 0.1939 - val_acc: 0.9455\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9648\n",
      "Epoch 00008: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1100 - acc: 0.9647 - val_loss: 0.2901 - val_acc: 0.9159\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9623\n",
      "Epoch 00009: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1146 - acc: 0.9623 - val_loss: 0.1948 - val_acc: 0.9411\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9642\n",
      "Epoch 00010: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1079 - acc: 0.9642 - val_loss: 0.1851 - val_acc: 0.9455\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9735\n",
      "Epoch 00011: val_loss did not improve from 0.18240\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0832 - acc: 0.9735 - val_loss: 0.2091 - val_acc: 0.9366\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9660\n",
      "Epoch 00012: val_loss improved from 0.18240 to 0.16672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/012-0.1667.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.1042 - acc: 0.9660 - val_loss: 0.1667 - val_acc: 0.9520\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9837\n",
      "Epoch 00013: val_loss improved from 0.16672 to 0.14980, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/013-0.1498.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0550 - acc: 0.9837 - val_loss: 0.1498 - val_acc: 0.9576\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9754\n",
      "Epoch 00014: val_loss did not improve from 0.14980\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0774 - acc: 0.9754 - val_loss: 0.2127 - val_acc: 0.9399\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9849\n",
      "Epoch 00015: val_loss improved from 0.14980 to 0.13963, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/015-0.1396.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0494 - acc: 0.9849 - val_loss: 0.1396 - val_acc: 0.9611\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9790\n",
      "Epoch 00016: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0633 - acc: 0.9790 - val_loss: 0.1405 - val_acc: 0.9606\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9879\n",
      "Epoch 00017: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0403 - acc: 0.9878 - val_loss: 0.2010 - val_acc: 0.9471\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9731\n",
      "Epoch 00018: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0808 - acc: 0.9731 - val_loss: 0.1502 - val_acc: 0.9599\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9896\n",
      "Epoch 00019: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0356 - acc: 0.9896 - val_loss: 0.1478 - val_acc: 0.9581\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9838\n",
      "Epoch 00020: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0502 - acc: 0.9838 - val_loss: 0.1465 - val_acc: 0.9604\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9903\n",
      "Epoch 00021: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0321 - acc: 0.9903 - val_loss: 0.1473 - val_acc: 0.9609\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9919\n",
      "Epoch 00022: val_loss did not improve from 0.13963\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0273 - acc: 0.9918 - val_loss: 0.1472 - val_acc: 0.9609\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9844\n",
      "Epoch 00023: val_loss improved from 0.13963 to 0.13458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/023-0.1346.hdf5\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0488 - acc: 0.9844 - val_loss: 0.1346 - val_acc: 0.9646\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00024: val_loss improved from 0.13458 to 0.12397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/024-0.1240.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0258 - acc: 0.9920 - val_loss: 0.1240 - val_acc: 0.9669\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9894\n",
      "Epoch 00025: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0348 - acc: 0.9894 - val_loss: 0.1301 - val_acc: 0.9651\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 00026: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0309 - acc: 0.9901 - val_loss: 0.1464 - val_acc: 0.9604\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9889\n",
      "Epoch 00027: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0353 - acc: 0.9889 - val_loss: 0.1341 - val_acc: 0.9632\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9907\n",
      "Epoch 00028: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0297 - acc: 0.9906 - val_loss: 0.1243 - val_acc: 0.9648\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9935\n",
      "Epoch 00029: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0220 - acc: 0.9935 - val_loss: 0.1382 - val_acc: 0.9651\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9929\n",
      "Epoch 00030: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0239 - acc: 0.9929 - val_loss: 0.1478 - val_acc: 0.9641\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9893\n",
      "Epoch 00031: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0337 - acc: 0.9893 - val_loss: 0.1422 - val_acc: 0.9648\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9964\n",
      "Epoch 00032: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0138 - acc: 0.9964 - val_loss: 0.1614 - val_acc: 0.9599\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9901\n",
      "Epoch 00033: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.1943 - val_acc: 0.9488\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9914\n",
      "Epoch 00034: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0272 - acc: 0.9913 - val_loss: 0.1571 - val_acc: 0.9609\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9935\n",
      "Epoch 00035: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0213 - acc: 0.9935 - val_loss: 0.1253 - val_acc: 0.9662\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9908\n",
      "Epoch 00036: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0292 - acc: 0.9908 - val_loss: 0.1262 - val_acc: 0.9660\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9976\n",
      "Epoch 00037: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0093 - acc: 0.9976 - val_loss: 0.1260 - val_acc: 0.9704\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9952\n",
      "Epoch 00038: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0159 - acc: 0.9951 - val_loss: 0.1479 - val_acc: 0.9609\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9940\n",
      "Epoch 00039: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0199 - acc: 0.9940 - val_loss: 0.1243 - val_acc: 0.9679\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9981\n",
      "Epoch 00040: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0083 - acc: 0.9980 - val_loss: 0.1426 - val_acc: 0.9634\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0266 - acc: 0.9919 - val_loss: 0.1331 - val_acc: 0.9651\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 00042: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0135 - acc: 0.9956 - val_loss: 0.1489 - val_acc: 0.9641\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9935\n",
      "Epoch 00043: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0199 - acc: 0.9935 - val_loss: 0.1380 - val_acc: 0.9658\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9973\n",
      "Epoch 00044: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.1862 - val_acc: 0.9583\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9912\n",
      "Epoch 00045: val_loss did not improve from 0.12397\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0283 - acc: 0.9912 - val_loss: 0.1363 - val_acc: 0.9669\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9945\n",
      "Epoch 00046: val_loss improved from 0.12397 to 0.12016, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/046-0.1202.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0177 - acc: 0.9945 - val_loss: 0.1202 - val_acc: 0.9706\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
      "Epoch 00047: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.1389 - val_acc: 0.9651\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9973\n",
      "Epoch 00048: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0096 - acc: 0.9973 - val_loss: 0.1776 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9931\n",
      "Epoch 00049: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0233 - acc: 0.9931 - val_loss: 0.1237 - val_acc: 0.9695\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00050: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.1390 - val_acc: 0.9658\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0169 - acc: 0.9947 - val_loss: 0.1481 - val_acc: 0.9630\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9926\n",
      "Epoch 00052: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0230 - acc: 0.9926 - val_loss: 0.1468 - val_acc: 0.9679\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00053: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1296 - val_acc: 0.9690\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00054: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1483 - val_acc: 0.9644\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9952\n",
      "Epoch 00055: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0151 - acc: 0.9952 - val_loss: 0.1532 - val_acc: 0.9655\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9968\n",
      "Epoch 00056: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0115 - acc: 0.9968 - val_loss: 0.1553 - val_acc: 0.9648\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00057: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.1843 - val_acc: 0.9574\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0128 - acc: 0.9958 - val_loss: 0.1728 - val_acc: 0.9646\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0075 - acc: 0.9978 - val_loss: 0.1760 - val_acc: 0.9618\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0157 - acc: 0.9955 - val_loss: 0.1459 - val_acc: 0.9672\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9938\n",
      "Epoch 00061: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0214 - acc: 0.9938 - val_loss: 0.1478 - val_acc: 0.9669\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9959\n",
      "Epoch 00062: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1396 - val_acc: 0.9690\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9974\n",
      "Epoch 00063: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0084 - acc: 0.9974 - val_loss: 0.1303 - val_acc: 0.9674\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00064: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.1246 - val_acc: 0.9693\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00065: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1614 - val_acc: 0.9651\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00066: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0103 - acc: 0.9969 - val_loss: 0.1258 - val_acc: 0.9700\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9946\n",
      "Epoch 00067: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 0.1286 - val_acc: 0.9697\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00068: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1352 - val_acc: 0.9695\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1528 - val_acc: 0.9667\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00070: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1593 - val_acc: 0.9679\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9972\n",
      "Epoch 00071: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.1234 - val_acc: 0.9727\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 00072: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.1272 - val_acc: 0.9723\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9988\n",
      "Epoch 00073: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.1498 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0168 - acc: 0.9945 - val_loss: 0.1414 - val_acc: 0.9667\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
      "Epoch 00075: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1330 - val_acc: 0.9706\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 00076: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0048 - acc: 0.9984 - val_loss: 0.1475 - val_acc: 0.9674\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0108 - acc: 0.9963 - val_loss: 0.1414 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00078: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1493 - val_acc: 0.9700\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9984\n",
      "Epoch 00079: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.1646 - val_acc: 0.9648\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9960\n",
      "Epoch 00080: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 303s 8ms/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.1385 - val_acc: 0.9679\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9967\n",
      "Epoch 00081: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 302s 8ms/sample - loss: 0.0106 - acc: 0.9967 - val_loss: 0.1281 - val_acc: 0.9688\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9975\n",
      "Epoch 00082: val_loss did not improve from 0.12016\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.1445 - val_acc: 0.9669\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00083: val_loss improved from 0.12016 to 0.11328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/083-0.1133.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1133 - val_acc: 0.9713\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 00084: val_loss improved from 0.11328 to 0.11280, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/084-0.1128.hdf5\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.1128 - val_acc: 0.9702\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00085: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1588 - val_acc: 0.9665\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9963\n",
      "Epoch 00086: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0119 - acc: 0.9963 - val_loss: 0.1352 - val_acc: 0.9679\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 00087: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0043 - acc: 0.9987 - val_loss: 0.1391 - val_acc: 0.9690\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 00088: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0049 - acc: 0.9985 - val_loss: 0.1706 - val_acc: 0.9660\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9971\n",
      "Epoch 00089: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0085 - acc: 0.9971 - val_loss: 0.1682 - val_acc: 0.9632\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9942\n",
      "Epoch 00090: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0164 - acc: 0.9942 - val_loss: 0.1268 - val_acc: 0.9683\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00091: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1284 - val_acc: 0.9693\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9971\n",
      "Epoch 00092: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0088 - acc: 0.9971 - val_loss: 0.1404 - val_acc: 0.9681\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9985\n",
      "Epoch 00093: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0048 - acc: 0.9985 - val_loss: 0.1292 - val_acc: 0.9695\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9970\n",
      "Epoch 00094: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.1411 - val_acc: 0.9713\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00095: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1425 - val_acc: 0.9683\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9972\n",
      "Epoch 00096: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.1565 - val_acc: 0.9648\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 00097: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.1487 - val_acc: 0.9676\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9986\n",
      "Epoch 00098: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0042 - acc: 0.9986 - val_loss: 0.1908 - val_acc: 0.9588\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9972\n",
      "Epoch 00099: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.1472 - val_acc: 0.9690\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9974\n",
      "Epoch 00100: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0074 - acc: 0.9974 - val_loss: 0.1795 - val_acc: 0.9576\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
      "Epoch 00101: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.1876 - val_acc: 0.9623\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 00102: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0165 - acc: 0.9947 - val_loss: 0.1391 - val_acc: 0.9690\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00103: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1249 - val_acc: 0.9723\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00104: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.1365 - val_acc: 0.9711\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00105: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1465 - val_acc: 0.9716\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 00106: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1493 - val_acc: 0.9709\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00107: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1360 - val_acc: 0.9711\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00108: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.1534 - val_acc: 0.9690\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 00109: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1285 - val_acc: 0.9730\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00110: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1411 - val_acc: 0.9711\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00111: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1394 - val_acc: 0.9706\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 00112: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1574 - val_acc: 0.9672\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9976\n",
      "Epoch 00113: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1483 - val_acc: 0.9672\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9966\n",
      "Epoch 00114: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0107 - acc: 0.9966 - val_loss: 0.1197 - val_acc: 0.9718\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00115: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1292 - val_acc: 0.9697\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979\n",
      "Epoch 00116: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0078 - acc: 0.9979 - val_loss: 0.1329 - val_acc: 0.9697\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00117: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1255 - val_acc: 0.9718\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9978\n",
      "Epoch 00118: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0082 - acc: 0.9977 - val_loss: 0.1267 - val_acc: 0.9713\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9972\n",
      "Epoch 00119: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.1388 - val_acc: 0.9706\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00120: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 00121: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1278 - val_acc: 0.9704\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00122: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0119 - acc: 0.9964 - val_loss: 0.1286 - val_acc: 0.9709\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00123: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0125 - acc: 0.9962 - val_loss: 0.1339 - val_acc: 0.9686\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00124: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1199 - val_acc: 0.9732\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00125: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1277 - val_acc: 0.9709\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 00126: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 0.1580 - val_acc: 0.9669\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00127: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.1379 - val_acc: 0.9720\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9974\n",
      "Epoch 00128: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0084 - acc: 0.9974 - val_loss: 0.1424 - val_acc: 0.9697\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00129: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1534 - val_acc: 0.9674\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9974\n",
      "Epoch 00130: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0074 - acc: 0.9974 - val_loss: 0.1436 - val_acc: 0.9690\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00131: val_loss did not improve from 0.11280\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0072 - acc: 0.9976 - val_loss: 0.1180 - val_acc: 0.9716\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00132: val_loss improved from 0.11280 to 0.11231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv_checkpoint/132-0.1123.hdf5\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1123 - val_acc: 0.9737\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
      "Epoch 00133: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1409 - val_acc: 0.9695\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00134: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1246 - val_acc: 0.9725\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990\n",
      "Epoch 00135: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1550 - val_acc: 0.9662\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00136: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1222 - val_acc: 0.9718\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9981\n",
      "Epoch 00137: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0060 - acc: 0.9980 - val_loss: 0.1660 - val_acc: 0.9662\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9978\n",
      "Epoch 00138: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0066 - acc: 0.9977 - val_loss: 0.1316 - val_acc: 0.9704\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 00139: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 310s 8ms/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1419 - val_acc: 0.9695\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 00140: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.1531 - val_acc: 0.9669\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9980\n",
      "Epoch 00141: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1265 - val_acc: 0.9709\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00142: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1287 - val_acc: 0.9718\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9970\n",
      "Epoch 00143: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 0.1160 - val_acc: 0.9737\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00144: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.1351 - val_acc: 0.9725\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00145: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1449 - val_acc: 0.9706\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 00146: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1365 - val_acc: 0.9706\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00147: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1444 - val_acc: 0.9716\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9972\n",
      "Epoch 00148: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.1970 - val_acc: 0.9574\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9979\n",
      "Epoch 00149: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0056 - acc: 0.9979 - val_loss: 0.1549 - val_acc: 0.9662\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00150: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0065 - acc: 0.9979 - val_loss: 0.1310 - val_acc: 0.9718\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00151: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1647 - val_acc: 0.9655\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9985\n",
      "Epoch 00152: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1331 - val_acc: 0.9716\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9975\n",
      "Epoch 00153: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0078 - acc: 0.9974 - val_loss: 0.1422 - val_acc: 0.9711\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9977\n",
      "Epoch 00154: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0081 - acc: 0.9977 - val_loss: 0.1219 - val_acc: 0.9744\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00155: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.1345 - val_acc: 0.9732\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00156: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1266 - val_acc: 0.9760\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00157: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1300 - val_acc: 0.9739\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 00158: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0043 - acc: 0.9987 - val_loss: 0.1472 - val_acc: 0.9716\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 00159: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1689 - val_acc: 0.9655\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00160: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1395 - val_acc: 0.9709\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9975\n",
      "Epoch 00161: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0085 - acc: 0.9975 - val_loss: 0.1426 - val_acc: 0.9688\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 00162: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.1360 - val_acc: 0.9725\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 00163: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0030 - acc: 0.9990 - val_loss: 0.2048 - val_acc: 0.9637\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
      "Epoch 00164: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0081 - acc: 0.9975 - val_loss: 0.1427 - val_acc: 0.9716\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9975\n",
      "Epoch 00165: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.1369 - val_acc: 0.9723\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00166: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1305 - val_acc: 0.9734\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00167: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1351 - val_acc: 0.9734\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00168: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0026 - acc: 0.9992 - val_loss: 0.1519 - val_acc: 0.9716\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9980\n",
      "Epoch 00169: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 309s 8ms/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 0.1427 - val_acc: 0.9713\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00170: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.1567 - val_acc: 0.9690\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 00171: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.1575 - val_acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 00172: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 308s 8ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1451 - val_acc: 0.9688\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00173: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1360 - val_acc: 0.9730\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9983\n",
      "Epoch 00174: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0066 - acc: 0.9983 - val_loss: 0.1337 - val_acc: 0.9706\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00175: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1386 - val_acc: 0.9695\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 00176: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1448 - val_acc: 0.9709\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00177: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1557 - val_acc: 0.9679\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9980\n",
      "Epoch 00178: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0069 - acc: 0.9980 - val_loss: 0.1540 - val_acc: 0.9702\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00179: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 306s 8ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1379 - val_acc: 0.9730\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00180: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1760 - val_acc: 0.9686\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 00181: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0071 - acc: 0.9980 - val_loss: 0.2088 - val_acc: 0.9618\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 00182: val_loss did not improve from 0.11231\n",
      "36805/36805 [==============================] - 307s 8ms/sample - loss: 0.0063 - acc: 0.9983 - val_loss: 0.1466 - val_acc: 0.9686\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.1473 - acc: 0.9639\n",
      "Loss: 0.14725198276757542 Accuracy: 0.96386296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_128_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1865104     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,865,104\n",
      "Trainable params: 1,711,504\n",
      "Non-trainable params: 153,600\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 17s 4ms/sample - loss: 1.4796 - acc: 0.6258\n",
      "Loss: 1.4795852784427155 Accuracy: 0.62575287\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_98_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1655696     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,655,696\n",
      "Trainable params: 1,551,760\n",
      "Non-trainable params: 103,936\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 1.1398 - acc: 0.7034\n",
      "Loss: 1.1398343253110923 Accuracy: 0.7034268\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_108_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1375632     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,375,632\n",
      "Trainable params: 1,338,256\n",
      "Non-trainable params: 37,376\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.5696 - acc: 0.8663\n",
      "Loss: 0.5696179456180996 Accuracy: 0.8662513\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_120_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1546128     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,546,128\n",
      "Trainable params: 1,530,256\n",
      "Non-trainable params: 15,872\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.2327 - acc: 0.9427\n",
      "Loss: 0.23270402601837864 Accuracy: 0.9426791\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_134_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           1870224     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,870,224\n",
      "Trainable params: 1,860,496\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 0.1532 - acc: 0.9597\n",
      "Loss: 0.1532023442385434 Accuracy: 0.9597092\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_150_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           3039632     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,039,632\n",
      "Trainable params: 3,029,392\n",
      "Non-trainable params: 10,240\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1473 - acc: 0.9639\n",
      "Loss: 0.14725198276757542 Accuracy: 0.96386296\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_128_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_90_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_90_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1865104     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,865,104\n",
      "Trainable params: 1,711,504\n",
      "Non-trainable params: 153,600\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 19s 4ms/sample - loss: 2.9164 - acc: 0.6314\n",
      "Loss: 2.9164492518978573 Accuracy: 0.63136035\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_98_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_98_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1655696     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,655,696\n",
      "Trainable params: 1,551,760\n",
      "Non-trainable params: 103,936\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 2.0074 - acc: 0.7173\n",
      "Loss: 2.0073853999778613 Accuracy: 0.71734166\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_108_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_108_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1375632     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,375,632\n",
      "Trainable params: 1,338,256\n",
      "Non-trainable params: 37,376\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.7047 - acc: 0.8748\n",
      "Loss: 0.7046973497294439 Accuracy: 0.87476635\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_120_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_120_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1546128     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,546,128\n",
      "Trainable params: 1,530,256\n",
      "Non-trainable params: 15,872\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.2730 - acc: 0.9389\n",
      "Loss: 0.2730019376542601 Accuracy: 0.9389408\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_134_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_134_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           1870224     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,870,224\n",
      "Trainable params: 1,860,496\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 20s 4ms/sample - loss: 0.1799 - acc: 0.9605\n",
      "Loss: 0.17989624534375281 Accuracy: 0.96054\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_128_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_150_input (InputLayer)   (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_150_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 16)           3039632     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Concatenate)          (None, 16)           0           sequential_14[1][0]              \n",
      "                                                                 sequential_14[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,039,632\n",
      "Trainable params: 3,029,392\n",
      "Non-trainable params: 10,240\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.1591 - acc: 0.9670\n",
      "Loss: 0.15907619287407465 Accuracy: 0.9669782\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
