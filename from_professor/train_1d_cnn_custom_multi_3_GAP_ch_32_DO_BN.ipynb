{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 96)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1552        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,432\n",
      "Trainable params: 12,240\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1552        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,712\n",
      "Trainable params: 17,456\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2064        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,784\n",
      "Trainable params: 28,400\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 32)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 160)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 160)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2576        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,584\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 192)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           3088        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 71,408\n",
      "Trainable params: 70,768\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 192)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           3088        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 92,208\n",
      "Trainable params: 91,440\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.6701 - acc: 0.1436\n",
      "Epoch 00001: val_loss improved from inf to 2.53077, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/001-2.5308.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 2.6698 - acc: 0.1436 - val_loss: 2.5308 - val_acc: 0.2409\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4727 - acc: 0.2048\n",
      "Epoch 00002: val_loss improved from 2.53077 to 2.31038, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/002-2.3104.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 2.4727 - acc: 0.2048 - val_loss: 2.3104 - val_acc: 0.3585\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3659 - acc: 0.2392\n",
      "Epoch 00003: val_loss improved from 2.31038 to 2.22112, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/003-2.2211.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 2.3660 - acc: 0.2392 - val_loss: 2.2211 - val_acc: 0.3913\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2836 - acc: 0.2653\n",
      "Epoch 00004: val_loss improved from 2.22112 to 2.13999, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/004-2.1400.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 2.2837 - acc: 0.2652 - val_loss: 2.1400 - val_acc: 0.4286\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2126 - acc: 0.2930\n",
      "Epoch 00005: val_loss improved from 2.13999 to 2.06929, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/005-2.0693.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 2.2126 - acc: 0.2930 - val_loss: 2.0693 - val_acc: 0.4419\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1528 - acc: 0.3083\n",
      "Epoch 00006: val_loss improved from 2.06929 to 2.00440, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/006-2.0044.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 2.1529 - acc: 0.3083 - val_loss: 2.0044 - val_acc: 0.4575\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1062 - acc: 0.3224\n",
      "Epoch 00007: val_loss improved from 2.00440 to 1.94502, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/007-1.9450.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.1061 - acc: 0.3225 - val_loss: 1.9450 - val_acc: 0.4799\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0565 - acc: 0.3383\n",
      "Epoch 00008: val_loss improved from 1.94502 to 1.88557, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/008-1.8856.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 2.0565 - acc: 0.3383 - val_loss: 1.8856 - val_acc: 0.4887\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0113 - acc: 0.3535\n",
      "Epoch 00009: val_loss improved from 1.88557 to 1.84473, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/009-1.8447.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 2.0112 - acc: 0.3535 - val_loss: 1.8447 - val_acc: 0.4896\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9732 - acc: 0.3668\n",
      "Epoch 00010: val_loss improved from 1.84473 to 1.79977, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/010-1.7998.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.9734 - acc: 0.3667 - val_loss: 1.7998 - val_acc: 0.5071\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9405 - acc: 0.3738\n",
      "Epoch 00011: val_loss improved from 1.79977 to 1.75853, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/011-1.7585.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.9405 - acc: 0.3738 - val_loss: 1.7585 - val_acc: 0.4948\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9090 - acc: 0.3855\n",
      "Epoch 00012: val_loss improved from 1.75853 to 1.72727, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/012-1.7273.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.9085 - acc: 0.3858 - val_loss: 1.7273 - val_acc: 0.5351\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8735 - acc: 0.3961\n",
      "Epoch 00013: val_loss improved from 1.72727 to 1.71368, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/013-1.7137.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.8739 - acc: 0.3959 - val_loss: 1.7137 - val_acc: 0.4971\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8582 - acc: 0.4004\n",
      "Epoch 00014: val_loss improved from 1.71368 to 1.66734, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/014-1.6673.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.8581 - acc: 0.4005 - val_loss: 1.6673 - val_acc: 0.5455\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8323 - acc: 0.4098\n",
      "Epoch 00015: val_loss improved from 1.66734 to 1.65511, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/015-1.6551.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.8323 - acc: 0.4097 - val_loss: 1.6551 - val_acc: 0.5386\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8120 - acc: 0.4210\n",
      "Epoch 00016: val_loss improved from 1.65511 to 1.62315, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/016-1.6231.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.8121 - acc: 0.4209 - val_loss: 1.6231 - val_acc: 0.5507\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7888 - acc: 0.4271\n",
      "Epoch 00017: val_loss improved from 1.62315 to 1.59425, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/017-1.5942.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.7888 - acc: 0.4271 - val_loss: 1.5942 - val_acc: 0.5670\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7752 - acc: 0.4291\n",
      "Epoch 00018: val_loss did not improve from 1.59425\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.7753 - acc: 0.4291 - val_loss: 1.5975 - val_acc: 0.5320\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7568 - acc: 0.4364\n",
      "Epoch 00019: val_loss improved from 1.59425 to 1.55395, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/019-1.5539.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.7568 - acc: 0.4362 - val_loss: 1.5539 - val_acc: 0.5712\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7423 - acc: 0.4398\n",
      "Epoch 00020: val_loss improved from 1.55395 to 1.55017, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/020-1.5502.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.7424 - acc: 0.4398 - val_loss: 1.5502 - val_acc: 0.5558\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7260 - acc: 0.4482\n",
      "Epoch 00021: val_loss improved from 1.55017 to 1.52023, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/021-1.5202.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.7262 - acc: 0.4483 - val_loss: 1.5202 - val_acc: 0.5723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7160 - acc: 0.4512\n",
      "Epoch 00022: val_loss improved from 1.52023 to 1.50841, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/022-1.5084.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.7157 - acc: 0.4514 - val_loss: 1.5084 - val_acc: 0.5772\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7024 - acc: 0.4535\n",
      "Epoch 00023: val_loss improved from 1.50841 to 1.48821, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/023-1.4882.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.7024 - acc: 0.4535 - val_loss: 1.4882 - val_acc: 0.5872\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6926 - acc: 0.4608\n",
      "Epoch 00024: val_loss did not improve from 1.48821\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.6926 - acc: 0.4608 - val_loss: 1.5019 - val_acc: 0.5684\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6784 - acc: 0.4612\n",
      "Epoch 00025: val_loss improved from 1.48821 to 1.48263, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/025-1.4826.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.6784 - acc: 0.4611 - val_loss: 1.4826 - val_acc: 0.5763\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6675 - acc: 0.4655\n",
      "Epoch 00026: val_loss improved from 1.48263 to 1.46157, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/026-1.4616.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.6674 - acc: 0.4655 - val_loss: 1.4616 - val_acc: 0.5723\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6605 - acc: 0.4679\n",
      "Epoch 00027: val_loss improved from 1.46157 to 1.43864, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/027-1.4386.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.6606 - acc: 0.4677 - val_loss: 1.4386 - val_acc: 0.6000\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6473 - acc: 0.4728\n",
      "Epoch 00028: val_loss did not improve from 1.43864\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.6468 - acc: 0.4730 - val_loss: 1.4782 - val_acc: 0.5537\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6371 - acc: 0.4772\n",
      "Epoch 00029: val_loss improved from 1.43864 to 1.41611, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/029-1.4161.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.6368 - acc: 0.4774 - val_loss: 1.4161 - val_acc: 0.6105\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6282 - acc: 0.4792\n",
      "Epoch 00030: val_loss improved from 1.41611 to 1.40284, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/030-1.4028.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.6282 - acc: 0.4792 - val_loss: 1.4028 - val_acc: 0.6226\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6241 - acc: 0.4826\n",
      "Epoch 00031: val_loss improved from 1.40284 to 1.38350, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/031-1.3835.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.6242 - acc: 0.4826 - val_loss: 1.3835 - val_acc: 0.6171\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6113 - acc: 0.4866\n",
      "Epoch 00032: val_loss improved from 1.38350 to 1.38262, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/032-1.3826.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.6113 - acc: 0.4866 - val_loss: 1.3826 - val_acc: 0.6059\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6051 - acc: 0.4900\n",
      "Epoch 00033: val_loss did not improve from 1.38262\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.6052 - acc: 0.4899 - val_loss: 1.4737 - val_acc: 0.5271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5960 - acc: 0.4924\n",
      "Epoch 00034: val_loss did not improve from 1.38262\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.5960 - acc: 0.4925 - val_loss: 1.3862 - val_acc: 0.6103\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5865 - acc: 0.4922\n",
      "Epoch 00035: val_loss did not improve from 1.38262\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.5863 - acc: 0.4923 - val_loss: 1.4623 - val_acc: 0.5376\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5811 - acc: 0.4992\n",
      "Epoch 00036: val_loss improved from 1.38262 to 1.34113, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/036-1.3411.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.5810 - acc: 0.4992 - val_loss: 1.3411 - val_acc: 0.6350\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5724 - acc: 0.4990\n",
      "Epoch 00037: val_loss did not improve from 1.34113\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.5728 - acc: 0.4990 - val_loss: 1.3447 - val_acc: 0.6373\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5679 - acc: 0.5010\n",
      "Epoch 00038: val_loss did not improve from 1.34113\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.5679 - acc: 0.5010 - val_loss: 1.3450 - val_acc: 0.6268\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5595 - acc: 0.5011\n",
      "Epoch 00039: val_loss improved from 1.34113 to 1.33215, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/039-1.3321.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.5596 - acc: 0.5011 - val_loss: 1.3321 - val_acc: 0.6224\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5591 - acc: 0.5064\n",
      "Epoch 00040: val_loss improved from 1.33215 to 1.32175, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/040-1.3217.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.5592 - acc: 0.5063 - val_loss: 1.3217 - val_acc: 0.6403\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5538 - acc: 0.5064\n",
      "Epoch 00041: val_loss did not improve from 1.32175\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.5537 - acc: 0.5063 - val_loss: 1.3600 - val_acc: 0.5893\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5451 - acc: 0.5093\n",
      "Epoch 00042: val_loss did not improve from 1.32175\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.5450 - acc: 0.5092 - val_loss: 1.3287 - val_acc: 0.6189\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5347 - acc: 0.5112\n",
      "Epoch 00043: val_loss improved from 1.32175 to 1.30906, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/043-1.3091.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.5345 - acc: 0.5113 - val_loss: 1.3091 - val_acc: 0.6282\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5349 - acc: 0.5142\n",
      "Epoch 00044: val_loss improved from 1.30906 to 1.28688, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/044-1.2869.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.5348 - acc: 0.5143 - val_loss: 1.2869 - val_acc: 0.6485\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5270 - acc: 0.5141\n",
      "Epoch 00045: val_loss did not improve from 1.28688\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.5269 - acc: 0.5141 - val_loss: 1.3156 - val_acc: 0.6329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5229 - acc: 0.5172\n",
      "Epoch 00046: val_loss did not improve from 1.28688\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.5229 - acc: 0.5172 - val_loss: 1.4315 - val_acc: 0.5360\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5165 - acc: 0.5171\n",
      "Epoch 00047: val_loss improved from 1.28688 to 1.27106, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/047-1.2711.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.5164 - acc: 0.5171 - val_loss: 1.2711 - val_acc: 0.6564\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5147 - acc: 0.5198\n",
      "Epoch 00048: val_loss improved from 1.27106 to 1.26067, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/048-1.2607.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.5148 - acc: 0.5197 - val_loss: 1.2607 - val_acc: 0.6571\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5093 - acc: 0.5202\n",
      "Epoch 00049: val_loss did not improve from 1.26067\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.5092 - acc: 0.5202 - val_loss: 1.2838 - val_acc: 0.6436\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5046 - acc: 0.5230\n",
      "Epoch 00050: val_loss did not improve from 1.26067\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.5050 - acc: 0.5229 - val_loss: 1.2639 - val_acc: 0.6408\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4988 - acc: 0.5252\n",
      "Epoch 00051: val_loss did not improve from 1.26067\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.4992 - acc: 0.5251 - val_loss: 1.4293 - val_acc: 0.5495\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4914 - acc: 0.5252\n",
      "Epoch 00052: val_loss did not improve from 1.26067\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.4918 - acc: 0.5251 - val_loss: 1.2769 - val_acc: 0.6378\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4964 - acc: 0.5230\n",
      "Epoch 00053: val_loss improved from 1.26067 to 1.24650, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/053-1.2465.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4964 - acc: 0.5229 - val_loss: 1.2465 - val_acc: 0.6608\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4896 - acc: 0.5253\n",
      "Epoch 00054: val_loss improved from 1.24650 to 1.24419, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/054-1.2442.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4895 - acc: 0.5254 - val_loss: 1.2442 - val_acc: 0.6560\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4818 - acc: 0.5313\n",
      "Epoch 00055: val_loss improved from 1.24419 to 1.24151, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/055-1.2415.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.4818 - acc: 0.5313 - val_loss: 1.2415 - val_acc: 0.6452\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4784 - acc: 0.5309\n",
      "Epoch 00056: val_loss improved from 1.24151 to 1.24104, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/056-1.2410.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.4786 - acc: 0.5309 - val_loss: 1.2410 - val_acc: 0.6601\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4709 - acc: 0.5306\n",
      "Epoch 00057: val_loss did not improve from 1.24104\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.4715 - acc: 0.5304 - val_loss: 1.2861 - val_acc: 0.6219\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4758 - acc: 0.5340\n",
      "Epoch 00058: val_loss improved from 1.24104 to 1.22179, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/058-1.2218.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4758 - acc: 0.5340 - val_loss: 1.2218 - val_acc: 0.6685\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4647 - acc: 0.5349\n",
      "Epoch 00059: val_loss improved from 1.22179 to 1.21040, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/059-1.2104.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.4649 - acc: 0.5349 - val_loss: 1.2104 - val_acc: 0.6567\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4580 - acc: 0.5375\n",
      "Epoch 00060: val_loss improved from 1.21040 to 1.20886, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/060-1.2089.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4579 - acc: 0.5376 - val_loss: 1.2089 - val_acc: 0.6671\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4526 - acc: 0.5360\n",
      "Epoch 00061: val_loss did not improve from 1.20886\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4526 - acc: 0.5360 - val_loss: 1.3182 - val_acc: 0.5749\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4566 - acc: 0.5367\n",
      "Epoch 00062: val_loss did not improve from 1.20886\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.4567 - acc: 0.5366 - val_loss: 1.2307 - val_acc: 0.6329\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4498 - acc: 0.5427\n",
      "Epoch 00063: val_loss did not improve from 1.20886\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4499 - acc: 0.5428 - val_loss: 1.2307 - val_acc: 0.6448\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4506 - acc: 0.5438\n",
      "Epoch 00064: val_loss improved from 1.20886 to 1.19782, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/064-1.1978.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4511 - acc: 0.5437 - val_loss: 1.1978 - val_acc: 0.6678\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4480 - acc: 0.5407\n",
      "Epoch 00065: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4480 - acc: 0.5408 - val_loss: 1.3653 - val_acc: 0.5607\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4409 - acc: 0.5433\n",
      "Epoch 00066: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4408 - acc: 0.5434 - val_loss: 1.4371 - val_acc: 0.5213\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4378 - acc: 0.5444\n",
      "Epoch 00067: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.4378 - acc: 0.5444 - val_loss: 1.6695 - val_acc: 0.4468\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4359 - acc: 0.5466\n",
      "Epoch 00068: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.4357 - acc: 0.5467 - val_loss: 1.2352 - val_acc: 0.6478\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4324 - acc: 0.5481\n",
      "Epoch 00069: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4323 - acc: 0.5481 - val_loss: 1.1999 - val_acc: 0.6625\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4306 - acc: 0.5462\n",
      "Epoch 00070: val_loss did not improve from 1.19782\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.4305 - acc: 0.5463 - val_loss: 1.2123 - val_acc: 0.6536\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4240 - acc: 0.5516\n",
      "Epoch 00071: val_loss improved from 1.19782 to 1.18879, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/071-1.1888.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4238 - acc: 0.5518 - val_loss: 1.1888 - val_acc: 0.6606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4288 - acc: 0.5506\n",
      "Epoch 00072: val_loss improved from 1.18879 to 1.16473, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/072-1.1647.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.4284 - acc: 0.5506 - val_loss: 1.1647 - val_acc: 0.6692\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4192 - acc: 0.5485\n",
      "Epoch 00073: val_loss did not improve from 1.16473\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.4192 - acc: 0.5485 - val_loss: 1.2125 - val_acc: 0.6499\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4178 - acc: 0.5533\n",
      "Epoch 00074: val_loss did not improve from 1.16473\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4173 - acc: 0.5535 - val_loss: 1.2069 - val_acc: 0.6511\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4098 - acc: 0.5510\n",
      "Epoch 00075: val_loss improved from 1.16473 to 1.16104, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/075-1.1610.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4101 - acc: 0.5510 - val_loss: 1.1610 - val_acc: 0.6783\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4139 - acc: 0.5526\n",
      "Epoch 00076: val_loss did not improve from 1.16104\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4139 - acc: 0.5525 - val_loss: 1.1738 - val_acc: 0.6657\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4101 - acc: 0.5566\n",
      "Epoch 00077: val_loss improved from 1.16104 to 1.15920, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/077-1.1592.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.4102 - acc: 0.5566 - val_loss: 1.1592 - val_acc: 0.6678\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4039 - acc: 0.5553\n",
      "Epoch 00078: val_loss improved from 1.15920 to 1.15818, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/078-1.1582.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.4036 - acc: 0.5553 - val_loss: 1.1582 - val_acc: 0.6730\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4040 - acc: 0.5577\n",
      "Epoch 00079: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.4042 - acc: 0.5574 - val_loss: 1.1885 - val_acc: 0.6555\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3919 - acc: 0.5618\n",
      "Epoch 00080: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.3914 - acc: 0.5618 - val_loss: 1.2007 - val_acc: 0.6452\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3928 - acc: 0.5602\n",
      "Epoch 00081: val_loss improved from 1.15818 to 1.15524, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/081-1.1552.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3926 - acc: 0.5602 - val_loss: 1.1552 - val_acc: 0.6725\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3913 - acc: 0.5596\n",
      "Epoch 00082: val_loss improved from 1.15524 to 1.14720, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/082-1.1472.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3911 - acc: 0.5597 - val_loss: 1.1472 - val_acc: 0.6737\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3916 - acc: 0.5605\n",
      "Epoch 00083: val_loss improved from 1.14720 to 1.13620, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/083-1.1362.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3916 - acc: 0.5605 - val_loss: 1.1362 - val_acc: 0.6844\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3895 - acc: 0.5661\n",
      "Epoch 00084: val_loss did not improve from 1.13620\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3896 - acc: 0.5661 - val_loss: 1.1491 - val_acc: 0.6781\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3879 - acc: 0.5633\n",
      "Epoch 00085: val_loss did not improve from 1.13620\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3881 - acc: 0.5632 - val_loss: 1.1445 - val_acc: 0.6778\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3807 - acc: 0.5669\n",
      "Epoch 00086: val_loss did not improve from 1.13620\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3808 - acc: 0.5669 - val_loss: 1.1420 - val_acc: 0.6746\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3767 - acc: 0.5641\n",
      "Epoch 00087: val_loss did not improve from 1.13620\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3768 - acc: 0.5641 - val_loss: 1.1425 - val_acc: 0.6653\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3738 - acc: 0.5678\n",
      "Epoch 00088: val_loss improved from 1.13620 to 1.11253, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/088-1.1125.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3739 - acc: 0.5678 - val_loss: 1.1125 - val_acc: 0.6897\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3868 - acc: 0.5613\n",
      "Epoch 00089: val_loss did not improve from 1.11253\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3868 - acc: 0.5613 - val_loss: 1.1628 - val_acc: 0.6681\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3681 - acc: 0.5720\n",
      "Epoch 00090: val_loss did not improve from 1.11253\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3681 - acc: 0.5720 - val_loss: 1.3182 - val_acc: 0.5749\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3742 - acc: 0.5660\n",
      "Epoch 00091: val_loss did not improve from 1.11253\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.3739 - acc: 0.5660 - val_loss: 1.1193 - val_acc: 0.6771\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3692 - acc: 0.5681\n",
      "Epoch 00092: val_loss improved from 1.11253 to 1.10708, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/092-1.1071.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3693 - acc: 0.5680 - val_loss: 1.1071 - val_acc: 0.6858\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3709 - acc: 0.5660\n",
      "Epoch 00093: val_loss did not improve from 1.10708\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3712 - acc: 0.5659 - val_loss: 1.1147 - val_acc: 0.6723\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3560 - acc: 0.5737\n",
      "Epoch 00094: val_loss did not improve from 1.10708\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3559 - acc: 0.5737 - val_loss: 1.1264 - val_acc: 0.6844\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3537 - acc: 0.5737\n",
      "Epoch 00095: val_loss improved from 1.10708 to 1.10335, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/095-1.1034.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3548 - acc: 0.5735 - val_loss: 1.1034 - val_acc: 0.6851\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3636 - acc: 0.5721\n",
      "Epoch 00096: val_loss did not improve from 1.10335\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3635 - acc: 0.5720 - val_loss: 1.1058 - val_acc: 0.6909\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3582 - acc: 0.5719\n",
      "Epoch 00097: val_loss did not improve from 1.10335\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3586 - acc: 0.5719 - val_loss: 1.1152 - val_acc: 0.6879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.5713\n",
      "Epoch 00098: val_loss did not improve from 1.10335\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3533 - acc: 0.5711 - val_loss: 1.1123 - val_acc: 0.6860\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3577 - acc: 0.5758\n",
      "Epoch 00099: val_loss improved from 1.10335 to 1.09051, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/099-1.0905.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3582 - acc: 0.5758 - val_loss: 1.0905 - val_acc: 0.6932\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3533 - acc: 0.5756\n",
      "Epoch 00100: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3539 - acc: 0.5753 - val_loss: 1.1173 - val_acc: 0.6816\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3433 - acc: 0.5778\n",
      "Epoch 00101: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3433 - acc: 0.5778 - val_loss: 1.1206 - val_acc: 0.6625\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3506 - acc: 0.5749\n",
      "Epoch 00102: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3508 - acc: 0.5748 - val_loss: 1.1532 - val_acc: 0.6536\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.5772\n",
      "Epoch 00103: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.3435 - acc: 0.5772 - val_loss: 1.0957 - val_acc: 0.6853\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3404 - acc: 0.5745\n",
      "Epoch 00104: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3403 - acc: 0.5745 - val_loss: 1.0961 - val_acc: 0.6935\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3409 - acc: 0.5778\n",
      "Epoch 00105: val_loss did not improve from 1.09051\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.3410 - acc: 0.5777 - val_loss: 1.0971 - val_acc: 0.6846\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3453 - acc: 0.5777\n",
      "Epoch 00106: val_loss improved from 1.09051 to 1.08420, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/106-1.0842.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3453 - acc: 0.5776 - val_loss: 1.0842 - val_acc: 0.6956\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3349 - acc: 0.5780\n",
      "Epoch 00107: val_loss did not improve from 1.08420\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3347 - acc: 0.5781 - val_loss: 1.1239 - val_acc: 0.6678\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3338 - acc: 0.5813\n",
      "Epoch 00108: val_loss improved from 1.08420 to 1.07577, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/108-1.0758.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3336 - acc: 0.5813 - val_loss: 1.0758 - val_acc: 0.6944\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3331 - acc: 0.5796\n",
      "Epoch 00109: val_loss did not improve from 1.07577\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3340 - acc: 0.5796 - val_loss: 1.1308 - val_acc: 0.6541\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3397 - acc: 0.5782\n",
      "Epoch 00110: val_loss did not improve from 1.07577\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.3399 - acc: 0.5783 - val_loss: 1.0820 - val_acc: 0.6995\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3320 - acc: 0.5838\n",
      "Epoch 00111: val_loss did not improve from 1.07577\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3321 - acc: 0.5838 - val_loss: 1.1562 - val_acc: 0.6375\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3241 - acc: 0.5841\n",
      "Epoch 00112: val_loss improved from 1.07577 to 1.07110, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/112-1.0711.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3241 - acc: 0.5841 - val_loss: 1.0711 - val_acc: 0.6904\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3239 - acc: 0.5841\n",
      "Epoch 00113: val_loss did not improve from 1.07110\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.3240 - acc: 0.5841 - val_loss: 1.2955 - val_acc: 0.5579\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3302 - acc: 0.5852\n",
      "Epoch 00114: val_loss improved from 1.07110 to 1.06963, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/114-1.0696.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3301 - acc: 0.5852 - val_loss: 1.0696 - val_acc: 0.6997\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3194 - acc: 0.5851\n",
      "Epoch 00115: val_loss did not improve from 1.06963\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3194 - acc: 0.5851 - val_loss: 1.0733 - val_acc: 0.6949\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3198 - acc: 0.5840\n",
      "Epoch 00116: val_loss improved from 1.06963 to 1.06624, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/116-1.0662.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3197 - acc: 0.5841 - val_loss: 1.0662 - val_acc: 0.6960\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.5847\n",
      "Epoch 00117: val_loss did not improve from 1.06624\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3183 - acc: 0.5847 - val_loss: 1.0957 - val_acc: 0.6706\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3201 - acc: 0.5851\n",
      "Epoch 00118: val_loss did not improve from 1.06624\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3198 - acc: 0.5852 - val_loss: 1.1314 - val_acc: 0.6529\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3172 - acc: 0.5879\n",
      "Epoch 00119: val_loss improved from 1.06624 to 1.06222, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/119-1.0622.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3174 - acc: 0.5879 - val_loss: 1.0622 - val_acc: 0.6846\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3146 - acc: 0.5891\n",
      "Epoch 00120: val_loss did not improve from 1.06222\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3145 - acc: 0.5892 - val_loss: 1.2435 - val_acc: 0.5963\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3090 - acc: 0.5921\n",
      "Epoch 00121: val_loss did not improve from 1.06222\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3085 - acc: 0.5924 - val_loss: 1.1131 - val_acc: 0.6646\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3113 - acc: 0.5916\n",
      "Epoch 00122: val_loss did not improve from 1.06222\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.3121 - acc: 0.5912 - val_loss: 1.1546 - val_acc: 0.6613\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3119 - acc: 0.5871\n",
      "Epoch 00123: val_loss improved from 1.06222 to 1.04151, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/123-1.0415.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3121 - acc: 0.5870 - val_loss: 1.0415 - val_acc: 0.7098\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3073 - acc: 0.5915\n",
      "Epoch 00124: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.3072 - acc: 0.5915 - val_loss: 2.2859 - val_acc: 0.4016\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3056 - acc: 0.5868\n",
      "Epoch 00125: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3056 - acc: 0.5866 - val_loss: 1.0781 - val_acc: 0.6811\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3037 - acc: 0.5910\n",
      "Epoch 00126: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.3037 - acc: 0.5910 - val_loss: 1.0546 - val_acc: 0.6942\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2992 - acc: 0.5932\n",
      "Epoch 00127: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2992 - acc: 0.5933 - val_loss: 1.0563 - val_acc: 0.7016\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2969 - acc: 0.5930\n",
      "Epoch 00128: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2970 - acc: 0.5930 - val_loss: 1.0425 - val_acc: 0.7081\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2980 - acc: 0.5944\n",
      "Epoch 00129: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2981 - acc: 0.5942 - val_loss: 1.2566 - val_acc: 0.5954\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2965 - acc: 0.5943\n",
      "Epoch 00130: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2965 - acc: 0.5941 - val_loss: 1.0708 - val_acc: 0.6916\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2960 - acc: 0.5926\n",
      "Epoch 00131: val_loss did not improve from 1.04151\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2961 - acc: 0.5924 - val_loss: 3.1176 - val_acc: 0.3555\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2896 - acc: 0.5935\n",
      "Epoch 00132: val_loss improved from 1.04151 to 1.03409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/132-1.0341.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2897 - acc: 0.5935 - val_loss: 1.0341 - val_acc: 0.7042\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2936 - acc: 0.5933\n",
      "Epoch 00133: val_loss did not improve from 1.03409\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2935 - acc: 0.5933 - val_loss: 1.0904 - val_acc: 0.6767\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3017 - acc: 0.5944\n",
      "Epoch 00134: val_loss did not improve from 1.03409\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.3017 - acc: 0.5944 - val_loss: 1.0567 - val_acc: 0.6928\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2847 - acc: 0.5985\n",
      "Epoch 00135: val_loss improved from 1.03409 to 1.03186, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/135-1.0319.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2846 - acc: 0.5986 - val_loss: 1.0319 - val_acc: 0.7070\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2897 - acc: 0.5963\n",
      "Epoch 00136: val_loss did not improve from 1.03186\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2897 - acc: 0.5963 - val_loss: 1.0613 - val_acc: 0.6949\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2845 - acc: 0.5963\n",
      "Epoch 00137: val_loss improved from 1.03186 to 1.02032, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/137-1.0203.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2845 - acc: 0.5963 - val_loss: 1.0203 - val_acc: 0.7112\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2860 - acc: 0.5956\n",
      "Epoch 00138: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2861 - acc: 0.5957 - val_loss: 1.7875 - val_acc: 0.4840\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2931 - acc: 0.5972\n",
      "Epoch 00139: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2931 - acc: 0.5972 - val_loss: 1.7035 - val_acc: 0.4656\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2793 - acc: 0.6009\n",
      "Epoch 00140: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.2793 - acc: 0.6010 - val_loss: 1.0402 - val_acc: 0.7049\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2801 - acc: 0.5970\n",
      "Epoch 00141: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2801 - acc: 0.5970 - val_loss: 1.0686 - val_acc: 0.6921\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2758 - acc: 0.5985\n",
      "Epoch 00142: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2765 - acc: 0.5985 - val_loss: 1.2162 - val_acc: 0.6026\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2799 - acc: 0.5988\n",
      "Epoch 00143: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2799 - acc: 0.5988 - val_loss: 2.8563 - val_acc: 0.3545\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2762 - acc: 0.6014\n",
      "Epoch 00144: val_loss did not improve from 1.02032\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2761 - acc: 0.6015 - val_loss: 1.1633 - val_acc: 0.6250\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2752 - acc: 0.5984\n",
      "Epoch 00145: val_loss improved from 1.02032 to 1.00473, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/145-1.0047.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2753 - acc: 0.5984 - val_loss: 1.0047 - val_acc: 0.7128\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2785 - acc: 0.5992\n",
      "Epoch 00146: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2784 - acc: 0.5993 - val_loss: 1.2595 - val_acc: 0.6070\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2693 - acc: 0.6038\n",
      "Epoch 00147: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2696 - acc: 0.6037 - val_loss: 1.1976 - val_acc: 0.6152\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2707 - acc: 0.6018\n",
      "Epoch 00148: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2708 - acc: 0.6017 - val_loss: 1.0541 - val_acc: 0.6709\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2760 - acc: 0.6013\n",
      "Epoch 00149: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2763 - acc: 0.6012 - val_loss: 1.0633 - val_acc: 0.6949\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2704 - acc: 0.6029\n",
      "Epoch 00150: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2704 - acc: 0.6029 - val_loss: 1.0600 - val_acc: 0.6965\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2719 - acc: 0.6026\n",
      "Epoch 00151: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2717 - acc: 0.6027 - val_loss: 1.1353 - val_acc: 0.6320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2640 - acc: 0.6004\n",
      "Epoch 00152: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2641 - acc: 0.6004 - val_loss: 1.0270 - val_acc: 0.7077\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2688 - acc: 0.6052\n",
      "Epoch 00153: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2690 - acc: 0.6052 - val_loss: 1.3110 - val_acc: 0.5835\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2658 - acc: 0.6052\n",
      "Epoch 00154: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2662 - acc: 0.6052 - val_loss: 1.2534 - val_acc: 0.5991\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2584 - acc: 0.6048\n",
      "Epoch 00155: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2590 - acc: 0.6047 - val_loss: 1.0940 - val_acc: 0.6571\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2589 - acc: 0.6072\n",
      "Epoch 00156: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2591 - acc: 0.6072 - val_loss: 1.5319 - val_acc: 0.5229\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2603 - acc: 0.6068\n",
      "Epoch 00157: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2602 - acc: 0.6067 - val_loss: 1.0254 - val_acc: 0.7114\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2583 - acc: 0.6098\n",
      "Epoch 00158: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2583 - acc: 0.6098 - val_loss: 1.3810 - val_acc: 0.5646\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2611 - acc: 0.6091\n",
      "Epoch 00159: val_loss did not improve from 1.00473\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2611 - acc: 0.6091 - val_loss: 1.0771 - val_acc: 0.6592\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2592 - acc: 0.6090\n",
      "Epoch 00160: val_loss improved from 1.00473 to 0.98884, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/160-0.9888.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2592 - acc: 0.6090 - val_loss: 0.9888 - val_acc: 0.7284\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2617 - acc: 0.6065\n",
      "Epoch 00161: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2617 - acc: 0.6064 - val_loss: 1.0280 - val_acc: 0.7053\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2542 - acc: 0.6099\n",
      "Epoch 00162: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2542 - acc: 0.6099 - val_loss: 1.0278 - val_acc: 0.6895\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2554 - acc: 0.6095\n",
      "Epoch 00163: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2554 - acc: 0.6095 - val_loss: 1.0296 - val_acc: 0.7077\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2480 - acc: 0.6082\n",
      "Epoch 00164: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2480 - acc: 0.6082 - val_loss: 0.9930 - val_acc: 0.7268\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2586 - acc: 0.6067\n",
      "Epoch 00165: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2586 - acc: 0.6067 - val_loss: 0.9996 - val_acc: 0.7151\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2498 - acc: 0.6090\n",
      "Epoch 00166: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2499 - acc: 0.6089 - val_loss: 1.0184 - val_acc: 0.7126\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2557 - acc: 0.6068\n",
      "Epoch 00167: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2555 - acc: 0.6068 - val_loss: 1.0572 - val_acc: 0.6981\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2534 - acc: 0.6091\n",
      "Epoch 00168: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2535 - acc: 0.6090 - val_loss: 0.9940 - val_acc: 0.7140\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2545 - acc: 0.6103\n",
      "Epoch 00169: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2545 - acc: 0.6104 - val_loss: 1.1518 - val_acc: 0.6208\n",
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2475 - acc: 0.6100\n",
      "Epoch 00170: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2473 - acc: 0.6101 - val_loss: 1.0025 - val_acc: 0.7119\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2443 - acc: 0.6135\n",
      "Epoch 00171: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2438 - acc: 0.6135 - val_loss: 1.2637 - val_acc: 0.5823\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2405 - acc: 0.6120\n",
      "Epoch 00172: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2409 - acc: 0.6119 - val_loss: 1.1121 - val_acc: 0.6438\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2391 - acc: 0.6130\n",
      "Epoch 00173: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2391 - acc: 0.6130 - val_loss: 1.0110 - val_acc: 0.7156\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2446 - acc: 0.6136\n",
      "Epoch 00174: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2444 - acc: 0.6135 - val_loss: 0.9904 - val_acc: 0.7226\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2402 - acc: 0.6141\n",
      "Epoch 00175: val_loss did not improve from 0.98884\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2408 - acc: 0.6139 - val_loss: 1.0497 - val_acc: 0.6790\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2376 - acc: 0.6102\n",
      "Epoch 00176: val_loss improved from 0.98884 to 0.97881, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/176-0.9788.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2373 - acc: 0.6103 - val_loss: 0.9788 - val_acc: 0.7263\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2415 - acc: 0.6133\n",
      "Epoch 00177: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2412 - acc: 0.6135 - val_loss: 2.2821 - val_acc: 0.4121\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2354 - acc: 0.6154\n",
      "Epoch 00178: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2349 - acc: 0.6157 - val_loss: 1.6807 - val_acc: 0.5129\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2388 - acc: 0.6159\n",
      "Epoch 00179: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2386 - acc: 0.6159 - val_loss: 1.6058 - val_acc: 0.4826\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2352 - acc: 0.6162\n",
      "Epoch 00180: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2357 - acc: 0.6159 - val_loss: 1.0018 - val_acc: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2394 - acc: 0.6115\n",
      "Epoch 00181: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.2393 - acc: 0.6114 - val_loss: 1.0265 - val_acc: 0.7077\n",
      "Epoch 182/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2319 - acc: 0.6157\n",
      "Epoch 00182: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.2324 - acc: 0.6157 - val_loss: 1.8166 - val_acc: 0.4754\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2331 - acc: 0.6179\n",
      "Epoch 00183: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2331 - acc: 0.6179 - val_loss: 1.0047 - val_acc: 0.7188\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2340 - acc: 0.6158\n",
      "Epoch 00184: val_loss did not improve from 0.97881\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2341 - acc: 0.6158 - val_loss: 1.1628 - val_acc: 0.6341\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2314 - acc: 0.6147\n",
      "Epoch 00185: val_loss improved from 0.97881 to 0.96536, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/185-0.9654.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2315 - acc: 0.6147 - val_loss: 0.9654 - val_acc: 0.7342\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2270 - acc: 0.6177\n",
      "Epoch 00186: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2264 - acc: 0.6178 - val_loss: 0.9966 - val_acc: 0.7007\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2315 - acc: 0.6162\n",
      "Epoch 00187: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2318 - acc: 0.6161 - val_loss: 0.9850 - val_acc: 0.7165\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2313 - acc: 0.6164\n",
      "Epoch 00188: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2313 - acc: 0.6164 - val_loss: 1.0383 - val_acc: 0.6660\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2253 - acc: 0.6170\n",
      "Epoch 00189: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2252 - acc: 0.6171 - val_loss: 1.0254 - val_acc: 0.6918\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2236 - acc: 0.6173\n",
      "Epoch 00190: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2235 - acc: 0.6173 - val_loss: 1.0212 - val_acc: 0.6958\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.6177\n",
      "Epoch 00191: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2282 - acc: 0.6176 - val_loss: 1.3336 - val_acc: 0.5504\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2208 - acc: 0.6183\n",
      "Epoch 00192: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2207 - acc: 0.6182 - val_loss: 2.0194 - val_acc: 0.4379\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2211 - acc: 0.6194\n",
      "Epoch 00193: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2215 - acc: 0.6193 - val_loss: 0.9983 - val_acc: 0.7116\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2254 - acc: 0.6171\n",
      "Epoch 00194: val_loss did not improve from 0.96536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2253 - acc: 0.6170 - val_loss: 1.0385 - val_acc: 0.6918\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2224 - acc: 0.6193\n",
      "Epoch 00195: val_loss improved from 0.96536 to 0.96346, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/195-0.9635.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2218 - acc: 0.6195 - val_loss: 0.9635 - val_acc: 0.7265\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2209 - acc: 0.6207\n",
      "Epoch 00196: val_loss did not improve from 0.96346\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.2209 - acc: 0.6208 - val_loss: 1.0988 - val_acc: 0.6571\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2124 - acc: 0.6209\n",
      "Epoch 00197: val_loss did not improve from 0.96346\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2124 - acc: 0.6209 - val_loss: 0.9708 - val_acc: 0.7349\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2142 - acc: 0.6205\n",
      "Epoch 00198: val_loss did not improve from 0.96346\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2141 - acc: 0.6206 - val_loss: 1.0017 - val_acc: 0.6935\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2219 - acc: 0.6176\n",
      "Epoch 00199: val_loss did not improve from 0.96346\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2217 - acc: 0.6177 - val_loss: 1.1573 - val_acc: 0.6268\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2195 - acc: 0.6173\n",
      "Epoch 00200: val_loss did not improve from 0.96346\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.2196 - acc: 0.6173 - val_loss: 0.9855 - val_acc: 0.7191\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2127 - acc: 0.6228\n",
      "Epoch 00201: val_loss improved from 0.96346 to 0.95498, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/201-0.9550.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.2127 - acc: 0.6229 - val_loss: 0.9550 - val_acc: 0.7358\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2175 - acc: 0.6198\n",
      "Epoch 00202: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2175 - acc: 0.6198 - val_loss: 1.1022 - val_acc: 0.6429\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2184 - acc: 0.6214\n",
      "Epoch 00203: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2185 - acc: 0.6214 - val_loss: 0.9571 - val_acc: 0.7282\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2029 - acc: 0.6261\n",
      "Epoch 00204: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2031 - acc: 0.6260 - val_loss: 1.4749 - val_acc: 0.5334\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2155 - acc: 0.6227\n",
      "Epoch 00205: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2155 - acc: 0.6227 - val_loss: 1.0687 - val_acc: 0.6653\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2109 - acc: 0.6250\n",
      "Epoch 00206: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2108 - acc: 0.6250 - val_loss: 1.0796 - val_acc: 0.6641\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2080 - acc: 0.6228\n",
      "Epoch 00207: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2081 - acc: 0.6228 - val_loss: 1.0248 - val_acc: 0.6942\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2166 - acc: 0.6240\n",
      "Epoch 00208: val_loss did not improve from 0.95498\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.2164 - acc: 0.6241 - val_loss: 1.4009 - val_acc: 0.5597\n",
      "Epoch 209/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2127 - acc: 0.6226\n",
      "Epoch 00209: val_loss improved from 0.95498 to 0.92995, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/209-0.9299.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2128 - acc: 0.6226 - val_loss: 0.9299 - val_acc: 0.7407\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2071 - acc: 0.6246\n",
      "Epoch 00210: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2071 - acc: 0.6246 - val_loss: 0.9739 - val_acc: 0.7088\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2063 - acc: 0.6227\n",
      "Epoch 00211: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2059 - acc: 0.6228 - val_loss: 0.9330 - val_acc: 0.7424\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2083 - acc: 0.6235\n",
      "Epoch 00212: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2086 - acc: 0.6234 - val_loss: 0.9495 - val_acc: 0.7270\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2098 - acc: 0.6227\n",
      "Epoch 00213: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2101 - acc: 0.6226 - val_loss: 0.9491 - val_acc: 0.7328\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2058 - acc: 0.6247\n",
      "Epoch 00214: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.2064 - acc: 0.6247 - val_loss: 2.0440 - val_acc: 0.4552\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2064 - acc: 0.6279\n",
      "Epoch 00215: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.2065 - acc: 0.6279 - val_loss: 0.9501 - val_acc: 0.7356\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2042 - acc: 0.6236\n",
      "Epoch 00216: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2043 - acc: 0.6235 - val_loss: 1.1881 - val_acc: 0.6133\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1979 - acc: 0.6268\n",
      "Epoch 00217: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1980 - acc: 0.6268 - val_loss: 0.9959 - val_acc: 0.7137\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2058 - acc: 0.6272\n",
      "Epoch 00218: val_loss did not improve from 0.92995\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2056 - acc: 0.6273 - val_loss: 0.9758 - val_acc: 0.7237\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1940 - acc: 0.6300\n",
      "Epoch 00219: val_loss improved from 0.92995 to 0.92487, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/219-0.9249.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1941 - acc: 0.6299 - val_loss: 0.9249 - val_acc: 0.7419\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2014 - acc: 0.6237\n",
      "Epoch 00220: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2011 - acc: 0.6240 - val_loss: 1.0143 - val_acc: 0.6939\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2051 - acc: 0.6233\n",
      "Epoch 00221: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.2047 - acc: 0.6236 - val_loss: 0.9746 - val_acc: 0.7081\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1958 - acc: 0.6285\n",
      "Epoch 00222: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1956 - acc: 0.6285 - val_loss: 0.9474 - val_acc: 0.7365\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1963 - acc: 0.6281\n",
      "Epoch 00223: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1961 - acc: 0.6282 - val_loss: 1.1007 - val_acc: 0.6529\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.6255\n",
      "Epoch 00224: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.2057 - acc: 0.6255 - val_loss: 1.7396 - val_acc: 0.4859\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1968 - acc: 0.6285\n",
      "Epoch 00225: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1967 - acc: 0.6285 - val_loss: 0.9734 - val_acc: 0.7284\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1972 - acc: 0.6281\n",
      "Epoch 00226: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1973 - acc: 0.6280 - val_loss: 0.9389 - val_acc: 0.7338\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6326\n",
      "Epoch 00227: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1910 - acc: 0.6326 - val_loss: 0.9891 - val_acc: 0.7086\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1936 - acc: 0.6332\n",
      "Epoch 00228: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1937 - acc: 0.6331 - val_loss: 0.9624 - val_acc: 0.7296\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1922 - acc: 0.6273\n",
      "Epoch 00229: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1919 - acc: 0.6274 - val_loss: 0.9411 - val_acc: 0.7379\n",
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1966 - acc: 0.6296\n",
      "Epoch 00230: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1972 - acc: 0.6294 - val_loss: 1.0031 - val_acc: 0.6928\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1912 - acc: 0.6294\n",
      "Epoch 00231: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.1911 - acc: 0.6294 - val_loss: 0.9584 - val_acc: 0.7272\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1933 - acc: 0.6285\n",
      "Epoch 00232: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1932 - acc: 0.6285 - val_loss: 0.9493 - val_acc: 0.7345\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1915 - acc: 0.6271\n",
      "Epoch 00233: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1917 - acc: 0.6270 - val_loss: 1.3482 - val_acc: 0.5807\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1886 - acc: 0.6293\n",
      "Epoch 00234: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1884 - acc: 0.6293 - val_loss: 0.9535 - val_acc: 0.7254\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1932 - acc: 0.6313\n",
      "Epoch 00235: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1928 - acc: 0.6313 - val_loss: 0.9908 - val_acc: 0.7135\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1982 - acc: 0.6303\n",
      "Epoch 00236: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1980 - acc: 0.6303 - val_loss: 0.9688 - val_acc: 0.7184\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1850 - acc: 0.6315\n",
      "Epoch 00237: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1846 - acc: 0.6316 - val_loss: 1.5781 - val_acc: 0.5136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1870 - acc: 0.6283\n",
      "Epoch 00238: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1873 - acc: 0.6284 - val_loss: 0.9611 - val_acc: 0.7272\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6310\n",
      "Epoch 00239: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1857 - acc: 0.6310 - val_loss: 0.9355 - val_acc: 0.7365\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1824 - acc: 0.6334\n",
      "Epoch 00240: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1830 - acc: 0.6331 - val_loss: 0.9626 - val_acc: 0.7235\n",
      "Epoch 241/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1829 - acc: 0.6325\n",
      "Epoch 00241: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1830 - acc: 0.6324 - val_loss: 0.9361 - val_acc: 0.7391\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1816 - acc: 0.6340\n",
      "Epoch 00242: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1814 - acc: 0.6342 - val_loss: 0.9334 - val_acc: 0.7435\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1835 - acc: 0.6331\n",
      "Epoch 00243: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1835 - acc: 0.6331 - val_loss: 0.9609 - val_acc: 0.7184\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.6307\n",
      "Epoch 00244: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1838 - acc: 0.6309 - val_loss: 0.9307 - val_acc: 0.7340\n",
      "Epoch 245/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1822 - acc: 0.6322\n",
      "Epoch 00245: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1822 - acc: 0.6321 - val_loss: 0.9275 - val_acc: 0.7382\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1839 - acc: 0.6329\n",
      "Epoch 00246: val_loss did not improve from 0.92487\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1838 - acc: 0.6328 - val_loss: 0.9572 - val_acc: 0.7282\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1850 - acc: 0.6305\n",
      "Epoch 00247: val_loss improved from 0.92487 to 0.92201, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/247-0.9220.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1850 - acc: 0.6305 - val_loss: 0.9220 - val_acc: 0.7372\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1863 - acc: 0.6297\n",
      "Epoch 00248: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1863 - acc: 0.6297 - val_loss: 0.9264 - val_acc: 0.7372\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1852 - acc: 0.6308\n",
      "Epoch 00249: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1854 - acc: 0.6306 - val_loss: 2.0144 - val_acc: 0.4689\n",
      "Epoch 250/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1814 - acc: 0.6339\n",
      "Epoch 00250: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1816 - acc: 0.6337 - val_loss: 1.0068 - val_acc: 0.6816\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1759 - acc: 0.6357\n",
      "Epoch 00251: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1764 - acc: 0.6357 - val_loss: 0.9362 - val_acc: 0.7403\n",
      "Epoch 252/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1822 - acc: 0.6356\n",
      "Epoch 00252: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1819 - acc: 0.6356 - val_loss: 2.6022 - val_acc: 0.4002\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1762 - acc: 0.6360\n",
      "Epoch 00253: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1761 - acc: 0.6360 - val_loss: 1.0332 - val_acc: 0.6732\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1791 - acc: 0.6314\n",
      "Epoch 00254: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1794 - acc: 0.6314 - val_loss: 1.0664 - val_acc: 0.6585\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1784 - acc: 0.6374\n",
      "Epoch 00255: val_loss did not improve from 0.92201\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1783 - acc: 0.6374 - val_loss: 1.0607 - val_acc: 0.6567\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1781 - acc: 0.6326\n",
      "Epoch 00256: val_loss improved from 0.92201 to 0.90310, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/256-0.9031.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1782 - acc: 0.6327 - val_loss: 0.9031 - val_acc: 0.7473\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1737 - acc: 0.6365\n",
      "Epoch 00257: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1736 - acc: 0.6366 - val_loss: 1.2831 - val_acc: 0.5933\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1701 - acc: 0.6383\n",
      "Epoch 00258: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1700 - acc: 0.6384 - val_loss: 1.0828 - val_acc: 0.6667\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1665 - acc: 0.6409\n",
      "Epoch 00259: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1669 - acc: 0.6408 - val_loss: 1.0167 - val_acc: 0.6816\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1781 - acc: 0.6352\n",
      "Epoch 00260: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1781 - acc: 0.6352 - val_loss: 0.9652 - val_acc: 0.7154\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1731 - acc: 0.6378\n",
      "Epoch 00261: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1728 - acc: 0.6379 - val_loss: 0.9230 - val_acc: 0.7405\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1766 - acc: 0.6334\n",
      "Epoch 00262: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1767 - acc: 0.6333 - val_loss: 0.9226 - val_acc: 0.7391\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1683 - acc: 0.6397\n",
      "Epoch 00263: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1683 - acc: 0.6398 - val_loss: 1.0168 - val_acc: 0.6865\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6352\n",
      "Epoch 00264: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1743 - acc: 0.6352 - val_loss: 0.9977 - val_acc: 0.6893\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1738 - acc: 0.6355\n",
      "Epoch 00265: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1736 - acc: 0.6355 - val_loss: 0.9125 - val_acc: 0.7496\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1683 - acc: 0.6367\n",
      "Epoch 00266: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1679 - acc: 0.6367 - val_loss: 0.9820 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1638 - acc: 0.6364\n",
      "Epoch 00267: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1633 - acc: 0.6366 - val_loss: 0.9703 - val_acc: 0.7230\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1730 - acc: 0.6365\n",
      "Epoch 00268: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1730 - acc: 0.6365 - val_loss: 1.1054 - val_acc: 0.6522\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1726 - acc: 0.6353\n",
      "Epoch 00269: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1724 - acc: 0.6353 - val_loss: 0.9293 - val_acc: 0.7289\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1704 - acc: 0.6363\n",
      "Epoch 00270: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1703 - acc: 0.6362 - val_loss: 1.4888 - val_acc: 0.5502\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1714 - acc: 0.6389\n",
      "Epoch 00271: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1710 - acc: 0.6391 - val_loss: 0.9206 - val_acc: 0.7442\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1691 - acc: 0.6380\n",
      "Epoch 00272: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1693 - acc: 0.6380 - val_loss: 0.9260 - val_acc: 0.7310\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6390\n",
      "Epoch 00273: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1639 - acc: 0.6390 - val_loss: 0.9441 - val_acc: 0.7268\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1629 - acc: 0.6383\n",
      "Epoch 00274: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1629 - acc: 0.6382 - val_loss: 0.9591 - val_acc: 0.7214\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1613 - acc: 0.6399\n",
      "Epoch 00275: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1611 - acc: 0.6399 - val_loss: 1.0557 - val_acc: 0.6751\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6395\n",
      "Epoch 00276: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1638 - acc: 0.6395 - val_loss: 0.9671 - val_acc: 0.7200\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1632 - acc: 0.6392\n",
      "Epoch 00277: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1629 - acc: 0.6393 - val_loss: 0.9378 - val_acc: 0.7342\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1667 - acc: 0.6390\n",
      "Epoch 00278: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1667 - acc: 0.6390 - val_loss: 1.0502 - val_acc: 0.6734\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1606 - acc: 0.6371\n",
      "Epoch 00279: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1606 - acc: 0.6372 - val_loss: 1.0129 - val_acc: 0.6935\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1618 - acc: 0.6395\n",
      "Epoch 00280: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1620 - acc: 0.6396 - val_loss: 0.9198 - val_acc: 0.7391\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1555 - acc: 0.6430\n",
      "Epoch 00281: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.1555 - acc: 0.6430 - val_loss: 1.2891 - val_acc: 0.5688\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1598 - acc: 0.6380\n",
      "Epoch 00282: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1598 - acc: 0.6381 - val_loss: 1.5353 - val_acc: 0.5169\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1529 - acc: 0.6426\n",
      "Epoch 00283: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1528 - acc: 0.6427 - val_loss: 0.9103 - val_acc: 0.7349\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6429\n",
      "Epoch 00284: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1558 - acc: 0.6429 - val_loss: 0.9302 - val_acc: 0.7291\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1592 - acc: 0.6389\n",
      "Epoch 00285: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1593 - acc: 0.6389 - val_loss: 1.8194 - val_acc: 0.4375\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1570 - acc: 0.6436\n",
      "Epoch 00286: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1570 - acc: 0.6437 - val_loss: 0.9540 - val_acc: 0.7186\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1564 - acc: 0.6410\n",
      "Epoch 00287: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1564 - acc: 0.6411 - val_loss: 0.9265 - val_acc: 0.7410\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1621 - acc: 0.6379\n",
      "Epoch 00288: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1620 - acc: 0.6379 - val_loss: 0.9185 - val_acc: 0.7433\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1590 - acc: 0.6412\n",
      "Epoch 00289: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1591 - acc: 0.6411 - val_loss: 0.9302 - val_acc: 0.7279\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1554 - acc: 0.6416\n",
      "Epoch 00290: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1555 - acc: 0.6416 - val_loss: 0.9799 - val_acc: 0.7135\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1572 - acc: 0.6401\n",
      "Epoch 00291: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1576 - acc: 0.6400 - val_loss: 0.9034 - val_acc: 0.7431\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1589 - acc: 0.6374\n",
      "Epoch 00292: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1590 - acc: 0.6374 - val_loss: 0.9723 - val_acc: 0.7021\n",
      "Epoch 293/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1588 - acc: 0.6394\n",
      "Epoch 00293: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1593 - acc: 0.6393 - val_loss: 1.0075 - val_acc: 0.6918\n",
      "Epoch 294/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1584 - acc: 0.6426\n",
      "Epoch 00294: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1584 - acc: 0.6427 - val_loss: 0.9094 - val_acc: 0.7487\n",
      "Epoch 295/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1476 - acc: 0.6457\n",
      "Epoch 00295: val_loss did not improve from 0.90310\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.1478 - acc: 0.6457 - val_loss: 0.9212 - val_acc: 0.7356\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1485 - acc: 0.6429\n",
      "Epoch 00296: val_loss improved from 0.90310 to 0.88332, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/296-0.8833.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.1489 - acc: 0.6427 - val_loss: 0.8833 - val_acc: 0.7538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1551 - acc: 0.6410\n",
      "Epoch 00297: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1552 - acc: 0.6410 - val_loss: 1.0373 - val_acc: 0.6625\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1499 - acc: 0.6447\n",
      "Epoch 00298: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1497 - acc: 0.6448 - val_loss: 0.9396 - val_acc: 0.7319\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1499 - acc: 0.6436\n",
      "Epoch 00299: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1496 - acc: 0.6438 - val_loss: 1.0359 - val_acc: 0.6965\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1505 - acc: 0.6432\n",
      "Epoch 00300: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1505 - acc: 0.6432 - val_loss: 0.9334 - val_acc: 0.7209\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1502 - acc: 0.6409\n",
      "Epoch 00301: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1501 - acc: 0.6409 - val_loss: 1.1390 - val_acc: 0.6441\n",
      "Epoch 302/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1455 - acc: 0.6450\n",
      "Epoch 00302: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1452 - acc: 0.6452 - val_loss: 0.9004 - val_acc: 0.7456\n",
      "Epoch 303/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1498 - acc: 0.6447\n",
      "Epoch 00303: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1500 - acc: 0.6446 - val_loss: 0.9683 - val_acc: 0.7137\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1478 - acc: 0.6433\n",
      "Epoch 00304: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1478 - acc: 0.6433 - val_loss: 0.9480 - val_acc: 0.7326\n",
      "Epoch 305/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1454 - acc: 0.6451\n",
      "Epoch 00305: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1462 - acc: 0.6449 - val_loss: 1.1524 - val_acc: 0.6455\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1457 - acc: 0.6440\n",
      "Epoch 00306: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1456 - acc: 0.6441 - val_loss: 0.9878 - val_acc: 0.7121\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1492 - acc: 0.6442\n",
      "Epoch 00307: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1492 - acc: 0.6442 - val_loss: 1.0208 - val_acc: 0.6928\n",
      "Epoch 308/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1492 - acc: 0.6453\n",
      "Epoch 00308: val_loss did not improve from 0.88332\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1492 - acc: 0.6453 - val_loss: 1.1238 - val_acc: 0.6608\n",
      "Epoch 309/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1442 - acc: 0.6468\n",
      "Epoch 00309: val_loss improved from 0.88332 to 0.88257, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/309-0.8826.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1439 - acc: 0.6469 - val_loss: 0.8826 - val_acc: 0.7536\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1472 - acc: 0.6448\n",
      "Epoch 00310: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1469 - acc: 0.6448 - val_loss: 0.9144 - val_acc: 0.7340\n",
      "Epoch 311/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1549 - acc: 0.6416\n",
      "Epoch 00311: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1549 - acc: 0.6417 - val_loss: 1.5552 - val_acc: 0.5146\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1461 - acc: 0.6464\n",
      "Epoch 00312: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1461 - acc: 0.6463 - val_loss: 1.0061 - val_acc: 0.6699\n",
      "Epoch 313/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1493 - acc: 0.6434\n",
      "Epoch 00313: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1492 - acc: 0.6434 - val_loss: 0.9361 - val_acc: 0.7314\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1431 - acc: 0.6449\n",
      "Epoch 00314: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1431 - acc: 0.6449 - val_loss: 0.9266 - val_acc: 0.7317\n",
      "Epoch 315/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1497 - acc: 0.6468\n",
      "Epoch 00315: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1498 - acc: 0.6469 - val_loss: 0.9795 - val_acc: 0.7035\n",
      "Epoch 316/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1414 - acc: 0.6469\n",
      "Epoch 00316: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1413 - acc: 0.6469 - val_loss: 1.1076 - val_acc: 0.6215\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1428 - acc: 0.6465\n",
      "Epoch 00317: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1433 - acc: 0.6464 - val_loss: 1.7783 - val_acc: 0.4892\n",
      "Epoch 318/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1410 - acc: 0.6458\n",
      "Epoch 00318: val_loss did not improve from 0.88257\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1408 - acc: 0.6459 - val_loss: 0.8979 - val_acc: 0.7426\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1400 - acc: 0.6454\n",
      "Epoch 00319: val_loss improved from 0.88257 to 0.88016, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/319-0.8802.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1400 - acc: 0.6454 - val_loss: 0.8802 - val_acc: 0.7531\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1501 - acc: 0.6475\n",
      "Epoch 00320: val_loss did not improve from 0.88016\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1500 - acc: 0.6475 - val_loss: 0.8812 - val_acc: 0.7519\n",
      "Epoch 321/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1354 - acc: 0.6496\n",
      "Epoch 00321: val_loss did not improve from 0.88016\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1357 - acc: 0.6496 - val_loss: 1.6938 - val_acc: 0.4934\n",
      "Epoch 322/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1365 - acc: 0.6482\n",
      "Epoch 00322: val_loss improved from 0.88016 to 0.87094, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/322-0.8709.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1368 - acc: 0.6482 - val_loss: 0.8709 - val_acc: 0.7580\n",
      "Epoch 323/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1352 - acc: 0.6476\n",
      "Epoch 00323: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1355 - acc: 0.6475 - val_loss: 0.9090 - val_acc: 0.7349\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1417 - acc: 0.6425\n",
      "Epoch 00324: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1417 - acc: 0.6425 - val_loss: 0.9019 - val_acc: 0.7508\n",
      "Epoch 325/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1427 - acc: 0.6451\n",
      "Epoch 00325: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1420 - acc: 0.6454 - val_loss: 0.8838 - val_acc: 0.7536\n",
      "Epoch 326/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1387 - acc: 0.6471\n",
      "Epoch 00326: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1389 - acc: 0.6470 - val_loss: 0.9165 - val_acc: 0.7361\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1393 - acc: 0.6471\n",
      "Epoch 00327: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1389 - acc: 0.6471 - val_loss: 0.9242 - val_acc: 0.7293\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1359 - acc: 0.6485\n",
      "Epoch 00328: val_loss did not improve from 0.87094\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1363 - acc: 0.6484 - val_loss: 0.9136 - val_acc: 0.7419\n",
      "Epoch 329/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1334 - acc: 0.6508\n",
      "Epoch 00329: val_loss improved from 0.87094 to 0.87038, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/329-0.8704.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1331 - acc: 0.6508 - val_loss: 0.8704 - val_acc: 0.7570\n",
      "Epoch 330/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1376 - acc: 0.6500\n",
      "Epoch 00330: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1378 - acc: 0.6499 - val_loss: 1.7220 - val_acc: 0.4908\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1337 - acc: 0.6484\n",
      "Epoch 00331: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1336 - acc: 0.6484 - val_loss: 0.8782 - val_acc: 0.7570\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1346 - acc: 0.6508\n",
      "Epoch 00332: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1346 - acc: 0.6508 - val_loss: 1.9309 - val_acc: 0.4177\n",
      "Epoch 333/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1352 - acc: 0.6488\n",
      "Epoch 00333: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1352 - acc: 0.6488 - val_loss: 0.8800 - val_acc: 0.7538\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1335 - acc: 0.6478\n",
      "Epoch 00334: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1332 - acc: 0.6478 - val_loss: 0.8984 - val_acc: 0.7435\n",
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1344 - acc: 0.6475\n",
      "Epoch 00335: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1347 - acc: 0.6474 - val_loss: 0.9127 - val_acc: 0.7393\n",
      "Epoch 336/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1294 - acc: 0.6512\n",
      "Epoch 00336: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1295 - acc: 0.6510 - val_loss: 0.8864 - val_acc: 0.7489\n",
      "Epoch 337/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1279 - acc: 0.6498\n",
      "Epoch 00337: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1278 - acc: 0.6498 - val_loss: 0.8952 - val_acc: 0.7468\n",
      "Epoch 338/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1273 - acc: 0.6506\n",
      "Epoch 00338: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1274 - acc: 0.6506 - val_loss: 0.9708 - val_acc: 0.7207\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1364 - acc: 0.6475\n",
      "Epoch 00339: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1364 - acc: 0.6475 - val_loss: 0.8891 - val_acc: 0.7452\n",
      "Epoch 340/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1345 - acc: 0.6497\n",
      "Epoch 00340: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1344 - acc: 0.6499 - val_loss: 0.9389 - val_acc: 0.7265\n",
      "Epoch 341/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1265 - acc: 0.6512\n",
      "Epoch 00341: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1268 - acc: 0.6512 - val_loss: 1.2427 - val_acc: 0.5989\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1215 - acc: 0.6534\n",
      "Epoch 00342: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1220 - acc: 0.6533 - val_loss: 0.9115 - val_acc: 0.7414\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1379 - acc: 0.6474\n",
      "Epoch 00343: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1375 - acc: 0.6474 - val_loss: 0.9686 - val_acc: 0.7021\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1309 - acc: 0.6504\n",
      "Epoch 00344: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.1310 - acc: 0.6503 - val_loss: 1.6720 - val_acc: 0.5062\n",
      "Epoch 345/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1248 - acc: 0.6515\n",
      "Epoch 00345: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1249 - acc: 0.6516 - val_loss: 2.1033 - val_acc: 0.4097\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6473\n",
      "Epoch 00346: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1304 - acc: 0.6473 - val_loss: 0.8767 - val_acc: 0.7563\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1339 - acc: 0.6475\n",
      "Epoch 00347: val_loss did not improve from 0.87038\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1338 - acc: 0.6475 - val_loss: 1.5341 - val_acc: 0.5586\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1260 - acc: 0.6511\n",
      "Epoch 00348: val_loss improved from 0.87038 to 0.86900, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/348-0.8690.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1264 - acc: 0.6511 - val_loss: 0.8690 - val_acc: 0.7559\n",
      "Epoch 349/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1221 - acc: 0.6514\n",
      "Epoch 00349: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1215 - acc: 0.6516 - val_loss: 1.3682 - val_acc: 0.5392\n",
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1287 - acc: 0.6478\n",
      "Epoch 00350: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1292 - acc: 0.6477 - val_loss: 0.8830 - val_acc: 0.7484\n",
      "Epoch 351/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1327 - acc: 0.6469\n",
      "Epoch 00351: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1327 - acc: 0.6469 - val_loss: 1.5548 - val_acc: 0.4952\n",
      "Epoch 352/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1263 - acc: 0.6527\n",
      "Epoch 00352: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1259 - acc: 0.6527 - val_loss: 0.9640 - val_acc: 0.7021\n",
      "Epoch 353/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1238 - acc: 0.6503\n",
      "Epoch 00353: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1236 - acc: 0.6503 - val_loss: 0.8821 - val_acc: 0.7584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1226 - acc: 0.6533\n",
      "Epoch 00354: val_loss did not improve from 0.86900\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1230 - acc: 0.6533 - val_loss: 2.6948 - val_acc: 0.3652\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1286 - acc: 0.6505\n",
      "Epoch 00355: val_loss improved from 0.86900 to 0.86199, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/355-0.8620.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1281 - acc: 0.6507 - val_loss: 0.8620 - val_acc: 0.7617\n",
      "Epoch 356/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1240 - acc: 0.6537\n",
      "Epoch 00356: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1243 - acc: 0.6537 - val_loss: 1.1891 - val_acc: 0.6159\n",
      "Epoch 357/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1204 - acc: 0.6528\n",
      "Epoch 00357: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1205 - acc: 0.6528 - val_loss: 0.8922 - val_acc: 0.7463\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1193 - acc: 0.6527\n",
      "Epoch 00358: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1194 - acc: 0.6527 - val_loss: 0.8648 - val_acc: 0.7580\n",
      "Epoch 359/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1262 - acc: 0.6534\n",
      "Epoch 00359: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1257 - acc: 0.6536 - val_loss: 1.3873 - val_acc: 0.5779\n",
      "Epoch 360/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1245 - acc: 0.6516\n",
      "Epoch 00360: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1243 - acc: 0.6517 - val_loss: 1.4801 - val_acc: 0.5399\n",
      "Epoch 361/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1179 - acc: 0.6568\n",
      "Epoch 00361: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1182 - acc: 0.6568 - val_loss: 1.0937 - val_acc: 0.6706\n",
      "Epoch 362/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1175 - acc: 0.6550\n",
      "Epoch 00362: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1175 - acc: 0.6550 - val_loss: 0.8700 - val_acc: 0.7533\n",
      "Epoch 363/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1217 - acc: 0.6545\n",
      "Epoch 00363: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1218 - acc: 0.6544 - val_loss: 1.0280 - val_acc: 0.6695\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1249 - acc: 0.6505\n",
      "Epoch 00364: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1250 - acc: 0.6505 - val_loss: 0.8872 - val_acc: 0.7403\n",
      "Epoch 365/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1249 - acc: 0.6508\n",
      "Epoch 00365: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1247 - acc: 0.6508 - val_loss: 1.2540 - val_acc: 0.6059\n",
      "Epoch 366/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1195 - acc: 0.6503\n",
      "Epoch 00366: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1191 - acc: 0.6506 - val_loss: 1.5639 - val_acc: 0.5413\n",
      "Epoch 367/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1265 - acc: 0.6539\n",
      "Epoch 00367: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1262 - acc: 0.6540 - val_loss: 1.1448 - val_acc: 0.6445\n",
      "Epoch 368/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1232 - acc: 0.6524\n",
      "Epoch 00368: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1231 - acc: 0.6524 - val_loss: 0.8682 - val_acc: 0.7515\n",
      "Epoch 369/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1214 - acc: 0.6558\n",
      "Epoch 00369: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1214 - acc: 0.6558 - val_loss: 1.9528 - val_acc: 0.4498\n",
      "Epoch 370/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6560\n",
      "Epoch 00370: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1170 - acc: 0.6558 - val_loss: 1.9092 - val_acc: 0.4482\n",
      "Epoch 371/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1202 - acc: 0.6533\n",
      "Epoch 00371: val_loss did not improve from 0.86199\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1202 - acc: 0.6533 - val_loss: 0.9204 - val_acc: 0.7263\n",
      "Epoch 372/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6553\n",
      "Epoch 00372: val_loss improved from 0.86199 to 0.85774, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/372-0.8577.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1132 - acc: 0.6553 - val_loss: 0.8577 - val_acc: 0.7619\n",
      "Epoch 373/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1205 - acc: 0.6540\n",
      "Epoch 00373: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1203 - acc: 0.6541 - val_loss: 0.9056 - val_acc: 0.7377\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1168 - acc: 0.6521\n",
      "Epoch 00374: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1168 - acc: 0.6521 - val_loss: 0.8661 - val_acc: 0.7591\n",
      "Epoch 375/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1202 - acc: 0.6552\n",
      "Epoch 00375: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1200 - acc: 0.6553 - val_loss: 0.8677 - val_acc: 0.7538\n",
      "Epoch 376/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1147 - acc: 0.6582\n",
      "Epoch 00376: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1148 - acc: 0.6579 - val_loss: 0.8780 - val_acc: 0.7522\n",
      "Epoch 377/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1121 - acc: 0.6550\n",
      "Epoch 00377: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1127 - acc: 0.6549 - val_loss: 0.8616 - val_acc: 0.7549\n",
      "Epoch 378/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1130 - acc: 0.6546\n",
      "Epoch 00378: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1134 - acc: 0.6546 - val_loss: 2.7252 - val_acc: 0.4205\n",
      "Epoch 379/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1139 - acc: 0.6566\n",
      "Epoch 00379: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1136 - acc: 0.6567 - val_loss: 1.0471 - val_acc: 0.6676\n",
      "Epoch 380/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1155 - acc: 0.6570\n",
      "Epoch 00380: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1152 - acc: 0.6571 - val_loss: 0.8797 - val_acc: 0.7503\n",
      "Epoch 381/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1183 - acc: 0.6555\n",
      "Epoch 00381: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1183 - acc: 0.6556 - val_loss: 0.8640 - val_acc: 0.7538\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1172 - acc: 0.6540\n",
      "Epoch 00382: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1173 - acc: 0.6540 - val_loss: 0.8973 - val_acc: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1177 - acc: 0.6546\n",
      "Epoch 00383: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1173 - acc: 0.6547 - val_loss: 0.8659 - val_acc: 0.7510\n",
      "Epoch 384/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1156 - acc: 0.6557\n",
      "Epoch 00384: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1153 - acc: 0.6556 - val_loss: 0.8599 - val_acc: 0.7624\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1152 - acc: 0.6570\n",
      "Epoch 00385: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1153 - acc: 0.6570 - val_loss: 0.8924 - val_acc: 0.7519\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1159 - acc: 0.6540\n",
      "Epoch 00386: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1161 - acc: 0.6539 - val_loss: 0.8782 - val_acc: 0.7482\n",
      "Epoch 387/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.6537\n",
      "Epoch 00387: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1132 - acc: 0.6534 - val_loss: 1.0080 - val_acc: 0.6809\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.6574\n",
      "Epoch 00388: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1076 - acc: 0.6574 - val_loss: 0.8809 - val_acc: 0.7449\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1142 - acc: 0.6561\n",
      "Epoch 00389: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1142 - acc: 0.6561 - val_loss: 0.8694 - val_acc: 0.7540\n",
      "Epoch 390/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1158 - acc: 0.6568\n",
      "Epoch 00390: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1154 - acc: 0.6568 - val_loss: 0.9124 - val_acc: 0.7470\n",
      "Epoch 391/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1165 - acc: 0.6570\n",
      "Epoch 00391: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1166 - acc: 0.6568 - val_loss: 0.8820 - val_acc: 0.7657\n",
      "Epoch 392/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1062 - acc: 0.6577\n",
      "Epoch 00392: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1061 - acc: 0.6577 - val_loss: 1.0177 - val_acc: 0.6767\n",
      "Epoch 393/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1108 - acc: 0.6580\n",
      "Epoch 00393: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1110 - acc: 0.6579 - val_loss: 0.9453 - val_acc: 0.7128\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1079 - acc: 0.6574\n",
      "Epoch 00394: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1080 - acc: 0.6574 - val_loss: 1.9076 - val_acc: 0.4864\n",
      "Epoch 395/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.6566\n",
      "Epoch 00395: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1138 - acc: 0.6566 - val_loss: 0.9466 - val_acc: 0.7221\n",
      "Epoch 396/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1119 - acc: 0.6600\n",
      "Epoch 00396: val_loss did not improve from 0.85774\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1120 - acc: 0.6600 - val_loss: 0.8739 - val_acc: 0.7435\n",
      "Epoch 397/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1165 - acc: 0.6558\n",
      "Epoch 00397: val_loss improved from 0.85774 to 0.85362, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/397-0.8536.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1169 - acc: 0.6557 - val_loss: 0.8536 - val_acc: 0.7596\n",
      "Epoch 398/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1117 - acc: 0.6544\n",
      "Epoch 00398: val_loss improved from 0.85362 to 0.84097, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/398-0.8410.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1117 - acc: 0.6545 - val_loss: 0.8410 - val_acc: 0.7675\n",
      "Epoch 399/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1081 - acc: 0.6569\n",
      "Epoch 00399: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1079 - acc: 0.6570 - val_loss: 0.9188 - val_acc: 0.7314\n",
      "Epoch 400/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.6585\n",
      "Epoch 00400: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1060 - acc: 0.6587 - val_loss: 1.1619 - val_acc: 0.6168\n",
      "Epoch 401/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6598\n",
      "Epoch 00401: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1034 - acc: 0.6598 - val_loss: 0.8844 - val_acc: 0.7498\n",
      "Epoch 402/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1078 - acc: 0.6563\n",
      "Epoch 00402: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1077 - acc: 0.6563 - val_loss: 0.9096 - val_acc: 0.7321\n",
      "Epoch 403/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6588\n",
      "Epoch 00403: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1026 - acc: 0.6590 - val_loss: 0.8697 - val_acc: 0.7487\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1045 - acc: 0.6585\n",
      "Epoch 00404: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1047 - acc: 0.6584 - val_loss: 0.8590 - val_acc: 0.7575\n",
      "Epoch 405/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1041 - acc: 0.6594\n",
      "Epoch 00405: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1047 - acc: 0.6591 - val_loss: 0.8531 - val_acc: 0.7671\n",
      "Epoch 406/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1075 - acc: 0.6564\n",
      "Epoch 00406: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1078 - acc: 0.6566 - val_loss: 5.1690 - val_acc: 0.2795\n",
      "Epoch 407/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.6613\n",
      "Epoch 00407: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0970 - acc: 0.6612 - val_loss: 0.9009 - val_acc: 0.7438\n",
      "Epoch 408/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1037 - acc: 0.6603\n",
      "Epoch 00408: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1041 - acc: 0.6603 - val_loss: 0.8572 - val_acc: 0.7685\n",
      "Epoch 409/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1126 - acc: 0.6564\n",
      "Epoch 00409: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1131 - acc: 0.6563 - val_loss: 1.1179 - val_acc: 0.6441\n",
      "Epoch 410/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.6582\n",
      "Epoch 00410: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1051 - acc: 0.6580 - val_loss: 0.8628 - val_acc: 0.7591\n",
      "Epoch 411/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1075 - acc: 0.6565\n",
      "Epoch 00411: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1082 - acc: 0.6563 - val_loss: 0.8935 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1100 - acc: 0.6587\n",
      "Epoch 00412: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1100 - acc: 0.6587 - val_loss: 0.8843 - val_acc: 0.7498\n",
      "Epoch 413/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1041 - acc: 0.6577\n",
      "Epoch 00413: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1041 - acc: 0.6577 - val_loss: 1.7090 - val_acc: 0.5059\n",
      "Epoch 414/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1027 - acc: 0.6593\n",
      "Epoch 00414: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1026 - acc: 0.6592 - val_loss: 0.9046 - val_acc: 0.7384\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1014 - acc: 0.6573\n",
      "Epoch 00415: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1014 - acc: 0.6572 - val_loss: 0.8534 - val_acc: 0.7554\n",
      "Epoch 416/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1049 - acc: 0.6618\n",
      "Epoch 00416: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1049 - acc: 0.6618 - val_loss: 0.8738 - val_acc: 0.7538\n",
      "Epoch 417/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0989 - acc: 0.6607\n",
      "Epoch 00417: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.0992 - acc: 0.6605 - val_loss: 0.8618 - val_acc: 0.7575\n",
      "Epoch 418/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1059 - acc: 0.6560\n",
      "Epoch 00418: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.1059 - acc: 0.6560 - val_loss: 0.8474 - val_acc: 0.7608\n",
      "Epoch 419/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1014 - acc: 0.6580\n",
      "Epoch 00419: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1013 - acc: 0.6580 - val_loss: 0.8503 - val_acc: 0.7596\n",
      "Epoch 420/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1004 - acc: 0.6585\n",
      "Epoch 00420: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1000 - acc: 0.6587 - val_loss: 0.8561 - val_acc: 0.7594\n",
      "Epoch 421/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.6616\n",
      "Epoch 00421: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0933 - acc: 0.6616 - val_loss: 0.9019 - val_acc: 0.7363\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6610\n",
      "Epoch 00422: val_loss did not improve from 0.84097\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0975 - acc: 0.6610 - val_loss: 0.9312 - val_acc: 0.7112\n",
      "Epoch 423/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1006 - acc: 0.6600\n",
      "Epoch 00423: val_loss improved from 0.84097 to 0.84044, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/423-0.8404.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1006 - acc: 0.6599 - val_loss: 0.8404 - val_acc: 0.7734\n",
      "Epoch 424/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0997 - acc: 0.6642\n",
      "Epoch 00424: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0997 - acc: 0.6640 - val_loss: 0.9679 - val_acc: 0.7023\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.6613\n",
      "Epoch 00425: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0992 - acc: 0.6613 - val_loss: 0.9003 - val_acc: 0.7449\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6601\n",
      "Epoch 00426: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0973 - acc: 0.6601 - val_loss: 1.3667 - val_acc: 0.5684\n",
      "Epoch 427/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0971 - acc: 0.6611\n",
      "Epoch 00427: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0971 - acc: 0.6611 - val_loss: 0.9009 - val_acc: 0.7365\n",
      "Epoch 428/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1010 - acc: 0.6549\n",
      "Epoch 00428: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1014 - acc: 0.6550 - val_loss: 0.8525 - val_acc: 0.7584\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0898 - acc: 0.6619\n",
      "Epoch 00429: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0903 - acc: 0.6619 - val_loss: 1.0169 - val_acc: 0.6818\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1054 - acc: 0.6595\n",
      "Epoch 00430: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.1054 - acc: 0.6594 - val_loss: 0.8562 - val_acc: 0.7512\n",
      "Epoch 431/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0954 - acc: 0.6635\n",
      "Epoch 00431: val_loss did not improve from 0.84044\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0954 - acc: 0.6636 - val_loss: 1.6910 - val_acc: 0.4736\n",
      "Epoch 432/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0985 - acc: 0.6611\n",
      "Epoch 00432: val_loss improved from 0.84044 to 0.83992, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/432-0.8399.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.0987 - acc: 0.6610 - val_loss: 0.8399 - val_acc: 0.7708\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0957 - acc: 0.6612\n",
      "Epoch 00433: val_loss improved from 0.83992 to 0.83081, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/433-0.8308.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.0957 - acc: 0.6612 - val_loss: 0.8308 - val_acc: 0.7685\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0916 - acc: 0.6626\n",
      "Epoch 00434: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0916 - acc: 0.6625 - val_loss: 0.8466 - val_acc: 0.7657\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0937 - acc: 0.6633\n",
      "Epoch 00435: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0937 - acc: 0.6634 - val_loss: 1.7493 - val_acc: 0.5045\n",
      "Epoch 436/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1004 - acc: 0.6576\n",
      "Epoch 00436: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1002 - acc: 0.6576 - val_loss: 1.0158 - val_acc: 0.6799\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0989 - acc: 0.6583\n",
      "Epoch 00437: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0988 - acc: 0.6583 - val_loss: 1.0390 - val_acc: 0.6667\n",
      "Epoch 438/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.6619\n",
      "Epoch 00438: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0938 - acc: 0.6618 - val_loss: 0.8757 - val_acc: 0.7503\n",
      "Epoch 439/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0961 - acc: 0.6623\n",
      "Epoch 00439: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0961 - acc: 0.6623 - val_loss: 0.8652 - val_acc: 0.7522\n",
      "Epoch 440/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6626\n",
      "Epoch 00440: val_loss did not improve from 0.83081\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0950 - acc: 0.6625 - val_loss: 1.2786 - val_acc: 0.6105\n",
      "Epoch 441/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0936 - acc: 0.6613\n",
      "Epoch 00441: val_loss improved from 0.83081 to 0.82987, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/441-0.8299.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.0937 - acc: 0.6614 - val_loss: 0.8299 - val_acc: 0.7768\n",
      "Epoch 442/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0970 - acc: 0.6630\n",
      "Epoch 00442: val_loss did not improve from 0.82987\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0974 - acc: 0.6629 - val_loss: 0.9512 - val_acc: 0.6988\n",
      "Epoch 443/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1009 - acc: 0.6599\n",
      "Epoch 00443: val_loss did not improve from 0.82987\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.1008 - acc: 0.6599 - val_loss: 0.8603 - val_acc: 0.7568\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0944 - acc: 0.6618\n",
      "Epoch 00444: val_loss did not improve from 0.82987\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0944 - acc: 0.6619 - val_loss: 1.3511 - val_acc: 0.5747\n",
      "Epoch 445/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0934 - acc: 0.6605\n",
      "Epoch 00445: val_loss improved from 0.82987 to 0.82945, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/445-0.8294.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0928 - acc: 0.6607 - val_loss: 0.8294 - val_acc: 0.7710\n",
      "Epoch 446/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0916 - acc: 0.6613\n",
      "Epoch 00446: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0912 - acc: 0.6615 - val_loss: 0.8453 - val_acc: 0.7650\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0892 - acc: 0.6649\n",
      "Epoch 00447: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0896 - acc: 0.6649 - val_loss: 0.8688 - val_acc: 0.7645\n",
      "Epoch 448/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.6656\n",
      "Epoch 00448: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0908 - acc: 0.6655 - val_loss: 0.8298 - val_acc: 0.7724\n",
      "Epoch 449/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0931 - acc: 0.6667\n",
      "Epoch 00449: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0931 - acc: 0.6666 - val_loss: 0.8480 - val_acc: 0.7582\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0928 - acc: 0.6629\n",
      "Epoch 00450: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0928 - acc: 0.6630 - val_loss: 0.9066 - val_acc: 0.7244\n",
      "Epoch 451/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.6628\n",
      "Epoch 00451: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0902 - acc: 0.6629 - val_loss: 0.8445 - val_acc: 0.7615\n",
      "Epoch 452/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.6646\n",
      "Epoch 00452: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0878 - acc: 0.6643 - val_loss: 0.8577 - val_acc: 0.7561\n",
      "Epoch 453/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.6634\n",
      "Epoch 00453: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0899 - acc: 0.6635 - val_loss: 0.8920 - val_acc: 0.7515\n",
      "Epoch 454/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0880 - acc: 0.6654\n",
      "Epoch 00454: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0879 - acc: 0.6655 - val_loss: 0.8504 - val_acc: 0.7596\n",
      "Epoch 455/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6616\n",
      "Epoch 00455: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0924 - acc: 0.6613 - val_loss: 0.8529 - val_acc: 0.7582\n",
      "Epoch 456/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0951 - acc: 0.6611\n",
      "Epoch 00456: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0951 - acc: 0.6611 - val_loss: 0.8664 - val_acc: 0.7652\n",
      "Epoch 457/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0894 - acc: 0.6626\n",
      "Epoch 00457: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0897 - acc: 0.6627 - val_loss: 0.8525 - val_acc: 0.7540\n",
      "Epoch 458/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0923 - acc: 0.6593\n",
      "Epoch 00458: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0923 - acc: 0.6593 - val_loss: 0.8566 - val_acc: 0.7589\n",
      "Epoch 459/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0800 - acc: 0.6678\n",
      "Epoch 00459: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0798 - acc: 0.6677 - val_loss: 0.8504 - val_acc: 0.7587\n",
      "Epoch 460/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0845 - acc: 0.6661\n",
      "Epoch 00460: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.0843 - acc: 0.6662 - val_loss: 0.9878 - val_acc: 0.6823\n",
      "Epoch 461/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0911 - acc: 0.6599\n",
      "Epoch 00461: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0910 - acc: 0.6599 - val_loss: 1.2083 - val_acc: 0.6504\n",
      "Epoch 462/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6628\n",
      "Epoch 00462: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0995 - acc: 0.6627 - val_loss: 0.8484 - val_acc: 0.7631\n",
      "Epoch 463/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0872 - acc: 0.6652\n",
      "Epoch 00463: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0878 - acc: 0.6651 - val_loss: 0.8800 - val_acc: 0.7361\n",
      "Epoch 464/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0917 - acc: 0.6622\n",
      "Epoch 00464: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0915 - acc: 0.6622 - val_loss: 0.8590 - val_acc: 0.7608\n",
      "Epoch 465/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.6655\n",
      "Epoch 00465: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0899 - acc: 0.6656 - val_loss: 1.0410 - val_acc: 0.6813\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0941 - acc: 0.6604\n",
      "Epoch 00466: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.0941 - acc: 0.6604 - val_loss: 0.9063 - val_acc: 0.7377\n",
      "Epoch 467/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.6658\n",
      "Epoch 00467: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0840 - acc: 0.6658 - val_loss: 1.4051 - val_acc: 0.5691\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0844 - acc: 0.6658\n",
      "Epoch 00468: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0844 - acc: 0.6658 - val_loss: 0.8354 - val_acc: 0.7699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0875 - acc: 0.6651\n",
      "Epoch 00469: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0875 - acc: 0.6652 - val_loss: 0.8543 - val_acc: 0.7647\n",
      "Epoch 470/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.6643\n",
      "Epoch 00470: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0838 - acc: 0.6645 - val_loss: 0.8944 - val_acc: 0.7412\n",
      "Epoch 471/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0891 - acc: 0.6633\n",
      "Epoch 00471: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0892 - acc: 0.6633 - val_loss: 0.8963 - val_acc: 0.7449\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0814 - acc: 0.6623\n",
      "Epoch 00472: val_loss did not improve from 0.82945\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0814 - acc: 0.6624 - val_loss: 0.9268 - val_acc: 0.7154\n",
      "Epoch 473/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0813 - acc: 0.6634\n",
      "Epoch 00473: val_loss improved from 0.82945 to 0.82629, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/473-0.8263.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0814 - acc: 0.6634 - val_loss: 0.8263 - val_acc: 0.7682\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.6632\n",
      "Epoch 00474: val_loss did not improve from 0.82629\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0842 - acc: 0.6633 - val_loss: 0.8467 - val_acc: 0.7647\n",
      "Epoch 475/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0861 - acc: 0.6634\n",
      "Epoch 00475: val_loss did not improve from 0.82629\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0859 - acc: 0.6635 - val_loss: 0.9370 - val_acc: 0.7228\n",
      "Epoch 476/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0878 - acc: 0.6623\n",
      "Epoch 00476: val_loss did not improve from 0.82629\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0879 - acc: 0.6622 - val_loss: 1.2313 - val_acc: 0.6126\n",
      "Epoch 477/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0813 - acc: 0.6637\n",
      "Epoch 00477: val_loss improved from 0.82629 to 0.82301, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/477-0.8230.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.0812 - acc: 0.6639 - val_loss: 0.8230 - val_acc: 0.7708\n",
      "Epoch 478/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0824 - acc: 0.6659\n",
      "Epoch 00478: val_loss did not improve from 0.82301\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0827 - acc: 0.6659 - val_loss: 0.8523 - val_acc: 0.7587\n",
      "Epoch 479/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.6629\n",
      "Epoch 00479: val_loss did not improve from 0.82301\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.0853 - acc: 0.6630 - val_loss: 0.8508 - val_acc: 0.7568\n",
      "Epoch 480/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0856 - acc: 0.6668\n",
      "Epoch 00480: val_loss did not improve from 0.82301\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.0853 - acc: 0.6668 - val_loss: 0.8290 - val_acc: 0.7661\n",
      "Epoch 481/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.6606\n",
      "Epoch 00481: val_loss improved from 0.82301 to 0.81536, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv_checkpoint/481-0.8154.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.0931 - acc: 0.6606 - val_loss: 0.8154 - val_acc: 0.7775\n",
      "Epoch 482/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0843 - acc: 0.6656\n",
      "Epoch 00482: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0841 - acc: 0.6655 - val_loss: 1.3130 - val_acc: 0.5644\n",
      "Epoch 483/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0819 - acc: 0.6641\n",
      "Epoch 00483: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0819 - acc: 0.6641 - val_loss: 1.5224 - val_acc: 0.5036\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0847 - acc: 0.6650\n",
      "Epoch 00484: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.0847 - acc: 0.6650 - val_loss: 0.9046 - val_acc: 0.7321\n",
      "Epoch 485/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0826 - acc: 0.6668\n",
      "Epoch 00485: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0829 - acc: 0.6668 - val_loss: 0.8418 - val_acc: 0.7664\n",
      "Epoch 486/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0849 - acc: 0.6671\n",
      "Epoch 00486: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0853 - acc: 0.6669 - val_loss: 0.8403 - val_acc: 0.7631\n",
      "Epoch 487/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0829 - acc: 0.6673\n",
      "Epoch 00487: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.0829 - acc: 0.6674 - val_loss: 1.6453 - val_acc: 0.5376\n",
      "Epoch 488/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0858 - acc: 0.6672\n",
      "Epoch 00488: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0861 - acc: 0.6673 - val_loss: 0.8227 - val_acc: 0.7729\n",
      "Epoch 489/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0867 - acc: 0.6637\n",
      "Epoch 00489: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 1.0879 - acc: 0.6635 - val_loss: 1.1761 - val_acc: 0.6271\n",
      "Epoch 490/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0811 - acc: 0.6645\n",
      "Epoch 00490: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0811 - acc: 0.6645 - val_loss: 1.0149 - val_acc: 0.6730\n",
      "Epoch 491/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0825 - acc: 0.6653\n",
      "Epoch 00491: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0827 - acc: 0.6651 - val_loss: 0.8258 - val_acc: 0.7673\n",
      "Epoch 492/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0818 - acc: 0.6637\n",
      "Epoch 00492: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0818 - acc: 0.6639 - val_loss: 1.3619 - val_acc: 0.5931\n",
      "Epoch 493/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0783 - acc: 0.6663\n",
      "Epoch 00493: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0779 - acc: 0.6664 - val_loss: 0.9282 - val_acc: 0.7293\n",
      "Epoch 494/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0869 - acc: 0.6652\n",
      "Epoch 00494: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0868 - acc: 0.6652 - val_loss: 1.1748 - val_acc: 0.6434\n",
      "Epoch 495/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.6677\n",
      "Epoch 00495: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.0808 - acc: 0.6677 - val_loss: 0.8313 - val_acc: 0.7689\n",
      "Epoch 496/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0786 - acc: 0.6671\n",
      "Epoch 00496: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 1.0788 - acc: 0.6672 - val_loss: 1.2319 - val_acc: 0.6124\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0745 - acc: 0.6695\n",
      "Epoch 00497: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0744 - acc: 0.6695 - val_loss: 0.8606 - val_acc: 0.7549\n",
      "Epoch 498/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0790 - acc: 0.6655\n",
      "Epoch 00498: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0792 - acc: 0.6655 - val_loss: 0.8384 - val_acc: 0.7694\n",
      "Epoch 499/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0769 - acc: 0.6675\n",
      "Epoch 00499: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.0770 - acc: 0.6674 - val_loss: 0.8302 - val_acc: 0.7682\n",
      "Epoch 500/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0831 - acc: 0.6676\n",
      "Epoch 00500: val_loss did not improve from 0.81536\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 1.0826 - acc: 0.6679 - val_loss: 0.9518 - val_acc: 0.7254\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 389us/sample - loss: 0.8598 - acc: 0.7466\n",
      "Loss: 0.8598382489455947 Accuracy: 0.7466251\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.6535 - acc: 0.1496\n",
      "Epoch 00001: val_loss improved from inf to 2.44409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/001-2.4441.hdf5\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 2.6531 - acc: 0.1498 - val_loss: 2.4441 - val_acc: 0.3040\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3406 - acc: 0.2442\n",
      "Epoch 00002: val_loss improved from 2.44409 to 2.12482, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/002-2.1248.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 2.3403 - acc: 0.2442 - val_loss: 2.1248 - val_acc: 0.4118\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1939 - acc: 0.2920\n",
      "Epoch 00003: val_loss improved from 2.12482 to 1.98249, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/003-1.9825.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 2.1939 - acc: 0.2919 - val_loss: 1.9825 - val_acc: 0.4419\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0831 - acc: 0.3269\n",
      "Epoch 00004: val_loss improved from 1.98249 to 1.86156, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/004-1.8616.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 2.0831 - acc: 0.3269 - val_loss: 1.8616 - val_acc: 0.4847\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9916 - acc: 0.3559\n",
      "Epoch 00005: val_loss improved from 1.86156 to 1.77660, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/005-1.7766.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.9919 - acc: 0.3558 - val_loss: 1.7766 - val_acc: 0.5195\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9148 - acc: 0.3850\n",
      "Epoch 00006: val_loss improved from 1.77660 to 1.69167, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/006-1.6917.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.9145 - acc: 0.3851 - val_loss: 1.6917 - val_acc: 0.5376\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8457 - acc: 0.4076\n",
      "Epoch 00007: val_loss improved from 1.69167 to 1.63154, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/007-1.6315.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.8455 - acc: 0.4077 - val_loss: 1.6315 - val_acc: 0.5684\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7908 - acc: 0.4265\n",
      "Epoch 00008: val_loss improved from 1.63154 to 1.56628, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/008-1.5663.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.7912 - acc: 0.4266 - val_loss: 1.5663 - val_acc: 0.5814\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7390 - acc: 0.4418\n",
      "Epoch 00009: val_loss improved from 1.56628 to 1.52945, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/009-1.5294.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.7389 - acc: 0.4418 - val_loss: 1.5294 - val_acc: 0.6003\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6957 - acc: 0.4601\n",
      "Epoch 00010: val_loss improved from 1.52945 to 1.48630, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/010-1.4863.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.6957 - acc: 0.4600 - val_loss: 1.4863 - val_acc: 0.6040\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6566 - acc: 0.4716\n",
      "Epoch 00011: val_loss improved from 1.48630 to 1.44551, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/011-1.4455.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.6566 - acc: 0.4716 - val_loss: 1.4455 - val_acc: 0.6154\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6163 - acc: 0.4862\n",
      "Epoch 00012: val_loss improved from 1.44551 to 1.38764, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/012-1.3876.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.6163 - acc: 0.4862 - val_loss: 1.3876 - val_acc: 0.6171\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5799 - acc: 0.4982\n",
      "Epoch 00013: val_loss improved from 1.38764 to 1.35904, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/013-1.3590.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.5797 - acc: 0.4982 - val_loss: 1.3590 - val_acc: 0.6429\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5441 - acc: 0.5133\n",
      "Epoch 00014: val_loss improved from 1.35904 to 1.30531, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/014-1.3053.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.5441 - acc: 0.5133 - val_loss: 1.3053 - val_acc: 0.6548\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5240 - acc: 0.5175\n",
      "Epoch 00015: val_loss improved from 1.30531 to 1.29296, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/015-1.2930.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.5241 - acc: 0.5175 - val_loss: 1.2930 - val_acc: 0.6639\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4957 - acc: 0.5307\n",
      "Epoch 00016: val_loss improved from 1.29296 to 1.26672, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/016-1.2667.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.4957 - acc: 0.5306 - val_loss: 1.2667 - val_acc: 0.6664\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4804 - acc: 0.5340\n",
      "Epoch 00017: val_loss improved from 1.26672 to 1.24601, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/017-1.2460.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.4806 - acc: 0.5338 - val_loss: 1.2460 - val_acc: 0.6539\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4520 - acc: 0.5417\n",
      "Epoch 00018: val_loss improved from 1.24601 to 1.23152, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/018-1.2315.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.4526 - acc: 0.5417 - val_loss: 1.2315 - val_acc: 0.6762\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4320 - acc: 0.5517\n",
      "Epoch 00019: val_loss improved from 1.23152 to 1.18601, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/019-1.1860.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.4323 - acc: 0.5517 - val_loss: 1.1860 - val_acc: 0.6879\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4078 - acc: 0.5558\n",
      "Epoch 00020: val_loss did not improve from 1.18601\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.4076 - acc: 0.5560 - val_loss: 1.2181 - val_acc: 0.6438\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3953 - acc: 0.5628\n",
      "Epoch 00021: val_loss did not improve from 1.18601\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.3955 - acc: 0.5626 - val_loss: 1.3109 - val_acc: 0.5819\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3799 - acc: 0.5675\n",
      "Epoch 00022: val_loss did not improve from 1.18601\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 1.3801 - acc: 0.5674 - val_loss: 1.2539 - val_acc: 0.6184\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3575 - acc: 0.5773\n",
      "Epoch 00023: val_loss improved from 1.18601 to 1.09972, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/023-1.0997.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 1.3574 - acc: 0.5773 - val_loss: 1.0997 - val_acc: 0.7028\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3479 - acc: 0.5798\n",
      "Epoch 00024: val_loss did not improve from 1.09972\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.3477 - acc: 0.5799 - val_loss: 1.1019 - val_acc: 0.6958\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3325 - acc: 0.5875\n",
      "Epoch 00025: val_loss did not improve from 1.09972\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.3325 - acc: 0.5874 - val_loss: 1.1211 - val_acc: 0.6997\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3187 - acc: 0.5907\n",
      "Epoch 00026: val_loss improved from 1.09972 to 1.06709, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/026-1.0671.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.3187 - acc: 0.5907 - val_loss: 1.0671 - val_acc: 0.7165\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3066 - acc: 0.5938\n",
      "Epoch 00027: val_loss improved from 1.06709 to 1.04574, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/027-1.0457.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.3068 - acc: 0.5937 - val_loss: 1.0457 - val_acc: 0.7135\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2994 - acc: 0.5950\n",
      "Epoch 00028: val_loss did not improve from 1.04574\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.2993 - acc: 0.5952 - val_loss: 1.0592 - val_acc: 0.7039\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2867 - acc: 0.5992\n",
      "Epoch 00029: val_loss did not improve from 1.04574\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.2870 - acc: 0.5992 - val_loss: 1.0609 - val_acc: 0.7165\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2715 - acc: 0.6039\n",
      "Epoch 00030: val_loss improved from 1.04574 to 1.03510, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/030-1.0351.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.2723 - acc: 0.6038 - val_loss: 1.0351 - val_acc: 0.7158\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2628 - acc: 0.6082\n",
      "Epoch 00031: val_loss improved from 1.03510 to 1.02917, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/031-1.0292.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.2628 - acc: 0.6081 - val_loss: 1.0292 - val_acc: 0.7147\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2533 - acc: 0.6096\n",
      "Epoch 00032: val_loss improved from 1.02917 to 1.00736, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/032-1.0074.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.2533 - acc: 0.6097 - val_loss: 1.0074 - val_acc: 0.7300\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2426 - acc: 0.6138\n",
      "Epoch 00033: val_loss improved from 1.00736 to 0.98030, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/033-0.9803.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.2426 - acc: 0.6137 - val_loss: 0.9803 - val_acc: 0.7379\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2404 - acc: 0.6136\n",
      "Epoch 00034: val_loss did not improve from 0.98030\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.2404 - acc: 0.6136 - val_loss: 1.0000 - val_acc: 0.7244\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2236 - acc: 0.6225\n",
      "Epoch 00035: val_loss improved from 0.98030 to 0.97322, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/035-0.9732.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.2239 - acc: 0.6224 - val_loss: 0.9732 - val_acc: 0.7403\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2234 - acc: 0.6227\n",
      "Epoch 00036: val_loss did not improve from 0.97322\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.2234 - acc: 0.6228 - val_loss: 1.0065 - val_acc: 0.7130\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2164 - acc: 0.6247\n",
      "Epoch 00037: val_loss did not improve from 0.97322\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.2163 - acc: 0.6248 - val_loss: 1.0059 - val_acc: 0.7305\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2071 - acc: 0.6270\n",
      "Epoch 00038: val_loss did not improve from 0.97322\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.2075 - acc: 0.6270 - val_loss: 0.9968 - val_acc: 0.7032\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1991 - acc: 0.6308\n",
      "Epoch 00039: val_loss improved from 0.97322 to 0.94859, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/039-0.9486.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.1991 - acc: 0.6308 - val_loss: 0.9486 - val_acc: 0.7445\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1908 - acc: 0.6306\n",
      "Epoch 00040: val_loss did not improve from 0.94859\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.1909 - acc: 0.6306 - val_loss: 0.9639 - val_acc: 0.7331\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1817 - acc: 0.6358\n",
      "Epoch 00041: val_loss improved from 0.94859 to 0.92813, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/041-0.9281.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.1819 - acc: 0.6359 - val_loss: 0.9281 - val_acc: 0.7549\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1784 - acc: 0.6386\n",
      "Epoch 00042: val_loss did not improve from 0.92813\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.1786 - acc: 0.6386 - val_loss: 0.9684 - val_acc: 0.7172\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1697 - acc: 0.6398\n",
      "Epoch 00043: val_loss did not improve from 0.92813\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1700 - acc: 0.6396 - val_loss: 0.9325 - val_acc: 0.7545\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1665 - acc: 0.6419\n",
      "Epoch 00044: val_loss did not improve from 0.92813\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.1663 - acc: 0.6420 - val_loss: 0.9304 - val_acc: 0.7424\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6402\n",
      "Epoch 00045: val_loss improved from 0.92813 to 0.91344, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/045-0.9134.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.1609 - acc: 0.6402 - val_loss: 0.9134 - val_acc: 0.7563\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1483 - acc: 0.6479\n",
      "Epoch 00046: val_loss improved from 0.91344 to 0.91307, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/046-0.9131.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.1484 - acc: 0.6479 - val_loss: 0.9131 - val_acc: 0.7501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1523 - acc: 0.6457\n",
      "Epoch 00047: val_loss improved from 0.91307 to 0.88678, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/047-0.8868.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.1523 - acc: 0.6456 - val_loss: 0.8868 - val_acc: 0.7563\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1425 - acc: 0.6498\n",
      "Epoch 00048: val_loss did not improve from 0.88678\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.1424 - acc: 0.6498 - val_loss: 0.8872 - val_acc: 0.7675\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1337 - acc: 0.6520\n",
      "Epoch 00049: val_loss improved from 0.88678 to 0.88180, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/049-0.8818.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1339 - acc: 0.6519 - val_loss: 0.8818 - val_acc: 0.7619\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1308 - acc: 0.6548\n",
      "Epoch 00050: val_loss did not improve from 0.88180\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.1308 - acc: 0.6548 - val_loss: 1.1705 - val_acc: 0.6222\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1289 - acc: 0.6537\n",
      "Epoch 00051: val_loss improved from 0.88180 to 0.88064, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/051-0.8806.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.1290 - acc: 0.6536 - val_loss: 0.8806 - val_acc: 0.7619\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1250 - acc: 0.6538\n",
      "Epoch 00052: val_loss improved from 0.88064 to 0.87160, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/052-0.8716.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.1251 - acc: 0.6539 - val_loss: 0.8716 - val_acc: 0.7645\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1152 - acc: 0.6577\n",
      "Epoch 00053: val_loss improved from 0.87160 to 0.86477, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/053-0.8648.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.1149 - acc: 0.6578 - val_loss: 0.8648 - val_acc: 0.7666\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1159 - acc: 0.6585\n",
      "Epoch 00054: val_loss improved from 0.86477 to 0.86097, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/054-0.8610.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1166 - acc: 0.6584 - val_loss: 0.8610 - val_acc: 0.7622\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1124 - acc: 0.6603\n",
      "Epoch 00055: val_loss did not improve from 0.86097\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.1125 - acc: 0.6603 - val_loss: 0.8812 - val_acc: 0.7668\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1056 - acc: 0.6637\n",
      "Epoch 00056: val_loss improved from 0.86097 to 0.85916, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/056-0.8592.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.1058 - acc: 0.6637 - val_loss: 0.8592 - val_acc: 0.7731\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1065 - acc: 0.6605\n",
      "Epoch 00057: val_loss did not improve from 0.85916\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1065 - acc: 0.6605 - val_loss: 0.9138 - val_acc: 0.7342\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.6625\n",
      "Epoch 00058: val_loss did not improve from 0.85916\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.0983 - acc: 0.6625 - val_loss: 1.0141 - val_acc: 0.6858\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.6656\n",
      "Epoch 00059: val_loss did not improve from 0.85916\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.0889 - acc: 0.6656 - val_loss: 0.8713 - val_acc: 0.7570\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0907 - acc: 0.6662\n",
      "Epoch 00060: val_loss did not improve from 0.85916\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0908 - acc: 0.6661 - val_loss: 0.8687 - val_acc: 0.7580\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0834 - acc: 0.6665\n",
      "Epoch 00061: val_loss did not improve from 0.85916\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0836 - acc: 0.6664 - val_loss: 0.8637 - val_acc: 0.7473\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0844 - acc: 0.6644\n",
      "Epoch 00062: val_loss improved from 0.85916 to 0.84642, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/062-0.8464.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0843 - acc: 0.6645 - val_loss: 0.8464 - val_acc: 0.7629\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0795 - acc: 0.6673\n",
      "Epoch 00063: val_loss improved from 0.84642 to 0.82920, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/063-0.8292.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.0789 - acc: 0.6674 - val_loss: 0.8292 - val_acc: 0.7785\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0716 - acc: 0.6705\n",
      "Epoch 00064: val_loss did not improve from 0.82920\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0717 - acc: 0.6705 - val_loss: 0.9140 - val_acc: 0.7319\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0715 - acc: 0.6733\n",
      "Epoch 00065: val_loss did not improve from 0.82920\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0716 - acc: 0.6733 - val_loss: 1.5097 - val_acc: 0.5518\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0682 - acc: 0.6717\n",
      "Epoch 00066: val_loss did not improve from 0.82920\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0678 - acc: 0.6719 - val_loss: 0.8416 - val_acc: 0.7584\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0591 - acc: 0.6759\n",
      "Epoch 00067: val_loss did not improve from 0.82920\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.0592 - acc: 0.6758 - val_loss: 1.0438 - val_acc: 0.6718\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0657 - acc: 0.6727\n",
      "Epoch 00068: val_loss improved from 0.82920 to 0.82833, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/068-0.8283.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0661 - acc: 0.6727 - val_loss: 0.8283 - val_acc: 0.7727\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0551 - acc: 0.6756\n",
      "Epoch 00069: val_loss improved from 0.82833 to 0.82715, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/069-0.8272.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.0552 - acc: 0.6756 - val_loss: 0.8272 - val_acc: 0.7750\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0538 - acc: 0.6754\n",
      "Epoch 00070: val_loss improved from 0.82715 to 0.82674, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/070-0.8267.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0533 - acc: 0.6756 - val_loss: 0.8267 - val_acc: 0.7687\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0445 - acc: 0.6834\n",
      "Epoch 00071: val_loss improved from 0.82674 to 0.79985, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/071-0.7998.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0446 - acc: 0.6834 - val_loss: 0.7998 - val_acc: 0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0438 - acc: 0.6821\n",
      "Epoch 00072: val_loss did not improve from 0.79985\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.0441 - acc: 0.6820 - val_loss: 0.8151 - val_acc: 0.7822\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0418 - acc: 0.6825\n",
      "Epoch 00073: val_loss did not improve from 0.79985\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0424 - acc: 0.6824 - val_loss: 0.8087 - val_acc: 0.7845\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0416 - acc: 0.6816\n",
      "Epoch 00074: val_loss did not improve from 0.79985\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0416 - acc: 0.6816 - val_loss: 0.8141 - val_acc: 0.7734\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0349 - acc: 0.6847\n",
      "Epoch 00075: val_loss did not improve from 0.79985\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0349 - acc: 0.6847 - val_loss: 0.8197 - val_acc: 0.7680\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0342 - acc: 0.6819\n",
      "Epoch 00076: val_loss did not improve from 0.79985\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0343 - acc: 0.6819 - val_loss: 0.8061 - val_acc: 0.7775\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0377 - acc: 0.6811\n",
      "Epoch 00077: val_loss improved from 0.79985 to 0.78216, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/077-0.7822.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0376 - acc: 0.6811 - val_loss: 0.7822 - val_acc: 0.7957\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0259 - acc: 0.6890\n",
      "Epoch 00078: val_loss improved from 0.78216 to 0.78088, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/078-0.7809.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.0260 - acc: 0.6889 - val_loss: 0.7809 - val_acc: 0.7957\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0254 - acc: 0.6854\n",
      "Epoch 00079: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.0259 - acc: 0.6853 - val_loss: 0.7946 - val_acc: 0.7848\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0296 - acc: 0.6844\n",
      "Epoch 00080: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0298 - acc: 0.6842 - val_loss: 0.8341 - val_acc: 0.7626\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0208 - acc: 0.6899\n",
      "Epoch 00081: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0209 - acc: 0.6899 - val_loss: 0.8393 - val_acc: 0.7582\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0234 - acc: 0.6878\n",
      "Epoch 00082: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.0236 - acc: 0.6878 - val_loss: 0.8066 - val_acc: 0.7785\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0170 - acc: 0.6877\n",
      "Epoch 00083: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.0170 - acc: 0.6877 - val_loss: 0.8716 - val_acc: 0.7438\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0161 - acc: 0.6887\n",
      "Epoch 00084: val_loss did not improve from 0.78088\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.0161 - acc: 0.6887 - val_loss: 0.7929 - val_acc: 0.7841\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0120 - acc: 0.6915\n",
      "Epoch 00085: val_loss improved from 0.78088 to 0.77300, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/085-0.7730.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.0120 - acc: 0.6915 - val_loss: 0.7730 - val_acc: 0.7971\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0130 - acc: 0.6905\n",
      "Epoch 00086: val_loss improved from 0.77300 to 0.76107, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/086-0.7611.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.0131 - acc: 0.6904 - val_loss: 0.7611 - val_acc: 0.8001\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0119 - acc: 0.6896\n",
      "Epoch 00087: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.0120 - acc: 0.6896 - val_loss: 0.7784 - val_acc: 0.7899\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0029 - acc: 0.6938\n",
      "Epoch 00088: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0028 - acc: 0.6938 - val_loss: 0.7927 - val_acc: 0.7761\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9986 - acc: 0.6966\n",
      "Epoch 00089: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9986 - acc: 0.6966 - val_loss: 0.9579 - val_acc: 0.6983\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0046 - acc: 0.6948\n",
      "Epoch 00090: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0048 - acc: 0.6948 - val_loss: 0.7735 - val_acc: 0.7920\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0009 - acc: 0.6946\n",
      "Epoch 00091: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 1.0009 - acc: 0.6946 - val_loss: 0.8051 - val_acc: 0.7766\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9988 - acc: 0.6949\n",
      "Epoch 00092: val_loss did not improve from 0.76107\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.9988 - acc: 0.6950 - val_loss: 0.8008 - val_acc: 0.7699\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9921 - acc: 0.6980\n",
      "Epoch 00093: val_loss improved from 0.76107 to 0.75425, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/093-0.7543.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9924 - acc: 0.6981 - val_loss: 0.7543 - val_acc: 0.7969\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9919 - acc: 0.6960\n",
      "Epoch 00094: val_loss did not improve from 0.75425\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9913 - acc: 0.6962 - val_loss: 0.8128 - val_acc: 0.7680\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9878 - acc: 0.6996\n",
      "Epoch 00095: val_loss did not improve from 0.75425\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9875 - acc: 0.6997 - val_loss: 0.7738 - val_acc: 0.7934\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9906 - acc: 0.6976\n",
      "Epoch 00096: val_loss did not improve from 0.75425\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9906 - acc: 0.6976 - val_loss: 0.7683 - val_acc: 0.7915\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9891 - acc: 0.6972\n",
      "Epoch 00097: val_loss did not improve from 0.75425\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9891 - acc: 0.6972 - val_loss: 0.7738 - val_acc: 0.7899\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9849 - acc: 0.6988\n",
      "Epoch 00098: val_loss improved from 0.75425 to 0.73937, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/098-0.7394.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9846 - acc: 0.6989 - val_loss: 0.7394 - val_acc: 0.8071\n",
      "Epoch 99/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9809 - acc: 0.7021\n",
      "Epoch 00099: val_loss did not improve from 0.73937\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9806 - acc: 0.7023 - val_loss: 0.8112 - val_acc: 0.7650\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9819 - acc: 0.7016\n",
      "Epoch 00100: val_loss did not improve from 0.73937\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.9824 - acc: 0.7014 - val_loss: 0.7894 - val_acc: 0.7734\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9754 - acc: 0.7054\n",
      "Epoch 00101: val_loss did not improve from 0.73937\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9753 - acc: 0.7054 - val_loss: 0.8140 - val_acc: 0.7622\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9800 - acc: 0.7009\n",
      "Epoch 00102: val_loss improved from 0.73937 to 0.73761, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/102-0.7376.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9800 - acc: 0.7009 - val_loss: 0.7376 - val_acc: 0.8034\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9704 - acc: 0.7043\n",
      "Epoch 00103: val_loss improved from 0.73761 to 0.73648, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/103-0.7365.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9704 - acc: 0.7043 - val_loss: 0.7365 - val_acc: 0.8041\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9669 - acc: 0.7058\n",
      "Epoch 00104: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9670 - acc: 0.7059 - val_loss: 0.7747 - val_acc: 0.7897\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9701 - acc: 0.7041\n",
      "Epoch 00105: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9704 - acc: 0.7040 - val_loss: 0.7561 - val_acc: 0.7936\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9647 - acc: 0.7062\n",
      "Epoch 00106: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9648 - acc: 0.7062 - val_loss: 0.7548 - val_acc: 0.7973\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9628 - acc: 0.7072\n",
      "Epoch 00107: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9629 - acc: 0.7072 - val_loss: 0.7434 - val_acc: 0.8029\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.7042\n",
      "Epoch 00108: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9664 - acc: 0.7042 - val_loss: 0.9238 - val_acc: 0.7077\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9661 - acc: 0.7095\n",
      "Epoch 00109: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.9660 - acc: 0.7095 - val_loss: 0.7673 - val_acc: 0.7871\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9611 - acc: 0.7086\n",
      "Epoch 00110: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9606 - acc: 0.7086 - val_loss: 0.7637 - val_acc: 0.7878\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.7095\n",
      "Epoch 00111: val_loss did not improve from 0.73648\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9610 - acc: 0.7096 - val_loss: 0.7908 - val_acc: 0.7899\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9545 - acc: 0.7111\n",
      "Epoch 00112: val_loss improved from 0.73648 to 0.72769, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/112-0.7277.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9546 - acc: 0.7111 - val_loss: 0.7277 - val_acc: 0.8032\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9581 - acc: 0.7072\n",
      "Epoch 00113: val_loss did not improve from 0.72769\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9583 - acc: 0.7072 - val_loss: 0.8257 - val_acc: 0.7449\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9580 - acc: 0.7073\n",
      "Epoch 00114: val_loss did not improve from 0.72769\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9580 - acc: 0.7072 - val_loss: 0.8640 - val_acc: 0.7419\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9554 - acc: 0.7085\n",
      "Epoch 00115: val_loss did not improve from 0.72769\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9559 - acc: 0.7084 - val_loss: 0.7726 - val_acc: 0.7752\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9502 - acc: 0.7097\n",
      "Epoch 00116: val_loss improved from 0.72769 to 0.70885, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/116-0.7088.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9499 - acc: 0.7098 - val_loss: 0.7088 - val_acc: 0.8150\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.7082\n",
      "Epoch 00117: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9612 - acc: 0.7082 - val_loss: 0.9255 - val_acc: 0.7116\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.7064\n",
      "Epoch 00118: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9538 - acc: 0.7064 - val_loss: 0.8817 - val_acc: 0.7279\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9486 - acc: 0.7121\n",
      "Epoch 00119: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9489 - acc: 0.7120 - val_loss: 0.7866 - val_acc: 0.7801\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9485 - acc: 0.7156\n",
      "Epoch 00120: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9488 - acc: 0.7155 - val_loss: 0.7205 - val_acc: 0.8020\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9418 - acc: 0.7142\n",
      "Epoch 00121: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9420 - acc: 0.7141 - val_loss: 0.7305 - val_acc: 0.7964\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.7155\n",
      "Epoch 00122: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9365 - acc: 0.7155 - val_loss: 0.7260 - val_acc: 0.7978\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9430 - acc: 0.7122\n",
      "Epoch 00123: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9430 - acc: 0.7122 - val_loss: 0.7712 - val_acc: 0.7845\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9444 - acc: 0.7141\n",
      "Epoch 00124: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9444 - acc: 0.7141 - val_loss: 0.7270 - val_acc: 0.8041\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9412 - acc: 0.7140\n",
      "Epoch 00125: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9411 - acc: 0.7141 - val_loss: 0.8275 - val_acc: 0.7552\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.7137\n",
      "Epoch 00126: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9420 - acc: 0.7137 - val_loss: 0.7104 - val_acc: 0.8076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9380 - acc: 0.7172\n",
      "Epoch 00127: val_loss improved from 0.70885 to 0.70684, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/127-0.7068.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9380 - acc: 0.7171 - val_loss: 0.7068 - val_acc: 0.8123\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9283 - acc: 0.7175\n",
      "Epoch 00128: val_loss improved from 0.70684 to 0.69811, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/128-0.6981.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.9283 - acc: 0.7175 - val_loss: 0.6981 - val_acc: 0.8146\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9327 - acc: 0.7173\n",
      "Epoch 00129: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9328 - acc: 0.7173 - val_loss: 0.7378 - val_acc: 0.7997\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9290 - acc: 0.7165\n",
      "Epoch 00130: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.9291 - acc: 0.7164 - val_loss: 0.7180 - val_acc: 0.8113\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9302 - acc: 0.7151\n",
      "Epoch 00131: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9303 - acc: 0.7150 - val_loss: 0.7061 - val_acc: 0.8092\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.7206\n",
      "Epoch 00132: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9268 - acc: 0.7205 - val_loss: 0.7270 - val_acc: 0.7997\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.7199\n",
      "Epoch 00133: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9242 - acc: 0.7197 - val_loss: 0.7383 - val_acc: 0.7948\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7150\n",
      "Epoch 00134: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.9315 - acc: 0.7151 - val_loss: 0.8043 - val_acc: 0.7720\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9155 - acc: 0.7208\n",
      "Epoch 00135: val_loss did not improve from 0.69811\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9156 - acc: 0.7208 - val_loss: 0.7048 - val_acc: 0.8118\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9239 - acc: 0.7210\n",
      "Epoch 00136: val_loss improved from 0.69811 to 0.68665, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/136-0.6867.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9238 - acc: 0.7211 - val_loss: 0.6867 - val_acc: 0.8181\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9263 - acc: 0.7177\n",
      "Epoch 00137: val_loss did not improve from 0.68665\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9262 - acc: 0.7177 - val_loss: 0.7021 - val_acc: 0.8113\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9192 - acc: 0.7209\n",
      "Epoch 00138: val_loss did not improve from 0.68665\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9192 - acc: 0.7209 - val_loss: 0.7144 - val_acc: 0.8053\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.7205\n",
      "Epoch 00139: val_loss did not improve from 0.68665\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9189 - acc: 0.7204 - val_loss: 0.7648 - val_acc: 0.7794\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7190\n",
      "Epoch 00140: val_loss did not improve from 0.68665\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.9162 - acc: 0.7191 - val_loss: 0.7045 - val_acc: 0.8064\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9182 - acc: 0.7227\n",
      "Epoch 00141: val_loss improved from 0.68665 to 0.68076, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/141-0.6808.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.9181 - acc: 0.7227 - val_loss: 0.6808 - val_acc: 0.8206\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9150 - acc: 0.7196\n",
      "Epoch 00142: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9150 - acc: 0.7195 - val_loss: 0.7209 - val_acc: 0.8029\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9176 - acc: 0.7236\n",
      "Epoch 00143: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9181 - acc: 0.7236 - val_loss: 0.6932 - val_acc: 0.8153\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9206 - acc: 0.7198\n",
      "Epoch 00144: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.9212 - acc: 0.7197 - val_loss: 0.7007 - val_acc: 0.8078\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9122 - acc: 0.7218\n",
      "Epoch 00145: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9124 - acc: 0.7217 - val_loss: 0.7393 - val_acc: 0.7952\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.7221\n",
      "Epoch 00146: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9205 - acc: 0.7220 - val_loss: 0.7442 - val_acc: 0.7869\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9087 - acc: 0.7238\n",
      "Epoch 00147: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9091 - acc: 0.7238 - val_loss: 0.6877 - val_acc: 0.8167\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9076 - acc: 0.7255\n",
      "Epoch 00148: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9076 - acc: 0.7254 - val_loss: 0.6864 - val_acc: 0.8157\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9073 - acc: 0.7261\n",
      "Epoch 00149: val_loss did not improve from 0.68076\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.9072 - acc: 0.7261 - val_loss: 1.2261 - val_acc: 0.6401\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7258\n",
      "Epoch 00150: val_loss improved from 0.68076 to 0.67940, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/150-0.6794.hdf5\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.9044 - acc: 0.7258 - val_loss: 0.6794 - val_acc: 0.8251\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9114 - acc: 0.7236\n",
      "Epoch 00151: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9110 - acc: 0.7237 - val_loss: 0.7770 - val_acc: 0.7720\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7271\n",
      "Epoch 00152: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.9078 - acc: 0.7271 - val_loss: 0.7109 - val_acc: 0.8097\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.7245\n",
      "Epoch 00153: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.9029 - acc: 0.7245 - val_loss: 0.7031 - val_acc: 0.8102\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9024 - acc: 0.7264\n",
      "Epoch 00154: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.9030 - acc: 0.7263 - val_loss: 0.6888 - val_acc: 0.8206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7285\n",
      "Epoch 00155: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8972 - acc: 0.7284 - val_loss: 0.7015 - val_acc: 0.8130\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8975 - acc: 0.7283\n",
      "Epoch 00156: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8975 - acc: 0.7283 - val_loss: 0.6949 - val_acc: 0.8213\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8925 - acc: 0.7302\n",
      "Epoch 00157: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8925 - acc: 0.7301 - val_loss: 0.6886 - val_acc: 0.8113\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7280\n",
      "Epoch 00158: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8981 - acc: 0.7279 - val_loss: 0.8200 - val_acc: 0.7575\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8985 - acc: 0.7257\n",
      "Epoch 00159: val_loss did not improve from 0.67940\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8992 - acc: 0.7256 - val_loss: 0.8035 - val_acc: 0.7526\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8933 - acc: 0.7287\n",
      "Epoch 00160: val_loss improved from 0.67940 to 0.66669, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/160-0.6667.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8931 - acc: 0.7288 - val_loss: 0.6667 - val_acc: 0.8227\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7286\n",
      "Epoch 00161: val_loss did not improve from 0.66669\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8926 - acc: 0.7284 - val_loss: 0.7048 - val_acc: 0.8095\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8943 - acc: 0.7293\n",
      "Epoch 00162: val_loss did not improve from 0.66669\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8943 - acc: 0.7293 - val_loss: 0.7254 - val_acc: 0.7955\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7289\n",
      "Epoch 00163: val_loss did not improve from 0.66669\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.8923 - acc: 0.7288 - val_loss: 0.7243 - val_acc: 0.8004\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8930 - acc: 0.7268\n",
      "Epoch 00164: val_loss did not improve from 0.66669\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8933 - acc: 0.7268 - val_loss: 0.7481 - val_acc: 0.7817\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8944 - acc: 0.7303\n",
      "Epoch 00165: val_loss did not improve from 0.66669\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8944 - acc: 0.7303 - val_loss: 0.7552 - val_acc: 0.7845\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7306\n",
      "Epoch 00166: val_loss improved from 0.66669 to 0.66409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/166-0.6641.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8913 - acc: 0.7306 - val_loss: 0.6641 - val_acc: 0.8258\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8873 - acc: 0.7311\n",
      "Epoch 00167: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8874 - acc: 0.7311 - val_loss: 0.6799 - val_acc: 0.8204\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7308\n",
      "Epoch 00168: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8908 - acc: 0.7308 - val_loss: 0.9128 - val_acc: 0.7179\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8886 - acc: 0.7324\n",
      "Epoch 00169: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8888 - acc: 0.7323 - val_loss: 0.8401 - val_acc: 0.7424\n",
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.7308\n",
      "Epoch 00170: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8875 - acc: 0.7308 - val_loss: 0.6744 - val_acc: 0.8227\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8845 - acc: 0.7317\n",
      "Epoch 00171: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8848 - acc: 0.7316 - val_loss: 0.6826 - val_acc: 0.8120\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7281\n",
      "Epoch 00172: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8850 - acc: 0.7281 - val_loss: 0.7560 - val_acc: 0.7866\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8842 - acc: 0.7331\n",
      "Epoch 00173: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8838 - acc: 0.7333 - val_loss: 0.6829 - val_acc: 0.8188\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8843 - acc: 0.7336\n",
      "Epoch 00174: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8844 - acc: 0.7337 - val_loss: 0.7061 - val_acc: 0.8036\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.7343\n",
      "Epoch 00175: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8760 - acc: 0.7343 - val_loss: 0.7305 - val_acc: 0.7908\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8780 - acc: 0.7304\n",
      "Epoch 00176: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8783 - acc: 0.7303 - val_loss: 0.6760 - val_acc: 0.8181\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8825 - acc: 0.7334\n",
      "Epoch 00177: val_loss improved from 0.66409 to 0.65673, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/177-0.6567.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8828 - acc: 0.7334 - val_loss: 0.6567 - val_acc: 0.8295\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8823 - acc: 0.7349\n",
      "Epoch 00178: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8820 - acc: 0.7349 - val_loss: 0.6595 - val_acc: 0.8281\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.7342\n",
      "Epoch 00179: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8812 - acc: 0.7341 - val_loss: 0.6737 - val_acc: 0.8178\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8726 - acc: 0.7331\n",
      "Epoch 00180: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8726 - acc: 0.7332 - val_loss: 0.6637 - val_acc: 0.8204\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8793 - acc: 0.7334\n",
      "Epoch 00181: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8793 - acc: 0.7334 - val_loss: 0.7416 - val_acc: 0.7864\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.7380\n",
      "Epoch 00182: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8758 - acc: 0.7380 - val_loss: 0.6645 - val_acc: 0.8195\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8752 - acc: 0.7337\n",
      "Epoch 00183: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8754 - acc: 0.7337 - val_loss: 0.8952 - val_acc: 0.7142\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8776 - acc: 0.7340\n",
      "Epoch 00184: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8779 - acc: 0.7339 - val_loss: 0.6576 - val_acc: 0.8248\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7334\n",
      "Epoch 00185: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8735 - acc: 0.7334 - val_loss: 0.7496 - val_acc: 0.7915\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.7351\n",
      "Epoch 00186: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8698 - acc: 0.7352 - val_loss: 0.7332 - val_acc: 0.7850\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.7362\n",
      "Epoch 00187: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8756 - acc: 0.7363 - val_loss: 0.7331 - val_acc: 0.7869\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.7367\n",
      "Epoch 00188: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8707 - acc: 0.7367 - val_loss: 0.6915 - val_acc: 0.8111\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7382\n",
      "Epoch 00189: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8689 - acc: 0.7381 - val_loss: 0.6857 - val_acc: 0.8155\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7375\n",
      "Epoch 00190: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8774 - acc: 0.7373 - val_loss: 0.6570 - val_acc: 0.8190\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8642 - acc: 0.7392\n",
      "Epoch 00191: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8648 - acc: 0.7391 - val_loss: 0.6680 - val_acc: 0.8272\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8672 - acc: 0.7357\n",
      "Epoch 00192: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8674 - acc: 0.7356 - val_loss: 0.6702 - val_acc: 0.8174\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8710 - acc: 0.7367\n",
      "Epoch 00193: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8709 - acc: 0.7367 - val_loss: 1.3401 - val_acc: 0.5819\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8649 - acc: 0.7385\n",
      "Epoch 00194: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8648 - acc: 0.7385 - val_loss: 0.7204 - val_acc: 0.7978\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8655 - acc: 0.7397\n",
      "Epoch 00195: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8656 - acc: 0.7397 - val_loss: 0.6624 - val_acc: 0.8216\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7365\n",
      "Epoch 00196: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8633 - acc: 0.7365 - val_loss: 0.6787 - val_acc: 0.8204\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8633 - acc: 0.7409\n",
      "Epoch 00197: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8633 - acc: 0.7409 - val_loss: 0.6746 - val_acc: 0.8134\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.7387\n",
      "Epoch 00198: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8662 - acc: 0.7387 - val_loss: 0.6811 - val_acc: 0.8146\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8635 - acc: 0.7368\n",
      "Epoch 00199: val_loss did not improve from 0.65673\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8635 - acc: 0.7368 - val_loss: 1.7555 - val_acc: 0.5637\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8592 - acc: 0.7400\n",
      "Epoch 00200: val_loss improved from 0.65673 to 0.63747, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/200-0.6375.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8593 - acc: 0.7400 - val_loss: 0.6375 - val_acc: 0.8330\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8589 - acc: 0.7403\n",
      "Epoch 00201: val_loss did not improve from 0.63747\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8589 - acc: 0.7403 - val_loss: 0.6540 - val_acc: 0.8272\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8623 - acc: 0.7365\n",
      "Epoch 00202: val_loss did not improve from 0.63747\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8623 - acc: 0.7365 - val_loss: 0.6542 - val_acc: 0.8225\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8570 - acc: 0.7398\n",
      "Epoch 00203: val_loss did not improve from 0.63747\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8573 - acc: 0.7397 - val_loss: 0.6480 - val_acc: 0.8281\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8593 - acc: 0.7376\n",
      "Epoch 00204: val_loss did not improve from 0.63747\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8593 - acc: 0.7376 - val_loss: 0.6527 - val_acc: 0.8192\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8561 - acc: 0.7391\n",
      "Epoch 00205: val_loss improved from 0.63747 to 0.63734, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/205-0.6373.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8561 - acc: 0.7391 - val_loss: 0.6373 - val_acc: 0.8314\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7415\n",
      "Epoch 00206: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8565 - acc: 0.7416 - val_loss: 0.6946 - val_acc: 0.8006\n",
      "Epoch 207/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8519 - acc: 0.7406\n",
      "Epoch 00207: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8518 - acc: 0.7406 - val_loss: 0.6399 - val_acc: 0.8286\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8587 - acc: 0.7397\n",
      "Epoch 00208: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8586 - acc: 0.7397 - val_loss: 0.6406 - val_acc: 0.8304\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8517 - acc: 0.7431\n",
      "Epoch 00209: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8517 - acc: 0.7431 - val_loss: 0.6650 - val_acc: 0.8162\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8512 - acc: 0.7426\n",
      "Epoch 00210: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8513 - acc: 0.7425 - val_loss: 0.6599 - val_acc: 0.8255\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8547 - acc: 0.7404\n",
      "Epoch 00211: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8546 - acc: 0.7404 - val_loss: 0.7197 - val_acc: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7390\n",
      "Epoch 00212: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8564 - acc: 0.7390 - val_loss: 0.7631 - val_acc: 0.7759\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8505 - acc: 0.7403\n",
      "Epoch 00213: val_loss did not improve from 0.63734\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8506 - acc: 0.7403 - val_loss: 0.9004 - val_acc: 0.7130\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8491 - acc: 0.7433\n",
      "Epoch 00214: val_loss improved from 0.63734 to 0.63011, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/214-0.6301.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8490 - acc: 0.7432 - val_loss: 0.6301 - val_acc: 0.8334\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7451\n",
      "Epoch 00215: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8477 - acc: 0.7451 - val_loss: 0.6439 - val_acc: 0.8311\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8516 - acc: 0.7421\n",
      "Epoch 00216: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8516 - acc: 0.7421 - val_loss: 0.6579 - val_acc: 0.8197\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7423\n",
      "Epoch 00217: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8479 - acc: 0.7420 - val_loss: 0.6548 - val_acc: 0.8220\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8475 - acc: 0.7417\n",
      "Epoch 00218: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8475 - acc: 0.7417 - val_loss: 0.7323 - val_acc: 0.7803\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7423\n",
      "Epoch 00219: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8548 - acc: 0.7422 - val_loss: 0.7460 - val_acc: 0.7843\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8436 - acc: 0.7451\n",
      "Epoch 00220: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8436 - acc: 0.7451 - val_loss: 0.9964 - val_acc: 0.6872\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8458 - acc: 0.7432\n",
      "Epoch 00221: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.8457 - acc: 0.7432 - val_loss: 0.7064 - val_acc: 0.7997\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8519 - acc: 0.7427\n",
      "Epoch 00222: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8523 - acc: 0.7427 - val_loss: 0.8651 - val_acc: 0.7303\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8498 - acc: 0.7417\n",
      "Epoch 00223: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8498 - acc: 0.7417 - val_loss: 0.7087 - val_acc: 0.7971\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7434\n",
      "Epoch 00224: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8456 - acc: 0.7433 - val_loss: 0.6399 - val_acc: 0.8276\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7455\n",
      "Epoch 00225: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8455 - acc: 0.7455 - val_loss: 0.6368 - val_acc: 0.8353\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.7443\n",
      "Epoch 00226: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8397 - acc: 0.7442 - val_loss: 0.6770 - val_acc: 0.8153\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8415 - acc: 0.7458\n",
      "Epoch 00227: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8415 - acc: 0.7458 - val_loss: 0.7277 - val_acc: 0.7929\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8401 - acc: 0.7444\n",
      "Epoch 00228: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8403 - acc: 0.7444 - val_loss: 0.6510 - val_acc: 0.8206\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8438 - acc: 0.7430\n",
      "Epoch 00229: val_loss did not improve from 0.63011\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8438 - acc: 0.7430 - val_loss: 0.6843 - val_acc: 0.8157\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8425 - acc: 0.7451\n",
      "Epoch 00230: val_loss improved from 0.63011 to 0.62305, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/230-0.6231.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8425 - acc: 0.7451 - val_loss: 0.6231 - val_acc: 0.8365\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8441 - acc: 0.7416\n",
      "Epoch 00231: val_loss did not improve from 0.62305\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8439 - acc: 0.7417 - val_loss: 0.6779 - val_acc: 0.8085\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8308 - acc: 0.7490\n",
      "Epoch 00232: val_loss did not improve from 0.62305\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8304 - acc: 0.7491 - val_loss: 0.6722 - val_acc: 0.8183\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7449\n",
      "Epoch 00233: val_loss did not improve from 0.62305\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8371 - acc: 0.7450 - val_loss: 0.6445 - val_acc: 0.8316\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7456\n",
      "Epoch 00234: val_loss improved from 0.62305 to 0.62147, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/234-0.6215.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8418 - acc: 0.7454 - val_loss: 0.6215 - val_acc: 0.8365\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7447\n",
      "Epoch 00235: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8399 - acc: 0.7447 - val_loss: 0.6779 - val_acc: 0.8120\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8358 - acc: 0.7504\n",
      "Epoch 00236: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8359 - acc: 0.7503 - val_loss: 0.6310 - val_acc: 0.8348\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8365 - acc: 0.7479\n",
      "Epoch 00237: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8365 - acc: 0.7479 - val_loss: 0.6871 - val_acc: 0.8092\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.7444\n",
      "Epoch 00238: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8385 - acc: 0.7444 - val_loss: 0.6221 - val_acc: 0.8304\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8354 - acc: 0.7498\n",
      "Epoch 00239: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8351 - acc: 0.7500 - val_loss: 0.6249 - val_acc: 0.8332\n",
      "Epoch 240/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8408 - acc: 0.7464\n",
      "Epoch 00240: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8407 - acc: 0.7464 - val_loss: 0.8102 - val_acc: 0.7598\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7460\n",
      "Epoch 00241: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8342 - acc: 0.7460 - val_loss: 0.9019 - val_acc: 0.7214\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8279 - acc: 0.7464\n",
      "Epoch 00242: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8278 - acc: 0.7465 - val_loss: 0.6454 - val_acc: 0.8279\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8257 - acc: 0.7483\n",
      "Epoch 00243: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8257 - acc: 0.7483 - val_loss: 0.6359 - val_acc: 0.8290\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.7459\n",
      "Epoch 00244: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8335 - acc: 0.7460 - val_loss: 0.8173 - val_acc: 0.7531\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8320 - acc: 0.7466\n",
      "Epoch 00245: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8320 - acc: 0.7466 - val_loss: 0.6450 - val_acc: 0.8241\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8291 - acc: 0.7497\n",
      "Epoch 00246: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8290 - acc: 0.7497 - val_loss: 0.6289 - val_acc: 0.8372\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7462\n",
      "Epoch 00247: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8352 - acc: 0.7461 - val_loss: 0.6485 - val_acc: 0.8160\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8384 - acc: 0.7456\n",
      "Epoch 00248: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8390 - acc: 0.7456 - val_loss: 0.6527 - val_acc: 0.8192\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8318 - acc: 0.7501\n",
      "Epoch 00249: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8317 - acc: 0.7501 - val_loss: 0.6520 - val_acc: 0.8190\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8279 - acc: 0.7505\n",
      "Epoch 00250: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8278 - acc: 0.7506 - val_loss: 0.6894 - val_acc: 0.8013\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8243 - acc: 0.7499\n",
      "Epoch 00251: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8240 - acc: 0.7501 - val_loss: 0.6379 - val_acc: 0.8267\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7496\n",
      "Epoch 00252: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8266 - acc: 0.7495 - val_loss: 0.6235 - val_acc: 0.8339\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8318 - acc: 0.7494\n",
      "Epoch 00253: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8318 - acc: 0.7494 - val_loss: 0.6418 - val_acc: 0.8251\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8274 - acc: 0.7492\n",
      "Epoch 00254: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8274 - acc: 0.7492 - val_loss: 1.1270 - val_acc: 0.6408\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8221 - acc: 0.7512\n",
      "Epoch 00255: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8220 - acc: 0.7512 - val_loss: 0.6319 - val_acc: 0.8288\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7497\n",
      "Epoch 00256: val_loss did not improve from 0.62147\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8230 - acc: 0.7498 - val_loss: 0.6389 - val_acc: 0.8262\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7477\n",
      "Epoch 00257: val_loss improved from 0.62147 to 0.61778, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/257-0.6178.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8286 - acc: 0.7476 - val_loss: 0.6178 - val_acc: 0.8365\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8254 - acc: 0.7507\n",
      "Epoch 00258: val_loss did not improve from 0.61778\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8251 - acc: 0.7507 - val_loss: 0.6246 - val_acc: 0.8318\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8247 - acc: 0.7496\n",
      "Epoch 00259: val_loss did not improve from 0.61778\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8248 - acc: 0.7496 - val_loss: 0.6374 - val_acc: 0.8246\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7520\n",
      "Epoch 00260: val_loss did not improve from 0.61778\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.8194 - acc: 0.7519 - val_loss: 0.6987 - val_acc: 0.8092\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8237 - acc: 0.7504\n",
      "Epoch 00261: val_loss did not improve from 0.61778\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8237 - acc: 0.7504 - val_loss: 0.6183 - val_acc: 0.8318\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8269 - acc: 0.7499\n",
      "Epoch 00262: val_loss improved from 0.61778 to 0.61316, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/262-0.6132.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8271 - acc: 0.7498 - val_loss: 0.6132 - val_acc: 0.8358\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7506\n",
      "Epoch 00263: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8237 - acc: 0.7506 - val_loss: 0.6694 - val_acc: 0.8206\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.7484\n",
      "Epoch 00264: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8270 - acc: 0.7484 - val_loss: 0.6431 - val_acc: 0.8225\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8197 - acc: 0.7512\n",
      "Epoch 00265: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8196 - acc: 0.7513 - val_loss: 0.6297 - val_acc: 0.8283\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8261 - acc: 0.7501\n",
      "Epoch 00266: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8261 - acc: 0.7501 - val_loss: 0.6641 - val_acc: 0.8132\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.7548\n",
      "Epoch 00267: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8121 - acc: 0.7548 - val_loss: 0.6249 - val_acc: 0.8374\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8237 - acc: 0.7509\n",
      "Epoch 00268: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8238 - acc: 0.7509 - val_loss: 0.6209 - val_acc: 0.8351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8219 - acc: 0.7497\n",
      "Epoch 00269: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8219 - acc: 0.7497 - val_loss: 0.6561 - val_acc: 0.8241\n",
      "Epoch 270/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8192 - acc: 0.7521\n",
      "Epoch 00270: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8190 - acc: 0.7520 - val_loss: 0.6226 - val_acc: 0.8351\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8182 - acc: 0.7523\n",
      "Epoch 00271: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8183 - acc: 0.7523 - val_loss: 0.6288 - val_acc: 0.8314\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7506\n",
      "Epoch 00272: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8203 - acc: 0.7505 - val_loss: 0.6494 - val_acc: 0.8211\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8144 - acc: 0.7542\n",
      "Epoch 00273: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8143 - acc: 0.7542 - val_loss: 0.6332 - val_acc: 0.8246\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7550\n",
      "Epoch 00274: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8148 - acc: 0.7550 - val_loss: 0.6210 - val_acc: 0.8297\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7554\n",
      "Epoch 00275: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8113 - acc: 0.7554 - val_loss: 0.6154 - val_acc: 0.8325\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7546\n",
      "Epoch 00276: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8137 - acc: 0.7546 - val_loss: 0.6324 - val_acc: 0.8281\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8207 - acc: 0.7511\n",
      "Epoch 00277: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8207 - acc: 0.7511 - val_loss: 0.6487 - val_acc: 0.8213\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8166 - acc: 0.7518\n",
      "Epoch 00278: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8167 - acc: 0.7518 - val_loss: 0.6371 - val_acc: 0.8241\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.7518\n",
      "Epoch 00279: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8150 - acc: 0.7518 - val_loss: 0.9835 - val_acc: 0.6972\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8125 - acc: 0.7552\n",
      "Epoch 00280: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8126 - acc: 0.7552 - val_loss: 0.6639 - val_acc: 0.8167\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8107 - acc: 0.7517\n",
      "Epoch 00281: val_loss did not improve from 0.61316\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8107 - acc: 0.7517 - val_loss: 0.6255 - val_acc: 0.8255\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7539\n",
      "Epoch 00282: val_loss improved from 0.61316 to 0.61163, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/282-0.6116.hdf5\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8197 - acc: 0.7539 - val_loss: 0.6116 - val_acc: 0.8381\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7549\n",
      "Epoch 00283: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8170 - acc: 0.7548 - val_loss: 0.6226 - val_acc: 0.8351\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8111 - acc: 0.7565\n",
      "Epoch 00284: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8110 - acc: 0.7565 - val_loss: 0.6256 - val_acc: 0.8351\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8126 - acc: 0.7530\n",
      "Epoch 00285: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8132 - acc: 0.7531 - val_loss: 0.6949 - val_acc: 0.8018\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8049 - acc: 0.7573\n",
      "Epoch 00286: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8048 - acc: 0.7573 - val_loss: 0.6235 - val_acc: 0.8328\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8088 - acc: 0.7548\n",
      "Epoch 00287: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8088 - acc: 0.7548 - val_loss: 0.9060 - val_acc: 0.7016\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8076 - acc: 0.7538\n",
      "Epoch 00288: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8076 - acc: 0.7539 - val_loss: 0.7662 - val_acc: 0.7657\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8064 - acc: 0.7532\n",
      "Epoch 00289: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8064 - acc: 0.7532 - val_loss: 0.6871 - val_acc: 0.7943\n",
      "Epoch 290/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8035 - acc: 0.7571\n",
      "Epoch 00290: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8044 - acc: 0.7570 - val_loss: 0.6870 - val_acc: 0.7945\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7534\n",
      "Epoch 00291: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.8177 - acc: 0.7533 - val_loss: 0.6864 - val_acc: 0.8060\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8037 - acc: 0.7585\n",
      "Epoch 00292: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8038 - acc: 0.7585 - val_loss: 0.6727 - val_acc: 0.8062\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8042 - acc: 0.7572\n",
      "Epoch 00293: val_loss did not improve from 0.61163\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.8044 - acc: 0.7572 - val_loss: 0.6786 - val_acc: 0.8095\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8069 - acc: 0.7536\n",
      "Epoch 00294: val_loss improved from 0.61163 to 0.60647, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/294-0.6065.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8068 - acc: 0.7536 - val_loss: 0.6065 - val_acc: 0.8351\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8059 - acc: 0.7539\n",
      "Epoch 00295: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8058 - acc: 0.7539 - val_loss: 0.6314 - val_acc: 0.8269\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7551\n",
      "Epoch 00296: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8055 - acc: 0.7551 - val_loss: 0.6087 - val_acc: 0.8369\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8023 - acc: 0.7590\n",
      "Epoch 00297: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8026 - acc: 0.7591 - val_loss: 0.6193 - val_acc: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8047 - acc: 0.7552\n",
      "Epoch 00298: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8047 - acc: 0.7552 - val_loss: 0.6754 - val_acc: 0.8074\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7570\n",
      "Epoch 00299: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8008 - acc: 0.7569 - val_loss: 0.6297 - val_acc: 0.8286\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8124 - acc: 0.7540\n",
      "Epoch 00300: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8126 - acc: 0.7539 - val_loss: 0.6522 - val_acc: 0.8178\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8064 - acc: 0.7567\n",
      "Epoch 00301: val_loss did not improve from 0.60647\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8065 - acc: 0.7566 - val_loss: 0.6217 - val_acc: 0.8309\n",
      "Epoch 302/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7969 - acc: 0.7593\n",
      "Epoch 00302: val_loss improved from 0.60647 to 0.59938, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/302-0.5994.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7970 - acc: 0.7592 - val_loss: 0.5994 - val_acc: 0.8383\n",
      "Epoch 303/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8115 - acc: 0.7550\n",
      "Epoch 00303: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8117 - acc: 0.7551 - val_loss: 0.6045 - val_acc: 0.8446\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8010 - acc: 0.7579\n",
      "Epoch 00304: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8010 - acc: 0.7579 - val_loss: 0.7113 - val_acc: 0.7962\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8047 - acc: 0.7559\n",
      "Epoch 00305: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8049 - acc: 0.7558 - val_loss: 0.7042 - val_acc: 0.7922\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7572\n",
      "Epoch 00306: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7967 - acc: 0.7571 - val_loss: 0.7315 - val_acc: 0.7782\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.7588\n",
      "Epoch 00307: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7960 - acc: 0.7588 - val_loss: 0.6044 - val_acc: 0.8395\n",
      "Epoch 308/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7963 - acc: 0.7587\n",
      "Epoch 00308: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7963 - acc: 0.7587 - val_loss: 0.7434 - val_acc: 0.7743\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7961 - acc: 0.7601\n",
      "Epoch 00309: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7963 - acc: 0.7600 - val_loss: 0.7474 - val_acc: 0.7715\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7992 - acc: 0.7557\n",
      "Epoch 00310: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7994 - acc: 0.7557 - val_loss: 0.6165 - val_acc: 0.8323\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8069 - acc: 0.7525\n",
      "Epoch 00311: val_loss did not improve from 0.59938\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8069 - acc: 0.7525 - val_loss: 0.7115 - val_acc: 0.7873\n",
      "Epoch 312/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7596\n",
      "Epoch 00312: val_loss improved from 0.59938 to 0.58823, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv_checkpoint/312-0.5882.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7977 - acc: 0.7597 - val_loss: 0.5882 - val_acc: 0.8439\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7999 - acc: 0.7600\n",
      "Epoch 00313: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8005 - acc: 0.7599 - val_loss: 0.6201 - val_acc: 0.8290\n",
      "Epoch 314/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7984 - acc: 0.7578\n",
      "Epoch 00314: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7989 - acc: 0.7575 - val_loss: 0.7988 - val_acc: 0.7533\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8017 - acc: 0.7583\n",
      "Epoch 00315: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8016 - acc: 0.7583 - val_loss: 0.6007 - val_acc: 0.8416\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7982 - acc: 0.7577\n",
      "Epoch 00316: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7982 - acc: 0.7577 - val_loss: 0.8256 - val_acc: 0.7480\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7588\n",
      "Epoch 00317: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7927 - acc: 0.7589 - val_loss: 0.6410 - val_acc: 0.8211\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.7613\n",
      "Epoch 00318: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7938 - acc: 0.7613 - val_loss: 0.6198 - val_acc: 0.8330\n",
      "Epoch 319/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.7582\n",
      "Epoch 00319: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7933 - acc: 0.7580 - val_loss: 0.6038 - val_acc: 0.8379\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7990 - acc: 0.7593\n",
      "Epoch 00320: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7989 - acc: 0.7594 - val_loss: 0.5923 - val_acc: 0.8383\n",
      "Epoch 321/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7940 - acc: 0.7600\n",
      "Epoch 00321: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7939 - acc: 0.7601 - val_loss: 1.7938 - val_acc: 0.5705\n",
      "Epoch 322/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7912 - acc: 0.7584\n",
      "Epoch 00322: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7919 - acc: 0.7583 - val_loss: 0.8329 - val_acc: 0.7498\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7886 - acc: 0.7611\n",
      "Epoch 00323: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7885 - acc: 0.7611 - val_loss: 0.6072 - val_acc: 0.8332\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.7607\n",
      "Epoch 00324: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7924 - acc: 0.7607 - val_loss: 0.6076 - val_acc: 0.8325\n",
      "Epoch 325/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7966 - acc: 0.7596\n",
      "Epoch 00325: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7966 - acc: 0.7597 - val_loss: 0.6308 - val_acc: 0.8232\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7594\n",
      "Epoch 00326: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8006 - acc: 0.7593 - val_loss: 0.6438 - val_acc: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7900 - acc: 0.7577\n",
      "Epoch 00327: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7900 - acc: 0.7577 - val_loss: 0.6144 - val_acc: 0.8358\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7893 - acc: 0.7612\n",
      "Epoch 00328: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7892 - acc: 0.7612 - val_loss: 0.6460 - val_acc: 0.8204\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7575\n",
      "Epoch 00329: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7933 - acc: 0.7575 - val_loss: 0.5950 - val_acc: 0.8414\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7917 - acc: 0.7597\n",
      "Epoch 00330: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7917 - acc: 0.7596 - val_loss: 0.5941 - val_acc: 0.8395\n",
      "Epoch 331/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7605\n",
      "Epoch 00331: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7830 - acc: 0.7604 - val_loss: 0.6505 - val_acc: 0.8164\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7593\n",
      "Epoch 00332: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7947 - acc: 0.7593 - val_loss: 0.6792 - val_acc: 0.8043\n",
      "Epoch 333/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7603\n",
      "Epoch 00333: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7931 - acc: 0.7603 - val_loss: 0.5933 - val_acc: 0.8428\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7604\n",
      "Epoch 00334: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7943 - acc: 0.7603 - val_loss: 0.6967 - val_acc: 0.8032\n",
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7604\n",
      "Epoch 00335: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7930 - acc: 0.7604 - val_loss: 0.6214 - val_acc: 0.8309\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7849 - acc: 0.7629\n",
      "Epoch 00336: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7848 - acc: 0.7629 - val_loss: 0.6125 - val_acc: 0.8365\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7598\n",
      "Epoch 00337: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7897 - acc: 0.7597 - val_loss: 0.6034 - val_acc: 0.8402\n",
      "Epoch 338/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7631\n",
      "Epoch 00338: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7840 - acc: 0.7632 - val_loss: 0.6260 - val_acc: 0.8269\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.7596\n",
      "Epoch 00339: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7891 - acc: 0.7596 - val_loss: 0.6046 - val_acc: 0.8353\n",
      "Epoch 340/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7591\n",
      "Epoch 00340: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7876 - acc: 0.7592 - val_loss: 0.7268 - val_acc: 0.7873\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7824 - acc: 0.7621\n",
      "Epoch 00341: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7825 - acc: 0.7621 - val_loss: 0.6026 - val_acc: 0.8421\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7599\n",
      "Epoch 00342: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7864 - acc: 0.7598 - val_loss: 0.6088 - val_acc: 0.8411\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7849 - acc: 0.7624\n",
      "Epoch 00343: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7846 - acc: 0.7626 - val_loss: 0.6278 - val_acc: 0.8337\n",
      "Epoch 344/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7632\n",
      "Epoch 00344: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7803 - acc: 0.7632 - val_loss: 0.7411 - val_acc: 0.7864\n",
      "Epoch 345/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7610\n",
      "Epoch 00345: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7847 - acc: 0.7611 - val_loss: 0.6081 - val_acc: 0.8386\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7907 - acc: 0.7629\n",
      "Epoch 00346: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7908 - acc: 0.7628 - val_loss: 0.5909 - val_acc: 0.8456\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7604\n",
      "Epoch 00347: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7833 - acc: 0.7605 - val_loss: 0.6237 - val_acc: 0.8304\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7613\n",
      "Epoch 00348: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7845 - acc: 0.7613 - val_loss: 0.5974 - val_acc: 0.8355\n",
      "Epoch 349/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7816 - acc: 0.7646\n",
      "Epoch 00349: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7818 - acc: 0.7645 - val_loss: 0.6859 - val_acc: 0.8132\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7624\n",
      "Epoch 00350: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7786 - acc: 0.7625 - val_loss: 0.6185 - val_acc: 0.8328\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7840 - acc: 0.7632\n",
      "Epoch 00351: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7840 - acc: 0.7632 - val_loss: 0.6581 - val_acc: 0.8190\n",
      "Epoch 352/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7865 - acc: 0.7629\n",
      "Epoch 00352: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7866 - acc: 0.7630 - val_loss: 0.6525 - val_acc: 0.8092\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7861 - acc: 0.7600\n",
      "Epoch 00353: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7863 - acc: 0.7600 - val_loss: 0.5886 - val_acc: 0.8414\n",
      "Epoch 354/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7639\n",
      "Epoch 00354: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7785 - acc: 0.7639 - val_loss: 0.7657 - val_acc: 0.7654\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7639\n",
      "Epoch 00355: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7785 - acc: 0.7639 - val_loss: 0.5955 - val_acc: 0.8444\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7601\n",
      "Epoch 00356: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7824 - acc: 0.7601 - val_loss: 0.6122 - val_acc: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7618\n",
      "Epoch 00357: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7842 - acc: 0.7618 - val_loss: 0.5885 - val_acc: 0.8421\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7792 - acc: 0.7629\n",
      "Epoch 00358: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7792 - acc: 0.7629 - val_loss: 0.6315 - val_acc: 0.8265\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7776 - acc: 0.7660\n",
      "Epoch 00359: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7776 - acc: 0.7659 - val_loss: 0.7544 - val_acc: 0.7782\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7808 - acc: 0.7624\n",
      "Epoch 00360: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7808 - acc: 0.7624 - val_loss: 0.6501 - val_acc: 0.8276\n",
      "Epoch 361/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7765 - acc: 0.7624\n",
      "Epoch 00361: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7764 - acc: 0.7624 - val_loss: 0.6109 - val_acc: 0.8416\n",
      "Epoch 362/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.7627\n",
      "Epoch 00362: val_loss did not improve from 0.58823\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7791 - acc: 0.7626 - val_loss: 0.7858 - val_acc: 0.7619\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 397us/sample - loss: 0.6458 - acc: 0.8083\n",
      "Loss: 0.6457661219227475 Accuracy: 0.80830735\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5002 - acc: 0.2086\n",
      "Epoch 00001: val_loss improved from inf to 2.25048, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/001-2.2505.hdf5\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 2.5001 - acc: 0.2086 - val_loss: 2.2505 - val_acc: 0.3422\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1004 - acc: 0.3268\n",
      "Epoch 00002: val_loss improved from 2.25048 to 1.79026, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/002-1.7903.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 2.1004 - acc: 0.3268 - val_loss: 1.7903 - val_acc: 0.5064\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8911 - acc: 0.4028\n",
      "Epoch 00003: val_loss improved from 1.79026 to 1.64680, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/003-1.6468.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.8910 - acc: 0.4028 - val_loss: 1.6468 - val_acc: 0.5500\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7381 - acc: 0.4557\n",
      "Epoch 00004: val_loss improved from 1.64680 to 1.49746, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/004-1.4975.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 1.7381 - acc: 0.4557 - val_loss: 1.4975 - val_acc: 0.6101\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6277 - acc: 0.4958\n",
      "Epoch 00005: val_loss improved from 1.49746 to 1.39283, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/005-1.3928.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 1.6275 - acc: 0.4957 - val_loss: 1.3928 - val_acc: 0.6583\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5388 - acc: 0.5224\n",
      "Epoch 00006: val_loss improved from 1.39283 to 1.29397, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/006-1.2940.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 1.5388 - acc: 0.5224 - val_loss: 1.2940 - val_acc: 0.6539\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4654 - acc: 0.5465\n",
      "Epoch 00007: val_loss improved from 1.29397 to 1.22167, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/007-1.2217.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.4657 - acc: 0.5461 - val_loss: 1.2217 - val_acc: 0.6795\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3969 - acc: 0.5730\n",
      "Epoch 00008: val_loss did not improve from 1.22167\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.3968 - acc: 0.5730 - val_loss: 1.2534 - val_acc: 0.6639\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3397 - acc: 0.5899\n",
      "Epoch 00009: val_loss improved from 1.22167 to 1.10644, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/009-1.1064.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.3397 - acc: 0.5899 - val_loss: 1.1064 - val_acc: 0.7174\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2953 - acc: 0.6050\n",
      "Epoch 00010: val_loss did not improve from 1.10644\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 1.2954 - acc: 0.6050 - val_loss: 1.1356 - val_acc: 0.6755\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2530 - acc: 0.6187\n",
      "Epoch 00011: val_loss improved from 1.10644 to 1.04717, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/011-1.0472.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.2534 - acc: 0.6185 - val_loss: 1.0472 - val_acc: 0.7268\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2184 - acc: 0.6291\n",
      "Epoch 00012: val_loss did not improve from 1.04717\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 1.2180 - acc: 0.6292 - val_loss: 1.2408 - val_acc: 0.6019\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1825 - acc: 0.6379\n",
      "Epoch 00013: val_loss improved from 1.04717 to 0.96080, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/013-0.9608.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.1825 - acc: 0.6378 - val_loss: 0.9608 - val_acc: 0.7403\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1648 - acc: 0.6468\n",
      "Epoch 00014: val_loss improved from 0.96080 to 0.94085, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/014-0.9409.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 1.1649 - acc: 0.6469 - val_loss: 0.9409 - val_acc: 0.7577\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1293 - acc: 0.6588\n",
      "Epoch 00015: val_loss improved from 0.94085 to 0.92819, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/015-0.9282.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 1.1294 - acc: 0.6587 - val_loss: 0.9282 - val_acc: 0.7512\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1141 - acc: 0.6639\n",
      "Epoch 00016: val_loss improved from 0.92819 to 0.90988, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/016-0.9099.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 1.1140 - acc: 0.6639 - val_loss: 0.9099 - val_acc: 0.7622\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0907 - acc: 0.6725\n",
      "Epoch 00017: val_loss improved from 0.90988 to 0.87429, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/017-0.8743.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 1.0906 - acc: 0.6725 - val_loss: 0.8743 - val_acc: 0.7696\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0649 - acc: 0.6793\n",
      "Epoch 00018: val_loss did not improve from 0.87429\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 1.0650 - acc: 0.6793 - val_loss: 0.8755 - val_acc: 0.7622\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0475 - acc: 0.6840\n",
      "Epoch 00019: val_loss improved from 0.87429 to 0.86924, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/019-0.8692.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 1.0475 - acc: 0.6840 - val_loss: 0.8692 - val_acc: 0.7515\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0332 - acc: 0.6901\n",
      "Epoch 00020: val_loss did not improve from 0.86924\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.0333 - acc: 0.6902 - val_loss: 0.9024 - val_acc: 0.7403\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0149 - acc: 0.6949\n",
      "Epoch 00021: val_loss improved from 0.86924 to 0.78765, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/021-0.7877.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 1.0150 - acc: 0.6949 - val_loss: 0.7877 - val_acc: 0.7834\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0027 - acc: 0.6987\n",
      "Epoch 00022: val_loss did not improve from 0.78765\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 1.0026 - acc: 0.6986 - val_loss: 0.9215 - val_acc: 0.7268\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9927 - acc: 0.7033\n",
      "Epoch 00023: val_loss did not improve from 0.78765\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.9926 - acc: 0.7033 - val_loss: 0.8276 - val_acc: 0.7601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9810 - acc: 0.7033\n",
      "Epoch 00024: val_loss improved from 0.78765 to 0.78724, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/024-0.7872.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.9810 - acc: 0.7033 - val_loss: 0.7872 - val_acc: 0.7794\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9653 - acc: 0.7132\n",
      "Epoch 00025: val_loss did not improve from 0.78724\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.9664 - acc: 0.7132 - val_loss: 0.8330 - val_acc: 0.7570\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9593 - acc: 0.7134\n",
      "Epoch 00026: val_loss did not improve from 0.78724\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.9592 - acc: 0.7135 - val_loss: 1.4557 - val_acc: 0.5521\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.7187\n",
      "Epoch 00027: val_loss improved from 0.78724 to 0.75051, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/027-0.7505.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.9397 - acc: 0.7188 - val_loss: 0.7505 - val_acc: 0.7918\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.7224\n",
      "Epoch 00028: val_loss improved from 0.75051 to 0.74599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/028-0.7460.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.9322 - acc: 0.7224 - val_loss: 0.7460 - val_acc: 0.7941\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9264 - acc: 0.7225\n",
      "Epoch 00029: val_loss did not improve from 0.74599\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.9266 - acc: 0.7225 - val_loss: 0.7955 - val_acc: 0.7782\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9117 - acc: 0.7259\n",
      "Epoch 00030: val_loss improved from 0.74599 to 0.74100, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/030-0.7410.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.9118 - acc: 0.7259 - val_loss: 0.7410 - val_acc: 0.7887\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9077 - acc: 0.7253\n",
      "Epoch 00031: val_loss improved from 0.74100 to 0.68860, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/031-0.6886.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.9078 - acc: 0.7252 - val_loss: 0.6886 - val_acc: 0.8085\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8959 - acc: 0.7327\n",
      "Epoch 00032: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.8959 - acc: 0.7327 - val_loss: 0.7246 - val_acc: 0.7945\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8904 - acc: 0.7332\n",
      "Epoch 00033: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.8901 - acc: 0.7333 - val_loss: 0.7063 - val_acc: 0.7962\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8794 - acc: 0.7382\n",
      "Epoch 00034: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.8800 - acc: 0.7382 - val_loss: 0.7585 - val_acc: 0.7859\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7385\n",
      "Epoch 00035: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.8805 - acc: 0.7385 - val_loss: 0.7881 - val_acc: 0.7685\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7439\n",
      "Epoch 00036: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.8653 - acc: 0.7439 - val_loss: 0.7123 - val_acc: 0.7911\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8555 - acc: 0.7442\n",
      "Epoch 00037: val_loss improved from 0.68860 to 0.66300, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/037-0.6630.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.8555 - acc: 0.7441 - val_loss: 0.6630 - val_acc: 0.8204\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7450\n",
      "Epoch 00038: val_loss did not improve from 0.66300\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.8604 - acc: 0.7450 - val_loss: 0.6716 - val_acc: 0.8113\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8515 - acc: 0.7484\n",
      "Epoch 00039: val_loss did not improve from 0.66300\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.8517 - acc: 0.7483 - val_loss: 0.6817 - val_acc: 0.8123\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8415 - acc: 0.7507\n",
      "Epoch 00040: val_loss did not improve from 0.66300\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.8417 - acc: 0.7507 - val_loss: 0.8177 - val_acc: 0.7468\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8315 - acc: 0.7536\n",
      "Epoch 00041: val_loss did not improve from 0.66300\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.8314 - acc: 0.7536 - val_loss: 0.8002 - val_acc: 0.7508\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8193 - acc: 0.7569\n",
      "Epoch 00042: val_loss improved from 0.66300 to 0.65050, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/042-0.6505.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.8194 - acc: 0.7569 - val_loss: 0.6505 - val_acc: 0.8227\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8187 - acc: 0.7580\n",
      "Epoch 00043: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.8186 - acc: 0.7580 - val_loss: 0.6751 - val_acc: 0.8113\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.7557\n",
      "Epoch 00044: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.8225 - acc: 0.7557 - val_loss: 0.7583 - val_acc: 0.7757\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8066 - acc: 0.7613\n",
      "Epoch 00045: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.8067 - acc: 0.7613 - val_loss: 0.6555 - val_acc: 0.8223\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8107 - acc: 0.7595\n",
      "Epoch 00046: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.8107 - acc: 0.7595 - val_loss: 0.7474 - val_acc: 0.7664\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7962 - acc: 0.7632\n",
      "Epoch 00047: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.7962 - acc: 0.7631 - val_loss: 0.7387 - val_acc: 0.7782\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7926 - acc: 0.7674\n",
      "Epoch 00048: val_loss did not improve from 0.65050\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.7927 - acc: 0.7675 - val_loss: 0.7064 - val_acc: 0.7897\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7926 - acc: 0.7658\n",
      "Epoch 00049: val_loss improved from 0.65050 to 0.61184, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/049-0.6118.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.7926 - acc: 0.7658 - val_loss: 0.6118 - val_acc: 0.8323\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7709\n",
      "Epoch 00050: val_loss did not improve from 0.61184\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.7852 - acc: 0.7709 - val_loss: 0.6452 - val_acc: 0.8174\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7677\n",
      "Epoch 00051: val_loss improved from 0.61184 to 0.59656, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/051-0.5966.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.7780 - acc: 0.7677 - val_loss: 0.5966 - val_acc: 0.8416\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7776 - acc: 0.7695\n",
      "Epoch 00052: val_loss did not improve from 0.59656\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.7779 - acc: 0.7695 - val_loss: 0.9833 - val_acc: 0.6907\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7741 - acc: 0.7726\n",
      "Epoch 00053: val_loss did not improve from 0.59656\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.7741 - acc: 0.7726 - val_loss: 0.9022 - val_acc: 0.7135\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7669 - acc: 0.7743\n",
      "Epoch 00054: val_loss improved from 0.59656 to 0.59644, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/054-0.5964.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.7676 - acc: 0.7743 - val_loss: 0.5964 - val_acc: 0.8402\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7640 - acc: 0.7737\n",
      "Epoch 00055: val_loss did not improve from 0.59644\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.7638 - acc: 0.7737 - val_loss: 0.6157 - val_acc: 0.8321\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.7751\n",
      "Epoch 00056: val_loss did not improve from 0.59644\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.7627 - acc: 0.7751 - val_loss: 0.6331 - val_acc: 0.8239\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7571 - acc: 0.7789\n",
      "Epoch 00057: val_loss did not improve from 0.59644\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.7570 - acc: 0.7789 - val_loss: 0.6751 - val_acc: 0.8099\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7504 - acc: 0.7785\n",
      "Epoch 00058: val_loss improved from 0.59644 to 0.58538, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/058-0.5854.hdf5\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.7503 - acc: 0.7785 - val_loss: 0.5854 - val_acc: 0.8402\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.7796\n",
      "Epoch 00059: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.7474 - acc: 0.7796 - val_loss: 0.6198 - val_acc: 0.8253\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7454 - acc: 0.7797\n",
      "Epoch 00060: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.7453 - acc: 0.7797 - val_loss: 0.5945 - val_acc: 0.8393\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7368 - acc: 0.7863\n",
      "Epoch 00061: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.7372 - acc: 0.7863 - val_loss: 0.7179 - val_acc: 0.7822\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7845\n",
      "Epoch 00062: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.7398 - acc: 0.7845 - val_loss: 0.6653 - val_acc: 0.8020\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7310 - acc: 0.7864\n",
      "Epoch 00063: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.7312 - acc: 0.7863 - val_loss: 0.8810 - val_acc: 0.7317\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7885\n",
      "Epoch 00064: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7234 - acc: 0.7885 - val_loss: 0.7732 - val_acc: 0.7661\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7278 - acc: 0.7874\n",
      "Epoch 00065: val_loss improved from 0.58538 to 0.56547, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/065-0.5655.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.7279 - acc: 0.7873 - val_loss: 0.5655 - val_acc: 0.8481\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7113 - acc: 0.7924\n",
      "Epoch 00066: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7116 - acc: 0.7923 - val_loss: 0.6557 - val_acc: 0.8143\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7218 - acc: 0.7884\n",
      "Epoch 00067: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7217 - acc: 0.7883 - val_loss: 0.5714 - val_acc: 0.8479\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7150 - acc: 0.7922\n",
      "Epoch 00068: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.7152 - acc: 0.7923 - val_loss: 0.5797 - val_acc: 0.8351\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7082 - acc: 0.7934\n",
      "Epoch 00069: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.7078 - acc: 0.7936 - val_loss: 0.7099 - val_acc: 0.7817\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7064 - acc: 0.7924\n",
      "Epoch 00070: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.7064 - acc: 0.7925 - val_loss: 0.6336 - val_acc: 0.8181\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6999 - acc: 0.7954\n",
      "Epoch 00071: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7001 - acc: 0.7954 - val_loss: 1.0344 - val_acc: 0.6751\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7008 - acc: 0.7960\n",
      "Epoch 00072: val_loss improved from 0.56547 to 0.54053, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/072-0.5405.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.7006 - acc: 0.7961 - val_loss: 0.5405 - val_acc: 0.8598\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7000 - acc: 0.7948\n",
      "Epoch 00073: val_loss did not improve from 0.54053\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6999 - acc: 0.7948 - val_loss: 0.6080 - val_acc: 0.8265\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7958\n",
      "Epoch 00074: val_loss improved from 0.54053 to 0.52561, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/074-0.5256.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.6941 - acc: 0.7958 - val_loss: 0.5256 - val_acc: 0.8586\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6904 - acc: 0.8002\n",
      "Epoch 00075: val_loss did not improve from 0.52561\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6909 - acc: 0.8000 - val_loss: 0.6116 - val_acc: 0.8211\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.8024\n",
      "Epoch 00076: val_loss did not improve from 0.52561\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.6856 - acc: 0.8024 - val_loss: 0.5472 - val_acc: 0.8449\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.8000\n",
      "Epoch 00077: val_loss did not improve from 0.52561\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6856 - acc: 0.8000 - val_loss: 0.5276 - val_acc: 0.8656\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.8034\n",
      "Epoch 00078: val_loss did not improve from 0.52561\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6828 - acc: 0.8035 - val_loss: 0.7470 - val_acc: 0.7713\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6818 - acc: 0.8026\n",
      "Epoch 00079: val_loss did not improve from 0.52561\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6818 - acc: 0.8026 - val_loss: 0.6777 - val_acc: 0.8048\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6813 - acc: 0.8019\n",
      "Epoch 00080: val_loss improved from 0.52561 to 0.51261, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/080-0.5126.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.6813 - acc: 0.8018 - val_loss: 0.5126 - val_acc: 0.8672\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.8013\n",
      "Epoch 00081: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6746 - acc: 0.8013 - val_loss: 0.5189 - val_acc: 0.8658\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.8052\n",
      "Epoch 00082: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6641 - acc: 0.8051 - val_loss: 0.5244 - val_acc: 0.8593\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.8058- ETA: 1s - loss: 0.66\n",
      "Epoch 00083: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6644 - acc: 0.8058 - val_loss: 0.7556 - val_acc: 0.7659\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.8062\n",
      "Epoch 00084: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6652 - acc: 0.8062 - val_loss: 0.5400 - val_acc: 0.8586\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6673 - acc: 0.8055\n",
      "Epoch 00085: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6673 - acc: 0.8055 - val_loss: 0.5468 - val_acc: 0.8512\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.8082\n",
      "Epoch 00086: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.6584 - acc: 0.8081 - val_loss: 0.7328 - val_acc: 0.7799\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6554 - acc: 0.8105\n",
      "Epoch 00087: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6553 - acc: 0.8105 - val_loss: 0.5349 - val_acc: 0.8542\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8107\n",
      "Epoch 00088: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6484 - acc: 0.8107 - val_loss: 0.5400 - val_acc: 0.8516\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.8103\n",
      "Epoch 00089: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6542 - acc: 0.8103 - val_loss: 0.5518 - val_acc: 0.8488\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.8119\n",
      "Epoch 00090: val_loss did not improve from 0.51261\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6476 - acc: 0.8121 - val_loss: 0.5233 - val_acc: 0.8530\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6402 - acc: 0.8147\n",
      "Epoch 00091: val_loss improved from 0.51261 to 0.49850, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/091-0.4985.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6402 - acc: 0.8147 - val_loss: 0.4985 - val_acc: 0.8705\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6435 - acc: 0.8116\n",
      "Epoch 00092: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6435 - acc: 0.8116 - val_loss: 0.8824 - val_acc: 0.7361\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.8158\n",
      "Epoch 00093: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6361 - acc: 0.8158 - val_loss: 0.5029 - val_acc: 0.8700\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.8129\n",
      "Epoch 00094: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6372 - acc: 0.8128 - val_loss: 0.6475 - val_acc: 0.8109\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6337 - acc: 0.8154\n",
      "Epoch 00095: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.6339 - acc: 0.8153 - val_loss: 0.5150 - val_acc: 0.8679\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.8168\n",
      "Epoch 00096: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6346 - acc: 0.8168 - val_loss: 0.5028 - val_acc: 0.8649\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6270 - acc: 0.8184\n",
      "Epoch 00097: val_loss did not improve from 0.49850\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6270 - acc: 0.8184 - val_loss: 0.5151 - val_acc: 0.8539\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.8173\n",
      "Epoch 00098: val_loss improved from 0.49850 to 0.48858, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/098-0.4886.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6286 - acc: 0.8173 - val_loss: 0.4886 - val_acc: 0.8705\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.8185\n",
      "Epoch 00099: val_loss did not improve from 0.48858\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6259 - acc: 0.8185 - val_loss: 0.5111 - val_acc: 0.8647\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6216 - acc: 0.8192\n",
      "Epoch 00100: val_loss did not improve from 0.48858\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6218 - acc: 0.8191 - val_loss: 0.5577 - val_acc: 0.8495\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.8224\n",
      "Epoch 00101: val_loss improved from 0.48858 to 0.48761, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/101-0.4876.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6145 - acc: 0.8224 - val_loss: 0.4876 - val_acc: 0.8744\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.8179\n",
      "Epoch 00102: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6198 - acc: 0.8179 - val_loss: 0.6803 - val_acc: 0.7985\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.8211\n",
      "Epoch 00103: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6134 - acc: 0.8210 - val_loss: 0.5165 - val_acc: 0.8595\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8229\n",
      "Epoch 00104: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6104 - acc: 0.8228 - val_loss: 0.6647 - val_acc: 0.7934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.8224\n",
      "Epoch 00105: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6121 - acc: 0.8224 - val_loss: 1.7377 - val_acc: 0.5623\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8238\n",
      "Epoch 00106: val_loss improved from 0.48761 to 0.47987, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/106-0.4799.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6066 - acc: 0.8238 - val_loss: 0.4799 - val_acc: 0.8754\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.8249\n",
      "Epoch 00107: val_loss did not improve from 0.47987\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6068 - acc: 0.8248 - val_loss: 0.6013 - val_acc: 0.8134\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6007 - acc: 0.8258\n",
      "Epoch 00108: val_loss did not improve from 0.47987\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.6012 - acc: 0.8257 - val_loss: 0.4809 - val_acc: 0.8710\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.8246\n",
      "Epoch 00109: val_loss improved from 0.47987 to 0.46423, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/109-0.4642.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6035 - acc: 0.8247 - val_loss: 0.4642 - val_acc: 0.8786\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8256\n",
      "Epoch 00110: val_loss did not improve from 0.46423\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6030 - acc: 0.8256 - val_loss: 0.5097 - val_acc: 0.8577\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5974 - acc: 0.8275\n",
      "Epoch 00111: val_loss improved from 0.46423 to 0.45240, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/111-0.4524.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.5974 - acc: 0.8276 - val_loss: 0.4524 - val_acc: 0.8796\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8254\n",
      "Epoch 00112: val_loss did not improve from 0.45240\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5936 - acc: 0.8256 - val_loss: 0.4742 - val_acc: 0.8744\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8280\n",
      "Epoch 00113: val_loss improved from 0.45240 to 0.44264, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/113-0.4426.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5908 - acc: 0.8280 - val_loss: 0.4426 - val_acc: 0.8835\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8304\n",
      "Epoch 00114: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5891 - acc: 0.8304 - val_loss: 0.4913 - val_acc: 0.8644\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.8287\n",
      "Epoch 00115: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5895 - acc: 0.8287 - val_loss: 0.4637 - val_acc: 0.8747\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.8306\n",
      "Epoch 00116: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5856 - acc: 0.8305 - val_loss: 0.5569 - val_acc: 0.8460\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.8300\n",
      "Epoch 00117: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.5890 - acc: 0.8300 - val_loss: 0.7305 - val_acc: 0.7787\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.8299\n",
      "Epoch 00118: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5785 - acc: 0.8299 - val_loss: 0.5539 - val_acc: 0.8416\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.8319\n",
      "Epoch 00119: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5762 - acc: 0.8320 - val_loss: 0.6325 - val_acc: 0.8111\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5760 - acc: 0.8333\n",
      "Epoch 00120: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5762 - acc: 0.8333 - val_loss: 0.4466 - val_acc: 0.8819\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.8323\n",
      "Epoch 00121: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5735 - acc: 0.8323 - val_loss: 0.4735 - val_acc: 0.8754\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8357\n",
      "Epoch 00122: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5736 - acc: 0.8356 - val_loss: 0.6557 - val_acc: 0.7983\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8367\n",
      "Epoch 00123: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5699 - acc: 0.8367 - val_loss: 0.4589 - val_acc: 0.8782\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8333\n",
      "Epoch 00124: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5731 - acc: 0.8333 - val_loss: 0.4530 - val_acc: 0.8786\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.8350\n",
      "Epoch 00125: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5705 - acc: 0.8350 - val_loss: 0.5890 - val_acc: 0.8351\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.8346\n",
      "Epoch 00126: val_loss did not improve from 0.44264\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5675 - acc: 0.8346 - val_loss: 0.4528 - val_acc: 0.8798\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5680 - acc: 0.8379\n",
      "Epoch 00127: val_loss improved from 0.44264 to 0.43981, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/127-0.4398.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5678 - acc: 0.8380 - val_loss: 0.4398 - val_acc: 0.8838\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8351\n",
      "Epoch 00128: val_loss did not improve from 0.43981\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5635 - acc: 0.8351 - val_loss: 0.4508 - val_acc: 0.8847\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8349\n",
      "Epoch 00129: val_loss improved from 0.43981 to 0.43648, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/129-0.4365.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5624 - acc: 0.8349 - val_loss: 0.4365 - val_acc: 0.8854\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8401\n",
      "Epoch 00130: val_loss did not improve from 0.43648\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5549 - acc: 0.8402 - val_loss: 0.5886 - val_acc: 0.8286\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8377\n",
      "Epoch 00131: val_loss did not improve from 0.43648\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5556 - acc: 0.8377 - val_loss: 0.4814 - val_acc: 0.8696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8385\n",
      "Epoch 00132: val_loss did not improve from 0.43648\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5547 - acc: 0.8384 - val_loss: 0.4490 - val_acc: 0.8800\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8396\n",
      "Epoch 00133: val_loss did not improve from 0.43648\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5550 - acc: 0.8396 - val_loss: 0.4794 - val_acc: 0.8735\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8403\n",
      "Epoch 00134: val_loss improved from 0.43648 to 0.43490, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/134-0.4349.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5504 - acc: 0.8403 - val_loss: 0.4349 - val_acc: 0.8845\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8426\n",
      "Epoch 00135: val_loss did not improve from 0.43490\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5482 - acc: 0.8425 - val_loss: 0.4621 - val_acc: 0.8712\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8363\n",
      "Epoch 00136: val_loss did not improve from 0.43490\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5548 - acc: 0.8363 - val_loss: 0.4448 - val_acc: 0.8821\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.8433\n",
      "Epoch 00137: val_loss improved from 0.43490 to 0.43035, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/137-0.4303.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.5444 - acc: 0.8433 - val_loss: 0.4303 - val_acc: 0.8884\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.8433\n",
      "Epoch 00138: val_loss improved from 0.43035 to 0.42034, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/138-0.4203.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.5401 - acc: 0.8433 - val_loss: 0.4203 - val_acc: 0.8915\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.8410\n",
      "Epoch 00139: val_loss did not improve from 0.42034\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5405 - acc: 0.8409 - val_loss: 0.4503 - val_acc: 0.8784\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.8421\n",
      "Epoch 00140: val_loss did not improve from 0.42034\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5416 - acc: 0.8422 - val_loss: 0.4652 - val_acc: 0.8807\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8421\n",
      "Epoch 00141: val_loss did not improve from 0.42034\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5395 - acc: 0.8421 - val_loss: 0.5793 - val_acc: 0.8416\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.8424\n",
      "Epoch 00142: val_loss did not improve from 0.42034\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5418 - acc: 0.8424 - val_loss: 0.4384 - val_acc: 0.8835\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8437\n",
      "Epoch 00143: val_loss improved from 0.42034 to 0.41983, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/143-0.4198.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5347 - acc: 0.8436 - val_loss: 0.4198 - val_acc: 0.8905\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8442\n",
      "Epoch 00144: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5351 - acc: 0.8441 - val_loss: 0.4215 - val_acc: 0.8863\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8422\n",
      "Epoch 00145: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5392 - acc: 0.8422 - val_loss: 0.4418 - val_acc: 0.8835\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8452\n",
      "Epoch 00146: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5303 - acc: 0.8452 - val_loss: 0.4875 - val_acc: 0.8703\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.8464\n",
      "Epoch 00147: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5267 - acc: 0.8464 - val_loss: 0.4278 - val_acc: 0.8903\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8463\n",
      "Epoch 00148: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5220 - acc: 0.8464 - val_loss: 0.5705 - val_acc: 0.8295\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8436\n",
      "Epoch 00149: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5299 - acc: 0.8436 - val_loss: 0.4641 - val_acc: 0.8705\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.8498\n",
      "Epoch 00150: val_loss did not improve from 0.41983\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5245 - acc: 0.8497 - val_loss: 0.4817 - val_acc: 0.8633\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.8468\n",
      "Epoch 00151: val_loss improved from 0.41983 to 0.40457, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/151-0.4046.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5256 - acc: 0.8468 - val_loss: 0.4046 - val_acc: 0.8882\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8462\n",
      "Epoch 00152: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5261 - acc: 0.8462 - val_loss: 0.4565 - val_acc: 0.8793\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8495\n",
      "Epoch 00153: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5198 - acc: 0.8496 - val_loss: 0.4631 - val_acc: 0.8724\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8467\n",
      "Epoch 00154: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5206 - acc: 0.8467 - val_loss: 0.4209 - val_acc: 0.8819\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8494\n",
      "Epoch 00155: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5184 - acc: 0.8493 - val_loss: 0.4087 - val_acc: 0.8884\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8493\n",
      "Epoch 00156: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5148 - acc: 0.8493 - val_loss: 0.8018 - val_acc: 0.7556\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.8479\n",
      "Epoch 00157: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5199 - acc: 0.8478 - val_loss: 0.4201 - val_acc: 0.8859\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8479\n",
      "Epoch 00158: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5201 - acc: 0.8479 - val_loss: 0.4523 - val_acc: 0.8817\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.8492\n",
      "Epoch 00159: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5107 - acc: 0.8492 - val_loss: 1.0091 - val_acc: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8519\n",
      "Epoch 00160: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5092 - acc: 0.8519 - val_loss: 0.4100 - val_acc: 0.8912\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.8510\n",
      "Epoch 00161: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5122 - acc: 0.8509 - val_loss: 0.4160 - val_acc: 0.8891\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8505\n",
      "Epoch 00162: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5114 - acc: 0.8506 - val_loss: 0.4241 - val_acc: 0.8861\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8521\n",
      "Epoch 00163: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5054 - acc: 0.8520 - val_loss: 0.4139 - val_acc: 0.8915\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.8512\n",
      "Epoch 00164: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5057 - acc: 0.8513 - val_loss: 0.4051 - val_acc: 0.8915\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8533\n",
      "Epoch 00165: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5075 - acc: 0.8533 - val_loss: 0.4058 - val_acc: 0.8901\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8529\n",
      "Epoch 00166: val_loss did not improve from 0.40457\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5029 - acc: 0.8529 - val_loss: 0.4333 - val_acc: 0.8770\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8513\n",
      "Epoch 00167: val_loss improved from 0.40457 to 0.40018, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/167-0.4002.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5051 - acc: 0.8513 - val_loss: 0.4002 - val_acc: 0.8935\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.8549\n",
      "Epoch 00168: val_loss did not improve from 0.40018\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5004 - acc: 0.8548 - val_loss: 0.4136 - val_acc: 0.8894\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8516\n",
      "Epoch 00169: val_loss did not improve from 0.40018\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5013 - acc: 0.8516 - val_loss: 0.4130 - val_acc: 0.8884\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8548\n",
      "Epoch 00170: val_loss did not improve from 0.40018\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4949 - acc: 0.8548 - val_loss: 0.4224 - val_acc: 0.8917\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8551\n",
      "Epoch 00171: val_loss improved from 0.40018 to 0.39461, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/171-0.3946.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4947 - acc: 0.8551 - val_loss: 0.3946 - val_acc: 0.8931\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.8560\n",
      "Epoch 00172: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4930 - acc: 0.8561 - val_loss: 0.4606 - val_acc: 0.8717\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8545\n",
      "Epoch 00173: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4938 - acc: 0.8545 - val_loss: 0.4659 - val_acc: 0.8733\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8563\n",
      "Epoch 00174: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4959 - acc: 0.8562 - val_loss: 0.4244 - val_acc: 0.8826\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8555\n",
      "Epoch 00175: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4954 - acc: 0.8555 - val_loss: 0.4169 - val_acc: 0.8842\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8548\n",
      "Epoch 00176: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4923 - acc: 0.8547 - val_loss: 0.4456 - val_acc: 0.8763\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8537\n",
      "Epoch 00177: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4937 - acc: 0.8536 - val_loss: 0.4971 - val_acc: 0.8570\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4895 - acc: 0.8566\n",
      "Epoch 00178: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4897 - acc: 0.8566 - val_loss: 0.4669 - val_acc: 0.8807\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4933 - acc: 0.8547\n",
      "Epoch 00179: val_loss did not improve from 0.39461\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4931 - acc: 0.8547 - val_loss: 0.3965 - val_acc: 0.8947\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8567\n",
      "Epoch 00180: val_loss improved from 0.39461 to 0.38009, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/180-0.3801.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.4870 - acc: 0.8567 - val_loss: 0.3801 - val_acc: 0.8991\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8571\n",
      "Epoch 00181: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4858 - acc: 0.8573 - val_loss: 0.4112 - val_acc: 0.8908\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8583\n",
      "Epoch 00182: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4838 - acc: 0.8584 - val_loss: 0.4135 - val_acc: 0.8866\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8586\n",
      "Epoch 00183: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4836 - acc: 0.8586 - val_loss: 0.4150 - val_acc: 0.8859\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8582\n",
      "Epoch 00184: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4817 - acc: 0.8582 - val_loss: 0.4621 - val_acc: 0.8696\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.8577\n",
      "Epoch 00185: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4791 - acc: 0.8578 - val_loss: 0.4448 - val_acc: 0.8798\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8592\n",
      "Epoch 00186: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4790 - acc: 0.8591 - val_loss: 0.4350 - val_acc: 0.8831\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8590\n",
      "Epoch 00187: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4781 - acc: 0.8590 - val_loss: 0.3933 - val_acc: 0.8947\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.8604\n",
      "Epoch 00188: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4755 - acc: 0.8604 - val_loss: 0.3890 - val_acc: 0.8956\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8609\n",
      "Epoch 00189: val_loss did not improve from 0.38009\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4732 - acc: 0.8608 - val_loss: 2.5227 - val_acc: 0.5129\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8623\n",
      "Epoch 00190: val_loss improved from 0.38009 to 0.37405, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/190-0.3740.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4708 - acc: 0.8624 - val_loss: 0.3740 - val_acc: 0.9015\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8591\n",
      "Epoch 00191: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4742 - acc: 0.8591 - val_loss: 0.4214 - val_acc: 0.8861\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8603\n",
      "Epoch 00192: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4753 - acc: 0.8602 - val_loss: 0.3742 - val_acc: 0.8994\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8620\n",
      "Epoch 00193: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4683 - acc: 0.8619 - val_loss: 0.4161 - val_acc: 0.8863\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8623\n",
      "Epoch 00194: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4737 - acc: 0.8622 - val_loss: 0.3897 - val_acc: 0.8961\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8623\n",
      "Epoch 00195: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4649 - acc: 0.8624 - val_loss: 0.4099 - val_acc: 0.8875\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8615\n",
      "Epoch 00196: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4707 - acc: 0.8615 - val_loss: 0.3989 - val_acc: 0.8919\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8602\n",
      "Epoch 00197: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4746 - acc: 0.8603 - val_loss: 0.3902 - val_acc: 0.8940\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.8612\n",
      "Epoch 00198: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4695 - acc: 0.8612 - val_loss: 0.3838 - val_acc: 0.8996\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8638\n",
      "Epoch 00199: val_loss did not improve from 0.37405\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4647 - acc: 0.8637 - val_loss: 0.3764 - val_acc: 0.8977\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8621\n",
      "Epoch 00200: val_loss improved from 0.37405 to 0.37265, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/200-0.3727.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4673 - acc: 0.8621 - val_loss: 0.3727 - val_acc: 0.8984\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8656\n",
      "Epoch 00201: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4618 - acc: 0.8656 - val_loss: 0.3925 - val_acc: 0.8912\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4704 - acc: 0.8625\n",
      "Epoch 00202: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4702 - acc: 0.8625 - val_loss: 0.3764 - val_acc: 0.9001\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8661\n",
      "Epoch 00203: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4554 - acc: 0.8661 - val_loss: 0.3936 - val_acc: 0.9005\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8659\n",
      "Epoch 00204: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4616 - acc: 0.8659 - val_loss: 0.3790 - val_acc: 0.9022\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8673\n",
      "Epoch 00205: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4539 - acc: 0.8673 - val_loss: 0.3932 - val_acc: 0.8961\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8630\n",
      "Epoch 00206: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4632 - acc: 0.8631 - val_loss: 0.3771 - val_acc: 0.8952\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8672\n",
      "Epoch 00207: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4554 - acc: 0.8672 - val_loss: 0.3977 - val_acc: 0.8931\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.8639\n",
      "Epoch 00208: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4612 - acc: 0.8639 - val_loss: 0.3869 - val_acc: 0.8975\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8658\n",
      "Epoch 00209: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4571 - acc: 0.8658 - val_loss: 0.3918 - val_acc: 0.8945\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8669\n",
      "Epoch 00210: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4536 - acc: 0.8669 - val_loss: 0.3738 - val_acc: 0.8942\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.8663\n",
      "Epoch 00211: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4523 - acc: 0.8663 - val_loss: 0.4323 - val_acc: 0.8793\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.8672\n",
      "Epoch 00212: val_loss did not improve from 0.37265\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4506 - acc: 0.8672 - val_loss: 0.4107 - val_acc: 0.8901\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8662\n",
      "Epoch 00213: val_loss improved from 0.37265 to 0.37246, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/213-0.3725.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4516 - acc: 0.8662 - val_loss: 0.3725 - val_acc: 0.8975\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8660\n",
      "Epoch 00214: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4509 - acc: 0.8660 - val_loss: 0.5040 - val_acc: 0.8600\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8656\n",
      "Epoch 00215: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4527 - acc: 0.8656 - val_loss: 0.3816 - val_acc: 0.8954\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8681\n",
      "Epoch 00216: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4490 - acc: 0.8682 - val_loss: 0.4105 - val_acc: 0.8798\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8674\n",
      "Epoch 00217: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4444 - acc: 0.8674 - val_loss: 0.8461 - val_acc: 0.7531\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8681\n",
      "Epoch 00218: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4479 - acc: 0.8681 - val_loss: 0.3804 - val_acc: 0.8959\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8688\n",
      "Epoch 00219: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4466 - acc: 0.8687 - val_loss: 0.3755 - val_acc: 0.8966\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.8691\n",
      "Epoch 00220: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4471 - acc: 0.8691 - val_loss: 0.7463 - val_acc: 0.7871\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8701\n",
      "Epoch 00221: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4404 - acc: 0.8700 - val_loss: 0.4319 - val_acc: 0.8791\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8708\n",
      "Epoch 00222: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4408 - acc: 0.8709 - val_loss: 0.4066 - val_acc: 0.8859\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.8693\n",
      "Epoch 00223: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4455 - acc: 0.8692 - val_loss: 0.5780 - val_acc: 0.8330\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.8666\n",
      "Epoch 00224: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4431 - acc: 0.8666 - val_loss: 0.4134 - val_acc: 0.8859\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8704\n",
      "Epoch 00225: val_loss did not improve from 0.37246\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4384 - acc: 0.8704 - val_loss: 0.3786 - val_acc: 0.8982\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8696\n",
      "Epoch 00226: val_loss improved from 0.37246 to 0.36675, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/226-0.3668.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4395 - acc: 0.8696 - val_loss: 0.3668 - val_acc: 0.8982\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8673\n",
      "Epoch 00227: val_loss did not improve from 0.36675\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4418 - acc: 0.8673 - val_loss: 0.3725 - val_acc: 0.8982\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8677\n",
      "Epoch 00228: val_loss improved from 0.36675 to 0.35575, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/228-0.3558.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4436 - acc: 0.8677 - val_loss: 0.3558 - val_acc: 0.9040\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8720\n",
      "Epoch 00229: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4346 - acc: 0.8720 - val_loss: 0.3679 - val_acc: 0.9043\n",
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8696\n",
      "Epoch 00230: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4370 - acc: 0.8697 - val_loss: 0.4503 - val_acc: 0.8696\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8692\n",
      "Epoch 00231: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4349 - acc: 0.8692 - val_loss: 0.3712 - val_acc: 0.8940\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.8704\n",
      "Epoch 00232: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4357 - acc: 0.8703 - val_loss: 0.3559 - val_acc: 0.9064\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8709\n",
      "Epoch 00233: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4341 - acc: 0.8709 - val_loss: 0.3659 - val_acc: 0.9050\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8700\n",
      "Epoch 00234: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4310 - acc: 0.8699 - val_loss: 0.3805 - val_acc: 0.9001\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8728\n",
      "Epoch 00235: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4313 - acc: 0.8729 - val_loss: 0.5737 - val_acc: 0.8383\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8737\n",
      "Epoch 00236: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4348 - acc: 0.8737 - val_loss: 0.3713 - val_acc: 0.8968\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8715\n",
      "Epoch 00237: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4325 - acc: 0.8715 - val_loss: 0.6288 - val_acc: 0.8113\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8727\n",
      "Epoch 00238: val_loss improved from 0.35575 to 0.35549, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/238-0.3555.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4292 - acc: 0.8725 - val_loss: 0.3555 - val_acc: 0.9033\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8714\n",
      "Epoch 00239: val_loss did not improve from 0.35549\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4285 - acc: 0.8714 - val_loss: 0.4048 - val_acc: 0.8845\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8751\n",
      "Epoch 00240: val_loss improved from 0.35549 to 0.34955, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/240-0.3496.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4219 - acc: 0.8750 - val_loss: 0.3496 - val_acc: 0.9052\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8705\n",
      "Epoch 00241: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4387 - acc: 0.8705 - val_loss: 0.3668 - val_acc: 0.9022\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8730\n",
      "Epoch 00242: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4267 - acc: 0.8730 - val_loss: 0.3542 - val_acc: 0.8998\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8731\n",
      "Epoch 00243: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4293 - acc: 0.8730 - val_loss: 0.3631 - val_acc: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4215 - acc: 0.8741\n",
      "Epoch 00244: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4214 - acc: 0.8741 - val_loss: 0.5062 - val_acc: 0.8570\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8729\n",
      "Epoch 00245: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4270 - acc: 0.8729 - val_loss: 0.3704 - val_acc: 0.9001\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8738\n",
      "Epoch 00246: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4244 - acc: 0.8738 - val_loss: 0.3536 - val_acc: 0.9054\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8723\n",
      "Epoch 00247: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4252 - acc: 0.8723 - val_loss: 0.3542 - val_acc: 0.9003\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8757\n",
      "Epoch 00248: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4204 - acc: 0.8756 - val_loss: 0.3815 - val_acc: 0.8998\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8742\n",
      "Epoch 00249: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4253 - acc: 0.8743 - val_loss: 0.3836 - val_acc: 0.8963\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8757\n",
      "Epoch 00250: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4168 - acc: 0.8757 - val_loss: 0.4202 - val_acc: 0.8800\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8770\n",
      "Epoch 00251: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4160 - acc: 0.8769 - val_loss: 0.3893 - val_acc: 0.8949\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8771\n",
      "Epoch 00252: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4149 - acc: 0.8771 - val_loss: 0.4292 - val_acc: 0.8821\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8757\n",
      "Epoch 00253: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4224 - acc: 0.8757 - val_loss: 0.4181 - val_acc: 0.8856\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8773\n",
      "Epoch 00254: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4170 - acc: 0.8773 - val_loss: 0.3596 - val_acc: 0.9036\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8770\n",
      "Epoch 00255: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4187 - acc: 0.8770 - val_loss: 0.3669 - val_acc: 0.9001\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8755\n",
      "Epoch 00256: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4160 - acc: 0.8755 - val_loss: 0.4196 - val_acc: 0.8905\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8780\n",
      "Epoch 00257: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4161 - acc: 0.8780 - val_loss: 0.3575 - val_acc: 0.9003\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8763\n",
      "Epoch 00258: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4177 - acc: 0.8762 - val_loss: 0.3742 - val_acc: 0.9005\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8758\n",
      "Epoch 00259: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4146 - acc: 0.8758 - val_loss: 0.6220 - val_acc: 0.8102\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8783\n",
      "Epoch 00260: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4126 - acc: 0.8784 - val_loss: 0.3985 - val_acc: 0.8889\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8777\n",
      "Epoch 00261: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4102 - acc: 0.8778 - val_loss: 0.4161 - val_acc: 0.8889\n",
      "Epoch 262/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8761\n",
      "Epoch 00262: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4125 - acc: 0.8762 - val_loss: 0.3504 - val_acc: 0.9052\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8793\n",
      "Epoch 00263: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4086 - acc: 0.8793 - val_loss: 0.3746 - val_acc: 0.8994\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8770\n",
      "Epoch 00264: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4135 - acc: 0.8770 - val_loss: 0.3728 - val_acc: 0.8966\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8758\n",
      "Epoch 00265: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4128 - acc: 0.8757 - val_loss: 0.3788 - val_acc: 0.8952\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8779\n",
      "Epoch 00266: val_loss did not improve from 0.34955\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4108 - acc: 0.8779 - val_loss: 0.7579 - val_acc: 0.7741\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8780\n",
      "Epoch 00267: val_loss improved from 0.34955 to 0.34426, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/267-0.3443.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4092 - acc: 0.8780 - val_loss: 0.3443 - val_acc: 0.9057\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8789\n",
      "Epoch 00268: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4048 - acc: 0.8790 - val_loss: 0.3676 - val_acc: 0.8975\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8783\n",
      "Epoch 00269: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4049 - acc: 0.8782 - val_loss: 0.3630 - val_acc: 0.9012\n",
      "Epoch 270/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8772\n",
      "Epoch 00270: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4079 - acc: 0.8771 - val_loss: 0.4065 - val_acc: 0.8880\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8772\n",
      "Epoch 00271: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4054 - acc: 0.8771 - val_loss: 0.3561 - val_acc: 0.9012\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8795\n",
      "Epoch 00272: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4056 - acc: 0.8795 - val_loss: 0.3923 - val_acc: 0.8928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8778\n",
      "Epoch 00273: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4095 - acc: 0.8778 - val_loss: 0.3838 - val_acc: 0.8954\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8768\n",
      "Epoch 00274: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4096 - acc: 0.8768 - val_loss: 0.5547 - val_acc: 0.8351\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8795\n",
      "Epoch 00275: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4012 - acc: 0.8793 - val_loss: 0.3576 - val_acc: 0.9031\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8782\n",
      "Epoch 00276: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4084 - acc: 0.8781 - val_loss: 0.3459 - val_acc: 0.9106\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8803\n",
      "Epoch 00277: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4034 - acc: 0.8801 - val_loss: 0.4184 - val_acc: 0.8805\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8806\n",
      "Epoch 00278: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4004 - acc: 0.8806 - val_loss: 0.3723 - val_acc: 0.9003\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8804\n",
      "Epoch 00279: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4002 - acc: 0.8804 - val_loss: 0.3547 - val_acc: 0.9043\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8813\n",
      "Epoch 00280: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3938 - acc: 0.8813 - val_loss: 0.3588 - val_acc: 0.8989\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8803\n",
      "Epoch 00281: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4033 - acc: 0.8803 - val_loss: 0.5538 - val_acc: 0.8339\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8826\n",
      "Epoch 00282: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3940 - acc: 0.8826 - val_loss: 0.3469 - val_acc: 0.9099\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8798\n",
      "Epoch 00283: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4053 - acc: 0.8797 - val_loss: 0.3854 - val_acc: 0.8973\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8794\n",
      "Epoch 00284: val_loss did not improve from 0.34426\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4001 - acc: 0.8794 - val_loss: 0.4780 - val_acc: 0.8658\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8815\n",
      "Epoch 00285: val_loss improved from 0.34426 to 0.33359, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/285-0.3336.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3938 - acc: 0.8815 - val_loss: 0.3336 - val_acc: 0.9124\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8808\n",
      "Epoch 00286: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3994 - acc: 0.8808 - val_loss: 0.3442 - val_acc: 0.9045\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8818\n",
      "Epoch 00287: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3993 - acc: 0.8818 - val_loss: 0.3818 - val_acc: 0.8926\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3888 - acc: 0.8844\n",
      "Epoch 00288: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3890 - acc: 0.8842 - val_loss: 0.3498 - val_acc: 0.9080\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8797\n",
      "Epoch 00289: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3990 - acc: 0.8797 - val_loss: 0.3734 - val_acc: 0.8980\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8814\n",
      "Epoch 00290: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3922 - acc: 0.8815 - val_loss: 0.4075 - val_acc: 0.8949\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8821\n",
      "Epoch 00291: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3950 - acc: 0.8821 - val_loss: 0.3659 - val_acc: 0.9008\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8832\n",
      "Epoch 00292: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3925 - acc: 0.8831 - val_loss: 0.3588 - val_acc: 0.9068\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8790\n",
      "Epoch 00293: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4003 - acc: 0.8789 - val_loss: 0.3485 - val_acc: 0.9075\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8822\n",
      "Epoch 00294: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3918 - acc: 0.8822 - val_loss: 0.3443 - val_acc: 0.9073\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8827\n",
      "Epoch 00295: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3915 - acc: 0.8827 - val_loss: 0.3504 - val_acc: 0.9031\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8860\n",
      "Epoch 00296: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3850 - acc: 0.8861 - val_loss: 0.5092 - val_acc: 0.8574\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8838\n",
      "Epoch 00297: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3898 - acc: 0.8838 - val_loss: 0.3608 - val_acc: 0.9043\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8805\n",
      "Epoch 00298: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3921 - acc: 0.8805 - val_loss: 0.3461 - val_acc: 0.9087\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8828\n",
      "Epoch 00299: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3872 - acc: 0.8830 - val_loss: 0.3378 - val_acc: 0.9054\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8821\n",
      "Epoch 00300: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3935 - acc: 0.8821 - val_loss: 0.4193 - val_acc: 0.8847\n",
      "Epoch 301/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8849\n",
      "Epoch 00301: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3887 - acc: 0.8849 - val_loss: 0.3681 - val_acc: 0.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8852\n",
      "Epoch 00302: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3844 - acc: 0.8852 - val_loss: 0.3443 - val_acc: 0.9085\n",
      "Epoch 303/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8840\n",
      "Epoch 00303: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3858 - acc: 0.8841 - val_loss: 0.4087 - val_acc: 0.8842\n",
      "Epoch 304/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8847\n",
      "Epoch 00304: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3833 - acc: 0.8847 - val_loss: 0.3365 - val_acc: 0.9122\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8856\n",
      "Epoch 00305: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3820 - acc: 0.8856 - val_loss: 0.3506 - val_acc: 0.9054\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8852\n",
      "Epoch 00306: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3839 - acc: 0.8852 - val_loss: 0.3373 - val_acc: 0.9059\n",
      "Epoch 307/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8856\n",
      "Epoch 00307: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3805 - acc: 0.8856 - val_loss: 0.3505 - val_acc: 0.9068\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8850\n",
      "Epoch 00308: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3885 - acc: 0.8850 - val_loss: 0.4706 - val_acc: 0.8663\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8833\n",
      "Epoch 00309: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3850 - acc: 0.8832 - val_loss: 0.3519 - val_acc: 0.9031\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8846\n",
      "Epoch 00310: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3861 - acc: 0.8845 - val_loss: 0.3578 - val_acc: 0.9045\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8846\n",
      "Epoch 00311: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3827 - acc: 0.8846 - val_loss: 0.3965 - val_acc: 0.8952\n",
      "Epoch 312/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8865\n",
      "Epoch 00312: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3823 - acc: 0.8865 - val_loss: 0.3421 - val_acc: 0.9057\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8838\n",
      "Epoch 00313: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3817 - acc: 0.8838 - val_loss: 0.3785 - val_acc: 0.8982\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8865\n",
      "Epoch 00314: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3787 - acc: 0.8865 - val_loss: 0.3634 - val_acc: 0.9045\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8860\n",
      "Epoch 00315: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3787 - acc: 0.8859 - val_loss: 0.3487 - val_acc: 0.9068\n",
      "Epoch 316/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8870\n",
      "Epoch 00316: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3771 - acc: 0.8871 - val_loss: 0.3442 - val_acc: 0.9075\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8868\n",
      "Epoch 00317: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3787 - acc: 0.8867 - val_loss: 0.3393 - val_acc: 0.9087\n",
      "Epoch 318/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8854\n",
      "Epoch 00318: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3789 - acc: 0.8854 - val_loss: 0.3528 - val_acc: 0.9061\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8834\n",
      "Epoch 00319: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3839 - acc: 0.8834 - val_loss: 0.3448 - val_acc: 0.9099\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8832\n",
      "Epoch 00320: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3857 - acc: 0.8832 - val_loss: 0.4443 - val_acc: 0.8845\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8878\n",
      "Epoch 00321: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3724 - acc: 0.8878 - val_loss: 0.5676 - val_acc: 0.8390\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8854\n",
      "Epoch 00322: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3773 - acc: 0.8854 - val_loss: 0.3572 - val_acc: 0.9017\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8860\n",
      "Epoch 00323: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3810 - acc: 0.8860 - val_loss: 0.3389 - val_acc: 0.9078\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8860\n",
      "Epoch 00324: val_loss improved from 0.33359 to 0.32601, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv_checkpoint/324-0.3260.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3756 - acc: 0.8860 - val_loss: 0.3260 - val_acc: 0.9115\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8867\n",
      "Epoch 00325: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3779 - acc: 0.8866 - val_loss: 0.3765 - val_acc: 0.8989\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8877\n",
      "Epoch 00326: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3689 - acc: 0.8878 - val_loss: 0.3569 - val_acc: 0.9022\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8870\n",
      "Epoch 00327: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3753 - acc: 0.8870 - val_loss: 0.3597 - val_acc: 0.9012\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8862\n",
      "Epoch 00328: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3741 - acc: 0.8863 - val_loss: 0.3790 - val_acc: 0.9017\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8890\n",
      "Epoch 00329: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3709 - acc: 0.8890 - val_loss: 0.3515 - val_acc: 0.9057\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8877\n",
      "Epoch 00330: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3709 - acc: 0.8877 - val_loss: 0.3759 - val_acc: 0.8994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8876\n",
      "Epoch 00331: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3739 - acc: 0.8876 - val_loss: 0.3411 - val_acc: 0.9071\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8890\n",
      "Epoch 00332: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3713 - acc: 0.8890 - val_loss: 0.3524 - val_acc: 0.9047\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8899\n",
      "Epoch 00333: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3654 - acc: 0.8898 - val_loss: 0.3775 - val_acc: 0.8987\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8883\n",
      "Epoch 00334: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3731 - acc: 0.8882 - val_loss: 0.3416 - val_acc: 0.9089\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8890\n",
      "Epoch 00335: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3679 - acc: 0.8891 - val_loss: 0.4069 - val_acc: 0.8882\n",
      "Epoch 336/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8896\n",
      "Epoch 00336: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3647 - acc: 0.8896 - val_loss: 0.4555 - val_acc: 0.8700\n",
      "Epoch 337/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8881\n",
      "Epoch 00337: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3664 - acc: 0.8881 - val_loss: 0.3266 - val_acc: 0.9122\n",
      "Epoch 338/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8881\n",
      "Epoch 00338: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3653 - acc: 0.8882 - val_loss: 0.3500 - val_acc: 0.9022\n",
      "Epoch 339/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8907\n",
      "Epoch 00339: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3642 - acc: 0.8907 - val_loss: 0.3895 - val_acc: 0.8945\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8879\n",
      "Epoch 00340: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3677 - acc: 0.8878 - val_loss: 0.3412 - val_acc: 0.9061\n",
      "Epoch 341/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8910\n",
      "Epoch 00341: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3677 - acc: 0.8910 - val_loss: 0.3331 - val_acc: 0.9075\n",
      "Epoch 342/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8885\n",
      "Epoch 00342: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3652 - acc: 0.8884 - val_loss: 0.3731 - val_acc: 0.8991\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8924\n",
      "Epoch 00343: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3622 - acc: 0.8924 - val_loss: 0.3704 - val_acc: 0.9010\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8882\n",
      "Epoch 00344: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3681 - acc: 0.8882 - val_loss: 0.3337 - val_acc: 0.9089\n",
      "Epoch 345/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8925\n",
      "Epoch 00345: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3632 - acc: 0.8925 - val_loss: 0.3430 - val_acc: 0.9106\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8916\n",
      "Epoch 00346: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3598 - acc: 0.8916 - val_loss: 0.3344 - val_acc: 0.9113\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8905\n",
      "Epoch 00347: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3638 - acc: 0.8905 - val_loss: 0.3964 - val_acc: 0.8912\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8908\n",
      "Epoch 00348: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3632 - acc: 0.8908 - val_loss: 0.3691 - val_acc: 0.9001\n",
      "Epoch 349/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8918\n",
      "Epoch 00349: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3587 - acc: 0.8918 - val_loss: 0.3506 - val_acc: 0.9068\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8931\n",
      "Epoch 00350: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3570 - acc: 0.8931 - val_loss: 0.3843 - val_acc: 0.8987\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8913\n",
      "Epoch 00351: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3632 - acc: 0.8912 - val_loss: 0.3411 - val_acc: 0.9078\n",
      "Epoch 352/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8909\n",
      "Epoch 00352: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3571 - acc: 0.8909 - val_loss: 0.3810 - val_acc: 0.8947\n",
      "Epoch 353/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8909\n",
      "Epoch 00353: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3628 - acc: 0.8909 - val_loss: 0.3421 - val_acc: 0.9066\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8914\n",
      "Epoch 00354: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3633 - acc: 0.8913 - val_loss: 0.4964 - val_acc: 0.8619\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8913\n",
      "Epoch 00355: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3610 - acc: 0.8914 - val_loss: 0.3577 - val_acc: 0.9059\n",
      "Epoch 356/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8944\n",
      "Epoch 00356: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3535 - acc: 0.8944 - val_loss: 0.3406 - val_acc: 0.9110\n",
      "Epoch 357/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8935\n",
      "Epoch 00357: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3517 - acc: 0.8935 - val_loss: 0.3647 - val_acc: 0.9066\n",
      "Epoch 358/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8901\n",
      "Epoch 00358: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3592 - acc: 0.8901 - val_loss: 0.3894 - val_acc: 0.8908\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8922\n",
      "Epoch 00359: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3564 - acc: 0.8922 - val_loss: 0.5809 - val_acc: 0.8307\n",
      "Epoch 360/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8907\n",
      "Epoch 00360: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3591 - acc: 0.8907 - val_loss: 0.5475 - val_acc: 0.8474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8915\n",
      "Epoch 00361: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3575 - acc: 0.8915 - val_loss: 0.3485 - val_acc: 0.9054\n",
      "Epoch 362/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8933\n",
      "Epoch 00362: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3545 - acc: 0.8933 - val_loss: 0.4082 - val_acc: 0.8838\n",
      "Epoch 363/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8946\n",
      "Epoch 00363: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.3525 - acc: 0.8945 - val_loss: 0.3620 - val_acc: 0.9080\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8922\n",
      "Epoch 00364: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3543 - acc: 0.8922 - val_loss: 0.3378 - val_acc: 0.9101\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8934\n",
      "Epoch 00365: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3520 - acc: 0.8934 - val_loss: 0.3778 - val_acc: 0.8991\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8914\n",
      "Epoch 00366: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3583 - acc: 0.8914 - val_loss: 0.3363 - val_acc: 0.9113\n",
      "Epoch 367/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8924\n",
      "Epoch 00367: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3560 - acc: 0.8924 - val_loss: 0.4183 - val_acc: 0.8819\n",
      "Epoch 368/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8871\n",
      "Epoch 00368: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3690 - acc: 0.8871 - val_loss: 0.3608 - val_acc: 0.9026\n",
      "Epoch 369/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8957\n",
      "Epoch 00369: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3539 - acc: 0.8957 - val_loss: 0.3319 - val_acc: 0.9113\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8925\n",
      "Epoch 00370: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3541 - acc: 0.8925 - val_loss: 0.3796 - val_acc: 0.8947\n",
      "Epoch 371/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8948\n",
      "Epoch 00371: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3482 - acc: 0.8949 - val_loss: 0.4082 - val_acc: 0.8928\n",
      "Epoch 372/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8941\n",
      "Epoch 00372: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.3490 - acc: 0.8939 - val_loss: 0.3462 - val_acc: 0.9040\n",
      "Epoch 373/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8909\n",
      "Epoch 00373: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3533 - acc: 0.8908 - val_loss: 0.3942 - val_acc: 0.8912\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8925\n",
      "Epoch 00374: val_loss did not improve from 0.32601\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3516 - acc: 0.8924 - val_loss: 0.3837 - val_acc: 0.9024\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 423us/sample - loss: 0.3943 - acc: 0.8843\n",
      "Loss: 0.39425390006844746 Accuracy: 0.88431984\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3424 - acc: 0.2558\n",
      "Epoch 00001: val_loss improved from inf to 2.08196, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/001-2.0820.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 2.3423 - acc: 0.2558 - val_loss: 2.0820 - val_acc: 0.3687\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8076 - acc: 0.4308\n",
      "Epoch 00002: val_loss improved from 2.08196 to 1.47446, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/002-1.4745.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.8077 - acc: 0.4308 - val_loss: 1.4745 - val_acc: 0.6124\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5718 - acc: 0.5170\n",
      "Epoch 00003: val_loss improved from 1.47446 to 1.28323, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/003-1.2832.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.5718 - acc: 0.5170 - val_loss: 1.2832 - val_acc: 0.6774\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4074 - acc: 0.5741\n",
      "Epoch 00004: val_loss improved from 1.28323 to 1.16302, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/004-1.1630.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 1.4074 - acc: 0.5741 - val_loss: 1.1630 - val_acc: 0.6986\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2843 - acc: 0.6159\n",
      "Epoch 00005: val_loss improved from 1.16302 to 1.04116, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/005-1.0412.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.2847 - acc: 0.6157 - val_loss: 1.0412 - val_acc: 0.7370\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1959 - acc: 0.6450\n",
      "Epoch 00006: val_loss did not improve from 1.04116\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 1.1960 - acc: 0.6449 - val_loss: 1.0524 - val_acc: 0.6962\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1173 - acc: 0.6743\n",
      "Epoch 00007: val_loss improved from 1.04116 to 0.86871, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/007-0.8687.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 1.1173 - acc: 0.6743 - val_loss: 0.8687 - val_acc: 0.7817\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0484 - acc: 0.6960\n",
      "Epoch 00008: val_loss improved from 0.86871 to 0.84423, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/008-0.8442.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.0483 - acc: 0.6960 - val_loss: 0.8442 - val_acc: 0.7759\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9981 - acc: 0.7122\n",
      "Epoch 00009: val_loss improved from 0.84423 to 0.74819, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/009-0.7482.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.9980 - acc: 0.7123 - val_loss: 0.7482 - val_acc: 0.8204\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9467 - acc: 0.7281\n",
      "Epoch 00010: val_loss improved from 0.74819 to 0.73186, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/010-0.7319.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.9468 - acc: 0.7280 - val_loss: 0.7319 - val_acc: 0.8190\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7395\n",
      "Epoch 00011: val_loss improved from 0.73186 to 0.71937, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/011-0.7194.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.9044 - acc: 0.7394 - val_loss: 0.7194 - val_acc: 0.8104\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8662 - acc: 0.7538\n",
      "Epoch 00012: val_loss improved from 0.71937 to 0.68803, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/012-0.6880.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8663 - acc: 0.7538 - val_loss: 0.6880 - val_acc: 0.8183\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7643\n",
      "Epoch 00013: val_loss improved from 0.68803 to 0.62524, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/013-0.6252.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.8234 - acc: 0.7644 - val_loss: 0.6252 - val_acc: 0.8381\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7961 - acc: 0.7710\n",
      "Epoch 00014: val_loss improved from 0.62524 to 0.61240, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/014-0.6124.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7965 - acc: 0.7710 - val_loss: 0.6124 - val_acc: 0.8523\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7728 - acc: 0.7830\n",
      "Epoch 00015: val_loss improved from 0.61240 to 0.57275, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/015-0.5727.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7728 - acc: 0.7830 - val_loss: 0.5727 - val_acc: 0.8458\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7884\n",
      "Epoch 00016: val_loss did not improve from 0.57275\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.7470 - acc: 0.7884 - val_loss: 0.6514 - val_acc: 0.8185\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.7969\n",
      "Epoch 00017: val_loss improved from 0.57275 to 0.55202, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/017-0.5520.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7166 - acc: 0.7969 - val_loss: 0.5520 - val_acc: 0.8532\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6901 - acc: 0.8038\n",
      "Epoch 00018: val_loss did not improve from 0.55202\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.6902 - acc: 0.8037 - val_loss: 0.6280 - val_acc: 0.8137\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.8111\n",
      "Epoch 00019: val_loss improved from 0.55202 to 0.48986, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/019-0.4899.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.6748 - acc: 0.8111 - val_loss: 0.4899 - val_acc: 0.8772\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.8134\n",
      "Epoch 00020: val_loss improved from 0.48986 to 0.48507, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/020-0.4851.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.6593 - acc: 0.8133 - val_loss: 0.4851 - val_acc: 0.8784\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6417 - acc: 0.8200\n",
      "Epoch 00021: val_loss did not improve from 0.48507\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.6419 - acc: 0.8199 - val_loss: 0.5646 - val_acc: 0.8532\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.8199\n",
      "Epoch 00022: val_loss improved from 0.48507 to 0.44957, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/022-0.4496.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.6288 - acc: 0.8199 - val_loss: 0.4496 - val_acc: 0.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6176 - acc: 0.8242\n",
      "Epoch 00023: val_loss did not improve from 0.44957\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.6177 - acc: 0.8241 - val_loss: 0.4812 - val_acc: 0.8682\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5951 - acc: 0.8304\n",
      "Epoch 00024: val_loss improved from 0.44957 to 0.43207, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/024-0.4321.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.5952 - acc: 0.8304 - val_loss: 0.4321 - val_acc: 0.8859\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.8349\n",
      "Epoch 00025: val_loss did not improve from 0.43207\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.5819 - acc: 0.8349 - val_loss: 0.4370 - val_acc: 0.8901\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5759 - acc: 0.8335\n",
      "Epoch 00026: val_loss improved from 0.43207 to 0.41099, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/026-0.4110.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.5759 - acc: 0.8334 - val_loss: 0.4110 - val_acc: 0.8908\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8372\n",
      "Epoch 00027: val_loss improved from 0.41099 to 0.40673, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/027-0.4067.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.5633 - acc: 0.8372 - val_loss: 0.4067 - val_acc: 0.8982\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.8415\n",
      "Epoch 00028: val_loss improved from 0.40673 to 0.38592, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/028-0.3859.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.5544 - acc: 0.8415 - val_loss: 0.3859 - val_acc: 0.9005\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8439\n",
      "Epoch 00029: val_loss did not improve from 0.38592\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.5440 - acc: 0.8438 - val_loss: 0.4005 - val_acc: 0.9012\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8453\n",
      "Epoch 00030: val_loss did not improve from 0.38592\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5354 - acc: 0.8453 - val_loss: 0.3887 - val_acc: 0.9052\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.8481\n",
      "Epoch 00031: val_loss did not improve from 0.38592\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.5275 - acc: 0.8481 - val_loss: 0.6351 - val_acc: 0.7948\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8519\n",
      "Epoch 00032: val_loss did not improve from 0.38592\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.5166 - acc: 0.8518 - val_loss: 0.4169 - val_acc: 0.8828\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8536\n",
      "Epoch 00033: val_loss improved from 0.38592 to 0.37126, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/033-0.3713.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.5077 - acc: 0.8536 - val_loss: 0.3713 - val_acc: 0.9024\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8555\n",
      "Epoch 00034: val_loss improved from 0.37126 to 0.36705, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/034-0.3671.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4986 - acc: 0.8554 - val_loss: 0.3671 - val_acc: 0.9073\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8583\n",
      "Epoch 00035: val_loss did not improve from 0.36705\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4922 - acc: 0.8583 - val_loss: 0.3762 - val_acc: 0.8994\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8578\n",
      "Epoch 00036: val_loss did not improve from 0.36705\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4926 - acc: 0.8578 - val_loss: 0.3824 - val_acc: 0.8977\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8598\n",
      "Epoch 00037: val_loss did not improve from 0.36705\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.4812 - acc: 0.8598 - val_loss: 0.3745 - val_acc: 0.8982\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.8612\n",
      "Epoch 00038: val_loss improved from 0.36705 to 0.36208, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/038-0.3621.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4785 - acc: 0.8612 - val_loss: 0.3621 - val_acc: 0.9038\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8636\n",
      "Epoch 00039: val_loss did not improve from 0.36208\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4721 - acc: 0.8636 - val_loss: 0.3654 - val_acc: 0.9052\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8676\n",
      "Epoch 00040: val_loss did not improve from 0.36208\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.4620 - acc: 0.8676 - val_loss: 0.3682 - val_acc: 0.9026\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8693\n",
      "Epoch 00041: val_loss improved from 0.36208 to 0.33129, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/041-0.3313.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.4537 - acc: 0.8693 - val_loss: 0.3313 - val_acc: 0.9131\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8701\n",
      "Epoch 00042: val_loss did not improve from 0.33129\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4531 - acc: 0.8700 - val_loss: 0.3416 - val_acc: 0.9117\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8720\n",
      "Epoch 00043: val_loss improved from 0.33129 to 0.33086, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/043-0.3309.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.4459 - acc: 0.8721 - val_loss: 0.3309 - val_acc: 0.9126\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8700\n",
      "Epoch 00044: val_loss did not improve from 0.33086\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.4439 - acc: 0.8700 - val_loss: 0.3558 - val_acc: 0.9092\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8741\n",
      "Epoch 00045: val_loss improved from 0.33086 to 0.32370, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/045-0.3237.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.4335 - acc: 0.8741 - val_loss: 0.3237 - val_acc: 0.9099\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4300 - acc: 0.8757\n",
      "Epoch 00046: val_loss did not improve from 0.32370\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4301 - acc: 0.8756 - val_loss: 0.4354 - val_acc: 0.8633\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8765\n",
      "Epoch 00047: val_loss improved from 0.32370 to 0.31197, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/047-0.3120.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.4262 - acc: 0.8764 - val_loss: 0.3120 - val_acc: 0.9138\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8780\n",
      "Epoch 00048: val_loss did not improve from 0.31197\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.4189 - acc: 0.8780 - val_loss: 0.3502 - val_acc: 0.8994\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8789\n",
      "Epoch 00049: val_loss did not improve from 0.31197\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4155 - acc: 0.8789 - val_loss: 0.3466 - val_acc: 0.9057\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8807\n",
      "Epoch 00050: val_loss did not improve from 0.31197\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.4080 - acc: 0.8806 - val_loss: 0.3334 - val_acc: 0.9071\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8821\n",
      "Epoch 00051: val_loss improved from 0.31197 to 0.30962, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/051-0.3096.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.4075 - acc: 0.8821 - val_loss: 0.3096 - val_acc: 0.9222\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8802\n",
      "Epoch 00052: val_loss improved from 0.30962 to 0.28864, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/052-0.2886.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.4099 - acc: 0.8802 - val_loss: 0.2886 - val_acc: 0.9255\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8822\n",
      "Epoch 00053: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.4024 - acc: 0.8822 - val_loss: 0.3069 - val_acc: 0.9178\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8847\n",
      "Epoch 00054: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3918 - acc: 0.8847 - val_loss: 0.2893 - val_acc: 0.9217\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8888\n",
      "Epoch 00055: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.3887 - acc: 0.8888 - val_loss: 0.2891 - val_acc: 0.9210\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8873\n",
      "Epoch 00056: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.3911 - acc: 0.8872 - val_loss: 0.3729 - val_acc: 0.8915\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8865\n",
      "Epoch 00057: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3904 - acc: 0.8866 - val_loss: 0.2953 - val_acc: 0.9157\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8912\n",
      "Epoch 00058: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.3773 - acc: 0.8912 - val_loss: 0.2891 - val_acc: 0.9220\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8898\n",
      "Epoch 00059: val_loss did not improve from 0.28864\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.3787 - acc: 0.8897 - val_loss: 0.2975 - val_acc: 0.9203\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8877\n",
      "Epoch 00060: val_loss improved from 0.28864 to 0.28326, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/060-0.2833.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3861 - acc: 0.8878 - val_loss: 0.2833 - val_acc: 0.9264\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8924\n",
      "Epoch 00061: val_loss improved from 0.28326 to 0.28310, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/061-0.2831.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.3687 - acc: 0.8924 - val_loss: 0.2831 - val_acc: 0.9238\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8937\n",
      "Epoch 00062: val_loss did not improve from 0.28310\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.3691 - acc: 0.8937 - val_loss: 0.3004 - val_acc: 0.9187\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8936\n",
      "Epoch 00063: val_loss did not improve from 0.28310\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.3613 - acc: 0.8936 - val_loss: 0.2871 - val_acc: 0.9201\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8945\n",
      "Epoch 00064: val_loss improved from 0.28310 to 0.26784, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/064-0.2678.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.3616 - acc: 0.8945 - val_loss: 0.2678 - val_acc: 0.9278\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8923\n",
      "Epoch 00065: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3683 - acc: 0.8922 - val_loss: 0.2749 - val_acc: 0.9294\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8939\n",
      "Epoch 00066: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.3606 - acc: 0.8939 - val_loss: 0.2815 - val_acc: 0.9287\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8980\n",
      "Epoch 00067: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3485 - acc: 0.8980 - val_loss: 0.2912 - val_acc: 0.9245\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8970\n",
      "Epoch 00068: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.3471 - acc: 0.8969 - val_loss: 0.2683 - val_acc: 0.9278\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8961\n",
      "Epoch 00069: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3492 - acc: 0.8960 - val_loss: 0.2809 - val_acc: 0.9227\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8988\n",
      "Epoch 00070: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.3464 - acc: 0.8987 - val_loss: 0.5073 - val_acc: 0.8397\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8996\n",
      "Epoch 00071: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.3426 - acc: 0.8996 - val_loss: 0.2929 - val_acc: 0.9210\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.9013\n",
      "Epoch 00072: val_loss did not improve from 0.26784\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3344 - acc: 0.9013 - val_loss: 0.2759 - val_acc: 0.9273\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.9021\n",
      "Epoch 00073: val_loss improved from 0.26784 to 0.26599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/073-0.2660.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.3364 - acc: 0.9021 - val_loss: 0.2660 - val_acc: 0.9290\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.9018\n",
      "Epoch 00074: val_loss did not improve from 0.26599\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3365 - acc: 0.9017 - val_loss: 0.2739 - val_acc: 0.9245\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.9020\n",
      "Epoch 00075: val_loss did not improve from 0.26599\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3350 - acc: 0.9020 - val_loss: 0.2752 - val_acc: 0.9276\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.9026\n",
      "Epoch 00076: val_loss improved from 0.26599 to 0.25747, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/076-0.2575.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3308 - acc: 0.9025 - val_loss: 0.2575 - val_acc: 0.9327\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.9025\n",
      "Epoch 00077: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.3266 - acc: 0.9025 - val_loss: 0.2665 - val_acc: 0.9262\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9042\n",
      "Epoch 00078: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.3285 - acc: 0.9041 - val_loss: 0.2625 - val_acc: 0.9315\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.9058\n",
      "Epoch 00079: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3246 - acc: 0.9057 - val_loss: 0.2615 - val_acc: 0.9285\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.9058\n",
      "Epoch 00080: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.3205 - acc: 0.9058 - val_loss: 0.2615 - val_acc: 0.9313\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9082\n",
      "Epoch 00081: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.3116 - acc: 0.9082 - val_loss: 0.2642 - val_acc: 0.9292\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9055\n",
      "Epoch 00082: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.3212 - acc: 0.9055 - val_loss: 0.2745 - val_acc: 0.9255\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9096\n",
      "Epoch 00083: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.3138 - acc: 0.9095 - val_loss: 0.2785 - val_acc: 0.9245\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9052\n",
      "Epoch 00084: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.3143 - acc: 0.9052 - val_loss: 0.3673 - val_acc: 0.8966\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.9085\n",
      "Epoch 00085: val_loss did not improve from 0.25747\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.3095 - acc: 0.9084 - val_loss: 0.2725 - val_acc: 0.9259\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9090\n",
      "Epoch 00086: val_loss improved from 0.25747 to 0.24970, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/086-0.2497.hdf5\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.3080 - acc: 0.9091 - val_loss: 0.2497 - val_acc: 0.9338\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9095\n",
      "Epoch 00087: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.3068 - acc: 0.9095 - val_loss: 0.2678 - val_acc: 0.9306\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9103\n",
      "Epoch 00088: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3038 - acc: 0.9103 - val_loss: 0.3085 - val_acc: 0.9175\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9099\n",
      "Epoch 00089: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.3012 - acc: 0.9099 - val_loss: 0.2650 - val_acc: 0.9294\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9126\n",
      "Epoch 00090: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2977 - acc: 0.9126 - val_loss: 0.2731 - val_acc: 0.9213\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9129\n",
      "Epoch 00091: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2974 - acc: 0.9129 - val_loss: 0.2556 - val_acc: 0.9320\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9124\n",
      "Epoch 00092: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.2962 - acc: 0.9124 - val_loss: 0.2551 - val_acc: 0.9311\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9120\n",
      "Epoch 00093: val_loss did not improve from 0.24970\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2970 - acc: 0.9119 - val_loss: 0.2528 - val_acc: 0.9301\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9140\n",
      "Epoch 00094: val_loss improved from 0.24970 to 0.24726, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/094-0.2473.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.2909 - acc: 0.9140 - val_loss: 0.2473 - val_acc: 0.9343\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9142\n",
      "Epoch 00095: val_loss did not improve from 0.24726\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2869 - acc: 0.9142 - val_loss: 0.2585 - val_acc: 0.9280\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9115\n",
      "Epoch 00096: val_loss did not improve from 0.24726\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2899 - acc: 0.9115 - val_loss: 0.2496 - val_acc: 0.9324\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9167\n",
      "Epoch 00097: val_loss improved from 0.24726 to 0.24620, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/097-0.2462.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2828 - acc: 0.9166 - val_loss: 0.2462 - val_acc: 0.9329\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9150\n",
      "Epoch 00098: val_loss did not improve from 0.24620\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2876 - acc: 0.9150 - val_loss: 0.3243 - val_acc: 0.9147\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9162\n",
      "Epoch 00099: val_loss improved from 0.24620 to 0.23289, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/099-0.2329.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2844 - acc: 0.9162 - val_loss: 0.2329 - val_acc: 0.9334\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9185\n",
      "Epoch 00100: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2776 - acc: 0.9185 - val_loss: 0.2564 - val_acc: 0.9283\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9180\n",
      "Epoch 00101: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2780 - acc: 0.9180 - val_loss: 0.2427 - val_acc: 0.9362\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9154\n",
      "Epoch 00102: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2822 - acc: 0.9155 - val_loss: 0.2340 - val_acc: 0.9362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9177\n",
      "Epoch 00103: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2753 - acc: 0.9177 - val_loss: 0.2396 - val_acc: 0.9371\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9177\n",
      "Epoch 00104: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2778 - acc: 0.9177 - val_loss: 0.2634 - val_acc: 0.9278\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9177\n",
      "Epoch 00105: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2767 - acc: 0.9177 - val_loss: 0.2609 - val_acc: 0.9327\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9193\n",
      "Epoch 00106: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2733 - acc: 0.9192 - val_loss: 0.2443 - val_acc: 0.9348\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9190\n",
      "Epoch 00107: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2697 - acc: 0.9190 - val_loss: 0.2669 - val_acc: 0.9287\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9196\n",
      "Epoch 00108: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2706 - acc: 0.9195 - val_loss: 0.2381 - val_acc: 0.9327\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9211\n",
      "Epoch 00109: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2660 - acc: 0.9211 - val_loss: 0.2463 - val_acc: 0.9304\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9203\n",
      "Epoch 00110: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2676 - acc: 0.9203 - val_loss: 0.3488 - val_acc: 0.9003\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9215\n",
      "Epoch 00111: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2657 - acc: 0.9215 - val_loss: 0.2400 - val_acc: 0.9315\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9216\n",
      "Epoch 00112: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2654 - acc: 0.9216 - val_loss: 0.2773 - val_acc: 0.9255\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9228\n",
      "Epoch 00113: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2633 - acc: 0.9228 - val_loss: 0.2521 - val_acc: 0.9317\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9209\n",
      "Epoch 00114: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2606 - acc: 0.9209 - val_loss: 0.2394 - val_acc: 0.9373\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9237\n",
      "Epoch 00115: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2586 - acc: 0.9237 - val_loss: 0.2373 - val_acc: 0.9359\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9222\n",
      "Epoch 00116: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2589 - acc: 0.9222 - val_loss: 0.2385 - val_acc: 0.9371\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9236\n",
      "Epoch 00117: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2586 - acc: 0.9236 - val_loss: 0.2384 - val_acc: 0.9317\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9255\n",
      "Epoch 00118: val_loss did not improve from 0.23289\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2560 - acc: 0.9255 - val_loss: 0.2460 - val_acc: 0.9362\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9250\n",
      "Epoch 00119: val_loss improved from 0.23289 to 0.23115, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/119-0.2311.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2536 - acc: 0.9250 - val_loss: 0.2311 - val_acc: 0.9406\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9246\n",
      "Epoch 00120: val_loss did not improve from 0.23115\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2517 - acc: 0.9247 - val_loss: 0.2513 - val_acc: 0.9336\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9242\n",
      "Epoch 00121: val_loss improved from 0.23115 to 0.22928, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/121-0.2293.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2564 - acc: 0.9242 - val_loss: 0.2293 - val_acc: 0.9366\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9279\n",
      "Epoch 00122: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2429 - acc: 0.9278 - val_loss: 0.2439 - val_acc: 0.9338\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9261\n",
      "Epoch 00123: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2487 - acc: 0.9261 - val_loss: 0.2654 - val_acc: 0.9297\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9287\n",
      "Epoch 00124: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2446 - acc: 0.9287 - val_loss: 0.2630 - val_acc: 0.9280\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9267\n",
      "Epoch 00125: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2460 - acc: 0.9267 - val_loss: 0.2615 - val_acc: 0.9273\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9259\n",
      "Epoch 00126: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2497 - acc: 0.9259 - val_loss: 0.3078 - val_acc: 0.9108\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9260\n",
      "Epoch 00127: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2436 - acc: 0.9260 - val_loss: 0.2504 - val_acc: 0.9320\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9285\n",
      "Epoch 00128: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2416 - acc: 0.9285 - val_loss: 0.2297 - val_acc: 0.9408\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9279\n",
      "Epoch 00129: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2415 - acc: 0.9279 - val_loss: 0.2535 - val_acc: 0.9299\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9278\n",
      "Epoch 00130: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2410 - acc: 0.9278 - val_loss: 0.2467 - val_acc: 0.9345\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9278\n",
      "Epoch 00131: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2398 - acc: 0.9278 - val_loss: 0.2575 - val_acc: 0.9285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9284\n",
      "Epoch 00132: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2384 - acc: 0.9284 - val_loss: 0.2566 - val_acc: 0.9322\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9286\n",
      "Epoch 00133: val_loss did not improve from 0.22928\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2375 - acc: 0.9286 - val_loss: 0.2380 - val_acc: 0.9359\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9299\n",
      "Epoch 00134: val_loss improved from 0.22928 to 0.22832, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/134-0.2283.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2347 - acc: 0.9298 - val_loss: 0.2283 - val_acc: 0.9378\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9280\n",
      "Epoch 00135: val_loss did not improve from 0.22832\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2355 - acc: 0.9280 - val_loss: 0.2321 - val_acc: 0.9366\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9284\n",
      "Epoch 00136: val_loss improved from 0.22832 to 0.21419, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/136-0.2142.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.2342 - acc: 0.9284 - val_loss: 0.2142 - val_acc: 0.9408\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9303\n",
      "Epoch 00137: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2277 - acc: 0.9302 - val_loss: 0.2323 - val_acc: 0.9355\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9313\n",
      "Epoch 00138: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2303 - acc: 0.9313 - val_loss: 0.2197 - val_acc: 0.9399\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9305\n",
      "Epoch 00139: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2329 - acc: 0.9304 - val_loss: 0.2381 - val_acc: 0.9364\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9290\n",
      "Epoch 00140: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2344 - acc: 0.9290 - val_loss: 0.2287 - val_acc: 0.9418\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9307\n",
      "Epoch 00141: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2324 - acc: 0.9307 - val_loss: 0.3235 - val_acc: 0.9113\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9330\n",
      "Epoch 00142: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2253 - acc: 0.9330 - val_loss: 0.2315 - val_acc: 0.9380\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9350\n",
      "Epoch 00143: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2243 - acc: 0.9350 - val_loss: 0.2264 - val_acc: 0.9390\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9295\n",
      "Epoch 00144: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2316 - acc: 0.9295 - val_loss: 0.2469 - val_acc: 0.9343\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9339\n",
      "Epoch 00145: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2215 - acc: 0.9339 - val_loss: 0.2443 - val_acc: 0.9350\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9308\n",
      "Epoch 00146: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2254 - acc: 0.9308 - val_loss: 0.2245 - val_acc: 0.9392\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9332\n",
      "Epoch 00147: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2234 - acc: 0.9331 - val_loss: 0.2323 - val_acc: 0.9380\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9338\n",
      "Epoch 00148: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2204 - acc: 0.9338 - val_loss: 0.2403 - val_acc: 0.9366\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9340\n",
      "Epoch 00149: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2195 - acc: 0.9339 - val_loss: 0.2231 - val_acc: 0.9411\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9348\n",
      "Epoch 00150: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2167 - acc: 0.9348 - val_loss: 0.2325 - val_acc: 0.9378\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9336\n",
      "Epoch 00151: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2186 - acc: 0.9336 - val_loss: 0.2420 - val_acc: 0.9350\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9337\n",
      "Epoch 00152: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2197 - acc: 0.9337 - val_loss: 0.2210 - val_acc: 0.9401\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9332\n",
      "Epoch 00153: val_loss did not improve from 0.21419\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2222 - acc: 0.9332 - val_loss: 0.2198 - val_acc: 0.9420\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9340\n",
      "Epoch 00154: val_loss improved from 0.21419 to 0.21081, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/154-0.2108.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2170 - acc: 0.9340 - val_loss: 0.2108 - val_acc: 0.9425\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9355\n",
      "Epoch 00155: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2157 - acc: 0.9355 - val_loss: 0.2408 - val_acc: 0.9345\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9355\n",
      "Epoch 00156: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2108 - acc: 0.9355 - val_loss: 0.2273 - val_acc: 0.9390\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9362\n",
      "Epoch 00157: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2120 - acc: 0.9361 - val_loss: 0.2278 - val_acc: 0.9422\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9348\n",
      "Epoch 00158: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2140 - acc: 0.9348 - val_loss: 0.2206 - val_acc: 0.9399\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9365\n",
      "Epoch 00159: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2096 - acc: 0.9365 - val_loss: 0.2276 - val_acc: 0.9397\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9382\n",
      "Epoch 00160: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2102 - acc: 0.9382 - val_loss: 0.2547 - val_acc: 0.9338\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9368\n",
      "Epoch 00161: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2100 - acc: 0.9368 - val_loss: 0.2809 - val_acc: 0.9206\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9372\n",
      "Epoch 00162: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2070 - acc: 0.9372 - val_loss: 0.2576 - val_acc: 0.9320\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9376\n",
      "Epoch 00163: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2060 - acc: 0.9375 - val_loss: 0.2292 - val_acc: 0.9357\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9361\n",
      "Epoch 00164: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2081 - acc: 0.9361 - val_loss: 0.2369 - val_acc: 0.9350\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9383\n",
      "Epoch 00165: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2044 - acc: 0.9383 - val_loss: 0.2220 - val_acc: 0.9387\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9373\n",
      "Epoch 00166: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2057 - acc: 0.9373 - val_loss: 0.2347 - val_acc: 0.9357\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9379\n",
      "Epoch 00167: val_loss did not improve from 0.21081\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2044 - acc: 0.9379 - val_loss: 0.2207 - val_acc: 0.9448\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9377\n",
      "Epoch 00168: val_loss improved from 0.21081 to 0.20596, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv_checkpoint/168-0.2060.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2070 - acc: 0.9376 - val_loss: 0.2060 - val_acc: 0.9432\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9383\n",
      "Epoch 00169: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1995 - acc: 0.9382 - val_loss: 0.2216 - val_acc: 0.9397\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9393\n",
      "Epoch 00170: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2014 - acc: 0.9393 - val_loss: 0.2290 - val_acc: 0.9397\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9365\n",
      "Epoch 00171: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2065 - acc: 0.9365 - val_loss: 0.2241 - val_acc: 0.9422\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9390\n",
      "Epoch 00172: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2012 - acc: 0.9389 - val_loss: 0.2430 - val_acc: 0.9329\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9376\n",
      "Epoch 00173: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2022 - acc: 0.9376 - val_loss: 0.2480 - val_acc: 0.9327\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9387\n",
      "Epoch 00174: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1997 - acc: 0.9387 - val_loss: 0.2208 - val_acc: 0.9406\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9397\n",
      "Epoch 00175: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1958 - acc: 0.9397 - val_loss: 0.2240 - val_acc: 0.9390\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9407\n",
      "Epoch 00176: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1949 - acc: 0.9406 - val_loss: 0.2301 - val_acc: 0.9359\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9398\n",
      "Epoch 00177: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1980 - acc: 0.9398 - val_loss: 0.2140 - val_acc: 0.9443\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9400\n",
      "Epoch 00178: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1971 - acc: 0.9400 - val_loss: 0.2300 - val_acc: 0.9413\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9416\n",
      "Epoch 00179: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1922 - acc: 0.9416 - val_loss: 0.2440 - val_acc: 0.9362\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9404\n",
      "Epoch 00180: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1983 - acc: 0.9404 - val_loss: 0.2434 - val_acc: 0.9369\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9408\n",
      "Epoch 00181: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1934 - acc: 0.9409 - val_loss: 0.2162 - val_acc: 0.9415\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9415\n",
      "Epoch 00182: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1940 - acc: 0.9415 - val_loss: 0.2271 - val_acc: 0.9390\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9411\n",
      "Epoch 00183: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1911 - acc: 0.9411 - val_loss: 0.2761 - val_acc: 0.9299\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9407\n",
      "Epoch 00184: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1910 - acc: 0.9407 - val_loss: 0.2405 - val_acc: 0.9390\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9413\n",
      "Epoch 00185: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1902 - acc: 0.9412 - val_loss: 0.2393 - val_acc: 0.9317\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9377\n",
      "Epoch 00186: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.2009 - acc: 0.9377 - val_loss: 0.2401 - val_acc: 0.9404\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9417\n",
      "Epoch 00187: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1891 - acc: 0.9417 - val_loss: 0.2737 - val_acc: 0.9287\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9437\n",
      "Epoch 00188: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1838 - acc: 0.9437 - val_loss: 0.2308 - val_acc: 0.9392\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9451\n",
      "Epoch 00189: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1826 - acc: 0.9451 - val_loss: 0.2194 - val_acc: 0.9411\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9448\n",
      "Epoch 00190: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1814 - acc: 0.9448 - val_loss: 0.2320 - val_acc: 0.9401\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9438\n",
      "Epoch 00191: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1883 - acc: 0.9438 - val_loss: 0.2435 - val_acc: 0.9341\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9432\n",
      "Epoch 00192: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1882 - acc: 0.9432 - val_loss: 0.2350 - val_acc: 0.9352\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9429\n",
      "Epoch 00193: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1845 - acc: 0.9429 - val_loss: 0.2264 - val_acc: 0.9380\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9419\n",
      "Epoch 00194: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1859 - acc: 0.9420 - val_loss: 0.2341 - val_acc: 0.9336\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9444\n",
      "Epoch 00195: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1803 - acc: 0.9444 - val_loss: 0.2271 - val_acc: 0.9397\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9440\n",
      "Epoch 00196: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1850 - acc: 0.9440 - val_loss: 0.2263 - val_acc: 0.9399\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9444\n",
      "Epoch 00197: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1795 - acc: 0.9444 - val_loss: 0.2284 - val_acc: 0.9383\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9414\n",
      "Epoch 00198: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1890 - acc: 0.9413 - val_loss: 0.2184 - val_acc: 0.9420\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9443\n",
      "Epoch 00199: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1811 - acc: 0.9443 - val_loss: 0.2234 - val_acc: 0.9432\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9448\n",
      "Epoch 00200: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1756 - acc: 0.9447 - val_loss: 0.2549 - val_acc: 0.9383\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9466\n",
      "Epoch 00201: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1769 - acc: 0.9466 - val_loss: 0.2415 - val_acc: 0.9357\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9455\n",
      "Epoch 00202: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1831 - acc: 0.9455 - val_loss: 0.2241 - val_acc: 0.9392\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9455\n",
      "Epoch 00203: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1806 - acc: 0.9455 - val_loss: 0.2275 - val_acc: 0.9434\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9465\n",
      "Epoch 00204: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1746 - acc: 0.9465 - val_loss: 0.2272 - val_acc: 0.9385\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9477\n",
      "Epoch 00205: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1713 - acc: 0.9477 - val_loss: 0.2306 - val_acc: 0.9406\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9436\n",
      "Epoch 00206: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1808 - acc: 0.9435 - val_loss: 0.2286 - val_acc: 0.9380\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9438\n",
      "Epoch 00207: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1809 - acc: 0.9438 - val_loss: 0.2466 - val_acc: 0.9362\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9441\n",
      "Epoch 00208: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1801 - acc: 0.9441 - val_loss: 0.2325 - val_acc: 0.9378\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9468\n",
      "Epoch 00209: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1770 - acc: 0.9468 - val_loss: 0.2387 - val_acc: 0.9352\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9463\n",
      "Epoch 00210: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1716 - acc: 0.9463 - val_loss: 0.2167 - val_acc: 0.9427\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9459\n",
      "Epoch 00211: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1729 - acc: 0.9458 - val_loss: 0.2458 - val_acc: 0.9311\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9460\n",
      "Epoch 00212: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.1780 - acc: 0.9460 - val_loss: 0.2218 - val_acc: 0.9420\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9470\n",
      "Epoch 00213: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1696 - acc: 0.9470 - val_loss: 0.2627 - val_acc: 0.9350\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9471\n",
      "Epoch 00214: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1751 - acc: 0.9470 - val_loss: 0.2323 - val_acc: 0.9394\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9472\n",
      "Epoch 00215: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1695 - acc: 0.9472 - val_loss: 0.2743 - val_acc: 0.9308\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9476\n",
      "Epoch 00216: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1710 - acc: 0.9476 - val_loss: 0.2173 - val_acc: 0.9427\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9486\n",
      "Epoch 00217: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1680 - acc: 0.9486 - val_loss: 0.2301 - val_acc: 0.9429\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9474\n",
      "Epoch 00218: val_loss did not improve from 0.20596\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1689 - acc: 0.9474 - val_loss: 0.2602 - val_acc: 0.9341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 451us/sample - loss: 0.2416 - acc: 0.9269\n",
      "Loss: 0.24161738423419765 Accuracy: 0.92689514\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2312 - acc: 0.2940\n",
      "Epoch 00001: val_loss improved from inf to 1.80404, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/001-1.8040.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.2312 - acc: 0.2940 - val_loss: 1.8040 - val_acc: 0.4708\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5816 - acc: 0.5060\n",
      "Epoch 00002: val_loss improved from 1.80404 to 1.16131, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/002-1.1613.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 1.5817 - acc: 0.5060 - val_loss: 1.1613 - val_acc: 0.6918\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2821 - acc: 0.6138\n",
      "Epoch 00003: val_loss improved from 1.16131 to 1.02578, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/003-1.0258.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 1.2821 - acc: 0.6138 - val_loss: 1.0258 - val_acc: 0.7258\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0713 - acc: 0.6875\n",
      "Epoch 00004: val_loss improved from 1.02578 to 0.79237, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/004-0.7924.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 1.0714 - acc: 0.6874 - val_loss: 0.7924 - val_acc: 0.8018\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9229 - acc: 0.7373\n",
      "Epoch 00005: val_loss improved from 0.79237 to 0.66910, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/005-0.6691.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.9228 - acc: 0.7373 - val_loss: 0.6691 - val_acc: 0.8365\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8033 - acc: 0.7751\n",
      "Epoch 00006: val_loss improved from 0.66910 to 0.55310, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/006-0.5531.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.8033 - acc: 0.7750 - val_loss: 0.5531 - val_acc: 0.8724\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.7989\n",
      "Epoch 00007: val_loss improved from 0.55310 to 0.50017, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/007-0.5002.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.7204 - acc: 0.7989 - val_loss: 0.5002 - val_acc: 0.8819\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.8167\n",
      "Epoch 00008: val_loss improved from 0.50017 to 0.45322, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/008-0.4532.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.6577 - acc: 0.8167 - val_loss: 0.4532 - val_acc: 0.8868\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.8313\n",
      "Epoch 00009: val_loss did not improve from 0.45322\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.6022 - acc: 0.8313 - val_loss: 0.4722 - val_acc: 0.8784\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.8424\n",
      "Epoch 00010: val_loss improved from 0.45322 to 0.43992, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/010-0.4399.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.5586 - acc: 0.8425 - val_loss: 0.4399 - val_acc: 0.8812\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.8500\n",
      "Epoch 00011: val_loss improved from 0.43992 to 0.35294, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/011-0.3529.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5319 - acc: 0.8500 - val_loss: 0.3529 - val_acc: 0.9059\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8599\n",
      "Epoch 00012: val_loss improved from 0.35294 to 0.32901, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/012-0.3290.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.4989 - acc: 0.8599 - val_loss: 0.3290 - val_acc: 0.9150\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.8660\n",
      "Epoch 00013: val_loss did not improve from 0.32901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.4725 - acc: 0.8659 - val_loss: 0.3608 - val_acc: 0.9005\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.8726\n",
      "Epoch 00014: val_loss improved from 0.32901 to 0.31229, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/014-0.3123.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.4536 - acc: 0.8726 - val_loss: 0.3123 - val_acc: 0.9173\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8771\n",
      "Epoch 00015: val_loss improved from 0.31229 to 0.28002, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/015-0.2800.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.4326 - acc: 0.8771 - val_loss: 0.2800 - val_acc: 0.9278\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8816\n",
      "Epoch 00016: val_loss improved from 0.28002 to 0.27926, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/016-0.2793.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.4158 - acc: 0.8815 - val_loss: 0.2793 - val_acc: 0.9273\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8869\n",
      "Epoch 00017: val_loss improved from 0.27926 to 0.25881, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/017-0.2588.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.4001 - acc: 0.8869 - val_loss: 0.2588 - val_acc: 0.9336\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8907\n",
      "Epoch 00018: val_loss did not improve from 0.25881\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.3845 - acc: 0.8907 - val_loss: 0.2712 - val_acc: 0.9227\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8928\n",
      "Epoch 00019: val_loss did not improve from 0.25881\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3762 - acc: 0.8928 - val_loss: 0.2734 - val_acc: 0.9273\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8966\n",
      "Epoch 00020: val_loss improved from 0.25881 to 0.24646, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/020-0.2465.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.3620 - acc: 0.8966 - val_loss: 0.2465 - val_acc: 0.9329\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.9002\n",
      "Epoch 00021: val_loss improved from 0.24646 to 0.24156, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/021-0.2416.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3490 - acc: 0.9001 - val_loss: 0.2416 - val_acc: 0.9336\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.9010\n",
      "Epoch 00022: val_loss did not improve from 0.24156\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.3442 - acc: 0.9010 - val_loss: 0.2489 - val_acc: 0.9259\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.9038\n",
      "Epoch 00023: val_loss did not improve from 0.24156\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.3315 - acc: 0.9038 - val_loss: 0.2447 - val_acc: 0.9341\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9063\n",
      "Epoch 00024: val_loss improved from 0.24156 to 0.22812, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/024-0.2281.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3248 - acc: 0.9063 - val_loss: 0.2281 - val_acc: 0.9343\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.9061\n",
      "Epoch 00025: val_loss improved from 0.22812 to 0.21098, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/025-0.2110.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3242 - acc: 0.9061 - val_loss: 0.2110 - val_acc: 0.9362\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9113\n",
      "Epoch 00026: val_loss did not improve from 0.21098\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3132 - acc: 0.9113 - val_loss: 0.2366 - val_acc: 0.9350\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9110\n",
      "Epoch 00027: val_loss did not improve from 0.21098\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3013 - acc: 0.9109 - val_loss: 0.2193 - val_acc: 0.9364\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9140\n",
      "Epoch 00028: val_loss did not improve from 0.21098\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2971 - acc: 0.9140 - val_loss: 0.2405 - val_acc: 0.9331\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9178\n",
      "Epoch 00029: val_loss did not improve from 0.21098\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2881 - acc: 0.9178 - val_loss: 0.2151 - val_acc: 0.9390\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9183\n",
      "Epoch 00030: val_loss improved from 0.21098 to 0.20834, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/030-0.2083.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2813 - acc: 0.9184 - val_loss: 0.2083 - val_acc: 0.9413\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9201\n",
      "Epoch 00031: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2761 - acc: 0.9201 - val_loss: 0.2113 - val_acc: 0.9397\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9196\n",
      "Epoch 00032: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2768 - acc: 0.9195 - val_loss: 0.2918 - val_acc: 0.9094\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9217\n",
      "Epoch 00033: val_loss improved from 0.20834 to 0.19827, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/033-0.1983.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2688 - acc: 0.9217 - val_loss: 0.1983 - val_acc: 0.9397\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9240\n",
      "Epoch 00034: val_loss improved from 0.19827 to 0.18301, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/034-0.1830.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2621 - acc: 0.9240 - val_loss: 0.1830 - val_acc: 0.9481\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9238\n",
      "Epoch 00035: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2607 - acc: 0.9238 - val_loss: 0.2184 - val_acc: 0.9348\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9262\n",
      "Epoch 00036: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2544 - acc: 0.9262 - val_loss: 0.2004 - val_acc: 0.9439\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9271\n",
      "Epoch 00037: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2473 - acc: 0.9271 - val_loss: 0.2122 - val_acc: 0.9394\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9275\n",
      "Epoch 00038: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2480 - acc: 0.9275 - val_loss: 0.1939 - val_acc: 0.9485\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9302\n",
      "Epoch 00039: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2397 - acc: 0.9302 - val_loss: 0.1897 - val_acc: 0.9476\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9301\n",
      "Epoch 00040: val_loss did not improve from 0.18301\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2370 - acc: 0.9300 - val_loss: 0.1856 - val_acc: 0.9474\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9327\n",
      "Epoch 00041: val_loss improved from 0.18301 to 0.17611, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/041-0.1761.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2321 - acc: 0.9327 - val_loss: 0.1761 - val_acc: 0.9502\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9335\n",
      "Epoch 00042: val_loss did not improve from 0.17611\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2308 - acc: 0.9335 - val_loss: 0.1843 - val_acc: 0.9446\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9324\n",
      "Epoch 00043: val_loss improved from 0.17611 to 0.16567, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/043-0.1657.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2281 - acc: 0.9325 - val_loss: 0.1657 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9347\n",
      "Epoch 00044: val_loss did not improve from 0.16567\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2210 - acc: 0.9347 - val_loss: 0.1940 - val_acc: 0.9450\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9369\n",
      "Epoch 00045: val_loss improved from 0.16567 to 0.16515, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/045-0.1651.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2132 - acc: 0.9369 - val_loss: 0.1651 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9364\n",
      "Epoch 00046: val_loss did not improve from 0.16515\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2124 - acc: 0.9364 - val_loss: 0.1696 - val_acc: 0.9548\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9368\n",
      "Epoch 00047: val_loss did not improve from 0.16515\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2150 - acc: 0.9368 - val_loss: 0.1795 - val_acc: 0.9469\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9388\n",
      "Epoch 00048: val_loss did not improve from 0.16515\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2087 - acc: 0.9387 - val_loss: 0.1842 - val_acc: 0.9432\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9377\n",
      "Epoch 00049: val_loss improved from 0.16515 to 0.16368, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/049-0.1637.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2049 - acc: 0.9377 - val_loss: 0.1637 - val_acc: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9414\n",
      "Epoch 00050: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1995 - acc: 0.9414 - val_loss: 0.1815 - val_acc: 0.9443\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9396\n",
      "Epoch 00051: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2026 - acc: 0.9396 - val_loss: 0.1890 - val_acc: 0.9476\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9409\n",
      "Epoch 00052: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1974 - acc: 0.9409 - val_loss: 0.1666 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9406\n",
      "Epoch 00053: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1980 - acc: 0.9406 - val_loss: 0.1822 - val_acc: 0.9443\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9435\n",
      "Epoch 00054: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1878 - acc: 0.9435 - val_loss: 0.1741 - val_acc: 0.9509\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9443\n",
      "Epoch 00055: val_loss did not improve from 0.16368\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1884 - acc: 0.9442 - val_loss: 0.1688 - val_acc: 0.9492\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9450\n",
      "Epoch 00056: val_loss improved from 0.16368 to 0.15545, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/056-0.1554.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1851 - acc: 0.9450 - val_loss: 0.1554 - val_acc: 0.9534\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9454\n",
      "Epoch 00057: val_loss did not improve from 0.15545\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1853 - acc: 0.9454 - val_loss: 0.1676 - val_acc: 0.9518\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9460\n",
      "Epoch 00058: val_loss improved from 0.15545 to 0.15275, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/058-0.1528.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1820 - acc: 0.9460 - val_loss: 0.1528 - val_acc: 0.9502\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9493\n",
      "Epoch 00059: val_loss did not improve from 0.15275\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1752 - acc: 0.9492 - val_loss: 0.1824 - val_acc: 0.9439\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9463\n",
      "Epoch 00060: val_loss did not improve from 0.15275\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1773 - acc: 0.9462 - val_loss: 0.1901 - val_acc: 0.9443\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9478\n",
      "Epoch 00061: val_loss improved from 0.15275 to 0.15150, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/061-0.1515.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1746 - acc: 0.9478 - val_loss: 0.1515 - val_acc: 0.9574\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9488\n",
      "Epoch 00062: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1690 - acc: 0.9488 - val_loss: 0.1612 - val_acc: 0.9536\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9499\n",
      "Epoch 00063: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1658 - acc: 0.9499 - val_loss: 0.1605 - val_acc: 0.9532\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9488\n",
      "Epoch 00064: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1677 - acc: 0.9488 - val_loss: 0.1547 - val_acc: 0.9581\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9484\n",
      "Epoch 00065: val_loss improved from 0.15150 to 0.15138, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/065-0.1514.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1685 - acc: 0.9483 - val_loss: 0.1514 - val_acc: 0.9555\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9500\n",
      "Epoch 00066: val_loss did not improve from 0.15138\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1635 - acc: 0.9499 - val_loss: 0.1752 - val_acc: 0.9471\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9498\n",
      "Epoch 00067: val_loss did not improve from 0.15138\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1637 - acc: 0.9498 - val_loss: 0.1664 - val_acc: 0.9509\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9518\n",
      "Epoch 00068: val_loss improved from 0.15138 to 0.14980, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/068-0.1498.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1598 - acc: 0.9518 - val_loss: 0.1498 - val_acc: 0.9560\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9528\n",
      "Epoch 00069: val_loss did not improve from 0.14980\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1577 - acc: 0.9528 - val_loss: 0.1579 - val_acc: 0.9541\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9517\n",
      "Epoch 00070: val_loss did not improve from 0.14980\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1567 - acc: 0.9517 - val_loss: 0.1544 - val_acc: 0.9541\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9537\n",
      "Epoch 00071: val_loss did not improve from 0.14980\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1545 - acc: 0.9537 - val_loss: 0.1896 - val_acc: 0.9476\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9538\n",
      "Epoch 00072: val_loss improved from 0.14980 to 0.14457, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/072-0.1446.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1535 - acc: 0.9537 - val_loss: 0.1446 - val_acc: 0.9576\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9541\n",
      "Epoch 00073: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1520 - acc: 0.9541 - val_loss: 0.1466 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9542\n",
      "Epoch 00074: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1511 - acc: 0.9542 - val_loss: 0.1639 - val_acc: 0.9522\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9554\n",
      "Epoch 00075: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1444 - acc: 0.9554 - val_loss: 0.1527 - val_acc: 0.9555\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9574\n",
      "Epoch 00076: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1434 - acc: 0.9574 - val_loss: 0.1603 - val_acc: 0.9574\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9561\n",
      "Epoch 00077: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1408 - acc: 0.9561 - val_loss: 0.1741 - val_acc: 0.9488\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9563\n",
      "Epoch 00078: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1404 - acc: 0.9562 - val_loss: 0.1476 - val_acc: 0.9560\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9561\n",
      "Epoch 00079: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1483 - acc: 0.9561 - val_loss: 0.1585 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9584\n",
      "Epoch 00080: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1377 - acc: 0.9584 - val_loss: 0.1475 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9583\n",
      "Epoch 00081: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1373 - acc: 0.9583 - val_loss: 0.1499 - val_acc: 0.9583\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9591\n",
      "Epoch 00082: val_loss did not improve from 0.14457\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1345 - acc: 0.9591 - val_loss: 0.1469 - val_acc: 0.9590\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9582\n",
      "Epoch 00083: val_loss improved from 0.14457 to 0.14001, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/083-0.1400.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1339 - acc: 0.9582 - val_loss: 0.1400 - val_acc: 0.9569\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9581\n",
      "Epoch 00084: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1380 - acc: 0.9581 - val_loss: 0.1597 - val_acc: 0.9509\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9611\n",
      "Epoch 00085: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1290 - acc: 0.9611 - val_loss: 0.1526 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9596\n",
      "Epoch 00086: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1307 - acc: 0.9596 - val_loss: 0.1511 - val_acc: 0.9562\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9607\n",
      "Epoch 00087: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1275 - acc: 0.9606 - val_loss: 0.1657 - val_acc: 0.9515\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9605\n",
      "Epoch 00088: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1299 - acc: 0.9605 - val_loss: 0.1585 - val_acc: 0.9562\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9630\n",
      "Epoch 00089: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1208 - acc: 0.9630 - val_loss: 0.1485 - val_acc: 0.9555\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9630\n",
      "Epoch 00090: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1219 - acc: 0.9630 - val_loss: 0.1433 - val_acc: 0.9557\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9619\n",
      "Epoch 00091: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1220 - acc: 0.9619 - val_loss: 0.1492 - val_acc: 0.9567\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9616\n",
      "Epoch 00092: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1243 - acc: 0.9616 - val_loss: 0.1789 - val_acc: 0.9471\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9625\n",
      "Epoch 00093: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1210 - acc: 0.9625 - val_loss: 0.1619 - val_acc: 0.9515\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9643\n",
      "Epoch 00094: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1168 - acc: 0.9644 - val_loss: 0.1546 - val_acc: 0.9564\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9638\n",
      "Epoch 00095: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1186 - acc: 0.9638 - val_loss: 0.1736 - val_acc: 0.9504\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9625\n",
      "Epoch 00096: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1186 - acc: 0.9625 - val_loss: 0.1517 - val_acc: 0.9564\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9660\n",
      "Epoch 00097: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1115 - acc: 0.9660 - val_loss: 0.1540 - val_acc: 0.9555\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9640\n",
      "Epoch 00098: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1164 - acc: 0.9639 - val_loss: 0.1505 - val_acc: 0.9557\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9636\n",
      "Epoch 00099: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1160 - acc: 0.9636 - val_loss: 0.1459 - val_acc: 0.9581\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9646\n",
      "Epoch 00100: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1144 - acc: 0.9646 - val_loss: 0.1622 - val_acc: 0.9557\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9644\n",
      "Epoch 00101: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1148 - acc: 0.9644 - val_loss: 0.1458 - val_acc: 0.9613\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9651\n",
      "Epoch 00102: val_loss did not improve from 0.14001\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1110 - acc: 0.9651 - val_loss: 0.1420 - val_acc: 0.9574\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9670\n",
      "Epoch 00103: val_loss improved from 0.14001 to 0.13959, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv_checkpoint/103-0.1396.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1083 - acc: 0.9669 - val_loss: 0.1396 - val_acc: 0.9578\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9675\n",
      "Epoch 00104: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1076 - acc: 0.9675 - val_loss: 0.1713 - val_acc: 0.9504\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9651\n",
      "Epoch 00105: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1112 - acc: 0.9651 - val_loss: 0.1743 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9684\n",
      "Epoch 00106: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1037 - acc: 0.9683 - val_loss: 0.1803 - val_acc: 0.9462\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9646\n",
      "Epoch 00107: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1110 - acc: 0.9647 - val_loss: 0.1464 - val_acc: 0.9602\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9685\n",
      "Epoch 00108: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1017 - acc: 0.9685 - val_loss: 0.1468 - val_acc: 0.9606\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9690\n",
      "Epoch 00109: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1003 - acc: 0.9690 - val_loss: 0.1440 - val_acc: 0.9585\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9677\n",
      "Epoch 00110: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1045 - acc: 0.9677 - val_loss: 0.1501 - val_acc: 0.9588\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9696\n",
      "Epoch 00111: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0978 - acc: 0.9696 - val_loss: 0.1643 - val_acc: 0.9564\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9693\n",
      "Epoch 00112: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0999 - acc: 0.9692 - val_loss: 0.1647 - val_acc: 0.9557\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9689\n",
      "Epoch 00113: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1008 - acc: 0.9688 - val_loss: 0.2343 - val_acc: 0.9322\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9683\n",
      "Epoch 00114: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1007 - acc: 0.9683 - val_loss: 0.1612 - val_acc: 0.9541\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9699\n",
      "Epoch 00115: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0989 - acc: 0.9699 - val_loss: 0.1487 - val_acc: 0.9590\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9692\n",
      "Epoch 00116: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0996 - acc: 0.9692 - val_loss: 0.1642 - val_acc: 0.9541\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9674\n",
      "Epoch 00117: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1037 - acc: 0.9675 - val_loss: 0.1620 - val_acc: 0.9532\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9720\n",
      "Epoch 00118: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0917 - acc: 0.9720 - val_loss: 0.1587 - val_acc: 0.9534\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9711\n",
      "Epoch 00119: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0942 - acc: 0.9710 - val_loss: 0.1987 - val_acc: 0.9443\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9697\n",
      "Epoch 00120: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0981 - acc: 0.9697 - val_loss: 0.1784 - val_acc: 0.9532\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9710\n",
      "Epoch 00121: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0948 - acc: 0.9710 - val_loss: 0.1560 - val_acc: 0.9592\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9726\n",
      "Epoch 00122: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0915 - acc: 0.9726 - val_loss: 0.1477 - val_acc: 0.9581\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9726\n",
      "Epoch 00123: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0897 - acc: 0.9726 - val_loss: 0.1504 - val_acc: 0.9609\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9716\n",
      "Epoch 00124: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0893 - acc: 0.9716 - val_loss: 0.1455 - val_acc: 0.9597\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9720\n",
      "Epoch 00125: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0902 - acc: 0.9720 - val_loss: 0.1439 - val_acc: 0.9581\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9739\n",
      "Epoch 00126: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0845 - acc: 0.9739 - val_loss: 0.1535 - val_acc: 0.9543\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9726\n",
      "Epoch 00127: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0887 - acc: 0.9725 - val_loss: 0.1739 - val_acc: 0.9525\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9704\n",
      "Epoch 00128: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0980 - acc: 0.9704 - val_loss: 0.1582 - val_acc: 0.9574\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9740\n",
      "Epoch 00129: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0864 - acc: 0.9740 - val_loss: 0.1533 - val_acc: 0.9588\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9729\n",
      "Epoch 00130: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0866 - acc: 0.9729 - val_loss: 0.1440 - val_acc: 0.9583\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9741\n",
      "Epoch 00131: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0845 - acc: 0.9741 - val_loss: 0.1593 - val_acc: 0.9534\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9754\n",
      "Epoch 00132: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0811 - acc: 0.9754 - val_loss: 0.1600 - val_acc: 0.9585\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9719\n",
      "Epoch 00133: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0914 - acc: 0.9719 - val_loss: 0.1438 - val_acc: 0.9595\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9754\n",
      "Epoch 00134: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0816 - acc: 0.9754 - val_loss: 0.1591 - val_acc: 0.9562\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9748\n",
      "Epoch 00135: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0817 - acc: 0.9748 - val_loss: 0.1486 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9736\n",
      "Epoch 00136: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0856 - acc: 0.9736 - val_loss: 0.1467 - val_acc: 0.9592\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9738\n",
      "Epoch 00137: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0834 - acc: 0.9738 - val_loss: 0.1615 - val_acc: 0.9557\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9746\n",
      "Epoch 00138: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0829 - acc: 0.9745 - val_loss: 0.1494 - val_acc: 0.9581\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9744\n",
      "Epoch 00139: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 0.1458 - val_acc: 0.9590\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9760\n",
      "Epoch 00140: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0765 - acc: 0.9759 - val_loss: 0.1944 - val_acc: 0.9497\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9734\n",
      "Epoch 00141: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0866 - acc: 0.9734 - val_loss: 0.1441 - val_acc: 0.9625\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9764\n",
      "Epoch 00142: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0778 - acc: 0.9764 - val_loss: 0.1671 - val_acc: 0.9529\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9762\n",
      "Epoch 00143: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0760 - acc: 0.9762 - val_loss: 0.1572 - val_acc: 0.9604\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9772\n",
      "Epoch 00144: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0755 - acc: 0.9772 - val_loss: 0.1662 - val_acc: 0.9569\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9766\n",
      "Epoch 00145: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0755 - acc: 0.9766 - val_loss: 0.1595 - val_acc: 0.9574\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9737\n",
      "Epoch 00146: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0855 - acc: 0.9737 - val_loss: 0.1602 - val_acc: 0.9592\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9784\n",
      "Epoch 00147: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0710 - acc: 0.9784 - val_loss: 0.1749 - val_acc: 0.9550\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9783\n",
      "Epoch 00148: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0720 - acc: 0.9782 - val_loss: 0.1592 - val_acc: 0.9620\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9762\n",
      "Epoch 00149: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0775 - acc: 0.9762 - val_loss: 0.1653 - val_acc: 0.9536\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9756\n",
      "Epoch 00150: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0792 - acc: 0.9755 - val_loss: 0.1563 - val_acc: 0.9611\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9776\n",
      "Epoch 00151: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0722 - acc: 0.9776 - val_loss: 0.1550 - val_acc: 0.9574\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9775\n",
      "Epoch 00152: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0728 - acc: 0.9775 - val_loss: 0.1837 - val_acc: 0.9499\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9756\n",
      "Epoch 00153: val_loss did not improve from 0.13959\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0784 - acc: 0.9756 - val_loss: 0.1558 - val_acc: 0.9560\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 456us/sample - loss: 0.1874 - acc: 0.9448\n",
      "Loss: 0.1874421004689495 Accuracy: 0.944756\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2440 - acc: 0.2992\n",
      "Epoch 00001: val_loss improved from inf to 1.65167, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/001-1.6517.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.2440 - acc: 0.2992 - val_loss: 1.6517 - val_acc: 0.5530\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.5701\n",
      "Epoch 00002: val_loss improved from 1.65167 to 0.85174, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/002-0.8517.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 1.3771 - acc: 0.5701 - val_loss: 0.8517 - val_acc: 0.7771\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9918 - acc: 0.7048\n",
      "Epoch 00003: val_loss improved from 0.85174 to 0.59132, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/003-0.5913.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.9920 - acc: 0.7048 - val_loss: 0.5913 - val_acc: 0.8477\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7782 - acc: 0.7734\n",
      "Epoch 00004: val_loss improved from 0.59132 to 0.45701, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/004-0.4570.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.7782 - acc: 0.7734 - val_loss: 0.4570 - val_acc: 0.8859\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6519 - acc: 0.8119\n",
      "Epoch 00005: val_loss improved from 0.45701 to 0.41838, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/005-0.4184.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.6520 - acc: 0.8119 - val_loss: 0.4184 - val_acc: 0.8926\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.8388\n",
      "Epoch 00006: val_loss improved from 0.41838 to 0.36599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/006-0.3660.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.5604 - acc: 0.8388 - val_loss: 0.3660 - val_acc: 0.9026\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8583\n",
      "Epoch 00007: val_loss improved from 0.36599 to 0.31636, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/007-0.3164.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.4972 - acc: 0.8583 - val_loss: 0.3164 - val_acc: 0.9206\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8725\n",
      "Epoch 00008: val_loss improved from 0.31636 to 0.27768, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/008-0.2777.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.4492 - acc: 0.8725 - val_loss: 0.2777 - val_acc: 0.9273\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8837\n",
      "Epoch 00009: val_loss improved from 0.27768 to 0.24952, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/009-0.2495.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.4072 - acc: 0.8837 - val_loss: 0.2495 - val_acc: 0.9324\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8907\n",
      "Epoch 00010: val_loss improved from 0.24952 to 0.23217, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/010-0.2322.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.3822 - acc: 0.8906 - val_loss: 0.2322 - val_acc: 0.9376\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8986\n",
      "Epoch 00011: val_loss improved from 0.23217 to 0.22814, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/011-0.2281.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.3524 - acc: 0.8985 - val_loss: 0.2281 - val_acc: 0.9371\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9048\n",
      "Epoch 00012: val_loss improved from 0.22814 to 0.22361, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/012-0.2236.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.3303 - acc: 0.9048 - val_loss: 0.2236 - val_acc: 0.9373\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9114\n",
      "Epoch 00013: val_loss improved from 0.22361 to 0.20055, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/013-0.2005.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3098 - acc: 0.9113 - val_loss: 0.2005 - val_acc: 0.9441\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9148\n",
      "Epoch 00014: val_loss improved from 0.20055 to 0.19793, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/014-0.1979.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2947 - acc: 0.9148 - val_loss: 0.1979 - val_acc: 0.9408\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9210\n",
      "Epoch 00015: val_loss improved from 0.19793 to 0.19313, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/015-0.1931.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2766 - acc: 0.9209 - val_loss: 0.1931 - val_acc: 0.9446\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9207\n",
      "Epoch 00016: val_loss did not improve from 0.19313\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2685 - acc: 0.9207 - val_loss: 0.2054 - val_acc: 0.9439\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9260\n",
      "Epoch 00017: val_loss improved from 0.19313 to 0.18445, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/017-0.1845.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.2546 - acc: 0.9259 - val_loss: 0.1845 - val_acc: 0.9474\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9292\n",
      "Epoch 00018: val_loss improved from 0.18445 to 0.17518, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/018-0.1752.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2429 - acc: 0.9292 - val_loss: 0.1752 - val_acc: 0.9502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9324\n",
      "Epoch 00019: val_loss did not improve from 0.17518\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2306 - acc: 0.9324 - val_loss: 0.1985 - val_acc: 0.9418\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9337\n",
      "Epoch 00020: val_loss improved from 0.17518 to 0.15927, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/020-0.1593.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2231 - acc: 0.9337 - val_loss: 0.1593 - val_acc: 0.9536\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9362\n",
      "Epoch 00021: val_loss did not improve from 0.15927\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2148 - acc: 0.9363 - val_loss: 0.1599 - val_acc: 0.9546\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9386\n",
      "Epoch 00022: val_loss did not improve from 0.15927\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.2069 - acc: 0.9386 - val_loss: 0.2069 - val_acc: 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9421\n",
      "Epoch 00023: val_loss did not improve from 0.15927\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1982 - acc: 0.9422 - val_loss: 0.1613 - val_acc: 0.9518\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9432\n",
      "Epoch 00024: val_loss improved from 0.15927 to 0.15333, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/024-0.1533.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1924 - acc: 0.9432 - val_loss: 0.1533 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9435\n",
      "Epoch 00025: val_loss did not improve from 0.15333\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1861 - acc: 0.9434 - val_loss: 0.1702 - val_acc: 0.9481\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9465\n",
      "Epoch 00026: val_loss improved from 0.15333 to 0.14788, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/026-0.1479.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1793 - acc: 0.9465 - val_loss: 0.1479 - val_acc: 0.9576\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9483\n",
      "Epoch 00027: val_loss did not improve from 0.14788\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1731 - acc: 0.9482 - val_loss: 0.1523 - val_acc: 0.9548\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9494\n",
      "Epoch 00028: val_loss did not improve from 0.14788\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1711 - acc: 0.9494 - val_loss: 0.1791 - val_acc: 0.9492\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9512\n",
      "Epoch 00029: val_loss did not improve from 0.14788\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1639 - acc: 0.9512 - val_loss: 0.1641 - val_acc: 0.9490\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9530\n",
      "Epoch 00030: val_loss did not improve from 0.14788\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1567 - acc: 0.9530 - val_loss: 0.1513 - val_acc: 0.9536\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9530\n",
      "Epoch 00031: val_loss did not improve from 0.14788\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1553 - acc: 0.9530 - val_loss: 0.1727 - val_acc: 0.9509\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9566\n",
      "Epoch 00032: val_loss improved from 0.14788 to 0.14078, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/032-0.1408.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1477 - acc: 0.9566 - val_loss: 0.1408 - val_acc: 0.9557\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9560\n",
      "Epoch 00033: val_loss did not improve from 0.14078\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1484 - acc: 0.9560 - val_loss: 0.1450 - val_acc: 0.9585\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9567\n",
      "Epoch 00034: val_loss did not improve from 0.14078\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1432 - acc: 0.9567 - val_loss: 0.1497 - val_acc: 0.9560\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9593\n",
      "Epoch 00035: val_loss did not improve from 0.14078\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1365 - acc: 0.9593 - val_loss: 0.1568 - val_acc: 0.9557\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9600\n",
      "Epoch 00036: val_loss improved from 0.14078 to 0.13399, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/036-0.1340.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1331 - acc: 0.9600 - val_loss: 0.1340 - val_acc: 0.9618\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9607\n",
      "Epoch 00037: val_loss did not improve from 0.13399\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1302 - acc: 0.9607 - val_loss: 0.1643 - val_acc: 0.9543\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9601\n",
      "Epoch 00038: val_loss did not improve from 0.13399\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1306 - acc: 0.9601 - val_loss: 0.1366 - val_acc: 0.9588\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9627\n",
      "Epoch 00039: val_loss did not improve from 0.13399\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1221 - acc: 0.9627 - val_loss: 0.1410 - val_acc: 0.9583\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9643\n",
      "Epoch 00040: val_loss did not improve from 0.13399\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1194 - acc: 0.9642 - val_loss: 0.1453 - val_acc: 0.9562\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9635\n",
      "Epoch 00041: val_loss improved from 0.13399 to 0.13279, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/041-0.1328.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1193 - acc: 0.9635 - val_loss: 0.1328 - val_acc: 0.9604\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9652\n",
      "Epoch 00042: val_loss improved from 0.13279 to 0.12906, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/042-0.1291.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1143 - acc: 0.9652 - val_loss: 0.1291 - val_acc: 0.9616\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9640\n",
      "Epoch 00043: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1153 - acc: 0.9639 - val_loss: 0.1448 - val_acc: 0.9562\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9663\n",
      "Epoch 00044: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1089 - acc: 0.9663 - val_loss: 0.1462 - val_acc: 0.9571\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9670\n",
      "Epoch 00045: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1070 - acc: 0.9670 - val_loss: 0.1425 - val_acc: 0.9592\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9693\n",
      "Epoch 00046: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1003 - acc: 0.9693 - val_loss: 0.1297 - val_acc: 0.9616\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9690\n",
      "Epoch 00047: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1044 - acc: 0.9691 - val_loss: 0.1315 - val_acc: 0.9616\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9713\n",
      "Epoch 00048: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0960 - acc: 0.9713 - val_loss: 0.1457 - val_acc: 0.9595\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9703\n",
      "Epoch 00049: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0979 - acc: 0.9702 - val_loss: 0.1444 - val_acc: 0.9585\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9711\n",
      "Epoch 00050: val_loss did not improve from 0.12906\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0956 - acc: 0.9711 - val_loss: 0.1343 - val_acc: 0.9625\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9711\n",
      "Epoch 00051: val_loss improved from 0.12906 to 0.12731, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv_checkpoint/051-0.1273.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0944 - acc: 0.9711 - val_loss: 0.1273 - val_acc: 0.9637\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9700\n",
      "Epoch 00052: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.0960 - acc: 0.9700 - val_loss: 0.1476 - val_acc: 0.9585\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9727\n",
      "Epoch 00053: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0888 - acc: 0.9727 - val_loss: 0.1442 - val_acc: 0.9564\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9744\n",
      "Epoch 00054: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0859 - acc: 0.9744 - val_loss: 0.1287 - val_acc: 0.9618\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9750\n",
      "Epoch 00055: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0831 - acc: 0.9750 - val_loss: 0.1630 - val_acc: 0.9504\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9754\n",
      "Epoch 00056: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0808 - acc: 0.9754 - val_loss: 0.1540 - val_acc: 0.9567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9738\n",
      "Epoch 00057: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0855 - acc: 0.9738 - val_loss: 0.1702 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9709\n",
      "Epoch 00058: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0957 - acc: 0.9709 - val_loss: 0.1413 - val_acc: 0.9604\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9768\n",
      "Epoch 00059: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0766 - acc: 0.9768 - val_loss: 0.1465 - val_acc: 0.9588\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9769\n",
      "Epoch 00060: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0758 - acc: 0.9769 - val_loss: 0.1549 - val_acc: 0.9555\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9771\n",
      "Epoch 00061: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0748 - acc: 0.9771 - val_loss: 0.1337 - val_acc: 0.9609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9778\n",
      "Epoch 00062: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0722 - acc: 0.9778 - val_loss: 0.1695 - val_acc: 0.9550\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9789\n",
      "Epoch 00063: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0723 - acc: 0.9789 - val_loss: 0.1384 - val_acc: 0.9655\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9794\n",
      "Epoch 00064: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0688 - acc: 0.9794 - val_loss: 0.1395 - val_acc: 0.9592\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9779\n",
      "Epoch 00065: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0729 - acc: 0.9778 - val_loss: 0.1471 - val_acc: 0.9604\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9759\n",
      "Epoch 00066: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0771 - acc: 0.9759 - val_loss: 0.1659 - val_acc: 0.9541\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9803\n",
      "Epoch 00067: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0653 - acc: 0.9803 - val_loss: 0.1734 - val_acc: 0.9539\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9807\n",
      "Epoch 00068: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0660 - acc: 0.9807 - val_loss: 0.1414 - val_acc: 0.9604\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9789\n",
      "Epoch 00069: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0672 - acc: 0.9789 - val_loss: 0.1660 - val_acc: 0.9588\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9807\n",
      "Epoch 00070: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0632 - acc: 0.9806 - val_loss: 0.1446 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9785\n",
      "Epoch 00071: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0694 - acc: 0.9785 - val_loss: 0.1396 - val_acc: 0.9604\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9824\n",
      "Epoch 00072: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0602 - acc: 0.9824 - val_loss: 0.2210 - val_acc: 0.9394\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9789\n",
      "Epoch 00073: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0694 - acc: 0.9788 - val_loss: 0.1785 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9821\n",
      "Epoch 00074: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0607 - acc: 0.9820 - val_loss: 0.1353 - val_acc: 0.9662\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9826\n",
      "Epoch 00075: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0600 - acc: 0.9826 - val_loss: 0.1352 - val_acc: 0.9637\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9818\n",
      "Epoch 00076: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0609 - acc: 0.9817 - val_loss: 0.1538 - val_acc: 0.9606\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9822\n",
      "Epoch 00077: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0575 - acc: 0.9822 - val_loss: 0.1410 - val_acc: 0.9611\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9835\n",
      "Epoch 00078: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0547 - acc: 0.9835 - val_loss: 0.1551 - val_acc: 0.9583\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9824\n",
      "Epoch 00079: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0568 - acc: 0.9824 - val_loss: 0.1447 - val_acc: 0.9632\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9822\n",
      "Epoch 00080: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0590 - acc: 0.9822 - val_loss: 0.1697 - val_acc: 0.9557\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9849\n",
      "Epoch 00081: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0519 - acc: 0.9849 - val_loss: 0.1303 - val_acc: 0.9651\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9813\n",
      "Epoch 00082: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0603 - acc: 0.9813 - val_loss: 0.1370 - val_acc: 0.9623\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9841\n",
      "Epoch 00083: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0514 - acc: 0.9841 - val_loss: 0.1514 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9849\n",
      "Epoch 00084: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0482 - acc: 0.9849 - val_loss: 0.1732 - val_acc: 0.9525\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9851\n",
      "Epoch 00085: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0488 - acc: 0.9851 - val_loss: 0.1418 - val_acc: 0.9634\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9855\n",
      "Epoch 00086: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0486 - acc: 0.9855 - val_loss: 0.1592 - val_acc: 0.9583\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9851\n",
      "Epoch 00087: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0481 - acc: 0.9851 - val_loss: 0.1583 - val_acc: 0.9585\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 00088: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0503 - acc: 0.9848 - val_loss: 0.1614 - val_acc: 0.9592\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9851\n",
      "Epoch 00089: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0486 - acc: 0.9851 - val_loss: 0.1526 - val_acc: 0.9637\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9861\n",
      "Epoch 00090: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0448 - acc: 0.9861 - val_loss: 0.1557 - val_acc: 0.9620\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9868\n",
      "Epoch 00091: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0432 - acc: 0.9868 - val_loss: 0.1846 - val_acc: 0.9527\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9826\n",
      "Epoch 00092: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0545 - acc: 0.9826 - val_loss: 0.1400 - val_acc: 0.9639\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9871\n",
      "Epoch 00093: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0415 - acc: 0.9871 - val_loss: 0.1545 - val_acc: 0.9595\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9881\n",
      "Epoch 00094: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0406 - acc: 0.9881 - val_loss: 0.1867 - val_acc: 0.9567\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9850\n",
      "Epoch 00095: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0502 - acc: 0.9850 - val_loss: 0.1490 - val_acc: 0.9646\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9882\n",
      "Epoch 00096: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0393 - acc: 0.9882 - val_loss: 0.1620 - val_acc: 0.9616\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9877\n",
      "Epoch 00097: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0406 - acc: 0.9877 - val_loss: 0.1450 - val_acc: 0.9646\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9872\n",
      "Epoch 00098: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0427 - acc: 0.9872 - val_loss: 0.1539 - val_acc: 0.9618\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9881\n",
      "Epoch 00099: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0389 - acc: 0.9880 - val_loss: 0.1536 - val_acc: 0.9641\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9864\n",
      "Epoch 00100: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0430 - acc: 0.9864 - val_loss: 0.1516 - val_acc: 0.9648\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9888\n",
      "Epoch 00101: val_loss did not improve from 0.12731\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0384 - acc: 0.9888 - val_loss: 0.1678 - val_acc: 0.9609\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 482us/sample - loss: 0.1820 - acc: 0.9477\n",
      "Loss: 0.1819972254405512 Accuracy: 0.94766355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GAP_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 96)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,432\n",
      "Trainable params: 12,240\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 488us/sample - loss: 0.8598 - acc: 0.7466\n",
      "Loss: 0.8598382489455947 Accuracy: 0.7466251\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 96)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,712\n",
      "Trainable params: 17,456\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 519us/sample - loss: 0.6458 - acc: 0.8083\n",
      "Loss: 0.6457661219227475 Accuracy: 0.80830735\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,784\n",
      "Trainable params: 28,400\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 502us/sample - loss: 0.3943 - acc: 0.8843\n",
      "Loss: 0.39425390006844746 Accuracy: 0.88431984\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 160)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,584\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 520us/sample - loss: 0.2416 - acc: 0.9269\n",
      "Loss: 0.24161738423419765 Accuracy: 0.92689514\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 192)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 71,408\n",
      "Trainable params: 70,768\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 557us/sample - loss: 0.1874 - acc: 0.9448\n",
      "Loss: 0.1874421004689495 Accuracy: 0.944756\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 192)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 92,208\n",
      "Trainable params: 91,440\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 560us/sample - loss: 0.1820 - acc: 0.9477\n",
      "Loss: 0.1819972254405512 Accuracy: 0.94766355\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GAP_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 96)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,432\n",
      "Trainable params: 12,240\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 564us/sample - loss: 0.9664 - acc: 0.7026\n",
      "Loss: 0.9663750133658 Accuracy: 0.70259607\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 96)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,712\n",
      "Trainable params: 17,456\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 619us/sample - loss: 0.8193 - acc: 0.7466\n",
      "Loss: 0.8192981995649922 Accuracy: 0.7466251\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,784\n",
      "Trainable params: 28,400\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 580us/sample - loss: 0.4330 - acc: 0.8754\n",
      "Loss: 0.4330364235465029 Accuracy: 0.8753894\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 160)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,584\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 613us/sample - loss: 0.2766 - acc: 0.9184\n",
      "Loss: 0.27655456238199 Accuracy: 0.9183801\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 192)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 71,408\n",
      "Trainable params: 70,768\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 660us/sample - loss: 0.2063 - acc: 0.9458\n",
      "Loss: 0.20628099756263127 Accuracy: 0.9457944\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 192)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 92,208\n",
      "Trainable params: 91,440\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 693us/sample - loss: 0.2272 - acc: 0.9431\n",
      "Loss: 0.22719424288228604 Accuracy: 0.9430945\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
