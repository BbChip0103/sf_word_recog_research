{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 192)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           3088        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5774 - acc: 0.1011\n",
      "Epoch 00001: val_loss improved from inf to 2.45325, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/001-2.4532.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 7.5771 - acc: 0.1011 - val_loss: 2.4532 - val_acc: 0.2259\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.7147 - acc: 0.1234\n",
      "Epoch 00002: val_loss improved from 2.45325 to 2.24836, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/002-2.2484.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 4.7146 - acc: 0.1234 - val_loss: 2.2484 - val_acc: 0.2919\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7790 - acc: 0.1390\n",
      "Epoch 00003: val_loss improved from 2.24836 to 2.08582, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/003-2.0858.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 3.7788 - acc: 0.1390 - val_loss: 2.0858 - val_acc: 0.3666\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2855 - acc: 0.1566\n",
      "Epoch 00004: val_loss improved from 2.08582 to 1.98318, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/004-1.9832.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 3.2855 - acc: 0.1566 - val_loss: 1.9832 - val_acc: 0.4128\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8941 - acc: 0.1877\n",
      "Epoch 00005: val_loss improved from 1.98318 to 1.89067, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/005-1.8907.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.8941 - acc: 0.1877 - val_loss: 1.8907 - val_acc: 0.4407\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6315 - acc: 0.2149\n",
      "Epoch 00006: val_loss improved from 1.89067 to 1.81163, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/006-1.8116.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.6316 - acc: 0.2149 - val_loss: 1.8116 - val_acc: 0.4656\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4234 - acc: 0.2364\n",
      "Epoch 00007: val_loss improved from 1.81163 to 1.73618, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/007-1.7362.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.4235 - acc: 0.2364 - val_loss: 1.7362 - val_acc: 0.4941\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2572 - acc: 0.2677\n",
      "Epoch 00008: val_loss improved from 1.73618 to 1.66488, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/008-1.6649.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.2572 - acc: 0.2677 - val_loss: 1.6649 - val_acc: 0.5213\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1193 - acc: 0.3015\n",
      "Epoch 00009: val_loss improved from 1.66488 to 1.58492, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/009-1.5849.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.1192 - acc: 0.3015 - val_loss: 1.5849 - val_acc: 0.5535\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0185 - acc: 0.3299\n",
      "Epoch 00010: val_loss improved from 1.58492 to 1.53010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/010-1.5301.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.0187 - acc: 0.3298 - val_loss: 1.5301 - val_acc: 0.5712\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9388 - acc: 0.3502\n",
      "Epoch 00011: val_loss improved from 1.53010 to 1.46407, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/011-1.4641.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.9387 - acc: 0.3503 - val_loss: 1.4641 - val_acc: 0.5800\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8395 - acc: 0.3864\n",
      "Epoch 00012: val_loss improved from 1.46407 to 1.39295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/012-1.3929.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.8396 - acc: 0.3864 - val_loss: 1.3929 - val_acc: 0.6056\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7778 - acc: 0.4076\n",
      "Epoch 00013: val_loss improved from 1.39295 to 1.34079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/013-1.3408.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.7778 - acc: 0.4076 - val_loss: 1.3408 - val_acc: 0.6301\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7059 - acc: 0.4329\n",
      "Epoch 00014: val_loss improved from 1.34079 to 1.28693, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/014-1.2869.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.7058 - acc: 0.4329 - val_loss: 1.2869 - val_acc: 0.6355\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6497 - acc: 0.4529\n",
      "Epoch 00015: val_loss improved from 1.28693 to 1.26538, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/015-1.2654.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.6498 - acc: 0.4529 - val_loss: 1.2654 - val_acc: 0.6536\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6046 - acc: 0.4676\n",
      "Epoch 00016: val_loss improved from 1.26538 to 1.20443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/016-1.2044.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.6046 - acc: 0.4676 - val_loss: 1.2044 - val_acc: 0.6611\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5612 - acc: 0.4879\n",
      "Epoch 00017: val_loss improved from 1.20443 to 1.16444, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/017-1.1644.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5611 - acc: 0.4879 - val_loss: 1.1644 - val_acc: 0.6711\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5188 - acc: 0.5011\n",
      "Epoch 00018: val_loss improved from 1.16444 to 1.12472, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/018-1.1247.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5189 - acc: 0.5010 - val_loss: 1.1247 - val_acc: 0.6876\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4819 - acc: 0.5147\n",
      "Epoch 00019: val_loss improved from 1.12472 to 1.09514, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/019-1.0951.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4819 - acc: 0.5147 - val_loss: 1.0951 - val_acc: 0.6760\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4410 - acc: 0.5282\n",
      "Epoch 00020: val_loss improved from 1.09514 to 1.06283, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/020-1.0628.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4410 - acc: 0.5282 - val_loss: 1.0628 - val_acc: 0.7009\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4106 - acc: 0.5392\n",
      "Epoch 00021: val_loss improved from 1.06283 to 1.05496, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/021-1.0550.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4105 - acc: 0.5392 - val_loss: 1.0550 - val_acc: 0.7009\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3760 - acc: 0.5512\n",
      "Epoch 00022: val_loss improved from 1.05496 to 1.02702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/022-1.0270.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3760 - acc: 0.5511 - val_loss: 1.0270 - val_acc: 0.7135\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3488 - acc: 0.5626\n",
      "Epoch 00023: val_loss improved from 1.02702 to 1.01071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/023-1.0107.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3488 - acc: 0.5626 - val_loss: 1.0107 - val_acc: 0.7167\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3248 - acc: 0.5695\n",
      "Epoch 00024: val_loss did not improve from 1.01071\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3246 - acc: 0.5695 - val_loss: 1.0506 - val_acc: 0.6914\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3015 - acc: 0.5772\n",
      "Epoch 00025: val_loss improved from 1.01071 to 0.96439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/025-0.9644.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3016 - acc: 0.5772 - val_loss: 0.9644 - val_acc: 0.7261\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2812 - acc: 0.5867\n",
      "Epoch 00026: val_loss did not improve from 0.96439\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2812 - acc: 0.5867 - val_loss: 0.9759 - val_acc: 0.7261\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2644 - acc: 0.5923\n",
      "Epoch 00027: val_loss improved from 0.96439 to 0.92645, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/027-0.9265.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2643 - acc: 0.5924 - val_loss: 0.9265 - val_acc: 0.7424\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2404 - acc: 0.5982\n",
      "Epoch 00028: val_loss improved from 0.92645 to 0.90821, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/028-0.9082.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2404 - acc: 0.5982 - val_loss: 0.9082 - val_acc: 0.7501\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2320 - acc: 0.6046\n",
      "Epoch 00029: val_loss did not improve from 0.90821\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2320 - acc: 0.6046 - val_loss: 0.9205 - val_acc: 0.7475\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.6089\n",
      "Epoch 00030: val_loss improved from 0.90821 to 0.89979, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/030-0.8998.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2194 - acc: 0.6089 - val_loss: 0.8998 - val_acc: 0.7503\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2093 - acc: 0.6115\n",
      "Epoch 00031: val_loss improved from 0.89979 to 0.89207, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/031-0.8921.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2094 - acc: 0.6115 - val_loss: 0.8921 - val_acc: 0.7440\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1851 - acc: 0.6217\n",
      "Epoch 00032: val_loss improved from 0.89207 to 0.88835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/032-0.8883.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1851 - acc: 0.6217 - val_loss: 0.8883 - val_acc: 0.7508\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1719 - acc: 0.6226\n",
      "Epoch 00033: val_loss did not improve from 0.88835\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1720 - acc: 0.6226 - val_loss: 0.9334 - val_acc: 0.7407\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1637 - acc: 0.6270\n",
      "Epoch 00034: val_loss improved from 0.88835 to 0.85085, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/034-0.8509.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1636 - acc: 0.6271 - val_loss: 0.8509 - val_acc: 0.7608\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1573 - acc: 0.6313\n",
      "Epoch 00035: val_loss improved from 0.85085 to 0.84421, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/035-0.8442.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1573 - acc: 0.6313 - val_loss: 0.8442 - val_acc: 0.7622\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1390 - acc: 0.6331\n",
      "Epoch 00036: val_loss improved from 0.84421 to 0.83817, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/036-0.8382.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1392 - acc: 0.6331 - val_loss: 0.8382 - val_acc: 0.7603\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1334 - acc: 0.6379\n",
      "Epoch 00037: val_loss improved from 0.83817 to 0.82923, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/037-0.8292.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1334 - acc: 0.6379 - val_loss: 0.8292 - val_acc: 0.7720\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1226 - acc: 0.6422\n",
      "Epoch 00038: val_loss improved from 0.82923 to 0.81161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/038-0.8116.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1225 - acc: 0.6422 - val_loss: 0.8116 - val_acc: 0.7645\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6412\n",
      "Epoch 00039: val_loss did not improve from 0.81161\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1168 - acc: 0.6412 - val_loss: 0.8117 - val_acc: 0.7692\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1188 - acc: 0.6415\n",
      "Epoch 00040: val_loss improved from 0.81161 to 0.80779, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/040-0.8078.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1188 - acc: 0.6415 - val_loss: 0.8078 - val_acc: 0.7678\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1012 - acc: 0.6473\n",
      "Epoch 00041: val_loss improved from 0.80779 to 0.80169, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/041-0.8017.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1012 - acc: 0.6472 - val_loss: 0.8017 - val_acc: 0.7782\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0991 - acc: 0.6490\n",
      "Epoch 00042: val_loss did not improve from 0.80169\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0992 - acc: 0.6490 - val_loss: 0.8035 - val_acc: 0.7713\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0897 - acc: 0.6515\n",
      "Epoch 00043: val_loss improved from 0.80169 to 0.79083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/043-0.7908.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0896 - acc: 0.6516 - val_loss: 0.7908 - val_acc: 0.7734\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0810 - acc: 0.6561\n",
      "Epoch 00044: val_loss improved from 0.79083 to 0.77715, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/044-0.7771.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0810 - acc: 0.6561 - val_loss: 0.7771 - val_acc: 0.7782\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0774 - acc: 0.6551\n",
      "Epoch 00045: val_loss did not improve from 0.77715\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0774 - acc: 0.6551 - val_loss: 0.7894 - val_acc: 0.7720\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6595\n",
      "Epoch 00046: val_loss did not improve from 0.77715\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0699 - acc: 0.6595 - val_loss: 0.8101 - val_acc: 0.7734\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.6611\n",
      "Epoch 00047: val_loss improved from 0.77715 to 0.76435, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/047-0.7643.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0657 - acc: 0.6611 - val_loss: 0.7643 - val_acc: 0.7834\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.6596\n",
      "Epoch 00048: val_loss did not improve from 0.76435\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0678 - acc: 0.6596 - val_loss: 0.7674 - val_acc: 0.7768\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0556 - acc: 0.6628\n",
      "Epoch 00049: val_loss did not improve from 0.76435\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0556 - acc: 0.6628 - val_loss: 0.7881 - val_acc: 0.7843\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0399 - acc: 0.6682\n",
      "Epoch 00050: val_loss did not improve from 0.76435\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0400 - acc: 0.6681 - val_loss: 0.7719 - val_acc: 0.7771\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0481 - acc: 0.6688\n",
      "Epoch 00051: val_loss improved from 0.76435 to 0.74243, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/051-0.7424.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0482 - acc: 0.6688 - val_loss: 0.7424 - val_acc: 0.7883\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0411 - acc: 0.6692\n",
      "Epoch 00052: val_loss improved from 0.74243 to 0.73661, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/052-0.7366.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0411 - acc: 0.6692 - val_loss: 0.7366 - val_acc: 0.7880\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0340 - acc: 0.6721\n",
      "Epoch 00053: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0340 - acc: 0.6721 - val_loss: 0.7375 - val_acc: 0.7878\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0325 - acc: 0.6700\n",
      "Epoch 00054: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0325 - acc: 0.6700 - val_loss: 0.7663 - val_acc: 0.7841\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0290 - acc: 0.6708\n",
      "Epoch 00055: val_loss improved from 0.73661 to 0.73204, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/055-0.7320.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0289 - acc: 0.6708 - val_loss: 0.7320 - val_acc: 0.7943\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0213 - acc: 0.6736\n",
      "Epoch 00056: val_loss improved from 0.73204 to 0.72881, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/056-0.7288.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0214 - acc: 0.6736 - val_loss: 0.7288 - val_acc: 0.7945\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0223 - acc: 0.6761\n",
      "Epoch 00057: val_loss did not improve from 0.72881\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0224 - acc: 0.6761 - val_loss: 0.7385 - val_acc: 0.7957\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0157 - acc: 0.6780\n",
      "Epoch 00058: val_loss improved from 0.72881 to 0.72105, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/058-0.7210.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0157 - acc: 0.6779 - val_loss: 0.7210 - val_acc: 0.7952\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0057 - acc: 0.6834\n",
      "Epoch 00059: val_loss did not improve from 0.72105\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0057 - acc: 0.6834 - val_loss: 0.7471 - val_acc: 0.7824\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0036 - acc: 0.6770\n",
      "Epoch 00060: val_loss did not improve from 0.72105\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0037 - acc: 0.6770 - val_loss: 0.7301 - val_acc: 0.7897\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0010 - acc: 0.6811\n",
      "Epoch 00061: val_loss improved from 0.72105 to 0.71230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/061-0.7123.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0010 - acc: 0.6811 - val_loss: 0.7123 - val_acc: 0.7987\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9980 - acc: 0.6845\n",
      "Epoch 00062: val_loss improved from 0.71230 to 0.70403, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/062-0.7040.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9982 - acc: 0.6845 - val_loss: 0.7040 - val_acc: 0.7994\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9952 - acc: 0.6841\n",
      "Epoch 00063: val_loss did not improve from 0.70403\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9951 - acc: 0.6841 - val_loss: 0.7097 - val_acc: 0.7929\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9916 - acc: 0.6861\n",
      "Epoch 00064: val_loss did not improve from 0.70403\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9916 - acc: 0.6861 - val_loss: 0.7251 - val_acc: 0.7964\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9850 - acc: 0.6875\n",
      "Epoch 00065: val_loss improved from 0.70403 to 0.67869, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/065-0.6787.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9849 - acc: 0.6875 - val_loss: 0.6787 - val_acc: 0.8069\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9876 - acc: 0.6846\n",
      "Epoch 00066: val_loss did not improve from 0.67869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9875 - acc: 0.6846 - val_loss: 0.7404 - val_acc: 0.7873\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.6876\n",
      "Epoch 00067: val_loss did not improve from 0.67869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9845 - acc: 0.6876 - val_loss: 0.6912 - val_acc: 0.7985\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9868 - acc: 0.6866\n",
      "Epoch 00068: val_loss did not improve from 0.67869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9868 - acc: 0.6866 - val_loss: 0.7019 - val_acc: 0.7969\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9776 - acc: 0.6885\n",
      "Epoch 00069: val_loss did not improve from 0.67869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9777 - acc: 0.6885 - val_loss: 0.6881 - val_acc: 0.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9722 - acc: 0.6920\n",
      "Epoch 00070: val_loss did not improve from 0.67869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9722 - acc: 0.6920 - val_loss: 0.7211 - val_acc: 0.7878\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9704 - acc: 0.6913\n",
      "Epoch 00071: val_loss improved from 0.67869 to 0.66991, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/071-0.6699.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9704 - acc: 0.6914 - val_loss: 0.6699 - val_acc: 0.8078\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.6942\n",
      "Epoch 00072: val_loss did not improve from 0.66991\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9617 - acc: 0.6942 - val_loss: 0.6850 - val_acc: 0.8004\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9618 - acc: 0.6947\n",
      "Epoch 00073: val_loss did not improve from 0.66991\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9617 - acc: 0.6947 - val_loss: 0.6925 - val_acc: 0.8015\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9599 - acc: 0.6936\n",
      "Epoch 00074: val_loss did not improve from 0.66991\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9598 - acc: 0.6937 - val_loss: 0.7200 - val_acc: 0.7927\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9583 - acc: 0.6963\n",
      "Epoch 00075: val_loss improved from 0.66991 to 0.66371, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/075-0.6637.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9582 - acc: 0.6963 - val_loss: 0.6637 - val_acc: 0.8022\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9528 - acc: 0.6982\n",
      "Epoch 00076: val_loss did not improve from 0.66371\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9528 - acc: 0.6982 - val_loss: 0.7069 - val_acc: 0.7913\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9506 - acc: 0.6986\n",
      "Epoch 00077: val_loss improved from 0.66371 to 0.65589, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/077-0.6559.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9506 - acc: 0.6986 - val_loss: 0.6559 - val_acc: 0.8111\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9556 - acc: 0.6943\n",
      "Epoch 00078: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9556 - acc: 0.6944 - val_loss: 0.6904 - val_acc: 0.8015\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.7017\n",
      "Epoch 00079: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9392 - acc: 0.7016 - val_loss: 0.6942 - val_acc: 0.7976\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9436 - acc: 0.7026\n",
      "Epoch 00080: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9436 - acc: 0.7025 - val_loss: 0.6590 - val_acc: 0.8092\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9376 - acc: 0.7043\n",
      "Epoch 00081: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9377 - acc: 0.7043 - val_loss: 0.6643 - val_acc: 0.8088\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9438 - acc: 0.6991\n",
      "Epoch 00082: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9438 - acc: 0.6991 - val_loss: 0.6598 - val_acc: 0.8174\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7038\n",
      "Epoch 00083: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9355 - acc: 0.7038 - val_loss: 0.6779 - val_acc: 0.7997\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9414 - acc: 0.7025\n",
      "Epoch 00084: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9414 - acc: 0.7025 - val_loss: 0.7350 - val_acc: 0.7850\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9393 - acc: 0.7032\n",
      "Epoch 00085: val_loss did not improve from 0.65589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9393 - acc: 0.7032 - val_loss: 0.6782 - val_acc: 0.8055\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9311 - acc: 0.7047\n",
      "Epoch 00086: val_loss improved from 0.65589 to 0.64618, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/086-0.6462.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9312 - acc: 0.7047 - val_loss: 0.6462 - val_acc: 0.8176\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9292 - acc: 0.7056\n",
      "Epoch 00087: val_loss did not improve from 0.64618\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9292 - acc: 0.7056 - val_loss: 0.6706 - val_acc: 0.8088\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9235 - acc: 0.7077\n",
      "Epoch 00088: val_loss did not improve from 0.64618\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9235 - acc: 0.7077 - val_loss: 0.6645 - val_acc: 0.8106\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9246 - acc: 0.7047\n",
      "Epoch 00089: val_loss did not improve from 0.64618\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9247 - acc: 0.7047 - val_loss: 0.6495 - val_acc: 0.8132\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9174 - acc: 0.7094\n",
      "Epoch 00090: val_loss did not improve from 0.64618\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9173 - acc: 0.7094 - val_loss: 0.7036 - val_acc: 0.7973\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9178 - acc: 0.7086\n",
      "Epoch 00091: val_loss improved from 0.64618 to 0.64073, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/091-0.6407.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9178 - acc: 0.7086 - val_loss: 0.6407 - val_acc: 0.8130\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9160 - acc: 0.7115\n",
      "Epoch 00092: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9160 - acc: 0.7115 - val_loss: 0.6440 - val_acc: 0.8137\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.7057\n",
      "Epoch 00093: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9169 - acc: 0.7057 - val_loss: 0.6637 - val_acc: 0.7959\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9104 - acc: 0.7099\n",
      "Epoch 00094: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9103 - acc: 0.7099 - val_loss: 0.6761 - val_acc: 0.8139\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9085 - acc: 0.7104\n",
      "Epoch 00095: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9086 - acc: 0.7103 - val_loss: 0.6631 - val_acc: 0.8053\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9116 - acc: 0.7100\n",
      "Epoch 00096: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9117 - acc: 0.7099 - val_loss: 0.6538 - val_acc: 0.8076\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9063 - acc: 0.7141\n",
      "Epoch 00097: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9063 - acc: 0.7141 - val_loss: 0.6466 - val_acc: 0.8127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9065 - acc: 0.7133\n",
      "Epoch 00098: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9066 - acc: 0.7132 - val_loss: 0.6494 - val_acc: 0.8150\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9060 - acc: 0.7165\n",
      "Epoch 00099: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9061 - acc: 0.7165 - val_loss: 0.6498 - val_acc: 0.8120\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9041 - acc: 0.7140\n",
      "Epoch 00100: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9041 - acc: 0.7140 - val_loss: 0.6824 - val_acc: 0.7985\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8977 - acc: 0.7148\n",
      "Epoch 00101: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8978 - acc: 0.7147 - val_loss: 0.6496 - val_acc: 0.8162\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8971 - acc: 0.7175\n",
      "Epoch 00102: val_loss did not improve from 0.64073\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8971 - acc: 0.7175 - val_loss: 0.6505 - val_acc: 0.8157\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7182\n",
      "Epoch 00103: val_loss improved from 0.64073 to 0.64058, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/103-0.6406.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8924 - acc: 0.7181 - val_loss: 0.6406 - val_acc: 0.8137\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8965 - acc: 0.7169\n",
      "Epoch 00104: val_loss improved from 0.64058 to 0.63237, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/104-0.6324.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8964 - acc: 0.7169 - val_loss: 0.6324 - val_acc: 0.8123\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.7177\n",
      "Epoch 00105: val_loss did not improve from 0.63237\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8878 - acc: 0.7177 - val_loss: 0.6426 - val_acc: 0.8099\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8851 - acc: 0.7195\n",
      "Epoch 00106: val_loss did not improve from 0.63237\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8851 - acc: 0.7195 - val_loss: 0.6719 - val_acc: 0.7999\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8904 - acc: 0.7177\n",
      "Epoch 00107: val_loss did not improve from 0.63237\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8903 - acc: 0.7178 - val_loss: 0.6459 - val_acc: 0.8150\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8865 - acc: 0.7181\n",
      "Epoch 00108: val_loss did not improve from 0.63237\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8864 - acc: 0.7181 - val_loss: 0.6353 - val_acc: 0.8153\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7211\n",
      "Epoch 00109: val_loss improved from 0.63237 to 0.62878, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/109-0.6288.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8840 - acc: 0.7211 - val_loss: 0.6288 - val_acc: 0.8220\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.7205\n",
      "Epoch 00110: val_loss did not improve from 0.62878\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8801 - acc: 0.7204 - val_loss: 0.6452 - val_acc: 0.8050\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8797 - acc: 0.7198\n",
      "Epoch 00111: val_loss did not improve from 0.62878\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8798 - acc: 0.7198 - val_loss: 0.6482 - val_acc: 0.8111\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8808 - acc: 0.7177\n",
      "Epoch 00112: val_loss improved from 0.62878 to 0.61985, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/112-0.6198.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8808 - acc: 0.7177 - val_loss: 0.6198 - val_acc: 0.8202\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.7239\n",
      "Epoch 00113: val_loss did not improve from 0.61985\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8761 - acc: 0.7239 - val_loss: 0.6451 - val_acc: 0.8081\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.7222\n",
      "Epoch 00114: val_loss did not improve from 0.61985\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8781 - acc: 0.7222 - val_loss: 0.6571 - val_acc: 0.8048\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8724 - acc: 0.7235\n",
      "Epoch 00115: val_loss did not improve from 0.61985\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8723 - acc: 0.7235 - val_loss: 0.6409 - val_acc: 0.8130\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7203\n",
      "Epoch 00116: val_loss did not improve from 0.61985\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8822 - acc: 0.7203 - val_loss: 0.6812 - val_acc: 0.8004\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8717 - acc: 0.7217\n",
      "Epoch 00117: val_loss improved from 0.61985 to 0.59932, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/117-0.5993.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8717 - acc: 0.7217 - val_loss: 0.5993 - val_acc: 0.8258\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8702 - acc: 0.7226\n",
      "Epoch 00118: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8701 - acc: 0.7226 - val_loss: 0.6397 - val_acc: 0.8078\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8754 - acc: 0.7226\n",
      "Epoch 00119: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8757 - acc: 0.7225 - val_loss: 0.6365 - val_acc: 0.8127\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8654 - acc: 0.7255\n",
      "Epoch 00120: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8654 - acc: 0.7254 - val_loss: 0.6125 - val_acc: 0.8164\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8737 - acc: 0.7230\n",
      "Epoch 00121: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8736 - acc: 0.7231 - val_loss: 0.6248 - val_acc: 0.8113\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8651 - acc: 0.7234\n",
      "Epoch 00122: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8652 - acc: 0.7234 - val_loss: 0.6309 - val_acc: 0.8176\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8607 - acc: 0.7296\n",
      "Epoch 00123: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8607 - acc: 0.7296 - val_loss: 0.6144 - val_acc: 0.8216\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8599 - acc: 0.7271\n",
      "Epoch 00124: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8599 - acc: 0.7271 - val_loss: 0.6344 - val_acc: 0.8104\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8643 - acc: 0.7265\n",
      "Epoch 00125: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8642 - acc: 0.7266 - val_loss: 0.6376 - val_acc: 0.8143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8497 - acc: 0.7296\n",
      "Epoch 00126: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8498 - acc: 0.7296 - val_loss: 0.6023 - val_acc: 0.8202\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8534 - acc: 0.7311\n",
      "Epoch 00127: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8533 - acc: 0.7311 - val_loss: 0.6082 - val_acc: 0.8216\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8602 - acc: 0.7298\n",
      "Epoch 00128: val_loss did not improve from 0.59932\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8601 - acc: 0.7298 - val_loss: 0.6087 - val_acc: 0.8206\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8518 - acc: 0.7299\n",
      "Epoch 00129: val_loss improved from 0.59932 to 0.59873, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/129-0.5987.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8517 - acc: 0.7299 - val_loss: 0.5987 - val_acc: 0.8244\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8502 - acc: 0.7320\n",
      "Epoch 00130: val_loss improved from 0.59873 to 0.59143, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/130-0.5914.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8502 - acc: 0.7320 - val_loss: 0.5914 - val_acc: 0.8283\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8542 - acc: 0.7308\n",
      "Epoch 00131: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8542 - acc: 0.7308 - val_loss: 0.6231 - val_acc: 0.8150\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8524 - acc: 0.7292\n",
      "Epoch 00132: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8524 - acc: 0.7292 - val_loss: 0.6215 - val_acc: 0.8190\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8529 - acc: 0.7295\n",
      "Epoch 00133: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8529 - acc: 0.7295 - val_loss: 0.6149 - val_acc: 0.8123\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8465 - acc: 0.7312\n",
      "Epoch 00134: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8466 - acc: 0.7312 - val_loss: 0.6179 - val_acc: 0.8241\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8462 - acc: 0.7321\n",
      "Epoch 00135: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8464 - acc: 0.7321 - val_loss: 0.6297 - val_acc: 0.8195\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8395 - acc: 0.7329\n",
      "Epoch 00136: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8395 - acc: 0.7329 - val_loss: 0.6131 - val_acc: 0.8241\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.7336\n",
      "Epoch 00137: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8390 - acc: 0.7336 - val_loss: 0.6399 - val_acc: 0.8153\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8423 - acc: 0.7345\n",
      "Epoch 00138: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8424 - acc: 0.7345 - val_loss: 0.5983 - val_acc: 0.8232\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8365 - acc: 0.7348\n",
      "Epoch 00139: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8365 - acc: 0.7348 - val_loss: 0.6144 - val_acc: 0.8188\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8450 - acc: 0.7315\n",
      "Epoch 00140: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8449 - acc: 0.7315 - val_loss: 0.6336 - val_acc: 0.8127\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8356 - acc: 0.7354\n",
      "Epoch 00141: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8355 - acc: 0.7354 - val_loss: 0.6331 - val_acc: 0.8099\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8348 - acc: 0.7352\n",
      "Epoch 00142: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8350 - acc: 0.7351 - val_loss: 0.5958 - val_acc: 0.8276\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7327\n",
      "Epoch 00143: val_loss did not improve from 0.59143\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8385 - acc: 0.7327 - val_loss: 0.6128 - val_acc: 0.8260\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8365 - acc: 0.7330\n",
      "Epoch 00144: val_loss improved from 0.59143 to 0.59067, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/144-0.5907.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8364 - acc: 0.7331 - val_loss: 0.5907 - val_acc: 0.8302\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7382\n",
      "Epoch 00145: val_loss did not improve from 0.59067\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8351 - acc: 0.7382 - val_loss: 0.6008 - val_acc: 0.8253\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7361\n",
      "Epoch 00146: val_loss improved from 0.59067 to 0.56898, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/146-0.5690.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8295 - acc: 0.7361 - val_loss: 0.5690 - val_acc: 0.8290\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8328 - acc: 0.7323\n",
      "Epoch 00147: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8327 - acc: 0.7323 - val_loss: 0.5966 - val_acc: 0.8239\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7336\n",
      "Epoch 00148: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8313 - acc: 0.7337 - val_loss: 0.6086 - val_acc: 0.8199\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8214 - acc: 0.7383\n",
      "Epoch 00149: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8213 - acc: 0.7384 - val_loss: 0.6085 - val_acc: 0.8258\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8244 - acc: 0.7378\n",
      "Epoch 00150: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8245 - acc: 0.7378 - val_loss: 0.6107 - val_acc: 0.8246\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8252 - acc: 0.7373\n",
      "Epoch 00151: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8252 - acc: 0.7373 - val_loss: 0.5788 - val_acc: 0.8344\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8268 - acc: 0.7368\n",
      "Epoch 00152: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8268 - acc: 0.7368 - val_loss: 0.5868 - val_acc: 0.8216\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7384\n",
      "Epoch 00153: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8266 - acc: 0.7384 - val_loss: 0.6079 - val_acc: 0.8176\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7376\n",
      "Epoch 00154: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8211 - acc: 0.7376 - val_loss: 0.5882 - val_acc: 0.8248\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8224 - acc: 0.7380\n",
      "Epoch 00155: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8224 - acc: 0.7380 - val_loss: 0.6397 - val_acc: 0.8116\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8185 - acc: 0.7396\n",
      "Epoch 00156: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8185 - acc: 0.7396 - val_loss: 0.6156 - val_acc: 0.8213\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7411\n",
      "Epoch 00157: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8177 - acc: 0.7410 - val_loss: 0.5940 - val_acc: 0.8281\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8244 - acc: 0.7393\n",
      "Epoch 00158: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8244 - acc: 0.7393 - val_loss: 0.5938 - val_acc: 0.8248\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8131 - acc: 0.7428\n",
      "Epoch 00159: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8131 - acc: 0.7428 - val_loss: 0.5889 - val_acc: 0.8314\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8216 - acc: 0.7378\n",
      "Epoch 00160: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8216 - acc: 0.7378 - val_loss: 0.6110 - val_acc: 0.8286\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8180 - acc: 0.7391\n",
      "Epoch 00161: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8180 - acc: 0.7391 - val_loss: 0.5721 - val_acc: 0.8314\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7395\n",
      "Epoch 00162: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8152 - acc: 0.7395 - val_loss: 0.6098 - val_acc: 0.8197\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8094 - acc: 0.7419\n",
      "Epoch 00163: val_loss did not improve from 0.56898\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8094 - acc: 0.7419 - val_loss: 0.6323 - val_acc: 0.8102\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7459\n",
      "Epoch 00164: val_loss improved from 0.56898 to 0.56421, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/164-0.5642.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8055 - acc: 0.7459 - val_loss: 0.5642 - val_acc: 0.8316\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7417\n",
      "Epoch 00165: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8071 - acc: 0.7417 - val_loss: 0.5738 - val_acc: 0.8325\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8052 - acc: 0.7433\n",
      "Epoch 00166: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8053 - acc: 0.7432 - val_loss: 0.6055 - val_acc: 0.8237\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.7406\n",
      "Epoch 00167: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8117 - acc: 0.7406 - val_loss: 0.5837 - val_acc: 0.8295\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7454\n",
      "Epoch 00168: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8071 - acc: 0.7454 - val_loss: 0.6232 - val_acc: 0.8134\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7489\n",
      "Epoch 00169: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7938 - acc: 0.7488 - val_loss: 0.6002 - val_acc: 0.8178\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7952 - acc: 0.7466\n",
      "Epoch 00170: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7954 - acc: 0.7466 - val_loss: 0.5957 - val_acc: 0.8244\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.7407\n",
      "Epoch 00171: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8062 - acc: 0.7407 - val_loss: 0.5904 - val_acc: 0.8279\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7999 - acc: 0.7452\n",
      "Epoch 00172: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7999 - acc: 0.7452 - val_loss: 0.5940 - val_acc: 0.8281\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7985 - acc: 0.7476\n",
      "Epoch 00173: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7986 - acc: 0.7476 - val_loss: 0.5995 - val_acc: 0.8239\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7993 - acc: 0.7453\n",
      "Epoch 00174: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7993 - acc: 0.7453 - val_loss: 0.6318 - val_acc: 0.8162\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8038 - acc: 0.7451\n",
      "Epoch 00175: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8039 - acc: 0.7451 - val_loss: 0.5996 - val_acc: 0.8195\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7955 - acc: 0.7455\n",
      "Epoch 00176: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7955 - acc: 0.7455 - val_loss: 0.5694 - val_acc: 0.8297\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7480\n",
      "Epoch 00177: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7940 - acc: 0.7480 - val_loss: 0.5770 - val_acc: 0.8358\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7464\n",
      "Epoch 00178: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7929 - acc: 0.7464 - val_loss: 0.6052 - val_acc: 0.8218\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7920 - acc: 0.7473\n",
      "Epoch 00179: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7920 - acc: 0.7473 - val_loss: 0.5698 - val_acc: 0.8318\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7985 - acc: 0.7451\n",
      "Epoch 00180: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7985 - acc: 0.7451 - val_loss: 0.6024 - val_acc: 0.8220\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7483\n",
      "Epoch 00181: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7940 - acc: 0.7483 - val_loss: 0.5740 - val_acc: 0.8334\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7870 - acc: 0.7470\n",
      "Epoch 00182: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7871 - acc: 0.7470 - val_loss: 0.5656 - val_acc: 0.8321\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7911 - acc: 0.7489\n",
      "Epoch 00183: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7912 - acc: 0.7489 - val_loss: 0.5879 - val_acc: 0.8276\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7468\n",
      "Epoch 00184: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7906 - acc: 0.7468 - val_loss: 0.5944 - val_acc: 0.8220\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7865 - acc: 0.7493\n",
      "Epoch 00185: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7865 - acc: 0.7493 - val_loss: 0.6116 - val_acc: 0.8269\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7952 - acc: 0.7474\n",
      "Epoch 00186: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7951 - acc: 0.7474 - val_loss: 0.6238 - val_acc: 0.8192\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7478\n",
      "Epoch 00187: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7842 - acc: 0.7478 - val_loss: 0.6002 - val_acc: 0.8192\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7872 - acc: 0.7481\n",
      "Epoch 00188: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7872 - acc: 0.7482 - val_loss: 0.5695 - val_acc: 0.8267\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7504\n",
      "Epoch 00189: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7876 - acc: 0.7504 - val_loss: 0.6042 - val_acc: 0.8251\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7524\n",
      "Epoch 00190: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7830 - acc: 0.7524 - val_loss: 0.6270 - val_acc: 0.8202\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7822 - acc: 0.7531\n",
      "Epoch 00191: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7824 - acc: 0.7530 - val_loss: 0.6180 - val_acc: 0.8064\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7910 - acc: 0.7488\n",
      "Epoch 00192: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7910 - acc: 0.7488 - val_loss: 0.5756 - val_acc: 0.8407\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7830 - acc: 0.7504\n",
      "Epoch 00193: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7830 - acc: 0.7504 - val_loss: 0.6383 - val_acc: 0.8134\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.7498\n",
      "Epoch 00194: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7789 - acc: 0.7499 - val_loss: 0.6014 - val_acc: 0.8185\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7718 - acc: 0.7561\n",
      "Epoch 00195: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7719 - acc: 0.7560 - val_loss: 0.6241 - val_acc: 0.8090\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7501\n",
      "Epoch 00196: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7799 - acc: 0.7502 - val_loss: 0.5769 - val_acc: 0.8348\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7792 - acc: 0.7542\n",
      "Epoch 00197: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7791 - acc: 0.7543 - val_loss: 0.5926 - val_acc: 0.8213\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7719 - acc: 0.7527\n",
      "Epoch 00198: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7719 - acc: 0.7527 - val_loss: 0.5917 - val_acc: 0.8260\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7772 - acc: 0.7533\n",
      "Epoch 00199: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7773 - acc: 0.7532 - val_loss: 0.6119 - val_acc: 0.8304\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7735 - acc: 0.7523\n",
      "Epoch 00200: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7735 - acc: 0.7523 - val_loss: 0.5700 - val_acc: 0.8344\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7680 - acc: 0.7565\n",
      "Epoch 00201: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7680 - acc: 0.7565 - val_loss: 0.5969 - val_acc: 0.8244\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7554\n",
      "Epoch 00202: val_loss did not improve from 0.56421\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7788 - acc: 0.7553 - val_loss: 0.5847 - val_acc: 0.8316\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7712 - acc: 0.7545\n",
      "Epoch 00203: val_loss improved from 0.56421 to 0.56069, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/203-0.5607.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7711 - acc: 0.7545 - val_loss: 0.5607 - val_acc: 0.8393\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7541\n",
      "Epoch 00204: val_loss improved from 0.56069 to 0.54809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/204-0.5481.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7711 - acc: 0.7541 - val_loss: 0.5481 - val_acc: 0.8414\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7707 - acc: 0.7545\n",
      "Epoch 00205: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7707 - acc: 0.7545 - val_loss: 0.6121 - val_acc: 0.8197\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7543\n",
      "Epoch 00206: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7711 - acc: 0.7542 - val_loss: 0.5845 - val_acc: 0.8276\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7536\n",
      "Epoch 00207: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7709 - acc: 0.7536 - val_loss: 0.5867 - val_acc: 0.8307\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.7547\n",
      "Epoch 00208: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7716 - acc: 0.7547 - val_loss: 0.5696 - val_acc: 0.8332\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7689 - acc: 0.7539\n",
      "Epoch 00209: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7691 - acc: 0.7539 - val_loss: 0.5811 - val_acc: 0.8376\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7629 - acc: 0.7575\n",
      "Epoch 00210: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7629 - acc: 0.7575 - val_loss: 0.6029 - val_acc: 0.8251\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7580\n",
      "Epoch 00211: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7652 - acc: 0.7580 - val_loss: 0.5852 - val_acc: 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7593\n",
      "Epoch 00212: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7595 - acc: 0.7592 - val_loss: 0.5865 - val_acc: 0.8260\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7640 - acc: 0.7557\n",
      "Epoch 00213: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7640 - acc: 0.7557 - val_loss: 0.6107 - val_acc: 0.8244\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7582 - acc: 0.7561\n",
      "Epoch 00214: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7582 - acc: 0.7561 - val_loss: 0.6039 - val_acc: 0.8262\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7605\n",
      "Epoch 00215: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7595 - acc: 0.7605 - val_loss: 0.5643 - val_acc: 0.8355\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.7564\n",
      "Epoch 00216: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7578 - acc: 0.7564 - val_loss: 0.6576 - val_acc: 0.7962\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7559\n",
      "Epoch 00217: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7624 - acc: 0.7558 - val_loss: 0.6063 - val_acc: 0.8251\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7570 - acc: 0.7585\n",
      "Epoch 00218: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7570 - acc: 0.7585 - val_loss: 0.5770 - val_acc: 0.8369\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7588\n",
      "Epoch 00219: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7585 - acc: 0.7588 - val_loss: 0.5644 - val_acc: 0.8381\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7530 - acc: 0.7584\n",
      "Epoch 00220: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7530 - acc: 0.7584 - val_loss: 0.6026 - val_acc: 0.8334\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7534 - acc: 0.7626\n",
      "Epoch 00221: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7535 - acc: 0.7625 - val_loss: 0.5797 - val_acc: 0.8365\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7566\n",
      "Epoch 00222: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7584 - acc: 0.7566 - val_loss: 0.6273 - val_acc: 0.8043\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7495 - acc: 0.7586\n",
      "Epoch 00223: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7496 - acc: 0.7585 - val_loss: 0.5888 - val_acc: 0.8293\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7483 - acc: 0.7624\n",
      "Epoch 00224: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7484 - acc: 0.7624 - val_loss: 0.5668 - val_acc: 0.8262\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7608\n",
      "Epoch 00225: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7508 - acc: 0.7608 - val_loss: 0.5694 - val_acc: 0.8300\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7574\n",
      "Epoch 00226: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7573 - acc: 0.7574 - val_loss: 0.5631 - val_acc: 0.8286\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7561\n",
      "Epoch 00227: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7609 - acc: 0.7561 - val_loss: 0.5689 - val_acc: 0.8334\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7579\n",
      "Epoch 00228: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7552 - acc: 0.7579 - val_loss: 0.5858 - val_acc: 0.8367\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7568 - acc: 0.7595\n",
      "Epoch 00229: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7568 - acc: 0.7595 - val_loss: 0.6266 - val_acc: 0.8137\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7511 - acc: 0.7623\n",
      "Epoch 00230: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7510 - acc: 0.7623 - val_loss: 0.6233 - val_acc: 0.8111\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7480 - acc: 0.7615\n",
      "Epoch 00231: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7481 - acc: 0.7615 - val_loss: 0.6004 - val_acc: 0.8286\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7520 - acc: 0.7592\n",
      "Epoch 00232: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7522 - acc: 0.7591 - val_loss: 0.5826 - val_acc: 0.8321\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7588\n",
      "Epoch 00233: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7523 - acc: 0.7588 - val_loss: 0.5655 - val_acc: 0.8388\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.7604\n",
      "Epoch 00234: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7493 - acc: 0.7604 - val_loss: 0.6464 - val_acc: 0.8013\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7591\n",
      "Epoch 00235: val_loss improved from 0.54809 to 0.54707, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/235-0.5471.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7501 - acc: 0.7591 - val_loss: 0.5471 - val_acc: 0.8439\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7531 - acc: 0.7598\n",
      "Epoch 00236: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7530 - acc: 0.7598 - val_loss: 0.5659 - val_acc: 0.8369\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7387 - acc: 0.7654\n",
      "Epoch 00237: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7387 - acc: 0.7654 - val_loss: 0.5701 - val_acc: 0.8362\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7445 - acc: 0.7621\n",
      "Epoch 00238: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7444 - acc: 0.7621 - val_loss: 0.5951 - val_acc: 0.8328\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7409 - acc: 0.7653\n",
      "Epoch 00239: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7409 - acc: 0.7652 - val_loss: 0.5505 - val_acc: 0.8425\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7411 - acc: 0.7606\n",
      "Epoch 00240: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7411 - acc: 0.7606 - val_loss: 0.5667 - val_acc: 0.8267\n",
      "Epoch 241/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7658\n",
      "Epoch 00241: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7434 - acc: 0.7658 - val_loss: 0.6045 - val_acc: 0.8309\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7408 - acc: 0.7625\n",
      "Epoch 00242: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7408 - acc: 0.7625 - val_loss: 0.5497 - val_acc: 0.8390\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7401 - acc: 0.7626\n",
      "Epoch 00243: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7401 - acc: 0.7626 - val_loss: 0.5475 - val_acc: 0.8435\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7393 - acc: 0.7632\n",
      "Epoch 00244: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7394 - acc: 0.7631 - val_loss: 0.5986 - val_acc: 0.8253\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7421 - acc: 0.7643\n",
      "Epoch 00245: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7420 - acc: 0.7643 - val_loss: 0.5855 - val_acc: 0.8334\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7622\n",
      "Epoch 00246: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7370 - acc: 0.7622 - val_loss: 0.5514 - val_acc: 0.8395\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7428 - acc: 0.7631\n",
      "Epoch 00247: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7429 - acc: 0.7631 - val_loss: 0.5668 - val_acc: 0.8358\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7665\n",
      "Epoch 00248: val_loss did not improve from 0.54707\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7393 - acc: 0.7665 - val_loss: 0.5602 - val_acc: 0.8444\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7325 - acc: 0.7648\n",
      "Epoch 00249: val_loss improved from 0.54707 to 0.53996, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/249-0.5400.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7325 - acc: 0.7648 - val_loss: 0.5400 - val_acc: 0.8446\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7650\n",
      "Epoch 00250: val_loss improved from 0.53996 to 0.53316, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/250-0.5332.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7280 - acc: 0.7650 - val_loss: 0.5332 - val_acc: 0.8418\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.7652\n",
      "Epoch 00251: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7354 - acc: 0.7652 - val_loss: 0.5408 - val_acc: 0.8409\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7646\n",
      "Epoch 00252: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7323 - acc: 0.7646 - val_loss: 0.5944 - val_acc: 0.8237\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.7648\n",
      "Epoch 00253: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7314 - acc: 0.7648 - val_loss: 0.5389 - val_acc: 0.8451\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7335 - acc: 0.7642\n",
      "Epoch 00254: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7335 - acc: 0.7642 - val_loss: 0.5457 - val_acc: 0.8425\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7337 - acc: 0.7654\n",
      "Epoch 00255: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7337 - acc: 0.7653 - val_loss: 0.5784 - val_acc: 0.8265\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7317 - acc: 0.7638\n",
      "Epoch 00256: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7317 - acc: 0.7638 - val_loss: 0.6146 - val_acc: 0.8174\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7359 - acc: 0.7665\n",
      "Epoch 00257: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7359 - acc: 0.7665 - val_loss: 0.5696 - val_acc: 0.8332\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7278 - acc: 0.7699\n",
      "Epoch 00258: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7279 - acc: 0.7699 - val_loss: 0.5715 - val_acc: 0.8328\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7338 - acc: 0.7654\n",
      "Epoch 00259: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7337 - acc: 0.7654 - val_loss: 0.5454 - val_acc: 0.8339\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.7688\n",
      "Epoch 00260: val_loss did not improve from 0.53316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7286 - acc: 0.7688 - val_loss: 0.5615 - val_acc: 0.8369\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7291 - acc: 0.7678\n",
      "Epoch 00261: val_loss improved from 0.53316 to 0.53064, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/261-0.5306.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7291 - acc: 0.7679 - val_loss: 0.5306 - val_acc: 0.8509\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7335 - acc: 0.7674\n",
      "Epoch 00262: val_loss did not improve from 0.53064\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7335 - acc: 0.7674 - val_loss: 0.5596 - val_acc: 0.8330\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.7666\n",
      "Epoch 00263: val_loss did not improve from 0.53064\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7273 - acc: 0.7666 - val_loss: 0.5816 - val_acc: 0.8348\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7240 - acc: 0.7675\n",
      "Epoch 00264: val_loss improved from 0.53064 to 0.53017, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_3_conv_checkpoint/264-0.5302.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7240 - acc: 0.7675 - val_loss: 0.5302 - val_acc: 0.8477\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7203 - acc: 0.7710\n",
      "Epoch 00265: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7202 - acc: 0.7710 - val_loss: 0.5628 - val_acc: 0.8379\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.7708\n",
      "Epoch 00266: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7204 - acc: 0.7708 - val_loss: 0.5333 - val_acc: 0.8465\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.7684\n",
      "Epoch 00267: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7227 - acc: 0.7684 - val_loss: 0.5636 - val_acc: 0.8362\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7689\n",
      "Epoch 00268: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7238 - acc: 0.7688 - val_loss: 0.5924 - val_acc: 0.8309\n",
      "Epoch 269/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7663\n",
      "Epoch 00269: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7298 - acc: 0.7663 - val_loss: 0.5423 - val_acc: 0.8491\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7280 - acc: 0.7670\n",
      "Epoch 00270: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7279 - acc: 0.7671 - val_loss: 0.5849 - val_acc: 0.8188\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7231 - acc: 0.7682\n",
      "Epoch 00271: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7231 - acc: 0.7682 - val_loss: 0.5585 - val_acc: 0.8355\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7695\n",
      "Epoch 00272: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7241 - acc: 0.7694 - val_loss: 0.5905 - val_acc: 0.8323\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7211 - acc: 0.7687\n",
      "Epoch 00273: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7211 - acc: 0.7687 - val_loss: 0.5878 - val_acc: 0.8290\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7251 - acc: 0.7682\n",
      "Epoch 00274: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7252 - acc: 0.7682 - val_loss: 0.5515 - val_acc: 0.8323\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7151 - acc: 0.7699\n",
      "Epoch 00275: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7150 - acc: 0.7699 - val_loss: 0.5607 - val_acc: 0.8328\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7179 - acc: 0.7687\n",
      "Epoch 00276: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7182 - acc: 0.7686 - val_loss: 0.5619 - val_acc: 0.8341\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7714\n",
      "Epoch 00277: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7163 - acc: 0.7713 - val_loss: 0.6143 - val_acc: 0.8057\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7132 - acc: 0.7725\n",
      "Epoch 00278: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7133 - acc: 0.7725 - val_loss: 0.5393 - val_acc: 0.8502\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7149 - acc: 0.7697\n",
      "Epoch 00279: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7149 - acc: 0.7697 - val_loss: 0.6501 - val_acc: 0.8078\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7690\n",
      "Epoch 00280: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7172 - acc: 0.7690 - val_loss: 0.5554 - val_acc: 0.8330\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7149 - acc: 0.7720\n",
      "Epoch 00281: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7150 - acc: 0.7719 - val_loss: 0.5603 - val_acc: 0.8348\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7228 - acc: 0.7704\n",
      "Epoch 00282: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7228 - acc: 0.7704 - val_loss: 0.5414 - val_acc: 0.8432\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7163 - acc: 0.7712\n",
      "Epoch 00283: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7163 - acc: 0.7712 - val_loss: 0.6292 - val_acc: 0.8104\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7153 - acc: 0.7736\n",
      "Epoch 00284: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7152 - acc: 0.7736 - val_loss: 0.5494 - val_acc: 0.8346\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7083 - acc: 0.7749\n",
      "Epoch 00285: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7082 - acc: 0.7750 - val_loss: 0.5491 - val_acc: 0.8330\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7720\n",
      "Epoch 00286: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7120 - acc: 0.7720 - val_loss: 0.5549 - val_acc: 0.8383\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.7716\n",
      "Epoch 00287: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7116 - acc: 0.7716 - val_loss: 0.5763 - val_acc: 0.8304\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.7709\n",
      "Epoch 00288: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7151 - acc: 0.7709 - val_loss: 0.6668 - val_acc: 0.7939\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7141 - acc: 0.7719\n",
      "Epoch 00289: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7141 - acc: 0.7719 - val_loss: 0.5668 - val_acc: 0.8376\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.7733\n",
      "Epoch 00290: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7108 - acc: 0.7733 - val_loss: 0.5424 - val_acc: 0.8435\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7735\n",
      "Epoch 00291: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7057 - acc: 0.7734 - val_loss: 0.5622 - val_acc: 0.8386\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7078 - acc: 0.7744\n",
      "Epoch 00292: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7079 - acc: 0.7744 - val_loss: 0.5529 - val_acc: 0.8367\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7069 - acc: 0.7751\n",
      "Epoch 00293: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7069 - acc: 0.7751 - val_loss: 0.5961 - val_acc: 0.8274\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7120 - acc: 0.7727\n",
      "Epoch 00294: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7121 - acc: 0.7726 - val_loss: 0.6258 - val_acc: 0.8132\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7185 - acc: 0.7703\n",
      "Epoch 00295: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7185 - acc: 0.7703 - val_loss: 0.7131 - val_acc: 0.7699\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7125 - acc: 0.7736\n",
      "Epoch 00296: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7125 - acc: 0.7736 - val_loss: 0.5399 - val_acc: 0.8425\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7143 - acc: 0.7710\n",
      "Epoch 00297: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7143 - acc: 0.7711 - val_loss: 0.5625 - val_acc: 0.8381\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.7736\n",
      "Epoch 00298: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7030 - acc: 0.7736 - val_loss: 0.5959 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7050 - acc: 0.7742\n",
      "Epoch 00299: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7051 - acc: 0.7741 - val_loss: 0.5631 - val_acc: 0.8369\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7027 - acc: 0.7763\n",
      "Epoch 00300: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7026 - acc: 0.7763 - val_loss: 0.6262 - val_acc: 0.8139\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7060 - acc: 0.7727\n",
      "Epoch 00301: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7062 - acc: 0.7727 - val_loss: 0.6659 - val_acc: 0.7978\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7036 - acc: 0.7757\n",
      "Epoch 00302: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7036 - acc: 0.7757 - val_loss: 0.5358 - val_acc: 0.8428\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7054 - acc: 0.7744\n",
      "Epoch 00303: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7053 - acc: 0.7744 - val_loss: 0.5786 - val_acc: 0.8311\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7033 - acc: 0.7758\n",
      "Epoch 00304: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7033 - acc: 0.7758 - val_loss: 0.5955 - val_acc: 0.8141\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6998 - acc: 0.7762\n",
      "Epoch 00305: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6997 - acc: 0.7762 - val_loss: 0.5859 - val_acc: 0.8332\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7026 - acc: 0.7729\n",
      "Epoch 00306: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7026 - acc: 0.7729 - val_loss: 0.5933 - val_acc: 0.8241\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7067 - acc: 0.7749\n",
      "Epoch 00307: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7068 - acc: 0.7749 - val_loss: 0.5517 - val_acc: 0.8383\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7771\n",
      "Epoch 00308: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7006 - acc: 0.7771 - val_loss: 0.6357 - val_acc: 0.8162\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7050 - acc: 0.7751\n",
      "Epoch 00309: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7051 - acc: 0.7750 - val_loss: 0.5395 - val_acc: 0.8388\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.7778\n",
      "Epoch 00310: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6993 - acc: 0.7778 - val_loss: 0.5435 - val_acc: 0.8451\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7009 - acc: 0.7746\n",
      "Epoch 00311: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7008 - acc: 0.7746 - val_loss: 0.5625 - val_acc: 0.8386\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.7754\n",
      "Epoch 00312: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7039 - acc: 0.7754 - val_loss: 0.5685 - val_acc: 0.8379\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7786\n",
      "Epoch 00313: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6964 - acc: 0.7786 - val_loss: 0.5605 - val_acc: 0.8341\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6964 - acc: 0.7783\n",
      "Epoch 00314: val_loss did not improve from 0.53017\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6965 - acc: 0.7783 - val_loss: 0.5747 - val_acc: 0.8216\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 543us/sample - loss: 0.6157 - acc: 0.8120\n",
      "Loss: 0.6156512848251458 Accuracy: 0.8120457\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8527 - acc: 0.1046\n",
      "Epoch 00001: val_loss improved from inf to 2.41069, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/001-2.4107.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 5.8528 - acc: 0.1046 - val_loss: 2.4107 - val_acc: 0.1926\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5398 - acc: 0.1397\n",
      "Epoch 00002: val_loss improved from 2.41069 to 2.08064, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/002-2.0806.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 3.5397 - acc: 0.1397 - val_loss: 2.0806 - val_acc: 0.3652\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8966 - acc: 0.1771\n",
      "Epoch 00003: val_loss improved from 2.08064 to 1.89992, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/003-1.8999.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.8965 - acc: 0.1772 - val_loss: 1.8999 - val_acc: 0.4458\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6083 - acc: 0.2135\n",
      "Epoch 00004: val_loss improved from 1.89992 to 1.75172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/004-1.7517.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.6083 - acc: 0.2135 - val_loss: 1.7517 - val_acc: 0.4889\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3845 - acc: 0.2505\n",
      "Epoch 00005: val_loss improved from 1.75172 to 1.61436, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/005-1.6144.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.3844 - acc: 0.2505 - val_loss: 1.6144 - val_acc: 0.5323\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1968 - acc: 0.2897\n",
      "Epoch 00006: val_loss improved from 1.61436 to 1.49581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/006-1.4958.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.1968 - acc: 0.2897 - val_loss: 1.4958 - val_acc: 0.5688\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0434 - acc: 0.3305\n",
      "Epoch 00007: val_loss improved from 1.49581 to 1.41931, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/007-1.4193.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.0435 - acc: 0.3305 - val_loss: 1.4193 - val_acc: 0.5917\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8918 - acc: 0.3748\n",
      "Epoch 00008: val_loss improved from 1.41931 to 1.28960, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/008-1.2896.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8918 - acc: 0.3748 - val_loss: 1.2896 - val_acc: 0.6231\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7709 - acc: 0.4110\n",
      "Epoch 00009: val_loss improved from 1.28960 to 1.20736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/009-1.2074.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7708 - acc: 0.4111 - val_loss: 1.2074 - val_acc: 0.6476\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6647 - acc: 0.4473\n",
      "Epoch 00010: val_loss improved from 1.20736 to 1.10809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/010-1.1081.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6646 - acc: 0.4473 - val_loss: 1.1081 - val_acc: 0.6834\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5535 - acc: 0.4854\n",
      "Epoch 00011: val_loss improved from 1.10809 to 1.04893, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/011-1.0489.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5536 - acc: 0.4854 - val_loss: 1.0489 - val_acc: 0.6916\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4734 - acc: 0.5173\n",
      "Epoch 00012: val_loss improved from 1.04893 to 1.01346, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/012-1.0135.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4734 - acc: 0.5172 - val_loss: 1.0135 - val_acc: 0.7037\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4028 - acc: 0.5386\n",
      "Epoch 00013: val_loss improved from 1.01346 to 0.95813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/013-0.9581.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4029 - acc: 0.5386 - val_loss: 0.9581 - val_acc: 0.7368\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3432 - acc: 0.5580\n",
      "Epoch 00014: val_loss improved from 0.95813 to 0.88543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/014-0.8854.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3432 - acc: 0.5580 - val_loss: 0.8854 - val_acc: 0.7480\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2808 - acc: 0.5830\n",
      "Epoch 00015: val_loss improved from 0.88543 to 0.86426, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/015-0.8643.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2809 - acc: 0.5829 - val_loss: 0.8643 - val_acc: 0.7556\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2334 - acc: 0.5986\n",
      "Epoch 00016: val_loss improved from 0.86426 to 0.83526, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/016-0.8353.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2333 - acc: 0.5986 - val_loss: 0.8353 - val_acc: 0.7652\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1904 - acc: 0.6130\n",
      "Epoch 00017: val_loss improved from 0.83526 to 0.81738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/017-0.8174.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1904 - acc: 0.6130 - val_loss: 0.8174 - val_acc: 0.7738\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1465 - acc: 0.6301\n",
      "Epoch 00018: val_loss improved from 0.81738 to 0.76012, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/018-0.7601.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1464 - acc: 0.6302 - val_loss: 0.7601 - val_acc: 0.7834\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1197 - acc: 0.6407\n",
      "Epoch 00019: val_loss did not improve from 0.76012\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1196 - acc: 0.6408 - val_loss: 0.7644 - val_acc: 0.7871\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0812 - acc: 0.6545\n",
      "Epoch 00020: val_loss did not improve from 0.76012\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0812 - acc: 0.6544 - val_loss: 0.7785 - val_acc: 0.7747\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0592 - acc: 0.6617\n",
      "Epoch 00021: val_loss improved from 0.76012 to 0.72094, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/021-0.7209.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0592 - acc: 0.6617 - val_loss: 0.7209 - val_acc: 0.7980\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0375 - acc: 0.6698\n",
      "Epoch 00022: val_loss did not improve from 0.72094\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0375 - acc: 0.6699 - val_loss: 0.7231 - val_acc: 0.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0123 - acc: 0.6747\n",
      "Epoch 00023: val_loss improved from 0.72094 to 0.71864, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/023-0.7186.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0124 - acc: 0.6747 - val_loss: 0.7186 - val_acc: 0.7966\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9988 - acc: 0.6843\n",
      "Epoch 00024: val_loss improved from 0.71864 to 0.71090, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/024-0.7109.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9988 - acc: 0.6843 - val_loss: 0.7109 - val_acc: 0.7973\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9708 - acc: 0.6934\n",
      "Epoch 00025: val_loss improved from 0.71090 to 0.66215, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/025-0.6622.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9707 - acc: 0.6934 - val_loss: 0.6622 - val_acc: 0.8097\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.6981\n",
      "Epoch 00026: val_loss did not improve from 0.66215\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9537 - acc: 0.6981 - val_loss: 0.6753 - val_acc: 0.8102\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9372 - acc: 0.7036\n",
      "Epoch 00027: val_loss improved from 0.66215 to 0.64808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/027-0.6481.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9371 - acc: 0.7037 - val_loss: 0.6481 - val_acc: 0.8164\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.7062\n",
      "Epoch 00028: val_loss improved from 0.64808 to 0.62595, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/028-0.6259.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9268 - acc: 0.7062 - val_loss: 0.6259 - val_acc: 0.8311\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7139\n",
      "Epoch 00029: val_loss improved from 0.62595 to 0.59665, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/029-0.5967.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9057 - acc: 0.7140 - val_loss: 0.5967 - val_acc: 0.8260\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.7147\n",
      "Epoch 00030: val_loss did not improve from 0.59665\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8990 - acc: 0.7148 - val_loss: 0.6182 - val_acc: 0.8232\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8795 - acc: 0.7217\n",
      "Epoch 00031: val_loss did not improve from 0.59665\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8795 - acc: 0.7217 - val_loss: 0.6303 - val_acc: 0.8183\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7290\n",
      "Epoch 00032: val_loss improved from 0.59665 to 0.58464, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/032-0.5846.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8687 - acc: 0.7291 - val_loss: 0.5846 - val_acc: 0.8388\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8590 - acc: 0.7318\n",
      "Epoch 00033: val_loss did not improve from 0.58464\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8589 - acc: 0.7318 - val_loss: 0.5867 - val_acc: 0.8318\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8506 - acc: 0.7356\n",
      "Epoch 00034: val_loss improved from 0.58464 to 0.58310, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/034-0.5831.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8506 - acc: 0.7356 - val_loss: 0.5831 - val_acc: 0.8362\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7412\n",
      "Epoch 00035: val_loss improved from 0.58310 to 0.55068, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/035-0.5507.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8294 - acc: 0.7412 - val_loss: 0.5507 - val_acc: 0.8460\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8309 - acc: 0.7377\n",
      "Epoch 00036: val_loss did not improve from 0.55068\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8309 - acc: 0.7377 - val_loss: 0.5542 - val_acc: 0.8381\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8179 - acc: 0.7433\n",
      "Epoch 00037: val_loss improved from 0.55068 to 0.53498, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/037-0.5350.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8180 - acc: 0.7433 - val_loss: 0.5350 - val_acc: 0.8472\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8054 - acc: 0.7498\n",
      "Epoch 00038: val_loss did not improve from 0.53498\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8054 - acc: 0.7497 - val_loss: 0.5884 - val_acc: 0.8400\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7999 - acc: 0.7477\n",
      "Epoch 00039: val_loss did not improve from 0.53498\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7999 - acc: 0.7477 - val_loss: 0.5529 - val_acc: 0.8418\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.7548\n",
      "Epoch 00040: val_loss did not improve from 0.53498\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7914 - acc: 0.7548 - val_loss: 0.5479 - val_acc: 0.8388\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7843 - acc: 0.7549\n",
      "Epoch 00041: val_loss improved from 0.53498 to 0.52812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/041-0.5281.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7846 - acc: 0.7548 - val_loss: 0.5281 - val_acc: 0.8558\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7791 - acc: 0.7555\n",
      "Epoch 00042: val_loss did not improve from 0.52812\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7791 - acc: 0.7555 - val_loss: 0.5539 - val_acc: 0.8409\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7605\n",
      "Epoch 00043: val_loss improved from 0.52812 to 0.50904, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/043-0.5090.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7660 - acc: 0.7605 - val_loss: 0.5090 - val_acc: 0.8546\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7619\n",
      "Epoch 00044: val_loss improved from 0.50904 to 0.50140, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/044-0.5014.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7631 - acc: 0.7619 - val_loss: 0.5014 - val_acc: 0.8546\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7652\n",
      "Epoch 00045: val_loss did not improve from 0.50140\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7526 - acc: 0.7652 - val_loss: 0.5134 - val_acc: 0.8526\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7660\n",
      "Epoch 00046: val_loss did not improve from 0.50140\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7483 - acc: 0.7660 - val_loss: 0.5033 - val_acc: 0.8574\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7719\n",
      "Epoch 00047: val_loss improved from 0.50140 to 0.48033, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/047-0.4803.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7396 - acc: 0.7719 - val_loss: 0.4803 - val_acc: 0.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7711\n",
      "Epoch 00048: val_loss did not improve from 0.48033\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7370 - acc: 0.7711 - val_loss: 0.5242 - val_acc: 0.8463\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7246 - acc: 0.7756\n",
      "Epoch 00049: val_loss improved from 0.48033 to 0.47426, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/049-0.4743.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7246 - acc: 0.7756 - val_loss: 0.4743 - val_acc: 0.8612\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7177 - acc: 0.7789\n",
      "Epoch 00050: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7177 - acc: 0.7788 - val_loss: 0.4987 - val_acc: 0.8600\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7179 - acc: 0.7768\n",
      "Epoch 00051: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7179 - acc: 0.7768 - val_loss: 0.4979 - val_acc: 0.8602\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7779\n",
      "Epoch 00052: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7205 - acc: 0.7779 - val_loss: 0.4867 - val_acc: 0.8577\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7097 - acc: 0.7806\n",
      "Epoch 00053: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7098 - acc: 0.7805 - val_loss: 0.5003 - val_acc: 0.8537\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7054 - acc: 0.7814\n",
      "Epoch 00054: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7053 - acc: 0.7814 - val_loss: 0.4803 - val_acc: 0.8661\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7062 - acc: 0.7810\n",
      "Epoch 00055: val_loss did not improve from 0.47426\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7061 - acc: 0.7810 - val_loss: 0.5062 - val_acc: 0.8500\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.7847\n",
      "Epoch 00056: val_loss improved from 0.47426 to 0.46503, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/056-0.4650.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6922 - acc: 0.7847 - val_loss: 0.4650 - val_acc: 0.8612\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7848\n",
      "Epoch 00057: val_loss did not improve from 0.46503\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6959 - acc: 0.7848 - val_loss: 0.4860 - val_acc: 0.8626\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6884 - acc: 0.7860\n",
      "Epoch 00058: val_loss improved from 0.46503 to 0.46273, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/058-0.4627.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6884 - acc: 0.7860 - val_loss: 0.4627 - val_acc: 0.8684\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6814 - acc: 0.7890\n",
      "Epoch 00059: val_loss did not improve from 0.46273\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6814 - acc: 0.7890 - val_loss: 0.4654 - val_acc: 0.8670\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6740 - acc: 0.7911\n",
      "Epoch 00060: val_loss did not improve from 0.46273\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6740 - acc: 0.7911 - val_loss: 0.5074 - val_acc: 0.8539\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.7930\n",
      "Epoch 00061: val_loss did not improve from 0.46273\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6711 - acc: 0.7930 - val_loss: 0.4695 - val_acc: 0.8581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6637 - acc: 0.7953\n",
      "Epoch 00062: val_loss did not improve from 0.46273\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6636 - acc: 0.7953 - val_loss: 0.4662 - val_acc: 0.8672\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.7911\n",
      "Epoch 00063: val_loss improved from 0.46273 to 0.45205, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/063-0.4521.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6709 - acc: 0.7911 - val_loss: 0.4521 - val_acc: 0.8686\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7951\n",
      "Epoch 00064: val_loss did not improve from 0.45205\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6572 - acc: 0.7951 - val_loss: 0.4576 - val_acc: 0.8649\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6584 - acc: 0.7970\n",
      "Epoch 00065: val_loss improved from 0.45205 to 0.43855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/065-0.4385.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6584 - acc: 0.7970 - val_loss: 0.4385 - val_acc: 0.8721\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.7975\n",
      "Epoch 00066: val_loss did not improve from 0.43855\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6534 - acc: 0.7975 - val_loss: 0.4499 - val_acc: 0.8668\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.7980\n",
      "Epoch 00067: val_loss improved from 0.43855 to 0.43449, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/067-0.4345.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6451 - acc: 0.7981 - val_loss: 0.4345 - val_acc: 0.8735\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8003\n",
      "Epoch 00068: val_loss did not improve from 0.43449\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6440 - acc: 0.8003 - val_loss: 0.4526 - val_acc: 0.8621\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.7993\n",
      "Epoch 00069: val_loss did not improve from 0.43449\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6516 - acc: 0.7993 - val_loss: 0.4641 - val_acc: 0.8633\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8006\n",
      "Epoch 00070: val_loss did not improve from 0.43449\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6441 - acc: 0.8006 - val_loss: 0.4482 - val_acc: 0.8630\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.8030\n",
      "Epoch 00071: val_loss did not improve from 0.43449\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6368 - acc: 0.8030 - val_loss: 0.4612 - val_acc: 0.8607\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.8068\n",
      "Epoch 00072: val_loss did not improve from 0.43449\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6297 - acc: 0.8068 - val_loss: 0.4453 - val_acc: 0.8682\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6306 - acc: 0.8042\n",
      "Epoch 00073: val_loss improved from 0.43449 to 0.42587, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/073-0.4259.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6306 - acc: 0.8042 - val_loss: 0.4259 - val_acc: 0.8749\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6334 - acc: 0.8039\n",
      "Epoch 00074: val_loss did not improve from 0.42587\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6334 - acc: 0.8039 - val_loss: 0.4381 - val_acc: 0.8728\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.8053\n",
      "Epoch 00075: val_loss did not improve from 0.42587\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6243 - acc: 0.8053 - val_loss: 0.4569 - val_acc: 0.8626\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.8065\n",
      "Epoch 00076: val_loss did not improve from 0.42587\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6284 - acc: 0.8065 - val_loss: 0.4531 - val_acc: 0.8707\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.8077\n",
      "Epoch 00077: val_loss did not improve from 0.42587\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6175 - acc: 0.8077 - val_loss: 0.4319 - val_acc: 0.8735\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6168 - acc: 0.8106\n",
      "Epoch 00078: val_loss did not improve from 0.42587\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6167 - acc: 0.8106 - val_loss: 0.4293 - val_acc: 0.8714\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6151 - acc: 0.8096\n",
      "Epoch 00079: val_loss improved from 0.42587 to 0.41962, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/079-0.4196.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6150 - acc: 0.8096 - val_loss: 0.4196 - val_acc: 0.8786\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8119\n",
      "Epoch 00080: val_loss did not improve from 0.41962\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6106 - acc: 0.8119 - val_loss: 0.4491 - val_acc: 0.8675\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.8127\n",
      "Epoch 00081: val_loss did not improve from 0.41962\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6106 - acc: 0.8128 - val_loss: 0.4276 - val_acc: 0.8754\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.8127\n",
      "Epoch 00082: val_loss improved from 0.41962 to 0.40954, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/082-0.4095.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6086 - acc: 0.8127 - val_loss: 0.4095 - val_acc: 0.8782\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6061 - acc: 0.8131\n",
      "Epoch 00083: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6060 - acc: 0.8131 - val_loss: 0.4169 - val_acc: 0.8786\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6070 - acc: 0.8111\n",
      "Epoch 00084: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6071 - acc: 0.8111 - val_loss: 0.4274 - val_acc: 0.8758\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5991 - acc: 0.8138\n",
      "Epoch 00085: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5991 - acc: 0.8138 - val_loss: 0.4225 - val_acc: 0.8733\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8136\n",
      "Epoch 00086: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5964 - acc: 0.8136 - val_loss: 0.4216 - val_acc: 0.8733\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8145\n",
      "Epoch 00087: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5977 - acc: 0.8145 - val_loss: 0.4159 - val_acc: 0.8756\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.8180\n",
      "Epoch 00088: val_loss did not improve from 0.40954\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5886 - acc: 0.8179 - val_loss: 0.4173 - val_acc: 0.8779\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.8172\n",
      "Epoch 00089: val_loss improved from 0.40954 to 0.40499, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/089-0.4050.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5916 - acc: 0.8173 - val_loss: 0.4050 - val_acc: 0.8784\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5845 - acc: 0.8176\n",
      "Epoch 00090: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5845 - acc: 0.8176 - val_loss: 0.4236 - val_acc: 0.8765\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8203\n",
      "Epoch 00091: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5842 - acc: 0.8203 - val_loss: 0.4320 - val_acc: 0.8707\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5821 - acc: 0.8181\n",
      "Epoch 00092: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5821 - acc: 0.8180 - val_loss: 0.4063 - val_acc: 0.8796\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8187\n",
      "Epoch 00093: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5796 - acc: 0.8187 - val_loss: 0.4094 - val_acc: 0.8768\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.8213\n",
      "Epoch 00094: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5802 - acc: 0.8213 - val_loss: 0.4197 - val_acc: 0.8730\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.8226\n",
      "Epoch 00095: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5710 - acc: 0.8226 - val_loss: 0.4224 - val_acc: 0.8740\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8214\n",
      "Epoch 00096: val_loss did not improve from 0.40499\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5736 - acc: 0.8213 - val_loss: 0.4202 - val_acc: 0.8754\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.8214\n",
      "Epoch 00097: val_loss improved from 0.40499 to 0.40118, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/097-0.4012.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5803 - acc: 0.8214 - val_loss: 0.4012 - val_acc: 0.8796\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8231\n",
      "Epoch 00098: val_loss did not improve from 0.40118\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5644 - acc: 0.8231 - val_loss: 0.4530 - val_acc: 0.8647\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8233\n",
      "Epoch 00099: val_loss did not improve from 0.40118\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5695 - acc: 0.8233 - val_loss: 0.4209 - val_acc: 0.8726\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.8250\n",
      "Epoch 00100: val_loss did not improve from 0.40118\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5657 - acc: 0.8250 - val_loss: 0.4119 - val_acc: 0.8768\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.8271\n",
      "Epoch 00101: val_loss did not improve from 0.40118\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5610 - acc: 0.8271 - val_loss: 0.4028 - val_acc: 0.8798\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8264\n",
      "Epoch 00102: val_loss improved from 0.40118 to 0.39277, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/102-0.3928.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5590 - acc: 0.8265 - val_loss: 0.3928 - val_acc: 0.8810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.8277\n",
      "Epoch 00103: val_loss improved from 0.39277 to 0.38986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/103-0.3899.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5530 - acc: 0.8277 - val_loss: 0.3899 - val_acc: 0.8842\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8288\n",
      "Epoch 00104: val_loss improved from 0.38986 to 0.38761, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/104-0.3876.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5510 - acc: 0.8289 - val_loss: 0.3876 - val_acc: 0.8828\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8288\n",
      "Epoch 00105: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5508 - acc: 0.8287 - val_loss: 0.4161 - val_acc: 0.8779\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8280\n",
      "Epoch 00106: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5545 - acc: 0.8280 - val_loss: 0.4381 - val_acc: 0.8705\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.8310\n",
      "Epoch 00107: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5460 - acc: 0.8311 - val_loss: 0.3894 - val_acc: 0.8814\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.8301\n",
      "Epoch 00108: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5437 - acc: 0.8301 - val_loss: 0.3880 - val_acc: 0.8831\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.8311\n",
      "Epoch 00109: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5480 - acc: 0.8311 - val_loss: 0.3912 - val_acc: 0.8845\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.8273\n",
      "Epoch 00110: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5490 - acc: 0.8273 - val_loss: 0.3980 - val_acc: 0.8796\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8325\n",
      "Epoch 00111: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5458 - acc: 0.8325 - val_loss: 0.3976 - val_acc: 0.8793\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8345\n",
      "Epoch 00112: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5383 - acc: 0.8345 - val_loss: 0.4045 - val_acc: 0.8789\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.8321\n",
      "Epoch 00113: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5412 - acc: 0.8321 - val_loss: 0.4030 - val_acc: 0.8810\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.8299\n",
      "Epoch 00114: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5426 - acc: 0.8299 - val_loss: 0.4132 - val_acc: 0.8747\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8333\n",
      "Epoch 00115: val_loss did not improve from 0.38761\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5375 - acc: 0.8333 - val_loss: 0.4128 - val_acc: 0.8733\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.8357\n",
      "Epoch 00116: val_loss improved from 0.38761 to 0.38393, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/116-0.3839.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5313 - acc: 0.8357 - val_loss: 0.3839 - val_acc: 0.8889\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.8329\n",
      "Epoch 00117: val_loss improved from 0.38393 to 0.37750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/117-0.3775.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5380 - acc: 0.8329 - val_loss: 0.3775 - val_acc: 0.8868\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8347\n",
      "Epoch 00118: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5308 - acc: 0.8347 - val_loss: 0.4034 - val_acc: 0.8756\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8345\n",
      "Epoch 00119: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5315 - acc: 0.8345 - val_loss: 0.3830 - val_acc: 0.8831\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8336\n",
      "Epoch 00120: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5318 - acc: 0.8336 - val_loss: 0.3887 - val_acc: 0.8838\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8364\n",
      "Epoch 00121: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5278 - acc: 0.8364 - val_loss: 0.4187 - val_acc: 0.8740\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8386\n",
      "Epoch 00122: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5277 - acc: 0.8386 - val_loss: 0.3833 - val_acc: 0.8856\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8343\n",
      "Epoch 00123: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5286 - acc: 0.8343 - val_loss: 0.4072 - val_acc: 0.8775\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.8340\n",
      "Epoch 00124: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5264 - acc: 0.8340 - val_loss: 0.4074 - val_acc: 0.8807\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8407\n",
      "Epoch 00125: val_loss did not improve from 0.37750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5215 - acc: 0.8408 - val_loss: 0.3829 - val_acc: 0.8861\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8400\n",
      "Epoch 00126: val_loss improved from 0.37750 to 0.37616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/126-0.3762.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5086 - acc: 0.8400 - val_loss: 0.3762 - val_acc: 0.8854\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8360\n",
      "Epoch 00127: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5188 - acc: 0.8360 - val_loss: 0.3807 - val_acc: 0.8877\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8386\n",
      "Epoch 00128: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5157 - acc: 0.8386 - val_loss: 0.3779 - val_acc: 0.8856\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8388\n",
      "Epoch 00129: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5140 - acc: 0.8388 - val_loss: 0.4023 - val_acc: 0.8770\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8417\n",
      "Epoch 00130: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5077 - acc: 0.8417 - val_loss: 0.3819 - val_acc: 0.8880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8396\n",
      "Epoch 00131: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5131 - acc: 0.8396 - val_loss: 0.3857 - val_acc: 0.8831\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8391\n",
      "Epoch 00132: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5108 - acc: 0.8390 - val_loss: 0.3824 - val_acc: 0.8849\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8396\n",
      "Epoch 00133: val_loss did not improve from 0.37616\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5105 - acc: 0.8396 - val_loss: 0.3958 - val_acc: 0.8849\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8420\n",
      "Epoch 00134: val_loss improved from 0.37616 to 0.37481, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/134-0.3748.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5058 - acc: 0.8420 - val_loss: 0.3748 - val_acc: 0.8905\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8429\n",
      "Epoch 00135: val_loss did not improve from 0.37481\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5044 - acc: 0.8428 - val_loss: 0.3887 - val_acc: 0.8847\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8438\n",
      "Epoch 00136: val_loss did not improve from 0.37481\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5043 - acc: 0.8438 - val_loss: 0.4448 - val_acc: 0.8612\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8430\n",
      "Epoch 00137: val_loss did not improve from 0.37481\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5042 - acc: 0.8430 - val_loss: 0.3845 - val_acc: 0.8884\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8430\n",
      "Epoch 00138: val_loss improved from 0.37481 to 0.37076, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/138-0.3708.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5012 - acc: 0.8430 - val_loss: 0.3708 - val_acc: 0.8921\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8442\n",
      "Epoch 00139: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5000 - acc: 0.8442 - val_loss: 0.4014 - val_acc: 0.8814\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8451\n",
      "Epoch 00140: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4981 - acc: 0.8451 - val_loss: 0.3718 - val_acc: 0.8870\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8467\n",
      "Epoch 00141: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4962 - acc: 0.8467 - val_loss: 0.4018 - val_acc: 0.8791\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8445\n",
      "Epoch 00142: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4945 - acc: 0.8445 - val_loss: 0.3803 - val_acc: 0.8889\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8452\n",
      "Epoch 00143: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4917 - acc: 0.8452 - val_loss: 0.3740 - val_acc: 0.8868\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.8461\n",
      "Epoch 00144: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4933 - acc: 0.8461 - val_loss: 0.3962 - val_acc: 0.8859\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8459\n",
      "Epoch 00145: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4881 - acc: 0.8459 - val_loss: 0.3745 - val_acc: 0.8861\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8480\n",
      "Epoch 00146: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4857 - acc: 0.8480 - val_loss: 0.3811 - val_acc: 0.8842\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8479\n",
      "Epoch 00147: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4865 - acc: 0.8479 - val_loss: 0.3942 - val_acc: 0.8838\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8476\n",
      "Epoch 00148: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4860 - acc: 0.8476 - val_loss: 0.3778 - val_acc: 0.8912\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8469\n",
      "Epoch 00149: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4890 - acc: 0.8469 - val_loss: 0.3859 - val_acc: 0.8833\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8478\n",
      "Epoch 00150: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4832 - acc: 0.8478 - val_loss: 0.4008 - val_acc: 0.8784\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8504\n",
      "Epoch 00151: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4765 - acc: 0.8504 - val_loss: 0.3799 - val_acc: 0.8819\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8513\n",
      "Epoch 00152: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4746 - acc: 0.8513 - val_loss: 0.3803 - val_acc: 0.8849\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4811 - acc: 0.8508\n",
      "Epoch 00153: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4811 - acc: 0.8508 - val_loss: 0.3814 - val_acc: 0.8870\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8511\n",
      "Epoch 00154: val_loss did not improve from 0.37076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4800 - acc: 0.8511 - val_loss: 0.3787 - val_acc: 0.8866\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8488\n",
      "Epoch 00155: val_loss improved from 0.37076 to 0.37053, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/155-0.3705.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4806 - acc: 0.8488 - val_loss: 0.3705 - val_acc: 0.8863\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8515\n",
      "Epoch 00156: val_loss did not improve from 0.37053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4773 - acc: 0.8515 - val_loss: 0.4252 - val_acc: 0.8733\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8511\n",
      "Epoch 00157: val_loss did not improve from 0.37053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4712 - acc: 0.8511 - val_loss: 0.3936 - val_acc: 0.8845\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8508\n",
      "Epoch 00158: val_loss improved from 0.37053 to 0.36415, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/158-0.3642.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4732 - acc: 0.8508 - val_loss: 0.3642 - val_acc: 0.8919\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8523\n",
      "Epoch 00159: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4703 - acc: 0.8522 - val_loss: 0.3961 - val_acc: 0.8814\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8536\n",
      "Epoch 00160: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4685 - acc: 0.8536 - val_loss: 0.4163 - val_acc: 0.8744\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8543\n",
      "Epoch 00161: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4655 - acc: 0.8543 - val_loss: 0.3860 - val_acc: 0.8805\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8551\n",
      "Epoch 00162: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4634 - acc: 0.8551 - val_loss: 0.3752 - val_acc: 0.8859\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8553\n",
      "Epoch 00163: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4648 - acc: 0.8552 - val_loss: 0.4004 - val_acc: 0.8775\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4719 - acc: 0.8507\n",
      "Epoch 00164: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4721 - acc: 0.8506 - val_loss: 0.3816 - val_acc: 0.8889\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8529\n",
      "Epoch 00165: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4678 - acc: 0.8528 - val_loss: 0.3661 - val_acc: 0.8898\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8560\n",
      "Epoch 00166: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4626 - acc: 0.8560 - val_loss: 0.3659 - val_acc: 0.8952\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8526\n",
      "Epoch 00167: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4696 - acc: 0.8525 - val_loss: 0.3712 - val_acc: 0.8875\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8560\n",
      "Epoch 00168: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4594 - acc: 0.8560 - val_loss: 0.3731 - val_acc: 0.8891\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8554\n",
      "Epoch 00169: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4625 - acc: 0.8555 - val_loss: 0.3905 - val_acc: 0.8831\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8579\n",
      "Epoch 00170: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4610 - acc: 0.8579 - val_loss: 0.4044 - val_acc: 0.8775\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8551\n",
      "Epoch 00171: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4672 - acc: 0.8552 - val_loss: 0.3903 - val_acc: 0.8828\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.8573\n",
      "Epoch 00172: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4555 - acc: 0.8573 - val_loss: 0.4081 - val_acc: 0.8761\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4602 - acc: 0.8540\n",
      "Epoch 00173: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4602 - acc: 0.8541 - val_loss: 0.3643 - val_acc: 0.8896\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8584\n",
      "Epoch 00174: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4545 - acc: 0.8584 - val_loss: 0.3945 - val_acc: 0.8828\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8579\n",
      "Epoch 00175: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4513 - acc: 0.8579 - val_loss: 0.3731 - val_acc: 0.8912\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8620\n",
      "Epoch 00176: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4475 - acc: 0.8620 - val_loss: 0.3835 - val_acc: 0.8875\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8581\n",
      "Epoch 00177: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4533 - acc: 0.8581 - val_loss: 0.3937 - val_acc: 0.8796\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.8564\n",
      "Epoch 00178: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4516 - acc: 0.8564 - val_loss: 0.3862 - val_acc: 0.8847\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8585\n",
      "Epoch 00179: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4574 - acc: 0.8584 - val_loss: 0.3805 - val_acc: 0.8938\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.8595\n",
      "Epoch 00180: val_loss did not improve from 0.36415\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4472 - acc: 0.8595 - val_loss: 0.3731 - val_acc: 0.8910\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8571\n",
      "Epoch 00181: val_loss improved from 0.36415 to 0.36167, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/181-0.3617.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4532 - acc: 0.8571 - val_loss: 0.3617 - val_acc: 0.8933\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8605\n",
      "Epoch 00182: val_loss improved from 0.36167 to 0.35805, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/182-0.3581.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4456 - acc: 0.8605 - val_loss: 0.3581 - val_acc: 0.8961\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8591\n",
      "Epoch 00183: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4468 - acc: 0.8591 - val_loss: 0.3859 - val_acc: 0.8840\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8601\n",
      "Epoch 00184: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4426 - acc: 0.8601 - val_loss: 0.3993 - val_acc: 0.8798\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8614\n",
      "Epoch 00185: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4438 - acc: 0.8614 - val_loss: 0.3792 - val_acc: 0.8915\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8619\n",
      "Epoch 00186: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4422 - acc: 0.8619 - val_loss: 0.3782 - val_acc: 0.8912\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8614\n",
      "Epoch 00187: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4398 - acc: 0.8614 - val_loss: 0.4149 - val_acc: 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8591\n",
      "Epoch 00188: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4427 - acc: 0.8591 - val_loss: 0.3622 - val_acc: 0.8915\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8614\n",
      "Epoch 00189: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4413 - acc: 0.8614 - val_loss: 0.3711 - val_acc: 0.8921\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8641\n",
      "Epoch 00190: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4351 - acc: 0.8641 - val_loss: 0.3701 - val_acc: 0.8870\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8600\n",
      "Epoch 00191: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4426 - acc: 0.8600 - val_loss: 0.3619 - val_acc: 0.8949\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8587\n",
      "Epoch 00192: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4446 - acc: 0.8586 - val_loss: 0.3627 - val_acc: 0.8954\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8640\n",
      "Epoch 00193: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4331 - acc: 0.8641 - val_loss: 0.3709 - val_acc: 0.8845\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8631\n",
      "Epoch 00194: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4377 - acc: 0.8631 - val_loss: 0.4234 - val_acc: 0.8737\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8638\n",
      "Epoch 00195: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4347 - acc: 0.8637 - val_loss: 0.3682 - val_acc: 0.8931\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.8626\n",
      "Epoch 00196: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4342 - acc: 0.8626 - val_loss: 0.3651 - val_acc: 0.8935\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8621\n",
      "Epoch 00197: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4366 - acc: 0.8621 - val_loss: 0.3824 - val_acc: 0.8938\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8653\n",
      "Epoch 00198: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4286 - acc: 0.8653 - val_loss: 0.3851 - val_acc: 0.8877\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8638\n",
      "Epoch 00199: val_loss did not improve from 0.35805\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4299 - acc: 0.8638 - val_loss: 0.3611 - val_acc: 0.8912\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8645\n",
      "Epoch 00200: val_loss improved from 0.35805 to 0.35750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_4_conv_checkpoint/200-0.3575.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4313 - acc: 0.8645 - val_loss: 0.3575 - val_acc: 0.8956\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8656\n",
      "Epoch 00201: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4273 - acc: 0.8656 - val_loss: 0.3688 - val_acc: 0.8959\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8655\n",
      "Epoch 00202: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4266 - acc: 0.8656 - val_loss: 0.3687 - val_acc: 0.8938\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8652\n",
      "Epoch 00203: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4296 - acc: 0.8652 - val_loss: 0.3831 - val_acc: 0.8910\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8661\n",
      "Epoch 00204: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4259 - acc: 0.8661 - val_loss: 0.3717 - val_acc: 0.8896\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8688\n",
      "Epoch 00205: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4184 - acc: 0.8688 - val_loss: 0.3804 - val_acc: 0.8866\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.8660\n",
      "Epoch 00206: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4230 - acc: 0.8660 - val_loss: 0.3753 - val_acc: 0.8880\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8648\n",
      "Epoch 00207: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4241 - acc: 0.8648 - val_loss: 0.3595 - val_acc: 0.8973\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8679\n",
      "Epoch 00208: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4229 - acc: 0.8678 - val_loss: 0.3700 - val_acc: 0.8884\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8671\n",
      "Epoch 00209: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4236 - acc: 0.8671 - val_loss: 0.3791 - val_acc: 0.8887\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8666\n",
      "Epoch 00210: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4254 - acc: 0.8666 - val_loss: 0.3657 - val_acc: 0.8891\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8657\n",
      "Epoch 00211: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4206 - acc: 0.8658 - val_loss: 0.3822 - val_acc: 0.8880\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8685\n",
      "Epoch 00212: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4212 - acc: 0.8685 - val_loss: 0.3700 - val_acc: 0.8942\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8687\n",
      "Epoch 00213: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4144 - acc: 0.8688 - val_loss: 0.3676 - val_acc: 0.8947\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8690\n",
      "Epoch 00214: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4155 - acc: 0.8690 - val_loss: 0.3594 - val_acc: 0.8970\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8692\n",
      "Epoch 00215: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4184 - acc: 0.8692 - val_loss: 0.3644 - val_acc: 0.8947\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8704\n",
      "Epoch 00216: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4083 - acc: 0.8704 - val_loss: 0.3713 - val_acc: 0.8891\n",
      "Epoch 217/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8693\n",
      "Epoch 00217: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4144 - acc: 0.8693 - val_loss: 0.3892 - val_acc: 0.8863\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8691\n",
      "Epoch 00218: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4161 - acc: 0.8690 - val_loss: 0.3590 - val_acc: 0.8915\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8700\n",
      "Epoch 00219: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4107 - acc: 0.8700 - val_loss: 0.3586 - val_acc: 0.8938\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8698\n",
      "Epoch 00220: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4127 - acc: 0.8697 - val_loss: 0.3676 - val_acc: 0.8947\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8717\n",
      "Epoch 00221: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4065 - acc: 0.8717 - val_loss: 0.3593 - val_acc: 0.8959\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8711\n",
      "Epoch 00222: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4063 - acc: 0.8712 - val_loss: 0.3602 - val_acc: 0.8956\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8714\n",
      "Epoch 00223: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4092 - acc: 0.8714 - val_loss: 0.3868 - val_acc: 0.8854\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8704\n",
      "Epoch 00224: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4096 - acc: 0.8704 - val_loss: 0.3720 - val_acc: 0.8901\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8711\n",
      "Epoch 00225: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4072 - acc: 0.8712 - val_loss: 0.3872 - val_acc: 0.8859\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8721\n",
      "Epoch 00226: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4058 - acc: 0.8721 - val_loss: 0.3917 - val_acc: 0.8847\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8738\n",
      "Epoch 00227: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4028 - acc: 0.8738 - val_loss: 0.3884 - val_acc: 0.8891\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8752\n",
      "Epoch 00228: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3988 - acc: 0.8752 - val_loss: 0.3735 - val_acc: 0.8884\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8692\n",
      "Epoch 00229: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4181 - acc: 0.8692 - val_loss: 0.4163 - val_acc: 0.8786\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8732\n",
      "Epoch 00230: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4049 - acc: 0.8732 - val_loss: 0.3793 - val_acc: 0.8875\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8723\n",
      "Epoch 00231: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4026 - acc: 0.8722 - val_loss: 0.3620 - val_acc: 0.8945\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8707\n",
      "Epoch 00232: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4063 - acc: 0.8707 - val_loss: 0.3619 - val_acc: 0.8921\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8733\n",
      "Epoch 00233: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3986 - acc: 0.8733 - val_loss: 0.3676 - val_acc: 0.8940\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8719\n",
      "Epoch 00234: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4002 - acc: 0.8719 - val_loss: 0.3778 - val_acc: 0.8891\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8731\n",
      "Epoch 00235: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4033 - acc: 0.8731 - val_loss: 0.4001 - val_acc: 0.8812\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8741\n",
      "Epoch 00236: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4002 - acc: 0.8741 - val_loss: 0.3807 - val_acc: 0.8884\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8733\n",
      "Epoch 00237: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4008 - acc: 0.8733 - val_loss: 0.3675 - val_acc: 0.8910\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8739\n",
      "Epoch 00238: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3968 - acc: 0.8740 - val_loss: 0.3614 - val_acc: 0.8961\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8753\n",
      "Epoch 00239: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3951 - acc: 0.8753 - val_loss: 0.4093 - val_acc: 0.8798\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8776\n",
      "Epoch 00240: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3922 - acc: 0.8776 - val_loss: 0.3696 - val_acc: 0.8961\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8758\n",
      "Epoch 00241: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3958 - acc: 0.8758 - val_loss: 0.3761 - val_acc: 0.8894\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8724\n",
      "Epoch 00242: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4020 - acc: 0.8724 - val_loss: 0.4317 - val_acc: 0.8707\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8738\n",
      "Epoch 00243: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3937 - acc: 0.8738 - val_loss: 0.4046 - val_acc: 0.8819\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8764\n",
      "Epoch 00244: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3937 - acc: 0.8764 - val_loss: 0.3880 - val_acc: 0.8863\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8764\n",
      "Epoch 00245: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3921 - acc: 0.8763 - val_loss: 0.3714 - val_acc: 0.8933\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8725\n",
      "Epoch 00246: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3971 - acc: 0.8725 - val_loss: 0.3578 - val_acc: 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8764\n",
      "Epoch 00247: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3890 - acc: 0.8763 - val_loss: 0.3722 - val_acc: 0.8928\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8764\n",
      "Epoch 00248: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3915 - acc: 0.8764 - val_loss: 0.3770 - val_acc: 0.8891\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8769\n",
      "Epoch 00249: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3897 - acc: 0.8769 - val_loss: 0.3939 - val_acc: 0.8852\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8730\n",
      "Epoch 00250: val_loss did not improve from 0.35750\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4014 - acc: 0.8730 - val_loss: 0.3938 - val_acc: 0.8849\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 571us/sample - loss: 0.4237 - acc: 0.8698\n",
      "Loss: 0.42367817113951606 Accuracy: 0.8697819\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9130 - acc: 0.1490\n",
      "Epoch 00001: val_loss improved from inf to 2.07288, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/001-2.0729.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 4.9128 - acc: 0.1490 - val_loss: 2.0729 - val_acc: 0.3173\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1548 - acc: 0.2552\n",
      "Epoch 00002: val_loss improved from 2.07288 to 1.50295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/002-1.5029.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 3.1548 - acc: 0.2552 - val_loss: 1.5029 - val_acc: 0.5532\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4782 - acc: 0.3548\n",
      "Epoch 00003: val_loss improved from 1.50295 to 1.15402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/003-1.1540.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.4781 - acc: 0.3548 - val_loss: 1.1540 - val_acc: 0.6671\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9887 - acc: 0.4402\n",
      "Epoch 00004: val_loss improved from 1.15402 to 0.85410, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/004-0.8541.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.9887 - acc: 0.4402 - val_loss: 0.8541 - val_acc: 0.7624\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5981 - acc: 0.5173\n",
      "Epoch 00005: val_loss improved from 0.85410 to 0.69543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/005-0.6954.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.5979 - acc: 0.5173 - val_loss: 0.6954 - val_acc: 0.7922\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3455 - acc: 0.5817\n",
      "Epoch 00006: val_loss improved from 0.69543 to 0.61468, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/006-0.6147.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3455 - acc: 0.5817 - val_loss: 0.6147 - val_acc: 0.8162\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1874 - acc: 0.6277\n",
      "Epoch 00007: val_loss improved from 0.61468 to 0.55755, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/007-0.5575.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1878 - acc: 0.6276 - val_loss: 0.5575 - val_acc: 0.8353\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0834 - acc: 0.6571\n",
      "Epoch 00008: val_loss improved from 0.55755 to 0.52370, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/008-0.5237.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0834 - acc: 0.6571 - val_loss: 0.5237 - val_acc: 0.8486\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9906 - acc: 0.6874\n",
      "Epoch 00009: val_loss did not improve from 0.52370\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9907 - acc: 0.6874 - val_loss: 0.5363 - val_acc: 0.8486\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7144\n",
      "Epoch 00010: val_loss improved from 0.52370 to 0.44831, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/010-0.4483.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9078 - acc: 0.7144 - val_loss: 0.4483 - val_acc: 0.8751\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8443 - acc: 0.7326\n",
      "Epoch 00011: val_loss did not improve from 0.44831\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8442 - acc: 0.7326 - val_loss: 0.4569 - val_acc: 0.8756\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7868 - acc: 0.7525\n",
      "Epoch 00012: val_loss improved from 0.44831 to 0.41886, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/012-0.4189.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7867 - acc: 0.7526 - val_loss: 0.4189 - val_acc: 0.8814\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7649\n",
      "Epoch 00013: val_loss did not improve from 0.41886\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7507 - acc: 0.7649 - val_loss: 0.4374 - val_acc: 0.8749\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7222 - acc: 0.7740\n",
      "Epoch 00014: val_loss improved from 0.41886 to 0.40624, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/014-0.4062.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7222 - acc: 0.7740 - val_loss: 0.4062 - val_acc: 0.8807\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.7856\n",
      "Epoch 00015: val_loss improved from 0.40624 to 0.37549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/015-0.3755.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6836 - acc: 0.7856 - val_loss: 0.3755 - val_acc: 0.8940\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6497 - acc: 0.7949\n",
      "Epoch 00016: val_loss did not improve from 0.37549\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6499 - acc: 0.7948 - val_loss: 0.3837 - val_acc: 0.8831\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6317 - acc: 0.8047\n",
      "Epoch 00017: val_loss improved from 0.37549 to 0.36515, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/017-0.3652.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6319 - acc: 0.8047 - val_loss: 0.3652 - val_acc: 0.8989\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6082 - acc: 0.8085\n",
      "Epoch 00018: val_loss improved from 0.36515 to 0.34575, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/018-0.3457.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6081 - acc: 0.8085 - val_loss: 0.3457 - val_acc: 0.9024\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8170\n",
      "Epoch 00019: val_loss did not improve from 0.34575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5861 - acc: 0.8170 - val_loss: 0.3530 - val_acc: 0.8945\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8233\n",
      "Epoch 00020: val_loss improved from 0.34575 to 0.33512, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/020-0.3351.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5707 - acc: 0.8233 - val_loss: 0.3351 - val_acc: 0.9036\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.8280\n",
      "Epoch 00021: val_loss improved from 0.33512 to 0.31211, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/021-0.3121.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5569 - acc: 0.8280 - val_loss: 0.3121 - val_acc: 0.9110\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8336\n",
      "Epoch 00022: val_loss improved from 0.31211 to 0.31161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/022-0.3116.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5362 - acc: 0.8337 - val_loss: 0.3116 - val_acc: 0.9103\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8362\n",
      "Epoch 00023: val_loss did not improve from 0.31161\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5270 - acc: 0.8362 - val_loss: 0.3406 - val_acc: 0.8994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8433\n",
      "Epoch 00024: val_loss improved from 0.31161 to 0.30875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/024-0.3087.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5085 - acc: 0.8433 - val_loss: 0.3087 - val_acc: 0.9113\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8441\n",
      "Epoch 00025: val_loss improved from 0.30875 to 0.29542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/025-0.2954.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4995 - acc: 0.8441 - val_loss: 0.2954 - val_acc: 0.9168\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8506\n",
      "Epoch 00026: val_loss did not improve from 0.29542\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4822 - acc: 0.8506 - val_loss: 0.3020 - val_acc: 0.9182\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.8558\n",
      "Epoch 00027: val_loss did not improve from 0.29542\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4675 - acc: 0.8558 - val_loss: 0.2960 - val_acc: 0.9101\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8545\n",
      "Epoch 00028: val_loss did not improve from 0.29542\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4679 - acc: 0.8545 - val_loss: 0.3090 - val_acc: 0.9080\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8597\n",
      "Epoch 00029: val_loss improved from 0.29542 to 0.28350, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/029-0.2835.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4537 - acc: 0.8597 - val_loss: 0.2835 - val_acc: 0.9117\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4487 - acc: 0.8608\n",
      "Epoch 00030: val_loss improved from 0.28350 to 0.27517, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/030-0.2752.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4488 - acc: 0.8607 - val_loss: 0.2752 - val_acc: 0.9213\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4377 - acc: 0.8658\n",
      "Epoch 00031: val_loss did not improve from 0.27517\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4377 - acc: 0.8658 - val_loss: 0.2915 - val_acc: 0.9173\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8683\n",
      "Epoch 00032: val_loss did not improve from 0.27517\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4252 - acc: 0.8684 - val_loss: 0.2928 - val_acc: 0.9126\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8702\n",
      "Epoch 00033: val_loss improved from 0.27517 to 0.26550, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/033-0.2655.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4142 - acc: 0.8702 - val_loss: 0.2655 - val_acc: 0.9236\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8709\n",
      "Epoch 00034: val_loss did not improve from 0.26550\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4164 - acc: 0.8709 - val_loss: 0.2760 - val_acc: 0.9185\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8731\n",
      "Epoch 00035: val_loss did not improve from 0.26550\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4139 - acc: 0.8731 - val_loss: 0.2818 - val_acc: 0.9194\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8783\n",
      "Epoch 00036: val_loss improved from 0.26550 to 0.25423, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/036-0.2542.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3947 - acc: 0.8783 - val_loss: 0.2542 - val_acc: 0.9290\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8811\n",
      "Epoch 00037: val_loss did not improve from 0.25423\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3877 - acc: 0.8811 - val_loss: 0.2691 - val_acc: 0.9238\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8802\n",
      "Epoch 00038: val_loss did not improve from 0.25423\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3862 - acc: 0.8802 - val_loss: 0.2614 - val_acc: 0.9271\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8845\n",
      "Epoch 00039: val_loss did not improve from 0.25423\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3777 - acc: 0.8846 - val_loss: 0.2652 - val_acc: 0.9257\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8849\n",
      "Epoch 00040: val_loss did not improve from 0.25423\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3703 - acc: 0.8849 - val_loss: 0.2660 - val_acc: 0.9185\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8883\n",
      "Epoch 00041: val_loss improved from 0.25423 to 0.23710, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/041-0.2371.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3679 - acc: 0.8884 - val_loss: 0.2371 - val_acc: 0.9304\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8876\n",
      "Epoch 00042: val_loss did not improve from 0.23710\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3679 - acc: 0.8875 - val_loss: 0.2493 - val_acc: 0.9301\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8883\n",
      "Epoch 00043: val_loss did not improve from 0.23710\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3628 - acc: 0.8883 - val_loss: 0.2403 - val_acc: 0.9350\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8901\n",
      "Epoch 00044: val_loss did not improve from 0.23710\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3566 - acc: 0.8901 - val_loss: 0.2385 - val_acc: 0.9290\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8958\n",
      "Epoch 00045: val_loss improved from 0.23710 to 0.23419, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/045-0.2342.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3439 - acc: 0.8957 - val_loss: 0.2342 - val_acc: 0.9334\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8941\n",
      "Epoch 00046: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3462 - acc: 0.8940 - val_loss: 0.2389 - val_acc: 0.9292\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8948\n",
      "Epoch 00047: val_loss improved from 0.23419 to 0.23088, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/047-0.2309.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3401 - acc: 0.8949 - val_loss: 0.2309 - val_acc: 0.9315\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8975\n",
      "Epoch 00048: val_loss did not improve from 0.23088\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3292 - acc: 0.8975 - val_loss: 0.2426 - val_acc: 0.9276\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8963\n",
      "Epoch 00049: val_loss did not improve from 0.23088\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3313 - acc: 0.8963 - val_loss: 0.2465 - val_acc: 0.9264\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9003\n",
      "Epoch 00050: val_loss did not improve from 0.23088\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3228 - acc: 0.9003 - val_loss: 0.2396 - val_acc: 0.9348\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8990\n",
      "Epoch 00051: val_loss did not improve from 0.23088\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3265 - acc: 0.8990 - val_loss: 0.2386 - val_acc: 0.9313\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9024\n",
      "Epoch 00052: val_loss improved from 0.23088 to 0.22669, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/052-0.2267.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3145 - acc: 0.9025 - val_loss: 0.2267 - val_acc: 0.9320\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9046\n",
      "Epoch 00053: val_loss did not improve from 0.22669\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3116 - acc: 0.9046 - val_loss: 0.2708 - val_acc: 0.9208\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9045\n",
      "Epoch 00054: val_loss did not improve from 0.22669\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3110 - acc: 0.9045 - val_loss: 0.2794 - val_acc: 0.9201\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9043\n",
      "Epoch 00055: val_loss improved from 0.22669 to 0.22437, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/055-0.2244.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3092 - acc: 0.9043 - val_loss: 0.2244 - val_acc: 0.9350\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9092\n",
      "Epoch 00056: val_loss improved from 0.22437 to 0.21470, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/056-0.2147.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2988 - acc: 0.9092 - val_loss: 0.2147 - val_acc: 0.9404\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9054\n",
      "Epoch 00057: val_loss did not improve from 0.21470\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3042 - acc: 0.9054 - val_loss: 0.2538 - val_acc: 0.9266\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9098\n",
      "Epoch 00058: val_loss did not improve from 0.21470\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2945 - acc: 0.9097 - val_loss: 0.2159 - val_acc: 0.9383\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9091\n",
      "Epoch 00059: val_loss did not improve from 0.21470\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2974 - acc: 0.9091 - val_loss: 0.2191 - val_acc: 0.9355\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9090\n",
      "Epoch 00060: val_loss did not improve from 0.21470\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2924 - acc: 0.9091 - val_loss: 0.2415 - val_acc: 0.9245\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9113\n",
      "Epoch 00061: val_loss improved from 0.21470 to 0.21034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/061-0.2103.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2877 - acc: 0.9113 - val_loss: 0.2103 - val_acc: 0.9380\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9129\n",
      "Epoch 00062: val_loss did not improve from 0.21034\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2823 - acc: 0.9129 - val_loss: 0.2153 - val_acc: 0.9401\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9131\n",
      "Epoch 00063: val_loss did not improve from 0.21034\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2785 - acc: 0.9131 - val_loss: 0.2248 - val_acc: 0.9334\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9146\n",
      "Epoch 00064: val_loss improved from 0.21034 to 0.20526, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/064-0.2053.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2770 - acc: 0.9147 - val_loss: 0.2053 - val_acc: 0.9399\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9154\n",
      "Epoch 00065: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2753 - acc: 0.9154 - val_loss: 0.2136 - val_acc: 0.9383\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9166\n",
      "Epoch 00066: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2716 - acc: 0.9166 - val_loss: 0.2206 - val_acc: 0.9352\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9167\n",
      "Epoch 00067: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2686 - acc: 0.9167 - val_loss: 0.2150 - val_acc: 0.9352\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9180\n",
      "Epoch 00068: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2701 - acc: 0.9180 - val_loss: 0.2245 - val_acc: 0.9362\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9189\n",
      "Epoch 00069: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2643 - acc: 0.9188 - val_loss: 0.2322 - val_acc: 0.9329\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9180\n",
      "Epoch 00070: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2623 - acc: 0.9180 - val_loss: 0.2239 - val_acc: 0.9322\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9200\n",
      "Epoch 00071: val_loss did not improve from 0.20526\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2600 - acc: 0.9200 - val_loss: 0.2083 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9210\n",
      "Epoch 00072: val_loss improved from 0.20526 to 0.20345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/072-0.2035.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2534 - acc: 0.9210 - val_loss: 0.2035 - val_acc: 0.9397\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9214\n",
      "Epoch 00073: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2546 - acc: 0.9214 - val_loss: 0.2103 - val_acc: 0.9404\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9215\n",
      "Epoch 00074: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2527 - acc: 0.9215 - val_loss: 0.2093 - val_acc: 0.9413\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9232\n",
      "Epoch 00075: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2486 - acc: 0.9232 - val_loss: 0.2165 - val_acc: 0.9345\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9227\n",
      "Epoch 00076: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2484 - acc: 0.9227 - val_loss: 0.2232 - val_acc: 0.9362\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9245\n",
      "Epoch 00077: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2432 - acc: 0.9244 - val_loss: 0.2088 - val_acc: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9253\n",
      "Epoch 00078: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2403 - acc: 0.9253 - val_loss: 0.2128 - val_acc: 0.9371\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9263\n",
      "Epoch 00079: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2384 - acc: 0.9263 - val_loss: 0.2047 - val_acc: 0.9394\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9260\n",
      "Epoch 00080: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2376 - acc: 0.9260 - val_loss: 0.2193 - val_acc: 0.9359\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9278\n",
      "Epoch 00081: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2355 - acc: 0.9278 - val_loss: 0.2153 - val_acc: 0.9359\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9256\n",
      "Epoch 00082: val_loss did not improve from 0.20345\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2384 - acc: 0.9256 - val_loss: 0.2082 - val_acc: 0.9413\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9267\n",
      "Epoch 00083: val_loss improved from 0.20345 to 0.20307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/083-0.2031.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2341 - acc: 0.9267 - val_loss: 0.2031 - val_acc: 0.9399\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9277\n",
      "Epoch 00084: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2305 - acc: 0.9278 - val_loss: 0.2241 - val_acc: 0.9317\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9297\n",
      "Epoch 00085: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2260 - acc: 0.9297 - val_loss: 0.2255 - val_acc: 0.9359\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9292\n",
      "Epoch 00086: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2249 - acc: 0.9292 - val_loss: 0.2183 - val_acc: 0.9378\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9304\n",
      "Epoch 00087: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2262 - acc: 0.9304 - val_loss: 0.2312 - val_acc: 0.9329\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9281\n",
      "Epoch 00088: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2256 - acc: 0.9281 - val_loss: 0.2106 - val_acc: 0.9383\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9295\n",
      "Epoch 00089: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2244 - acc: 0.9295 - val_loss: 0.2095 - val_acc: 0.9401\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9319\n",
      "Epoch 00090: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2195 - acc: 0.9319 - val_loss: 0.2207 - val_acc: 0.9306\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9340\n",
      "Epoch 00091: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2116 - acc: 0.9340 - val_loss: 0.2301 - val_acc: 0.9338\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9309\n",
      "Epoch 00092: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2209 - acc: 0.9309 - val_loss: 0.2273 - val_acc: 0.9362\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9339\n",
      "Epoch 00093: val_loss did not improve from 0.20307\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2141 - acc: 0.9339 - val_loss: 0.2084 - val_acc: 0.9408\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9337\n",
      "Epoch 00094: val_loss improved from 0.20307 to 0.19663, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_5_conv_checkpoint/094-0.1966.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2125 - acc: 0.9337 - val_loss: 0.1966 - val_acc: 0.9441\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9351\n",
      "Epoch 00095: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2076 - acc: 0.9351 - val_loss: 0.2212 - val_acc: 0.9324\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9337\n",
      "Epoch 00096: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2106 - acc: 0.9337 - val_loss: 0.2040 - val_acc: 0.9413\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9343\n",
      "Epoch 00097: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2085 - acc: 0.9342 - val_loss: 0.2158 - val_acc: 0.9378\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9359\n",
      "Epoch 00098: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2049 - acc: 0.9359 - val_loss: 0.2177 - val_acc: 0.9366\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9355\n",
      "Epoch 00099: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2030 - acc: 0.9355 - val_loss: 0.2130 - val_acc: 0.9406\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9352\n",
      "Epoch 00100: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2040 - acc: 0.9353 - val_loss: 0.2143 - val_acc: 0.9364\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9381\n",
      "Epoch 00101: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2010 - acc: 0.9381 - val_loss: 0.2364 - val_acc: 0.9338\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9375\n",
      "Epoch 00102: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1986 - acc: 0.9375 - val_loss: 0.2214 - val_acc: 0.9373\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9382\n",
      "Epoch 00103: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1952 - acc: 0.9382 - val_loss: 0.2041 - val_acc: 0.9422\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9390\n",
      "Epoch 00104: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1968 - acc: 0.9390 - val_loss: 0.2171 - val_acc: 0.9380\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9374\n",
      "Epoch 00105: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1965 - acc: 0.9374 - val_loss: 0.2009 - val_acc: 0.9420\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9408\n",
      "Epoch 00106: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1921 - acc: 0.9407 - val_loss: 0.2178 - val_acc: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9390\n",
      "Epoch 00107: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1963 - acc: 0.9390 - val_loss: 0.2145 - val_acc: 0.9399\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9388\n",
      "Epoch 00108: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1948 - acc: 0.9387 - val_loss: 0.2003 - val_acc: 0.9387\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9417\n",
      "Epoch 00109: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1868 - acc: 0.9417 - val_loss: 0.2036 - val_acc: 0.9427\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9402\n",
      "Epoch 00110: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1885 - acc: 0.9402 - val_loss: 0.2252 - val_acc: 0.9352\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9419\n",
      "Epoch 00111: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1869 - acc: 0.9419 - val_loss: 0.2183 - val_acc: 0.9352\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9437\n",
      "Epoch 00112: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1829 - acc: 0.9437 - val_loss: 0.2137 - val_acc: 0.9380\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9436\n",
      "Epoch 00113: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1786 - acc: 0.9436 - val_loss: 0.1972 - val_acc: 0.9422\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9434\n",
      "Epoch 00114: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1821 - acc: 0.9434 - val_loss: 0.2134 - val_acc: 0.9387\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9430\n",
      "Epoch 00115: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1802 - acc: 0.9430 - val_loss: 0.2091 - val_acc: 0.9427\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9431\n",
      "Epoch 00116: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1790 - acc: 0.9431 - val_loss: 0.2111 - val_acc: 0.9392\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9432\n",
      "Epoch 00117: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1782 - acc: 0.9432 - val_loss: 0.2057 - val_acc: 0.9434\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9458\n",
      "Epoch 00118: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1730 - acc: 0.9458 - val_loss: 0.2024 - val_acc: 0.9434\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9447\n",
      "Epoch 00119: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1761 - acc: 0.9447 - val_loss: 0.2400 - val_acc: 0.9338\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9444\n",
      "Epoch 00120: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1748 - acc: 0.9444 - val_loss: 0.2150 - val_acc: 0.9376\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9442\n",
      "Epoch 00121: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1751 - acc: 0.9442 - val_loss: 0.1969 - val_acc: 0.9448\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9464\n",
      "Epoch 00122: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1691 - acc: 0.9464 - val_loss: 0.2091 - val_acc: 0.9429\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9465\n",
      "Epoch 00123: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1692 - acc: 0.9465 - val_loss: 0.2075 - val_acc: 0.9397\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9479\n",
      "Epoch 00124: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1629 - acc: 0.9479 - val_loss: 0.2038 - val_acc: 0.9467\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9468\n",
      "Epoch 00125: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1676 - acc: 0.9469 - val_loss: 0.2086 - val_acc: 0.9392\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9466\n",
      "Epoch 00126: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1676 - acc: 0.9466 - val_loss: 0.2273 - val_acc: 0.9383\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9471\n",
      "Epoch 00127: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1670 - acc: 0.9471 - val_loss: 0.2301 - val_acc: 0.9352\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9478\n",
      "Epoch 00128: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1657 - acc: 0.9478 - val_loss: 0.2069 - val_acc: 0.9399\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9469\n",
      "Epoch 00129: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1643 - acc: 0.9469 - val_loss: 0.2028 - val_acc: 0.9436\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9481\n",
      "Epoch 00130: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1620 - acc: 0.9481 - val_loss: 0.2098 - val_acc: 0.9408\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9493\n",
      "Epoch 00131: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1594 - acc: 0.9493 - val_loss: 0.1988 - val_acc: 0.9434\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9488\n",
      "Epoch 00132: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1627 - acc: 0.9487 - val_loss: 0.2039 - val_acc: 0.9399\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9492\n",
      "Epoch 00133: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1594 - acc: 0.9492 - val_loss: 0.2069 - val_acc: 0.9422\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9494\n",
      "Epoch 00134: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1578 - acc: 0.9494 - val_loss: 0.2192 - val_acc: 0.9371\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9498\n",
      "Epoch 00135: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1563 - acc: 0.9498 - val_loss: 0.2068 - val_acc: 0.9420\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9516\n",
      "Epoch 00136: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1555 - acc: 0.9516 - val_loss: 0.2227 - val_acc: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9528\n",
      "Epoch 00137: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1533 - acc: 0.9528 - val_loss: 0.2024 - val_acc: 0.9425\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9502\n",
      "Epoch 00138: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1538 - acc: 0.9501 - val_loss: 0.2267 - val_acc: 0.9373\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9513\n",
      "Epoch 00139: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1529 - acc: 0.9513 - val_loss: 0.2254 - val_acc: 0.9371\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9514\n",
      "Epoch 00140: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1489 - acc: 0.9514 - val_loss: 0.2134 - val_acc: 0.9413\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9533\n",
      "Epoch 00141: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1481 - acc: 0.9533 - val_loss: 0.2167 - val_acc: 0.9394\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9530\n",
      "Epoch 00142: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1444 - acc: 0.9530 - val_loss: 0.2319 - val_acc: 0.9317\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9525\n",
      "Epoch 00143: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1476 - acc: 0.9525 - val_loss: 0.2321 - val_acc: 0.9336\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9528\n",
      "Epoch 00144: val_loss did not improve from 0.19663\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1438 - acc: 0.9528 - val_loss: 0.2112 - val_acc: 0.9432\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 640us/sample - loss: 0.2370 - acc: 0.9238\n",
      "Loss: 0.23701797800519633 Accuracy: 0.92377985\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0090 - acc: 0.2100\n",
      "Epoch 00001: val_loss improved from inf to 1.58583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/001-1.5858.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 4.0089 - acc: 0.2100 - val_loss: 1.5858 - val_acc: 0.5013\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1378 - acc: 0.4195\n",
      "Epoch 00002: val_loss improved from 1.58583 to 0.73989, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/002-0.7399.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.1377 - acc: 0.4196 - val_loss: 0.7399 - val_acc: 0.7789\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4640 - acc: 0.5679\n",
      "Epoch 00003: val_loss improved from 0.73989 to 0.49555, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/003-0.4956.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.4641 - acc: 0.5679 - val_loss: 0.4956 - val_acc: 0.8586\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6600\n",
      "Epoch 00004: val_loss improved from 0.49555 to 0.38707, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/004-0.3871.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1312 - acc: 0.6600 - val_loss: 0.3871 - val_acc: 0.8896\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9243 - acc: 0.7157\n",
      "Epoch 00005: val_loss improved from 0.38707 to 0.35353, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/005-0.3535.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9244 - acc: 0.7157 - val_loss: 0.3535 - val_acc: 0.8942\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8037 - acc: 0.7533\n",
      "Epoch 00006: val_loss did not improve from 0.35353\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8037 - acc: 0.7532 - val_loss: 0.4412 - val_acc: 0.8686\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7813\n",
      "Epoch 00007: val_loss improved from 0.35353 to 0.29250, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/007-0.2925.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7101 - acc: 0.7813 - val_loss: 0.2925 - val_acc: 0.9124\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6303 - acc: 0.8043\n",
      "Epoch 00008: val_loss improved from 0.29250 to 0.25794, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/008-0.2579.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6303 - acc: 0.8043 - val_loss: 0.2579 - val_acc: 0.9229\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8199\n",
      "Epoch 00009: val_loss improved from 0.25794 to 0.24138, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/009-0.2414.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5849 - acc: 0.8199 - val_loss: 0.2414 - val_acc: 0.9231\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.8352\n",
      "Epoch 00010: val_loss improved from 0.24138 to 0.23952, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/010-0.2395.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5406 - acc: 0.8352 - val_loss: 0.2395 - val_acc: 0.9283\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.8409\n",
      "Epoch 00011: val_loss improved from 0.23952 to 0.23720, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/011-0.2372.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5056 - acc: 0.8409 - val_loss: 0.2372 - val_acc: 0.9283\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.8536\n",
      "Epoch 00012: val_loss improved from 0.23720 to 0.21919, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/012-0.2192.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4763 - acc: 0.8536 - val_loss: 0.2192 - val_acc: 0.9362\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8585\n",
      "Epoch 00013: val_loss improved from 0.21919 to 0.21639, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/013-0.2164.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4512 - acc: 0.8586 - val_loss: 0.2164 - val_acc: 0.9364\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8683\n",
      "Epoch 00014: val_loss did not improve from 0.21639\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4211 - acc: 0.8684 - val_loss: 0.2250 - val_acc: 0.9304\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8742\n",
      "Epoch 00015: val_loss did not improve from 0.21639\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4044 - acc: 0.8742 - val_loss: 0.2252 - val_acc: 0.9313\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8800\n",
      "Epoch 00016: val_loss did not improve from 0.21639\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3853 - acc: 0.8800 - val_loss: 0.2233 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8839\n",
      "Epoch 00017: val_loss improved from 0.21639 to 0.20018, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/017-0.2002.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3742 - acc: 0.8839 - val_loss: 0.2002 - val_acc: 0.9390\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8910\n",
      "Epoch 00018: val_loss did not improve from 0.20018\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3533 - acc: 0.8910 - val_loss: 0.2145 - val_acc: 0.9338\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8947\n",
      "Epoch 00019: val_loss did not improve from 0.20018\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3401 - acc: 0.8947 - val_loss: 0.2403 - val_acc: 0.9248\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8977\n",
      "Epoch 00020: val_loss improved from 0.20018 to 0.17427, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/020-0.1743.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3262 - acc: 0.8977 - val_loss: 0.1743 - val_acc: 0.9492\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9024\n",
      "Epoch 00021: val_loss did not improve from 0.17427\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3143 - acc: 0.9024 - val_loss: 0.1852 - val_acc: 0.9422\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9035\n",
      "Epoch 00022: val_loss did not improve from 0.17427\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3080 - acc: 0.9035 - val_loss: 0.1800 - val_acc: 0.9439\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9093\n",
      "Epoch 00023: val_loss did not improve from 0.17427\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2937 - acc: 0.9093 - val_loss: 0.1924 - val_acc: 0.9467\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9102\n",
      "Epoch 00024: val_loss improved from 0.17427 to 0.16752, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/024-0.1675.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2892 - acc: 0.9102 - val_loss: 0.1675 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9140\n",
      "Epoch 00025: val_loss improved from 0.16752 to 0.15739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/025-0.1574.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2787 - acc: 0.9141 - val_loss: 0.1574 - val_acc: 0.9532\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9162\n",
      "Epoch 00026: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2670 - acc: 0.9162 - val_loss: 0.1696 - val_acc: 0.9490\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9187\n",
      "Epoch 00027: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2585 - acc: 0.9187 - val_loss: 0.1866 - val_acc: 0.9436\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9170\n",
      "Epoch 00028: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2649 - acc: 0.9170 - val_loss: 0.1898 - val_acc: 0.9429\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9227\n",
      "Epoch 00029: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2516 - acc: 0.9227 - val_loss: 0.1579 - val_acc: 0.9518\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9261\n",
      "Epoch 00030: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2386 - acc: 0.9260 - val_loss: 0.1682 - val_acc: 0.9502\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9243\n",
      "Epoch 00031: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2406 - acc: 0.9243 - val_loss: 0.1590 - val_acc: 0.9515\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9265\n",
      "Epoch 00032: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2300 - acc: 0.9265 - val_loss: 0.1634 - val_acc: 0.9499\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9295\n",
      "Epoch 00033: val_loss did not improve from 0.15739\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2248 - acc: 0.9295 - val_loss: 0.1730 - val_acc: 0.9455\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9312\n",
      "Epoch 00034: val_loss improved from 0.15739 to 0.14206, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/034-0.1421.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2192 - acc: 0.9312 - val_loss: 0.1421 - val_acc: 0.9569\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9346\n",
      "Epoch 00035: val_loss did not improve from 0.14206\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2096 - acc: 0.9345 - val_loss: 0.1567 - val_acc: 0.9536\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9335\n",
      "Epoch 00036: val_loss did not improve from 0.14206\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2101 - acc: 0.9335 - val_loss: 0.1883 - val_acc: 0.9425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9345\n",
      "Epoch 00037: val_loss did not improve from 0.14206\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2062 - acc: 0.9345 - val_loss: 0.1616 - val_acc: 0.9478\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9363\n",
      "Epoch 00038: val_loss improved from 0.14206 to 0.14013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/038-0.1401.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2041 - acc: 0.9363 - val_loss: 0.1401 - val_acc: 0.9553\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9396\n",
      "Epoch 00039: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1934 - acc: 0.9397 - val_loss: 0.1416 - val_acc: 0.9564\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9379\n",
      "Epoch 00040: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1959 - acc: 0.9379 - val_loss: 0.1531 - val_acc: 0.9532\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9389\n",
      "Epoch 00041: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1915 - acc: 0.9389 - val_loss: 0.1420 - val_acc: 0.9560\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9433\n",
      "Epoch 00042: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1834 - acc: 0.9433 - val_loss: 0.1438 - val_acc: 0.9548\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9414\n",
      "Epoch 00043: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1828 - acc: 0.9414 - val_loss: 0.1513 - val_acc: 0.9534\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9456\n",
      "Epoch 00044: val_loss improved from 0.14013 to 0.13832, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/044-0.1383.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1735 - acc: 0.9456 - val_loss: 0.1383 - val_acc: 0.9581\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9452\n",
      "Epoch 00045: val_loss improved from 0.13832 to 0.13643, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/045-0.1364.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1743 - acc: 0.9451 - val_loss: 0.1364 - val_acc: 0.9578\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9461\n",
      "Epoch 00046: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1723 - acc: 0.9461 - val_loss: 0.1460 - val_acc: 0.9553\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9468\n",
      "Epoch 00047: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1672 - acc: 0.9468 - val_loss: 0.1384 - val_acc: 0.9567\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9469\n",
      "Epoch 00048: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1660 - acc: 0.9469 - val_loss: 0.1710 - val_acc: 0.9457\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9492\n",
      "Epoch 00049: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1590 - acc: 0.9492 - val_loss: 0.1477 - val_acc: 0.9525\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9518\n",
      "Epoch 00050: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1530 - acc: 0.9518 - val_loss: 0.1610 - val_acc: 0.9504\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9506\n",
      "Epoch 00051: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1538 - acc: 0.9506 - val_loss: 0.1437 - val_acc: 0.9548\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9527\n",
      "Epoch 00052: val_loss did not improve from 0.13643\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1528 - acc: 0.9527 - val_loss: 0.1455 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9542\n",
      "Epoch 00053: val_loss improved from 0.13643 to 0.13407, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/053-0.1341.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1443 - acc: 0.9542 - val_loss: 0.1341 - val_acc: 0.9553\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9539\n",
      "Epoch 00054: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1435 - acc: 0.9539 - val_loss: 0.1385 - val_acc: 0.9553\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9563\n",
      "Epoch 00055: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1405 - acc: 0.9563 - val_loss: 0.1586 - val_acc: 0.9506\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9520\n",
      "Epoch 00056: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1491 - acc: 0.9520 - val_loss: 0.1591 - val_acc: 0.9527\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9575\n",
      "Epoch 00057: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1355 - acc: 0.9575 - val_loss: 0.1446 - val_acc: 0.9557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9576\n",
      "Epoch 00058: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1319 - acc: 0.9576 - val_loss: 0.1565 - val_acc: 0.9511\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9571\n",
      "Epoch 00059: val_loss did not improve from 0.13407\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1357 - acc: 0.9571 - val_loss: 0.1366 - val_acc: 0.9578\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9596\n",
      "Epoch 00060: val_loss improved from 0.13407 to 0.13071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_6_conv_checkpoint/060-0.1307.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1301 - acc: 0.9596 - val_loss: 0.1307 - val_acc: 0.9588\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9603\n",
      "Epoch 00061: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1237 - acc: 0.9603 - val_loss: 0.1434 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9605\n",
      "Epoch 00062: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1236 - acc: 0.9605 - val_loss: 0.1412 - val_acc: 0.9595\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9602\n",
      "Epoch 00063: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1232 - acc: 0.9602 - val_loss: 0.1583 - val_acc: 0.9504\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9622\n",
      "Epoch 00064: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1208 - acc: 0.9621 - val_loss: 0.1549 - val_acc: 0.9515\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9604\n",
      "Epoch 00065: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1232 - acc: 0.9604 - val_loss: 0.1388 - val_acc: 0.9555\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9620\n",
      "Epoch 00066: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1172 - acc: 0.9620 - val_loss: 0.1424 - val_acc: 0.9576\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9629\n",
      "Epoch 00067: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1150 - acc: 0.9629 - val_loss: 0.1453 - val_acc: 0.9543\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9651\n",
      "Epoch 00068: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1099 - acc: 0.9651 - val_loss: 0.1336 - val_acc: 0.9588\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9640\n",
      "Epoch 00069: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1115 - acc: 0.9640 - val_loss: 0.1398 - val_acc: 0.9592\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9669\n",
      "Epoch 00070: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1054 - acc: 0.9669 - val_loss: 0.1559 - val_acc: 0.9529\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9657\n",
      "Epoch 00071: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1052 - acc: 0.9657 - val_loss: 0.1393 - val_acc: 0.9569\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9644\n",
      "Epoch 00072: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1086 - acc: 0.9644 - val_loss: 0.1448 - val_acc: 0.9581\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9670\n",
      "Epoch 00073: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1059 - acc: 0.9670 - val_loss: 0.1368 - val_acc: 0.9588\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9684\n",
      "Epoch 00074: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0993 - acc: 0.9684 - val_loss: 0.1431 - val_acc: 0.9578\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9677\n",
      "Epoch 00075: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0983 - acc: 0.9677 - val_loss: 0.1343 - val_acc: 0.9597\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9686\n",
      "Epoch 00076: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0987 - acc: 0.9686 - val_loss: 0.1495 - val_acc: 0.9532\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9689\n",
      "Epoch 00077: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0978 - acc: 0.9689 - val_loss: 0.1318 - val_acc: 0.9620\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9670\n",
      "Epoch 00078: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1013 - acc: 0.9670 - val_loss: 0.1493 - val_acc: 0.9555\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9699\n",
      "Epoch 00079: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0943 - acc: 0.9699 - val_loss: 0.1619 - val_acc: 0.9536\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9687\n",
      "Epoch 00080: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0976 - acc: 0.9687 - val_loss: 0.1423 - val_acc: 0.9578\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9708\n",
      "Epoch 00081: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0898 - acc: 0.9708 - val_loss: 0.1412 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9715\n",
      "Epoch 00082: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0883 - acc: 0.9716 - val_loss: 0.1556 - val_acc: 0.9562\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9715\n",
      "Epoch 00083: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0878 - acc: 0.9715 - val_loss: 0.1371 - val_acc: 0.9597\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9715\n",
      "Epoch 00084: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0879 - acc: 0.9715 - val_loss: 0.1405 - val_acc: 0.9574\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9723\n",
      "Epoch 00085: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0869 - acc: 0.9723 - val_loss: 0.1525 - val_acc: 0.9592\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9724\n",
      "Epoch 00086: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0864 - acc: 0.9724 - val_loss: 0.1448 - val_acc: 0.9574\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9735\n",
      "Epoch 00087: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0833 - acc: 0.9735 - val_loss: 0.1405 - val_acc: 0.9585\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9735\n",
      "Epoch 00088: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0826 - acc: 0.9734 - val_loss: 0.1476 - val_acc: 0.9578\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9730\n",
      "Epoch 00089: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0869 - acc: 0.9730 - val_loss: 0.1680 - val_acc: 0.9485\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9749\n",
      "Epoch 00090: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0793 - acc: 0.9749 - val_loss: 0.1413 - val_acc: 0.9592\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9737\n",
      "Epoch 00091: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0804 - acc: 0.9737 - val_loss: 0.1336 - val_acc: 0.9597\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9755\n",
      "Epoch 00092: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0762 - acc: 0.9755 - val_loss: 0.1391 - val_acc: 0.9574\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9758\n",
      "Epoch 00093: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0766 - acc: 0.9758 - val_loss: 0.1446 - val_acc: 0.9576\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9758\n",
      "Epoch 00094: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0744 - acc: 0.9758 - val_loss: 0.1373 - val_acc: 0.9609\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9770\n",
      "Epoch 00095: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0713 - acc: 0.9770 - val_loss: 0.1403 - val_acc: 0.9595\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9759\n",
      "Epoch 00096: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0767 - acc: 0.9759 - val_loss: 0.1419 - val_acc: 0.9590\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9771\n",
      "Epoch 00097: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0711 - acc: 0.9771 - val_loss: 0.1442 - val_acc: 0.9581\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9776\n",
      "Epoch 00098: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0701 - acc: 0.9776 - val_loss: 0.1330 - val_acc: 0.9623\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9773\n",
      "Epoch 00099: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0687 - acc: 0.9773 - val_loss: 0.1447 - val_acc: 0.9562\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9768\n",
      "Epoch 00100: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0716 - acc: 0.9768 - val_loss: 0.1537 - val_acc: 0.9585\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9773\n",
      "Epoch 00101: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0708 - acc: 0.9772 - val_loss: 0.1679 - val_acc: 0.9539\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9773\n",
      "Epoch 00102: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0720 - acc: 0.9773 - val_loss: 0.1354 - val_acc: 0.9599\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9812\n",
      "Epoch 00103: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0606 - acc: 0.9812 - val_loss: 0.1523 - val_acc: 0.9583\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9784\n",
      "Epoch 00104: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0677 - acc: 0.9784 - val_loss: 0.1350 - val_acc: 0.9616\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9799\n",
      "Epoch 00105: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0631 - acc: 0.9799 - val_loss: 0.1340 - val_acc: 0.9620\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9783\n",
      "Epoch 00106: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0638 - acc: 0.9782 - val_loss: 0.1584 - val_acc: 0.9553\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9788\n",
      "Epoch 00107: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0680 - acc: 0.9788 - val_loss: 0.1465 - val_acc: 0.9625\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9818\n",
      "Epoch 00108: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0581 - acc: 0.9818 - val_loss: 0.1562 - val_acc: 0.9560\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9805\n",
      "Epoch 00109: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0618 - acc: 0.9805 - val_loss: 0.1384 - val_acc: 0.9606\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9816\n",
      "Epoch 00110: val_loss did not improve from 0.13071\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0568 - acc: 0.9816 - val_loss: 0.1401 - val_acc: 0.9613\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 684us/sample - loss: 0.1666 - acc: 0.9502\n",
      "Loss: 0.16661081556404864 Accuracy: 0.95015574\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0130 - acc: 0.3117\n",
      "Epoch 00001: val_loss improved from inf to 1.15885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/001-1.1589.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 3.0131 - acc: 0.3117 - val_loss: 1.1589 - val_acc: 0.6480\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.6049\n",
      "Epoch 00002: val_loss improved from 1.15885 to 0.40835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/002-0.4083.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.3525 - acc: 0.6049 - val_loss: 0.4083 - val_acc: 0.8777\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8932 - acc: 0.7328\n",
      "Epoch 00003: val_loss improved from 0.40835 to 0.37981, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/003-0.3798.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8932 - acc: 0.7328 - val_loss: 0.3798 - val_acc: 0.8821\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.7908\n",
      "Epoch 00004: val_loss improved from 0.37981 to 0.26875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/004-0.2687.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6872 - acc: 0.7907 - val_loss: 0.2687 - val_acc: 0.9213\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8263\n",
      "Epoch 00005: val_loss improved from 0.26875 to 0.23052, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/005-0.2305.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5724 - acc: 0.8263 - val_loss: 0.2305 - val_acc: 0.9259\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8519\n",
      "Epoch 00006: val_loss improved from 0.23052 to 0.20349, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/006-0.2035.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4929 - acc: 0.8519 - val_loss: 0.2035 - val_acc: 0.9401\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.8675\n",
      "Epoch 00007: val_loss improved from 0.20349 to 0.18638, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/007-0.1864.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4321 - acc: 0.8675 - val_loss: 0.1864 - val_acc: 0.9441\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8782\n",
      "Epoch 00008: val_loss did not improve from 0.18638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3965 - acc: 0.8782 - val_loss: 0.3157 - val_acc: 0.8982\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8878\n",
      "Epoch 00009: val_loss improved from 0.18638 to 0.17181, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/009-0.1718.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3650 - acc: 0.8878 - val_loss: 0.1718 - val_acc: 0.9495\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8979\n",
      "Epoch 00010: val_loss did not improve from 0.17181\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3295 - acc: 0.8979 - val_loss: 0.1814 - val_acc: 0.9462\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9074\n",
      "Epoch 00011: val_loss improved from 0.17181 to 0.16201, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/011-0.1620.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3010 - acc: 0.9074 - val_loss: 0.1620 - val_acc: 0.9497\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9107\n",
      "Epoch 00012: val_loss improved from 0.16201 to 0.16008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/012-0.1601.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2851 - acc: 0.9107 - val_loss: 0.1601 - val_acc: 0.9525\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9159\n",
      "Epoch 00013: val_loss improved from 0.16008 to 0.14544, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/013-0.1454.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2659 - acc: 0.9159 - val_loss: 0.1454 - val_acc: 0.9562\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9245\n",
      "Epoch 00014: val_loss did not improve from 0.14544\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2435 - acc: 0.9245 - val_loss: 0.1600 - val_acc: 0.9483\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9246\n",
      "Epoch 00015: val_loss improved from 0.14544 to 0.14013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/015-0.1401.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2371 - acc: 0.9245 - val_loss: 0.1401 - val_acc: 0.9560\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9290\n",
      "Epoch 00016: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2272 - acc: 0.9290 - val_loss: 0.1499 - val_acc: 0.9527\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9342\n",
      "Epoch 00017: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2106 - acc: 0.9342 - val_loss: 0.1699 - val_acc: 0.9469\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9375\n",
      "Epoch 00018: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1982 - acc: 0.9375 - val_loss: 0.1573 - val_acc: 0.9497\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9392\n",
      "Epoch 00019: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1938 - acc: 0.9392 - val_loss: 0.1412 - val_acc: 0.9564\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9425\n",
      "Epoch 00020: val_loss did not improve from 0.14013\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1841 - acc: 0.9425 - val_loss: 0.1455 - val_acc: 0.9541\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9435\n",
      "Epoch 00021: val_loss improved from 0.14013 to 0.13124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/021-0.1312.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1798 - acc: 0.9435 - val_loss: 0.1312 - val_acc: 0.9576\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9482\n",
      "Epoch 00022: val_loss did not improve from 0.13124\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1683 - acc: 0.9482 - val_loss: 0.1818 - val_acc: 0.9446\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9485\n",
      "Epoch 00023: val_loss did not improve from 0.13124\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1602 - acc: 0.9484 - val_loss: 0.1422 - val_acc: 0.9564\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9508\n",
      "Epoch 00024: val_loss improved from 0.13124 to 0.12955, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/024-0.1295.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1570 - acc: 0.9508 - val_loss: 0.1295 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9530\n",
      "Epoch 00025: val_loss improved from 0.12955 to 0.12589, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/025-0.1259.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1491 - acc: 0.9530 - val_loss: 0.1259 - val_acc: 0.9616\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9545\n",
      "Epoch 00026: val_loss did not improve from 0.12589\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1398 - acc: 0.9545 - val_loss: 0.1437 - val_acc: 0.9588\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9571\n",
      "Epoch 00027: val_loss did not improve from 0.12589\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1342 - acc: 0.9571 - val_loss: 0.1305 - val_acc: 0.9604\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9580\n",
      "Epoch 00028: val_loss did not improve from 0.12589\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1310 - acc: 0.9580 - val_loss: 0.1404 - val_acc: 0.9525\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9598\n",
      "Epoch 00029: val_loss improved from 0.12589 to 0.11882, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/029-0.1188.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1232 - acc: 0.9598 - val_loss: 0.1188 - val_acc: 0.9648\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9602\n",
      "Epoch 00030: val_loss did not improve from 0.11882\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1255 - acc: 0.9602 - val_loss: 0.1230 - val_acc: 0.9620\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9629\n",
      "Epoch 00031: val_loss did not improve from 0.11882\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1166 - acc: 0.9629 - val_loss: 0.1243 - val_acc: 0.9623\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9658\n",
      "Epoch 00032: val_loss did not improve from 0.11882\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1084 - acc: 0.9658 - val_loss: 0.1224 - val_acc: 0.9618\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9652\n",
      "Epoch 00033: val_loss improved from 0.11882 to 0.11647, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_7_conv_checkpoint/033-0.1165.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1080 - acc: 0.9652 - val_loss: 0.1165 - val_acc: 0.9632\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9672\n",
      "Epoch 00034: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1047 - acc: 0.9672 - val_loss: 0.1214 - val_acc: 0.9630\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9687\n",
      "Epoch 00035: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0970 - acc: 0.9687 - val_loss: 0.1259 - val_acc: 0.9595\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9694\n",
      "Epoch 00036: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0965 - acc: 0.9694 - val_loss: 0.1194 - val_acc: 0.9616\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9686\n",
      "Epoch 00037: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0952 - acc: 0.9686 - val_loss: 0.1277 - val_acc: 0.9599\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9711\n",
      "Epoch 00038: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0891 - acc: 0.9711 - val_loss: 0.1333 - val_acc: 0.9613\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9732\n",
      "Epoch 00039: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0846 - acc: 0.9732 - val_loss: 0.1302 - val_acc: 0.9585\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9708\n",
      "Epoch 00040: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0893 - acc: 0.9708 - val_loss: 0.1190 - val_acc: 0.9630\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9741\n",
      "Epoch 00041: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0793 - acc: 0.9741 - val_loss: 0.1181 - val_acc: 0.9623\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9758\n",
      "Epoch 00042: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0760 - acc: 0.9758 - val_loss: 0.1227 - val_acc: 0.9620\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9760\n",
      "Epoch 00043: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0760 - acc: 0.9760 - val_loss: 0.1290 - val_acc: 0.9625\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9758\n",
      "Epoch 00044: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0739 - acc: 0.9758 - val_loss: 0.1242 - val_acc: 0.9648\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9773\n",
      "Epoch 00045: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0707 - acc: 0.9773 - val_loss: 0.1311 - val_acc: 0.9634\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9795\n",
      "Epoch 00046: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0643 - acc: 0.9795 - val_loss: 0.1182 - val_acc: 0.9644\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9798\n",
      "Epoch 00047: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0649 - acc: 0.9798 - val_loss: 0.1194 - val_acc: 0.9637\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9803\n",
      "Epoch 00048: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0602 - acc: 0.9803 - val_loss: 0.1238 - val_acc: 0.9618\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9807\n",
      "Epoch 00049: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0606 - acc: 0.9807 - val_loss: 0.1368 - val_acc: 0.9588\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9810\n",
      "Epoch 00050: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0602 - acc: 0.9810 - val_loss: 0.1506 - val_acc: 0.9602\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9803\n",
      "Epoch 00051: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0608 - acc: 0.9803 - val_loss: 0.1277 - val_acc: 0.9625\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9823\n",
      "Epoch 00052: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0535 - acc: 0.9823 - val_loss: 0.1233 - val_acc: 0.9646\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9827\n",
      "Epoch 00053: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0533 - acc: 0.9827 - val_loss: 0.1308 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9815\n",
      "Epoch 00054: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0571 - acc: 0.9815 - val_loss: 0.1521 - val_acc: 0.9576\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9840\n",
      "Epoch 00055: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0511 - acc: 0.9840 - val_loss: 0.1306 - val_acc: 0.9632\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9806\n",
      "Epoch 00056: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0627 - acc: 0.9806 - val_loss: 0.1378 - val_acc: 0.9602\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9862\n",
      "Epoch 00057: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0472 - acc: 0.9863 - val_loss: 0.1203 - val_acc: 0.9655\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9857\n",
      "Epoch 00058: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0432 - acc: 0.9857 - val_loss: 0.1464 - val_acc: 0.9616\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9851\n",
      "Epoch 00059: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0468 - acc: 0.9851 - val_loss: 0.1313 - val_acc: 0.9655\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9860\n",
      "Epoch 00060: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0448 - acc: 0.9860 - val_loss: 0.1272 - val_acc: 0.9627\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9856\n",
      "Epoch 00061: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0449 - acc: 0.9856 - val_loss: 0.1182 - val_acc: 0.9660\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9865\n",
      "Epoch 00062: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0455 - acc: 0.9865 - val_loss: 0.1239 - val_acc: 0.9639\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9876\n",
      "Epoch 00063: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0398 - acc: 0.9875 - val_loss: 0.1256 - val_acc: 0.9644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9867\n",
      "Epoch 00064: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0417 - acc: 0.9867 - val_loss: 0.1376 - val_acc: 0.9637\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9868\n",
      "Epoch 00065: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0422 - acc: 0.9868 - val_loss: 0.1361 - val_acc: 0.9627\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9888\n",
      "Epoch 00066: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0370 - acc: 0.9888 - val_loss: 0.1566 - val_acc: 0.9606\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9865\n",
      "Epoch 00067: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0421 - acc: 0.9865 - val_loss: 0.1249 - val_acc: 0.9648\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9885\n",
      "Epoch 00068: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0375 - acc: 0.9885 - val_loss: 0.1349 - val_acc: 0.9644\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9891\n",
      "Epoch 00069: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0338 - acc: 0.9891 - val_loss: 0.1314 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9902\n",
      "Epoch 00070: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0317 - acc: 0.9902 - val_loss: 0.1614 - val_acc: 0.9599\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9880\n",
      "Epoch 00071: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0393 - acc: 0.9879 - val_loss: 0.1293 - val_acc: 0.9641\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9877\n",
      "Epoch 00072: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0389 - acc: 0.9876 - val_loss: 0.1321 - val_acc: 0.9667\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9900\n",
      "Epoch 00073: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0329 - acc: 0.9899 - val_loss: 0.1332 - val_acc: 0.9639\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9897\n",
      "Epoch 00074: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0326 - acc: 0.9897 - val_loss: 0.1288 - val_acc: 0.9658\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9920\n",
      "Epoch 00075: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0274 - acc: 0.9920 - val_loss: 0.1366 - val_acc: 0.9625\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9902\n",
      "Epoch 00076: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0317 - acc: 0.9902 - val_loss: 0.1480 - val_acc: 0.9630\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9907\n",
      "Epoch 00077: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0314 - acc: 0.9907 - val_loss: 0.1434 - val_acc: 0.9613\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9903\n",
      "Epoch 00078: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0319 - acc: 0.9903 - val_loss: 0.1386 - val_acc: 0.9644\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9917\n",
      "Epoch 00079: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0280 - acc: 0.9916 - val_loss: 0.1571 - val_acc: 0.9599\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9881\n",
      "Epoch 00080: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0395 - acc: 0.9881 - val_loss: 0.1333 - val_acc: 0.9644\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9908\n",
      "Epoch 00081: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0290 - acc: 0.9908 - val_loss: 0.1593 - val_acc: 0.9569\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9923\n",
      "Epoch 00082: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0249 - acc: 0.9923 - val_loss: 0.1863 - val_acc: 0.9564\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00083: val_loss did not improve from 0.11647\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.1378 - val_acc: 0.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 723us/sample - loss: 0.1475 - acc: 0.9535\n",
      "Loss: 0.1475257602962254 Accuracy: 0.9534787\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3610 - acc: 0.3840\n",
      "Epoch 00001: val_loss improved from inf to 0.99781, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/001-0.9978.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.3610 - acc: 0.3841 - val_loss: 0.9978 - val_acc: 0.6867\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9819 - acc: 0.6999\n",
      "Epoch 00002: val_loss improved from 0.99781 to 0.37722, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/002-0.3772.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.9820 - acc: 0.6999 - val_loss: 0.3772 - val_acc: 0.8800\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.8052\n",
      "Epoch 00003: val_loss improved from 0.37722 to 0.28790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/003-0.2879.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.6404 - acc: 0.8053 - val_loss: 0.2879 - val_acc: 0.9117\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8496\n",
      "Epoch 00004: val_loss improved from 0.28790 to 0.22858, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/004-0.2286.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4883 - acc: 0.8496 - val_loss: 0.2286 - val_acc: 0.9364\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8732\n",
      "Epoch 00005: val_loss improved from 0.22858 to 0.22212, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/005-0.2221.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4081 - acc: 0.8732 - val_loss: 0.2221 - val_acc: 0.9320\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8907\n",
      "Epoch 00006: val_loss improved from 0.22212 to 0.18830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/006-0.1883.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3485 - acc: 0.8907 - val_loss: 0.1883 - val_acc: 0.9429\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9060\n",
      "Epoch 00007: val_loss improved from 0.18830 to 0.17709, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/007-0.1771.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3031 - acc: 0.9060 - val_loss: 0.1771 - val_acc: 0.9497\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9160\n",
      "Epoch 00008: val_loss improved from 0.17709 to 0.16180, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/008-0.1618.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2749 - acc: 0.9160 - val_loss: 0.1618 - val_acc: 0.9534\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9229\n",
      "Epoch 00009: val_loss did not improve from 0.16180\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2424 - acc: 0.9229 - val_loss: 0.1816 - val_acc: 0.9429\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9307\n",
      "Epoch 00010: val_loss did not improve from 0.16180\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2188 - acc: 0.9307 - val_loss: 0.1656 - val_acc: 0.9492\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9354\n",
      "Epoch 00011: val_loss did not improve from 0.16180\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2059 - acc: 0.9354 - val_loss: 0.2460 - val_acc: 0.9245\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9401\n",
      "Epoch 00012: val_loss improved from 0.16180 to 0.14815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/012-0.1481.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1894 - acc: 0.9401 - val_loss: 0.1481 - val_acc: 0.9518\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9439\n",
      "Epoch 00013: val_loss did not improve from 0.14815\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1759 - acc: 0.9439 - val_loss: 0.1630 - val_acc: 0.9515\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9470\n",
      "Epoch 00014: val_loss did not improve from 0.14815\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1619 - acc: 0.9470 - val_loss: 0.1521 - val_acc: 0.9567\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9525\n",
      "Epoch 00015: val_loss improved from 0.14815 to 0.13780, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/015-0.1378.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1502 - acc: 0.9525 - val_loss: 0.1378 - val_acc: 0.9576\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9572\n",
      "Epoch 00016: val_loss improved from 0.13780 to 0.12420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/016-0.1242.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1363 - acc: 0.9572 - val_loss: 0.1242 - val_acc: 0.9639\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9593\n",
      "Epoch 00017: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1294 - acc: 0.9593 - val_loss: 0.1269 - val_acc: 0.9611\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9623\n",
      "Epoch 00018: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1206 - acc: 0.9623 - val_loss: 0.1511 - val_acc: 0.9522\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9627\n",
      "Epoch 00019: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1155 - acc: 0.9627 - val_loss: 0.1367 - val_acc: 0.9599\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9665\n",
      "Epoch 00020: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1054 - acc: 0.9666 - val_loss: 0.1476 - val_acc: 0.9562\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9687\n",
      "Epoch 00021: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1013 - acc: 0.9687 - val_loss: 0.1268 - val_acc: 0.9609\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9700\n",
      "Epoch 00022: val_loss did not improve from 0.12420\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0940 - acc: 0.9700 - val_loss: 0.1497 - val_acc: 0.9560\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9708\n",
      "Epoch 00023: val_loss improved from 0.12420 to 0.10917, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_BN_8_conv_checkpoint/023-0.1092.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0884 - acc: 0.9708 - val_loss: 0.1092 - val_acc: 0.9660\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9732\n",
      "Epoch 00024: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0840 - acc: 0.9732 - val_loss: 0.1377 - val_acc: 0.9597\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9743\n",
      "Epoch 00025: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0834 - acc: 0.9743 - val_loss: 0.1172 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9747\n",
      "Epoch 00026: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0800 - acc: 0.9747 - val_loss: 0.1195 - val_acc: 0.9653\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9762\n",
      "Epoch 00027: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0745 - acc: 0.9762 - val_loss: 0.1378 - val_acc: 0.9606\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9782\n",
      "Epoch 00028: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0689 - acc: 0.9782 - val_loss: 0.1193 - val_acc: 0.9625\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9801\n",
      "Epoch 00029: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0630 - acc: 0.9801 - val_loss: 0.1234 - val_acc: 0.9653\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9805\n",
      "Epoch 00030: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0633 - acc: 0.9805 - val_loss: 0.1265 - val_acc: 0.9611\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9809\n",
      "Epoch 00031: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0601 - acc: 0.9809 - val_loss: 0.1155 - val_acc: 0.9667\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9819\n",
      "Epoch 00032: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0569 - acc: 0.9819 - val_loss: 0.1135 - val_acc: 0.9665\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9799\n",
      "Epoch 00033: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0621 - acc: 0.9799 - val_loss: 0.1230 - val_acc: 0.9646\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9851\n",
      "Epoch 00034: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0489 - acc: 0.9851 - val_loss: 0.1137 - val_acc: 0.9665\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9848\n",
      "Epoch 00035: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0490 - acc: 0.9848 - val_loss: 0.1148 - val_acc: 0.9637\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9860\n",
      "Epoch 00036: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0475 - acc: 0.9860 - val_loss: 0.1286 - val_acc: 0.9634\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9873\n",
      "Epoch 00037: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0420 - acc: 0.9873 - val_loss: 0.1321 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9873\n",
      "Epoch 00038: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0429 - acc: 0.9873 - val_loss: 0.1474 - val_acc: 0.9623\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9866\n",
      "Epoch 00039: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0437 - acc: 0.9866 - val_loss: 0.1249 - val_acc: 0.9676\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9865\n",
      "Epoch 00040: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0413 - acc: 0.9865 - val_loss: 0.1157 - val_acc: 0.9655\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9871\n",
      "Epoch 00041: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0410 - acc: 0.9871 - val_loss: 0.1175 - val_acc: 0.9683\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9896\n",
      "Epoch 00042: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0348 - acc: 0.9896 - val_loss: 0.1158 - val_acc: 0.9662\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9898\n",
      "Epoch 00043: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0341 - acc: 0.9898 - val_loss: 0.1155 - val_acc: 0.9693\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9898\n",
      "Epoch 00044: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0340 - acc: 0.9898 - val_loss: 0.1308 - val_acc: 0.9651\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9896\n",
      "Epoch 00045: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0346 - acc: 0.9895 - val_loss: 0.1210 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9896\n",
      "Epoch 00046: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0357 - acc: 0.9896 - val_loss: 0.1213 - val_acc: 0.9662\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0351 - acc: 0.9895 - val_loss: 0.1271 - val_acc: 0.9700\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9937\n",
      "Epoch 00048: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0233 - acc: 0.9938 - val_loss: 0.1205 - val_acc: 0.9688\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9900\n",
      "Epoch 00049: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0315 - acc: 0.9900 - val_loss: 0.1675 - val_acc: 0.9567\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9930\n",
      "Epoch 00050: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 0.1111 - val_acc: 0.9741\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9924\n",
      "Epoch 00051: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0260 - acc: 0.9923 - val_loss: 0.1412 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9893\n",
      "Epoch 00052: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0335 - acc: 0.9893 - val_loss: 0.1356 - val_acc: 0.9632\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9910\n",
      "Epoch 00053: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0316 - acc: 0.9910 - val_loss: 0.1238 - val_acc: 0.9690\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9933\n",
      "Epoch 00054: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0219 - acc: 0.9933 - val_loss: 0.1528 - val_acc: 0.9618\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9937\n",
      "Epoch 00055: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0210 - acc: 0.9937 - val_loss: 0.1201 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 00056: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.1346 - val_acc: 0.9653\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 00057: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0240 - acc: 0.9927 - val_loss: 0.1387 - val_acc: 0.9634\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9939\n",
      "Epoch 00058: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0206 - acc: 0.9938 - val_loss: 0.1547 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9905\n",
      "Epoch 00059: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0321 - acc: 0.9905 - val_loss: 0.1161 - val_acc: 0.9718\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9949\n",
      "Epoch 00060: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0177 - acc: 0.9949 - val_loss: 0.1304 - val_acc: 0.9700\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0182 - acc: 0.9946 - val_loss: 0.1289 - val_acc: 0.9718\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9946\n",
      "Epoch 00062: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0187 - acc: 0.9946 - val_loss: 0.1309 - val_acc: 0.9672\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9937\n",
      "Epoch 00063: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0205 - acc: 0.9937 - val_loss: 0.1727 - val_acc: 0.9578\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9892\n",
      "Epoch 00064: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0348 - acc: 0.9892 - val_loss: 0.1342 - val_acc: 0.9667\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00065: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0181 - acc: 0.9946 - val_loss: 0.1500 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9957\n",
      "Epoch 00066: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.1449 - val_acc: 0.9658\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9944\n",
      "Epoch 00067: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0197 - acc: 0.9944 - val_loss: 0.1453 - val_acc: 0.9667\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9955\n",
      "Epoch 00068: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0164 - acc: 0.9955 - val_loss: 0.1285 - val_acc: 0.9695\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9957\n",
      "Epoch 00069: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0151 - acc: 0.9957 - val_loss: 0.1545 - val_acc: 0.9630\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9943\n",
      "Epoch 00070: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0195 - acc: 0.9943 - val_loss: 0.1650 - val_acc: 0.9623\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9904\n",
      "Epoch 00071: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0317 - acc: 0.9904 - val_loss: 0.1403 - val_acc: 0.9681\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9950\n",
      "Epoch 00072: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0169 - acc: 0.9950 - val_loss: 0.1445 - val_acc: 0.9690\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00073: val_loss did not improve from 0.10917\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.1373 - val_acc: 0.9695\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 777us/sample - loss: 0.1693 - acc: 0.9483\n",
      "Loss: 0.16933279489229897 Accuracy: 0.9482866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 726us/sample - loss: 0.6157 - acc: 0.8120\n",
      "Loss: 0.6156512848251458 Accuracy: 0.8120457\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 754us/sample - loss: 0.4237 - acc: 0.8698\n",
      "Loss: 0.42367817113951606 Accuracy: 0.8697819\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 789us/sample - loss: 0.2370 - acc: 0.9238\n",
      "Loss: 0.23701797800519633 Accuracy: 0.92377985\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 810us/sample - loss: 0.1666 - acc: 0.9502\n",
      "Loss: 0.16661081556404864 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 839us/sample - loss: 0.1475 - acc: 0.9535\n",
      "Loss: 0.1475257602962254 Accuracy: 0.9534787\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 870us/sample - loss: 0.1693 - acc: 0.9483\n",
      "Loss: 0.16933279489229897 Accuracy: 0.9482866\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 823us/sample - loss: 0.6531 - acc: 0.7940\n",
      "Loss: 0.6531486609457438 Accuracy: 0.79397714\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 862us/sample - loss: 0.4482 - acc: 0.8675\n",
      "Loss: 0.4481992211544873 Accuracy: 0.8674974\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 894us/sample - loss: 0.2600 - acc: 0.9225\n",
      "Loss: 0.25998547853958076 Accuracy: 0.92253375\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 988us/sample - loss: 0.1816 - acc: 0.9485\n",
      "Loss: 0.18163802131066564 Accuracy: 0.9484943\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 933us/sample - loss: 0.1599 - acc: 0.9580\n",
      "Loss: 0.1598537364012348 Accuracy: 0.95804775\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1869 - acc: 0.9589\n",
      "Loss: 0.1869317456870509 Accuracy: 0.9588785\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
