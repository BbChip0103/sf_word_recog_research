{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())   \n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9357 - acc: 0.3945\n",
      "Epoch 00001: val_loss improved from inf to 1.76198, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/001-1.7620.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 1.9356 - acc: 0.3945 - val_loss: 1.7620 - val_acc: 0.4440\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3355 - acc: 0.5843\n",
      "Epoch 00002: val_loss improved from 1.76198 to 1.75592, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/002-1.7559.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.3356 - acc: 0.5844 - val_loss: 1.7559 - val_acc: 0.4659\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0913 - acc: 0.6680\n",
      "Epoch 00003: val_loss improved from 1.75592 to 1.74470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/003-1.7447.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.0912 - acc: 0.6680 - val_loss: 1.7447 - val_acc: 0.5232\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9470 - acc: 0.7153\n",
      "Epoch 00004: val_loss improved from 1.74470 to 1.23686, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/004-1.2369.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.9472 - acc: 0.7152 - val_loss: 1.2369 - val_acc: 0.6257\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8578 - acc: 0.7422\n",
      "Epoch 00005: val_loss did not improve from 1.23686\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.8578 - acc: 0.7422 - val_loss: 2.0344 - val_acc: 0.5024\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7811 - acc: 0.7695\n",
      "Epoch 00006: val_loss improved from 1.23686 to 1.16804, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/006-1.1680.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7811 - acc: 0.7695 - val_loss: 1.1680 - val_acc: 0.6778\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7241 - acc: 0.7857\n",
      "Epoch 00007: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.7242 - acc: 0.7856 - val_loss: 2.2836 - val_acc: 0.4782\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6759 - acc: 0.8010\n",
      "Epoch 00008: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6758 - acc: 0.8011 - val_loss: 1.5493 - val_acc: 0.6063\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6341 - acc: 0.8117\n",
      "Epoch 00009: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6340 - acc: 0.8117 - val_loss: 1.2494 - val_acc: 0.6476\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8221\n",
      "Epoch 00010: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.6003 - acc: 0.8221 - val_loss: 1.3270 - val_acc: 0.6415\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.8315\n",
      "Epoch 00011: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5695 - acc: 0.8315 - val_loss: 1.1838 - val_acc: 0.6690\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.8374\n",
      "Epoch 00012: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5440 - acc: 0.8374 - val_loss: 1.6355 - val_acc: 0.6024\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8493\n",
      "Epoch 00013: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5084 - acc: 0.8493 - val_loss: 2.0370 - val_acc: 0.5500\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8579\n",
      "Epoch 00014: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4826 - acc: 0.8579 - val_loss: 1.1882 - val_acc: 0.6758\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4638 - acc: 0.8641\n",
      "Epoch 00015: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4639 - acc: 0.8640 - val_loss: 2.0076 - val_acc: 0.5311\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.8714\n",
      "Epoch 00016: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.4390 - acc: 0.8714 - val_loss: 1.6893 - val_acc: 0.5998\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8772\n",
      "Epoch 00017: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4138 - acc: 0.8772 - val_loss: 1.5180 - val_acc: 0.6334\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8845\n",
      "Epoch 00018: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3915 - acc: 0.8845 - val_loss: 1.2589 - val_acc: 0.6608\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8880\n",
      "Epoch 00019: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3773 - acc: 0.8879 - val_loss: 2.0237 - val_acc: 0.5837\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8962\n",
      "Epoch 00020: val_loss did not improve from 1.16804\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3570 - acc: 0.8962 - val_loss: 1.6710 - val_acc: 0.6273\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8996\n",
      "Epoch 00021: val_loss improved from 1.16804 to 1.14300, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/021-1.1430.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.3394 - acc: 0.8996 - val_loss: 1.1430 - val_acc: 0.7074\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9013\n",
      "Epoch 00022: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3373 - acc: 0.9013 - val_loss: 1.6491 - val_acc: 0.6049\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9098\n",
      "Epoch 00023: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3090 - acc: 0.9097 - val_loss: 1.8429 - val_acc: 0.5702\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9157\n",
      "Epoch 00024: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2926 - acc: 0.9157 - val_loss: 1.3433 - val_acc: 0.6685\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9202\n",
      "Epoch 00025: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2773 - acc: 0.9203 - val_loss: 1.5410 - val_acc: 0.6653\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9209\n",
      "Epoch 00026: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2664 - acc: 0.9209 - val_loss: 1.3298 - val_acc: 0.6746\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9289\n",
      "Epoch 00027: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2501 - acc: 0.9288 - val_loss: 2.0218 - val_acc: 0.6124\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9270\n",
      "Epoch 00028: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2475 - acc: 0.9270 - val_loss: 2.2728 - val_acc: 0.5737\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9311\n",
      "Epoch 00029: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2328 - acc: 0.9312 - val_loss: 2.1030 - val_acc: 0.5795\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9380\n",
      "Epoch 00030: val_loss did not improve from 1.14300\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2176 - acc: 0.9379 - val_loss: 1.5909 - val_acc: 0.6420\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9323\n",
      "Epoch 00031: val_loss improved from 1.14300 to 1.12090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/031-1.1209.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.2292 - acc: 0.9323 - val_loss: 1.1209 - val_acc: 0.7142\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9467\n",
      "Epoch 00032: val_loss did not improve from 1.12090\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1929 - acc: 0.9466 - val_loss: 2.2935 - val_acc: 0.5472\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9368\n",
      "Epoch 00033: val_loss improved from 1.12090 to 0.80120, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/033-0.8012.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2170 - acc: 0.9368 - val_loss: 0.8012 - val_acc: 0.7862\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9497\n",
      "Epoch 00034: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1778 - acc: 0.9497 - val_loss: 1.4269 - val_acc: 0.6618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9505\n",
      "Epoch 00035: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1731 - acc: 0.9505 - val_loss: 1.7141 - val_acc: 0.6317\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9508\n",
      "Epoch 00036: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1717 - acc: 0.9508 - val_loss: 1.4238 - val_acc: 0.6785\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9532\n",
      "Epoch 00037: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1611 - acc: 0.9532 - val_loss: 1.0254 - val_acc: 0.7594\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9533\n",
      "Epoch 00038: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1593 - acc: 0.9533 - val_loss: 2.3914 - val_acc: 0.5982\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9543\n",
      "Epoch 00039: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1558 - acc: 0.9542 - val_loss: 1.1721 - val_acc: 0.7270\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9588\n",
      "Epoch 00040: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1465 - acc: 0.9588 - val_loss: 1.2502 - val_acc: 0.7163\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9572\n",
      "Epoch 00041: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1498 - acc: 0.9571 - val_loss: 1.4278 - val_acc: 0.6655\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9595\n",
      "Epoch 00042: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1418 - acc: 0.9594 - val_loss: 2.0209 - val_acc: 0.6573\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9633\n",
      "Epoch 00043: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1297 - acc: 0.9633 - val_loss: 0.8217 - val_acc: 0.7827\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9651\n",
      "Epoch 00044: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1225 - acc: 0.9651 - val_loss: 1.0828 - val_acc: 0.7417\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9661\n",
      "Epoch 00045: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1191 - acc: 0.9661 - val_loss: 1.1151 - val_acc: 0.7445\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9644\n",
      "Epoch 00046: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1266 - acc: 0.9644 - val_loss: 1.2808 - val_acc: 0.7228\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9657\n",
      "Epoch 00047: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1193 - acc: 0.9657 - val_loss: 1.5302 - val_acc: 0.6641\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9674\n",
      "Epoch 00048: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1129 - acc: 0.9674 - val_loss: 1.7744 - val_acc: 0.6513\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9673\n",
      "Epoch 00049: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1132 - acc: 0.9673 - val_loss: 2.2129 - val_acc: 0.6096\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9671\n",
      "Epoch 00050: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1118 - acc: 0.9671 - val_loss: 1.2295 - val_acc: 0.7345\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9710\n",
      "Epoch 00051: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1025 - acc: 0.9710 - val_loss: 1.8376 - val_acc: 0.6464\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9741\n",
      "Epoch 00052: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0951 - acc: 0.9741 - val_loss: 1.2705 - val_acc: 0.7300\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9723\n",
      "Epoch 00053: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0977 - acc: 0.9723 - val_loss: 1.2756 - val_acc: 0.7214\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9679\n",
      "Epoch 00054: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1102 - acc: 0.9679 - val_loss: 1.9484 - val_acc: 0.6208\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9737\n",
      "Epoch 00055: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0947 - acc: 0.9737 - val_loss: 1.5562 - val_acc: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9730\n",
      "Epoch 00056: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0949 - acc: 0.9729 - val_loss: 0.8052 - val_acc: 0.8064\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9743\n",
      "Epoch 00057: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0897 - acc: 0.9743 - val_loss: 1.4112 - val_acc: 0.7170\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9759\n",
      "Epoch 00058: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0851 - acc: 0.9758 - val_loss: 1.7798 - val_acc: 0.6804\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9709\n",
      "Epoch 00059: val_loss did not improve from 0.80120\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1006 - acc: 0.9709 - val_loss: 1.1190 - val_acc: 0.7591\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9783\n",
      "Epoch 00060: val_loss improved from 0.80120 to 0.79977, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv_checkpoint/060-0.7998.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0777 - acc: 0.9782 - val_loss: 0.7998 - val_acc: 0.8088\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9765\n",
      "Epoch 00061: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0841 - acc: 0.9764 - val_loss: 1.4444 - val_acc: 0.6979\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9721\n",
      "Epoch 00062: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0980 - acc: 0.9720 - val_loss: 1.2011 - val_acc: 0.7515\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9732\n",
      "Epoch 00063: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0953 - acc: 0.9732 - val_loss: 1.1778 - val_acc: 0.7424\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9772\n",
      "Epoch 00064: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0791 - acc: 0.9771 - val_loss: 0.9840 - val_acc: 0.7689\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9800\n",
      "Epoch 00065: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0728 - acc: 0.9800 - val_loss: 0.9511 - val_acc: 0.7866\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9797\n",
      "Epoch 00066: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0717 - acc: 0.9797 - val_loss: 1.6099 - val_acc: 0.7058\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9801\n",
      "Epoch 00067: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0706 - acc: 0.9801 - val_loss: 1.7702 - val_acc: 0.6832\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9822\n",
      "Epoch 00068: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0665 - acc: 0.9822 - val_loss: 1.6337 - val_acc: 0.6962\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9799\n",
      "Epoch 00069: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0720 - acc: 0.9799 - val_loss: 0.9331 - val_acc: 0.7948\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9788\n",
      "Epoch 00070: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0717 - acc: 0.9788 - val_loss: 1.3885 - val_acc: 0.7133\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9758\n",
      "Epoch 00071: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0815 - acc: 0.9758 - val_loss: 1.1315 - val_acc: 0.7508\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9844\n",
      "Epoch 00072: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0585 - acc: 0.9843 - val_loss: 1.1052 - val_acc: 0.7624\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9777\n",
      "Epoch 00073: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0738 - acc: 0.9777 - val_loss: 2.0117 - val_acc: 0.6776\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9794\n",
      "Epoch 00074: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0707 - acc: 0.9794 - val_loss: 0.9837 - val_acc: 0.7878\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9828\n",
      "Epoch 00075: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0633 - acc: 0.9828 - val_loss: 1.3056 - val_acc: 0.7389\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9816\n",
      "Epoch 00076: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0663 - acc: 0.9816 - val_loss: 1.5355 - val_acc: 0.7214\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9816\n",
      "Epoch 00077: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0644 - acc: 0.9816 - val_loss: 1.0425 - val_acc: 0.7729\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9821\n",
      "Epoch 00078: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0637 - acc: 0.9821 - val_loss: 2.6255 - val_acc: 0.5996\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9810\n",
      "Epoch 00079: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0633 - acc: 0.9810 - val_loss: 1.3634 - val_acc: 0.7475\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9794\n",
      "Epoch 00080: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0722 - acc: 0.9794 - val_loss: 1.0814 - val_acc: 0.7715\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9809\n",
      "Epoch 00081: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0646 - acc: 0.9809 - val_loss: 0.8830 - val_acc: 0.8171\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9845\n",
      "Epoch 00082: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0545 - acc: 0.9845 - val_loss: 1.4866 - val_acc: 0.7254\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9845\n",
      "Epoch 00083: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0553 - acc: 0.9845 - val_loss: 1.3143 - val_acc: 0.7549\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9790\n",
      "Epoch 00084: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0699 - acc: 0.9790 - val_loss: 1.2094 - val_acc: 0.7540\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9869\n",
      "Epoch 00085: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0472 - acc: 0.9869 - val_loss: 1.0232 - val_acc: 0.7894\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9835\n",
      "Epoch 00086: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0559 - acc: 0.9835 - val_loss: 1.4152 - val_acc: 0.7372\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9859\n",
      "Epoch 00087: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0513 - acc: 0.9859 - val_loss: 1.2005 - val_acc: 0.7654\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9825\n",
      "Epoch 00088: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0574 - acc: 0.9825 - val_loss: 1.9277 - val_acc: 0.6739\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9825\n",
      "Epoch 00089: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0582 - acc: 0.9825 - val_loss: 1.0724 - val_acc: 0.7794\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9863\n",
      "Epoch 00090: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0505 - acc: 0.9863 - val_loss: 1.7366 - val_acc: 0.6949\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9826\n",
      "Epoch 00091: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0595 - acc: 0.9826 - val_loss: 1.0112 - val_acc: 0.7927\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9861\n",
      "Epoch 00092: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0498 - acc: 0.9861 - val_loss: 1.7125 - val_acc: 0.6911\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9851\n",
      "Epoch 00093: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0528 - acc: 0.9851 - val_loss: 1.0436 - val_acc: 0.7862\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9833\n",
      "Epoch 00094: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0543 - acc: 0.9833 - val_loss: 1.2401 - val_acc: 0.7508\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9867\n",
      "Epoch 00095: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0467 - acc: 0.9867 - val_loss: 1.4220 - val_acc: 0.7356\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9803\n",
      "Epoch 00096: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0642 - acc: 0.9803 - val_loss: 1.1564 - val_acc: 0.7743\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9864\n",
      "Epoch 00097: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0496 - acc: 0.9863 - val_loss: 1.1536 - val_acc: 0.7682\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9857\n",
      "Epoch 00098: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0506 - acc: 0.9857 - val_loss: 1.8430 - val_acc: 0.6869\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9886\n",
      "Epoch 00099: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0412 - acc: 0.9885 - val_loss: 1.0045 - val_acc: 0.7932\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9837\n",
      "Epoch 00100: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0549 - acc: 0.9836 - val_loss: 1.6459 - val_acc: 0.7268\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9868\n",
      "Epoch 00101: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0471 - acc: 0.9868 - val_loss: 1.7731 - val_acc: 0.6967\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9865\n",
      "Epoch 00102: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0471 - acc: 0.9865 - val_loss: 1.1188 - val_acc: 0.7727\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9873\n",
      "Epoch 00103: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0446 - acc: 0.9873 - val_loss: 1.3144 - val_acc: 0.7685\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9835\n",
      "Epoch 00104: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0534 - acc: 0.9835 - val_loss: 1.3452 - val_acc: 0.7522\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9865\n",
      "Epoch 00105: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0467 - acc: 0.9865 - val_loss: 1.0308 - val_acc: 0.7980\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9871\n",
      "Epoch 00106: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0466 - acc: 0.9871 - val_loss: 1.3122 - val_acc: 0.7699\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9867\n",
      "Epoch 00107: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0477 - acc: 0.9866 - val_loss: 1.1371 - val_acc: 0.7829\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9840\n",
      "Epoch 00108: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0558 - acc: 0.9841 - val_loss: 1.2538 - val_acc: 0.7636\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9896\n",
      "Epoch 00109: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0370 - acc: 0.9896 - val_loss: 1.3834 - val_acc: 0.7491\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9844\n",
      "Epoch 00110: val_loss did not improve from 0.79977\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0534 - acc: 0.9844 - val_loss: 1.1276 - val_acc: 0.7887\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.9171 - acc: 0.7780\n",
      "Loss: 0.9171340922824072 Accuracy: 0.77798545\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0179 - acc: 0.3688\n",
      "Epoch 00001: val_loss improved from inf to 1.73944, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/001-1.7394.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 2.0179 - acc: 0.3688 - val_loss: 1.7394 - val_acc: 0.4559\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3865 - acc: 0.5681\n",
      "Epoch 00002: val_loss did not improve from 1.73944\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.3864 - acc: 0.5682 - val_loss: 1.8575 - val_acc: 0.4754\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1295 - acc: 0.6554\n",
      "Epoch 00003: val_loss improved from 1.73944 to 1.50347, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/003-1.5035.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.1296 - acc: 0.6554 - val_loss: 1.5035 - val_acc: 0.5507\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9723 - acc: 0.7100\n",
      "Epoch 00004: val_loss improved from 1.50347 to 1.35717, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/004-1.3572.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.9722 - acc: 0.7100 - val_loss: 1.3572 - val_acc: 0.6210\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8548 - acc: 0.7468\n",
      "Epoch 00005: val_loss improved from 1.35717 to 1.01696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/005-1.0170.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.8548 - acc: 0.7468 - val_loss: 1.0170 - val_acc: 0.7032\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7739\n",
      "Epoch 00006: val_loss did not improve from 1.01696\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.7701 - acc: 0.7739 - val_loss: 1.0917 - val_acc: 0.6804\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.7976\n",
      "Epoch 00007: val_loss did not improve from 1.01696\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.7007 - acc: 0.7976 - val_loss: 1.1256 - val_acc: 0.6567\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6461 - acc: 0.8140\n",
      "Epoch 00008: val_loss improved from 1.01696 to 0.73919, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/008-0.7392.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.6461 - acc: 0.8140 - val_loss: 0.7392 - val_acc: 0.7855\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5909 - acc: 0.8282\n",
      "Epoch 00009: val_loss improved from 0.73919 to 0.73899, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/009-0.7390.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5912 - acc: 0.8281 - val_loss: 0.7390 - val_acc: 0.7803\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8387\n",
      "Epoch 00010: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5557 - acc: 0.8387 - val_loss: 0.8385 - val_acc: 0.7596\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8513\n",
      "Epoch 00011: val_loss improved from 0.73899 to 0.61085, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/011-0.6109.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5189 - acc: 0.8513 - val_loss: 0.6109 - val_acc: 0.8244\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8624\n",
      "Epoch 00012: val_loss improved from 0.61085 to 0.60445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/012-0.6045.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4792 - acc: 0.8624 - val_loss: 0.6045 - val_acc: 0.8248\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4483 - acc: 0.8723\n",
      "Epoch 00013: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4483 - acc: 0.8722 - val_loss: 0.7023 - val_acc: 0.8025\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8796\n",
      "Epoch 00014: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4183 - acc: 0.8795 - val_loss: 1.1053 - val_acc: 0.7002\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8832\n",
      "Epoch 00015: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4031 - acc: 0.8832 - val_loss: 0.7548 - val_acc: 0.7939\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8925\n",
      "Epoch 00016: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3738 - acc: 0.8924 - val_loss: 1.3799 - val_acc: 0.6765\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8973\n",
      "Epoch 00017: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3574 - acc: 0.8972 - val_loss: 0.7748 - val_acc: 0.7682\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.9023\n",
      "Epoch 00018: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3389 - acc: 0.9022 - val_loss: 0.8637 - val_acc: 0.7722\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9064\n",
      "Epoch 00019: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3211 - acc: 0.9064 - val_loss: 0.8681 - val_acc: 0.7566\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9114\n",
      "Epoch 00020: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3060 - acc: 0.9114 - val_loss: 1.1614 - val_acc: 0.7123\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9180\n",
      "Epoch 00021: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2865 - acc: 0.9180 - val_loss: 0.8792 - val_acc: 0.7692\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9214\n",
      "Epoch 00022: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2742 - acc: 0.9213 - val_loss: 0.9817 - val_acc: 0.7193\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9241\n",
      "Epoch 00023: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2640 - acc: 0.9241 - val_loss: 0.7984 - val_acc: 0.7857\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9272\n",
      "Epoch 00024: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2473 - acc: 0.9272 - val_loss: 0.8151 - val_acc: 0.7782\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9317\n",
      "Epoch 00025: val_loss did not improve from 0.60445\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2404 - acc: 0.9317 - val_loss: 0.8747 - val_acc: 0.7710\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9317\n",
      "Epoch 00026: val_loss improved from 0.60445 to 0.53661, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/026-0.5366.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2298 - acc: 0.9317 - val_loss: 0.5366 - val_acc: 0.8442\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9349\n",
      "Epoch 00027: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2229 - acc: 0.9349 - val_loss: 0.8532 - val_acc: 0.7680\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9333\n",
      "Epoch 00028: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2269 - acc: 0.9333 - val_loss: 0.7090 - val_acc: 0.8195\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9400\n",
      "Epoch 00029: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2068 - acc: 0.9400 - val_loss: 1.8418 - val_acc: 0.6569\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9460\n",
      "Epoch 00030: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1888 - acc: 0.9460 - val_loss: 0.7117 - val_acc: 0.8083\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9485\n",
      "Epoch 00031: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1796 - acc: 0.9485 - val_loss: 0.8324 - val_acc: 0.7794\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9443\n",
      "Epoch 00032: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1882 - acc: 0.9444 - val_loss: 0.5675 - val_acc: 0.8393\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9521\n",
      "Epoch 00033: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1678 - acc: 0.9522 - val_loss: 0.7896 - val_acc: 0.7964\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9539\n",
      "Epoch 00034: val_loss did not improve from 0.53661\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1604 - acc: 0.9539 - val_loss: 0.5777 - val_acc: 0.8467\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9536\n",
      "Epoch 00035: val_loss improved from 0.53661 to 0.43923, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/035-0.4392.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1580 - acc: 0.9535 - val_loss: 0.4392 - val_acc: 0.8742\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9547\n",
      "Epoch 00036: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1544 - acc: 0.9547 - val_loss: 0.5622 - val_acc: 0.8514\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9553\n",
      "Epoch 00037: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1522 - acc: 0.9553 - val_loss: 1.2831 - val_acc: 0.7163\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9562\n",
      "Epoch 00038: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1462 - acc: 0.9562 - val_loss: 0.9817 - val_acc: 0.7484\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9623\n",
      "Epoch 00039: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1285 - acc: 0.9623 - val_loss: 0.6019 - val_acc: 0.8442\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9616\n",
      "Epoch 00040: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1288 - acc: 0.9616 - val_loss: 0.7915 - val_acc: 0.7962\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9650\n",
      "Epoch 00041: val_loss did not improve from 0.43923\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1212 - acc: 0.9650 - val_loss: 0.5473 - val_acc: 0.8609\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9618\n",
      "Epoch 00042: val_loss improved from 0.43923 to 0.41990, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/042-0.4199.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1297 - acc: 0.9618 - val_loss: 0.4199 - val_acc: 0.8821\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9671\n",
      "Epoch 00043: val_loss improved from 0.41990 to 0.41095, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/043-0.4109.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1127 - acc: 0.9671 - val_loss: 0.4109 - val_acc: 0.8880\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9700\n",
      "Epoch 00044: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1067 - acc: 0.9700 - val_loss: 1.0251 - val_acc: 0.7650\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9685\n",
      "Epoch 00045: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1097 - acc: 0.9685 - val_loss: 0.5360 - val_acc: 0.8644\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9699\n",
      "Epoch 00046: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1050 - acc: 0.9699 - val_loss: 0.4990 - val_acc: 0.8749\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9670\n",
      "Epoch 00047: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1087 - acc: 0.9670 - val_loss: 1.1311 - val_acc: 0.7417\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9714\n",
      "Epoch 00048: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0970 - acc: 0.9714 - val_loss: 0.7403 - val_acc: 0.8227\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9751\n",
      "Epoch 00049: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0875 - acc: 0.9750 - val_loss: 1.0544 - val_acc: 0.7549\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9737\n",
      "Epoch 00050: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0933 - acc: 0.9736 - val_loss: 0.9607 - val_acc: 0.7724\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9704\n",
      "Epoch 00051: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0995 - acc: 0.9704 - val_loss: 0.8858 - val_acc: 0.7892\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9775\n",
      "Epoch 00052: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0796 - acc: 0.9775 - val_loss: 0.9500 - val_acc: 0.7831\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9742\n",
      "Epoch 00053: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0870 - acc: 0.9742 - val_loss: 0.6313 - val_acc: 0.8519\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9757\n",
      "Epoch 00054: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0810 - acc: 0.9756 - val_loss: 1.7196 - val_acc: 0.6904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9735\n",
      "Epoch 00055: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0899 - acc: 0.9735 - val_loss: 0.6598 - val_acc: 0.8456\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9756\n",
      "Epoch 00056: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0823 - acc: 0.9755 - val_loss: 0.9050 - val_acc: 0.7929\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9769\n",
      "Epoch 00057: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0772 - acc: 0.9769 - val_loss: 0.8985 - val_acc: 0.8137\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9760\n",
      "Epoch 00058: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0831 - acc: 0.9759 - val_loss: 1.1394 - val_acc: 0.7435\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9798\n",
      "Epoch 00059: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0710 - acc: 0.9798 - val_loss: 0.5863 - val_acc: 0.8558\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9812\n",
      "Epoch 00060: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0659 - acc: 0.9812 - val_loss: 0.8820 - val_acc: 0.7999\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9800\n",
      "Epoch 00061: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0698 - acc: 0.9799 - val_loss: 1.0361 - val_acc: 0.7773\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9749\n",
      "Epoch 00062: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0883 - acc: 0.9749 - val_loss: 0.7853 - val_acc: 0.8260\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9813\n",
      "Epoch 00063: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0668 - acc: 0.9813 - val_loss: 0.5984 - val_acc: 0.8588\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9835\n",
      "Epoch 00064: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0575 - acc: 0.9835 - val_loss: 0.4954 - val_acc: 0.8763\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9807\n",
      "Epoch 00065: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0687 - acc: 0.9807 - val_loss: 0.4887 - val_acc: 0.8814\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9836\n",
      "Epoch 00066: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0577 - acc: 0.9836 - val_loss: 0.6482 - val_acc: 0.8591\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9813\n",
      "Epoch 00067: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0654 - acc: 0.9813 - val_loss: 0.8734 - val_acc: 0.8169\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9859\n",
      "Epoch 00068: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0518 - acc: 0.9858 - val_loss: 0.5046 - val_acc: 0.8838\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9768\n",
      "Epoch 00069: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0776 - acc: 0.9768 - val_loss: 0.9236 - val_acc: 0.8055\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9866\n",
      "Epoch 00070: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0502 - acc: 0.9866 - val_loss: 0.6814 - val_acc: 0.8451\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9846\n",
      "Epoch 00071: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0537 - acc: 0.9846 - val_loss: 0.5129 - val_acc: 0.8840\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9789\n",
      "Epoch 00072: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0740 - acc: 0.9789 - val_loss: 0.7106 - val_acc: 0.8337\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9866\n",
      "Epoch 00073: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0478 - acc: 0.9866 - val_loss: 0.6185 - val_acc: 0.8498\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9851\n",
      "Epoch 00074: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0539 - acc: 0.9850 - val_loss: 0.9275 - val_acc: 0.7992\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9851\n",
      "Epoch 00075: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0519 - acc: 0.9851 - val_loss: 1.6409 - val_acc: 0.7263\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9873\n",
      "Epoch 00076: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0459 - acc: 0.9873 - val_loss: 0.5021 - val_acc: 0.8861\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9840\n",
      "Epoch 00077: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0551 - acc: 0.9840 - val_loss: 1.2367 - val_acc: 0.7580\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9849\n",
      "Epoch 00078: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0503 - acc: 0.9849 - val_loss: 0.4321 - val_acc: 0.8968\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9830\n",
      "Epoch 00079: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0583 - acc: 0.9830 - val_loss: 0.5490 - val_acc: 0.8735\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9880\n",
      "Epoch 00080: val_loss did not improve from 0.41095\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0429 - acc: 0.9880 - val_loss: 0.6180 - val_acc: 0.8696\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9878\n",
      "Epoch 00081: val_loss improved from 0.41095 to 0.41070, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/081-0.4107.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0456 - acc: 0.9878 - val_loss: 0.4107 - val_acc: 0.9045\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9883\n",
      "Epoch 00082: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0439 - acc: 0.9883 - val_loss: 1.2175 - val_acc: 0.7566\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9871\n",
      "Epoch 00083: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0457 - acc: 0.9871 - val_loss: 0.5141 - val_acc: 0.8840\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9872\n",
      "Epoch 00084: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0458 - acc: 0.9872 - val_loss: 0.4117 - val_acc: 0.9043\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9862\n",
      "Epoch 00085: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0489 - acc: 0.9862 - val_loss: 0.5507 - val_acc: 0.8677\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9838\n",
      "Epoch 00086: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0534 - acc: 0.9838 - val_loss: 0.5328 - val_acc: 0.8835\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9878\n",
      "Epoch 00087: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0406 - acc: 0.9878 - val_loss: 0.4695 - val_acc: 0.8933\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9849\n",
      "Epoch 00088: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0540 - acc: 0.9849 - val_loss: 0.4815 - val_acc: 0.8935\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9883\n",
      "Epoch 00089: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0395 - acc: 0.9883 - val_loss: 0.8141 - val_acc: 0.8369\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9895\n",
      "Epoch 00090: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0387 - acc: 0.9894 - val_loss: 0.4193 - val_acc: 0.9026\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9848\n",
      "Epoch 00091: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0535 - acc: 0.9847 - val_loss: 0.8325 - val_acc: 0.8304\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9837\n",
      "Epoch 00092: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0547 - acc: 0.9837 - val_loss: 0.5330 - val_acc: 0.8793\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9928\n",
      "Epoch 00093: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0281 - acc: 0.9928 - val_loss: 0.5510 - val_acc: 0.8798\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9877\n",
      "Epoch 00094: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0442 - acc: 0.9877 - val_loss: 0.5262 - val_acc: 0.8866\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9857\n",
      "Epoch 00095: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0486 - acc: 0.9856 - val_loss: 0.5955 - val_acc: 0.8668\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9879\n",
      "Epoch 00096: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0406 - acc: 0.9879 - val_loss: 0.7749 - val_acc: 0.8362\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9923\n",
      "Epoch 00097: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0303 - acc: 0.9923 - val_loss: 0.9239 - val_acc: 0.8055\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9883\n",
      "Epoch 00098: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0404 - acc: 0.9882 - val_loss: 0.6351 - val_acc: 0.8616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9844\n",
      "Epoch 00099: val_loss did not improve from 0.41070\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0566 - acc: 0.9844 - val_loss: 0.5920 - val_acc: 0.8707\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9875\n",
      "Epoch 00100: val_loss improved from 0.41070 to 0.40614, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/100-0.4061.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0434 - acc: 0.9875 - val_loss: 0.4061 - val_acc: 0.9075\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9917\n",
      "Epoch 00101: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0300 - acc: 0.9917 - val_loss: 0.5097 - val_acc: 0.8807\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9910\n",
      "Epoch 00102: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0332 - acc: 0.9910 - val_loss: 1.0645 - val_acc: 0.8008\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9866\n",
      "Epoch 00103: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0445 - acc: 0.9866 - val_loss: 0.5018 - val_acc: 0.8863\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9925\n",
      "Epoch 00104: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0275 - acc: 0.9925 - val_loss: 0.4680 - val_acc: 0.8987\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9870\n",
      "Epoch 00105: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0454 - acc: 0.9870 - val_loss: 0.5852 - val_acc: 0.8744\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9854\n",
      "Epoch 00106: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0505 - acc: 0.9854 - val_loss: 0.6659 - val_acc: 0.8612\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9925\n",
      "Epoch 00107: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0298 - acc: 0.9925 - val_loss: 0.8453 - val_acc: 0.8281\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9885\n",
      "Epoch 00108: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0399 - acc: 0.9885 - val_loss: 0.5442 - val_acc: 0.8831\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9908\n",
      "Epoch 00109: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0353 - acc: 0.9908 - val_loss: 0.4811 - val_acc: 0.8910\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9942\n",
      "Epoch 00110: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0228 - acc: 0.9942 - val_loss: 1.1610 - val_acc: 0.8076\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9847\n",
      "Epoch 00111: val_loss did not improve from 0.40614\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0506 - acc: 0.9846 - val_loss: 1.1102 - val_acc: 0.8018\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9882\n",
      "Epoch 00112: val_loss improved from 0.40614 to 0.40556, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/112-0.4056.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0399 - acc: 0.9882 - val_loss: 0.4056 - val_acc: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9874\n",
      "Epoch 00113: val_loss did not improve from 0.40556\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0449 - acc: 0.9874 - val_loss: 0.4819 - val_acc: 0.9001\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9887\n",
      "Epoch 00114: val_loss did not improve from 0.40556\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0391 - acc: 0.9887 - val_loss: 0.6529 - val_acc: 0.8567\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9901\n",
      "Epoch 00115: val_loss improved from 0.40556 to 0.33947, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv_checkpoint/115-0.3395.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0374 - acc: 0.9901 - val_loss: 0.3395 - val_acc: 0.9222\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9945\n",
      "Epoch 00116: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0225 - acc: 0.9945 - val_loss: 0.9749 - val_acc: 0.8043\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9871\n",
      "Epoch 00117: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0431 - acc: 0.9871 - val_loss: 0.5421 - val_acc: 0.8817\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9924\n",
      "Epoch 00118: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0280 - acc: 0.9924 - val_loss: 1.5946 - val_acc: 0.7303\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9848\n",
      "Epoch 00119: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0524 - acc: 0.9848 - val_loss: 0.7554 - val_acc: 0.8491\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9865\n",
      "Epoch 00120: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0461 - acc: 0.9865 - val_loss: 0.7375 - val_acc: 0.8502\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9949\n",
      "Epoch 00121: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0213 - acc: 0.9949 - val_loss: 0.5908 - val_acc: 0.8779\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9893\n",
      "Epoch 00122: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0388 - acc: 0.9893 - val_loss: 0.4514 - val_acc: 0.8994\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9926\n",
      "Epoch 00123: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0290 - acc: 0.9925 - val_loss: 0.5451 - val_acc: 0.8840\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9917\n",
      "Epoch 00124: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0306 - acc: 0.9917 - val_loss: 0.4390 - val_acc: 0.9075\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9808\n",
      "Epoch 00125: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0696 - acc: 0.9808 - val_loss: 0.4887 - val_acc: 0.8968\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9954\n",
      "Epoch 00126: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0186 - acc: 0.9954 - val_loss: 0.6663 - val_acc: 0.8640\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9927\n",
      "Epoch 00127: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0265 - acc: 0.9927 - val_loss: 1.1102 - val_acc: 0.7876\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9912\n",
      "Epoch 00128: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0302 - acc: 0.9912 - val_loss: 0.5680 - val_acc: 0.8796\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9916\n",
      "Epoch 00129: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0315 - acc: 0.9916 - val_loss: 0.6455 - val_acc: 0.8698\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9925\n",
      "Epoch 00130: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0279 - acc: 0.9925 - val_loss: 1.2336 - val_acc: 0.7764\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9926\n",
      "Epoch 00131: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0273 - acc: 0.9926 - val_loss: 0.8358 - val_acc: 0.8325\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9920\n",
      "Epoch 00132: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0279 - acc: 0.9920 - val_loss: 0.4542 - val_acc: 0.9064\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9907\n",
      "Epoch 00133: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0322 - acc: 0.9907 - val_loss: 0.5185 - val_acc: 0.8917\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9899\n",
      "Epoch 00134: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0348 - acc: 0.9899 - val_loss: 1.0318 - val_acc: 0.8111\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9936\n",
      "Epoch 00135: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0228 - acc: 0.9935 - val_loss: 0.4852 - val_acc: 0.8996\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9880\n",
      "Epoch 00136: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0421 - acc: 0.9880 - val_loss: 0.3991 - val_acc: 0.9119\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9950\n",
      "Epoch 00137: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0207 - acc: 0.9950 - val_loss: 0.6190 - val_acc: 0.8686\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9909\n",
      "Epoch 00138: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0317 - acc: 0.9908 - val_loss: 1.0429 - val_acc: 0.8097\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9880\n",
      "Epoch 00139: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0417 - acc: 0.9880 - val_loss: 0.3774 - val_acc: 0.9168\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9927\n",
      "Epoch 00140: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0286 - acc: 0.9927 - val_loss: 0.4457 - val_acc: 0.9050\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9912\n",
      "Epoch 00141: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0335 - acc: 0.9912 - val_loss: 0.8209 - val_acc: 0.8334\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9958\n",
      "Epoch 00142: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0173 - acc: 0.9958 - val_loss: 0.3487 - val_acc: 0.9243\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9959\n",
      "Epoch 00143: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0176 - acc: 0.9959 - val_loss: 0.5943 - val_acc: 0.8705\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9899\n",
      "Epoch 00144: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0338 - acc: 0.9899 - val_loss: 0.9386 - val_acc: 0.8148\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9864\n",
      "Epoch 00145: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0476 - acc: 0.9864 - val_loss: 0.4433 - val_acc: 0.9099\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9950\n",
      "Epoch 00146: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0191 - acc: 0.9950 - val_loss: 0.9093 - val_acc: 0.8314\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9881\n",
      "Epoch 00147: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0414 - acc: 0.9881 - val_loss: 0.3926 - val_acc: 0.9133\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9928\n",
      "Epoch 00148: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0263 - acc: 0.9928 - val_loss: 0.4276 - val_acc: 0.9126\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9905\n",
      "Epoch 00149: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0326 - acc: 0.9905 - val_loss: 0.5219 - val_acc: 0.8977\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9927\n",
      "Epoch 00150: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0247 - acc: 0.9927 - val_loss: 0.6029 - val_acc: 0.8779\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9949\n",
      "Epoch 00151: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0207 - acc: 0.9949 - val_loss: 0.6387 - val_acc: 0.8642\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9926\n",
      "Epoch 00152: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0258 - acc: 0.9926 - val_loss: 0.9299 - val_acc: 0.8304\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9932\n",
      "Epoch 00153: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0256 - acc: 0.9932 - val_loss: 0.5731 - val_acc: 0.8842\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9954\n",
      "Epoch 00154: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0191 - acc: 0.9954 - val_loss: 0.4653 - val_acc: 0.9040\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9898\n",
      "Epoch 00155: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0348 - acc: 0.9898 - val_loss: 0.7796 - val_acc: 0.8465\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9893\n",
      "Epoch 00156: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0363 - acc: 0.9892 - val_loss: 1.1569 - val_acc: 0.7934\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9899\n",
      "Epoch 00157: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0334 - acc: 0.9899 - val_loss: 0.3846 - val_acc: 0.9248\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9952\n",
      "Epoch 00158: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0169 - acc: 0.9952 - val_loss: 1.2760 - val_acc: 0.7731\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9923\n",
      "Epoch 00159: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0265 - acc: 0.9923 - val_loss: 0.4365 - val_acc: 0.9071\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9931\n",
      "Epoch 00160: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0243 - acc: 0.9931 - val_loss: 0.9202 - val_acc: 0.8339\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9924\n",
      "Epoch 00161: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0261 - acc: 0.9924 - val_loss: 0.4733 - val_acc: 0.9001\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9931\n",
      "Epoch 00162: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0243 - acc: 0.9931 - val_loss: 0.7525 - val_acc: 0.8423\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9941\n",
      "Epoch 00163: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0211 - acc: 0.9941 - val_loss: 0.7367 - val_acc: 0.8556\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9913\n",
      "Epoch 00164: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0304 - acc: 0.9913 - val_loss: 0.5011 - val_acc: 0.8991\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9930\n",
      "Epoch 00165: val_loss did not improve from 0.33947\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0259 - acc: 0.9930 - val_loss: 0.5055 - val_acc: 0.9064\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.4533 - acc: 0.9001\n",
      "Loss: 0.45326784061246694 Accuracy: 0.90010387\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9961 - acc: 0.3804\n",
      "Epoch 00001: val_loss improved from inf to 1.65901, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/001-1.6590.hdf5\n",
      "36805/36805 [==============================] - 199s 5ms/sample - loss: 1.9962 - acc: 0.3804 - val_loss: 1.6590 - val_acc: 0.4752\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3224 - acc: 0.5980\n",
      "Epoch 00002: val_loss improved from 1.65901 to 1.33849, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/002-1.3385.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 1.3223 - acc: 0.5980 - val_loss: 1.3385 - val_acc: 0.5821\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0195 - acc: 0.6964\n",
      "Epoch 00003: val_loss improved from 1.33849 to 0.89478, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/003-0.8948.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 1.0195 - acc: 0.6963 - val_loss: 0.8948 - val_acc: 0.7410\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7558\n",
      "Epoch 00004: val_loss did not improve from 0.89478\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.8325 - acc: 0.7557 - val_loss: 0.9038 - val_acc: 0.7254\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7986\n",
      "Epoch 00005: val_loss improved from 0.89478 to 0.65164, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/005-0.6516.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.6974 - acc: 0.7986 - val_loss: 0.6516 - val_acc: 0.8157\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.8284\n",
      "Epoch 00006: val_loss did not improve from 0.65164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5950 - acc: 0.8283 - val_loss: 0.9298 - val_acc: 0.7396\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8517\n",
      "Epoch 00007: val_loss did not improve from 0.65164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5196 - acc: 0.8517 - val_loss: 0.6592 - val_acc: 0.8050\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8691\n",
      "Epoch 00008: val_loss improved from 0.65164 to 0.45210, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/008-0.4521.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4589 - acc: 0.8691 - val_loss: 0.4521 - val_acc: 0.8707\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8790\n",
      "Epoch 00009: val_loss did not improve from 0.45210\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.4155 - acc: 0.8790 - val_loss: 0.5309 - val_acc: 0.8460\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8919\n",
      "Epoch 00010: val_loss did not improve from 0.45210\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.3753 - acc: 0.8919 - val_loss: 0.6744 - val_acc: 0.8015\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.9001\n",
      "Epoch 00011: val_loss improved from 0.45210 to 0.40415, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/011-0.4042.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.3508 - acc: 0.9001 - val_loss: 0.4042 - val_acc: 0.8798\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9070\n",
      "Epoch 00012: val_loss improved from 0.40415 to 0.40108, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/012-0.4011.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.3235 - acc: 0.9070 - val_loss: 0.4011 - val_acc: 0.8854\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9138\n",
      "Epoch 00013: val_loss did not improve from 0.40108\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2961 - acc: 0.9138 - val_loss: 0.4874 - val_acc: 0.8591\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9165\n",
      "Epoch 00014: val_loss did not improve from 0.40108\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2847 - acc: 0.9165 - val_loss: 0.4594 - val_acc: 0.8647\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9235\n",
      "Epoch 00015: val_loss did not improve from 0.40108\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2636 - acc: 0.9235 - val_loss: 0.4114 - val_acc: 0.8810\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9274\n",
      "Epoch 00016: val_loss improved from 0.40108 to 0.33573, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/016-0.3357.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2487 - acc: 0.9274 - val_loss: 0.3357 - val_acc: 0.8984\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9330\n",
      "Epoch 00017: val_loss did not improve from 0.33573\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2346 - acc: 0.9331 - val_loss: 0.5244 - val_acc: 0.8498\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9381\n",
      "Epoch 00018: val_loss did not improve from 0.33573\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2117 - acc: 0.9381 - val_loss: 0.4362 - val_acc: 0.8686\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9405\n",
      "Epoch 00019: val_loss improved from 0.33573 to 0.27627, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/019-0.2763.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2031 - acc: 0.9404 - val_loss: 0.2763 - val_acc: 0.9180\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9413\n",
      "Epoch 00020: val_loss did not improve from 0.27627\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2014 - acc: 0.9412 - val_loss: 0.5707 - val_acc: 0.8418\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9437\n",
      "Epoch 00021: val_loss did not improve from 0.27627\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1871 - acc: 0.9437 - val_loss: 0.4677 - val_acc: 0.8626\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9489\n",
      "Epoch 00022: val_loss improved from 0.27627 to 0.24825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/022-0.2483.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1744 - acc: 0.9489 - val_loss: 0.2483 - val_acc: 0.9280\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9487\n",
      "Epoch 00023: val_loss did not improve from 0.24825\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1756 - acc: 0.9487 - val_loss: 0.3342 - val_acc: 0.9073\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9538\n",
      "Epoch 00024: val_loss did not improve from 0.24825\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1551 - acc: 0.9538 - val_loss: 0.3107 - val_acc: 0.9138\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9585\n",
      "Epoch 00025: val_loss did not improve from 0.24825\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1429 - acc: 0.9584 - val_loss: 0.3723 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9573\n",
      "Epoch 00026: val_loss did not improve from 0.24825\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1448 - acc: 0.9573 - val_loss: 0.6452 - val_acc: 0.8174\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9599\n",
      "Epoch 00027: val_loss did not improve from 0.24825\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1349 - acc: 0.9599 - val_loss: 0.2969 - val_acc: 0.9201\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9639\n",
      "Epoch 00028: val_loss improved from 0.24825 to 0.23164, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/028-0.2316.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1231 - acc: 0.9639 - val_loss: 0.2316 - val_acc: 0.9341\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9622\n",
      "Epoch 00029: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1299 - acc: 0.9622 - val_loss: 0.3434 - val_acc: 0.8952\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9686\n",
      "Epoch 00030: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1089 - acc: 0.9686 - val_loss: 0.3828 - val_acc: 0.8873\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9651\n",
      "Epoch 00031: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1156 - acc: 0.9651 - val_loss: 0.4568 - val_acc: 0.8756\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9692\n",
      "Epoch 00032: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1053 - acc: 0.9692 - val_loss: 0.3774 - val_acc: 0.8947\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9708\n",
      "Epoch 00033: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1012 - acc: 0.9708 - val_loss: 0.6570 - val_acc: 0.8139\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9720\n",
      "Epoch 00034: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0954 - acc: 0.9720 - val_loss: 0.5301 - val_acc: 0.8598\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9735\n",
      "Epoch 00035: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0906 - acc: 0.9735 - val_loss: 0.5274 - val_acc: 0.8528\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9755\n",
      "Epoch 00036: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0849 - acc: 0.9755 - val_loss: 0.2675 - val_acc: 0.9264\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9743\n",
      "Epoch 00037: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0892 - acc: 0.9743 - val_loss: 0.2894 - val_acc: 0.9234\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9742\n",
      "Epoch 00038: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0876 - acc: 0.9742 - val_loss: 0.2459 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9793\n",
      "Epoch 00039: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0735 - acc: 0.9793 - val_loss: 0.5992 - val_acc: 0.8451\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9812\n",
      "Epoch 00040: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0686 - acc: 0.9812 - val_loss: 0.2800 - val_acc: 0.9180\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9818\n",
      "Epoch 00041: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0642 - acc: 0.9818 - val_loss: 0.3442 - val_acc: 0.9078\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9765\n",
      "Epoch 00042: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0787 - acc: 0.9765 - val_loss: 0.4190 - val_acc: 0.8875\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9801\n",
      "Epoch 00043: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0658 - acc: 0.9801 - val_loss: 0.3520 - val_acc: 0.9073\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9821\n",
      "Epoch 00044: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0605 - acc: 0.9821 - val_loss: 0.4500 - val_acc: 0.8894\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9823\n",
      "Epoch 00045: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0621 - acc: 0.9823 - val_loss: 0.8917 - val_acc: 0.7852\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9798\n",
      "Epoch 00046: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0693 - acc: 0.9798 - val_loss: 0.4421 - val_acc: 0.8880\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9829\n",
      "Epoch 00047: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0574 - acc: 0.9829 - val_loss: 0.5145 - val_acc: 0.8721\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9773\n",
      "Epoch 00048: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0783 - acc: 0.9773 - val_loss: 0.2626 - val_acc: 0.9359\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9847\n",
      "Epoch 00049: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0530 - acc: 0.9847 - val_loss: 0.6091 - val_acc: 0.8553\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9867\n",
      "Epoch 00050: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0446 - acc: 0.9867 - val_loss: 0.2640 - val_acc: 0.9297\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9867\n",
      "Epoch 00051: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0474 - acc: 0.9867 - val_loss: 0.4912 - val_acc: 0.8805\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9853\n",
      "Epoch 00052: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0510 - acc: 0.9853 - val_loss: 0.3843 - val_acc: 0.9012\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9857\n",
      "Epoch 00053: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0500 - acc: 0.9857 - val_loss: 0.3771 - val_acc: 0.9057\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9855\n",
      "Epoch 00054: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0517 - acc: 0.9855 - val_loss: 0.2715 - val_acc: 0.9308\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9891\n",
      "Epoch 00055: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0393 - acc: 0.9891 - val_loss: 0.3688 - val_acc: 0.9099\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9860\n",
      "Epoch 00056: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0468 - acc: 0.9860 - val_loss: 0.7291 - val_acc: 0.8209\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9889\n",
      "Epoch 00057: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0399 - acc: 0.9889 - val_loss: 0.2363 - val_acc: 0.9397\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9892\n",
      "Epoch 00058: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0409 - acc: 0.9891 - val_loss: 0.5709 - val_acc: 0.8684\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9871\n",
      "Epoch 00059: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0450 - acc: 0.9871 - val_loss: 0.5317 - val_acc: 0.8747\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9847\n",
      "Epoch 00060: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0521 - acc: 0.9847 - val_loss: 0.3023 - val_acc: 0.9187\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9858\n",
      "Epoch 00061: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0479 - acc: 0.9857 - val_loss: 0.2470 - val_acc: 0.9345\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9853\n",
      "Epoch 00062: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0509 - acc: 0.9853 - val_loss: 0.3584 - val_acc: 0.9101\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9934\n",
      "Epoch 00063: val_loss improved from 0.23164 to 0.21913, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/063-0.2191.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0267 - acc: 0.9934 - val_loss: 0.2191 - val_acc: 0.9408\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9921\n",
      "Epoch 00064: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0295 - acc: 0.9920 - val_loss: 0.3365 - val_acc: 0.9159\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9869\n",
      "Epoch 00065: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0464 - acc: 0.9869 - val_loss: 0.2317 - val_acc: 0.9385\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9903\n",
      "Epoch 00066: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0333 - acc: 0.9903 - val_loss: 0.4306 - val_acc: 0.8887\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9916\n",
      "Epoch 00067: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0310 - acc: 0.9916 - val_loss: 0.2774 - val_acc: 0.9290\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9898\n",
      "Epoch 00068: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0360 - acc: 0.9898 - val_loss: 0.3128 - val_acc: 0.9203\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9862\n",
      "Epoch 00069: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0449 - acc: 0.9862 - val_loss: 0.2767 - val_acc: 0.9294\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9916\n",
      "Epoch 00070: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0300 - acc: 0.9916 - val_loss: 0.3280 - val_acc: 0.9185\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9897\n",
      "Epoch 00071: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0352 - acc: 0.9897 - val_loss: 0.2474 - val_acc: 0.9378\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9883\n",
      "Epoch 00072: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0400 - acc: 0.9883 - val_loss: 0.2601 - val_acc: 0.9338\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9937\n",
      "Epoch 00073: val_loss did not improve from 0.21913\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0246 - acc: 0.9937 - val_loss: 0.2999 - val_acc: 0.9304\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9861\n",
      "Epoch 00074: val_loss improved from 0.21913 to 0.20926, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv_checkpoint/074-0.2093.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0483 - acc: 0.9861 - val_loss: 0.2093 - val_acc: 0.9492\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9948\n",
      "Epoch 00075: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0199 - acc: 0.9948 - val_loss: 0.4545 - val_acc: 0.8912\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9917\n",
      "Epoch 00076: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0277 - acc: 0.9917 - val_loss: 0.3286 - val_acc: 0.9187\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9922\n",
      "Epoch 00077: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0273 - acc: 0.9922 - val_loss: 0.2667 - val_acc: 0.9371\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9912\n",
      "Epoch 00078: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0295 - acc: 0.9912 - val_loss: 0.2614 - val_acc: 0.9380\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9923\n",
      "Epoch 00079: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0263 - acc: 0.9923 - val_loss: 0.4392 - val_acc: 0.8975\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9892\n",
      "Epoch 00080: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0373 - acc: 0.9892 - val_loss: 0.3729 - val_acc: 0.9101\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9938\n",
      "Epoch 00081: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0222 - acc: 0.9938 - val_loss: 0.4588 - val_acc: 0.8975\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9887\n",
      "Epoch 00082: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0380 - acc: 0.9886 - val_loss: 0.2546 - val_acc: 0.9329\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9916\n",
      "Epoch 00083: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0286 - acc: 0.9916 - val_loss: 0.3109 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9938\n",
      "Epoch 00084: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0212 - acc: 0.9938 - val_loss: 0.3485 - val_acc: 0.9168\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9926\n",
      "Epoch 00085: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0240 - acc: 0.9926 - val_loss: 0.3112 - val_acc: 0.9299\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9915\n",
      "Epoch 00086: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0281 - acc: 0.9915 - val_loss: 0.2516 - val_acc: 0.9373\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00087: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.3290 - val_acc: 0.9227\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 00088: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0277 - acc: 0.9914 - val_loss: 0.2584 - val_acc: 0.9383\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9913\n",
      "Epoch 00089: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0291 - acc: 0.9913 - val_loss: 0.3369 - val_acc: 0.9220\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9876\n",
      "Epoch 00090: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0429 - acc: 0.9876 - val_loss: 0.3178 - val_acc: 0.9189\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9960\n",
      "Epoch 00091: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0153 - acc: 0.9960 - val_loss: 0.2311 - val_acc: 0.9455\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9954\n",
      "Epoch 00092: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0173 - acc: 0.9953 - val_loss: 0.4421 - val_acc: 0.9029\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9886\n",
      "Epoch 00093: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0381 - acc: 0.9886 - val_loss: 0.2432 - val_acc: 0.9411\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00094: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0164 - acc: 0.9954 - val_loss: 0.4610 - val_acc: 0.8968\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9905\n",
      "Epoch 00095: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0318 - acc: 0.9905 - val_loss: 0.3897 - val_acc: 0.9108\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9964\n",
      "Epoch 00096: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0141 - acc: 0.9964 - val_loss: 0.3268 - val_acc: 0.9236\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9886\n",
      "Epoch 00097: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.3101 - val_acc: 0.9294\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9935\n",
      "Epoch 00098: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0213 - acc: 0.9935 - val_loss: 0.4748 - val_acc: 0.8996\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9935\n",
      "Epoch 00099: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0235 - acc: 0.9935 - val_loss: 0.3736 - val_acc: 0.9145\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9953\n",
      "Epoch 00100: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0177 - acc: 0.9953 - val_loss: 0.2931 - val_acc: 0.9334\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 00101: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0219 - acc: 0.9935 - val_loss: 0.9274 - val_acc: 0.8141\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9947\n",
      "Epoch 00102: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0180 - acc: 0.9946 - val_loss: 0.2996 - val_acc: 0.9359\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9883\n",
      "Epoch 00103: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0396 - acc: 0.9883 - val_loss: 0.2136 - val_acc: 0.9476\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9968\n",
      "Epoch 00104: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0132 - acc: 0.9968 - val_loss: 0.3488 - val_acc: 0.9248\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9931\n",
      "Epoch 00105: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0234 - acc: 0.9931 - val_loss: 0.3273 - val_acc: 0.9243\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 00106: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0301 - acc: 0.9907 - val_loss: 0.2954 - val_acc: 0.9320\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9945\n",
      "Epoch 00107: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0196 - acc: 0.9945 - val_loss: 0.2414 - val_acc: 0.9450\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 00108: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0216 - acc: 0.9937 - val_loss: 0.2271 - val_acc: 0.9462\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9963\n",
      "Epoch 00109: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0134 - acc: 0.9963 - val_loss: 0.3136 - val_acc: 0.9308\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9943\n",
      "Epoch 00110: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0199 - acc: 0.9943 - val_loss: 0.3567 - val_acc: 0.9157\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9944\n",
      "Epoch 00111: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0196 - acc: 0.9944 - val_loss: 0.4387 - val_acc: 0.9140\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9915\n",
      "Epoch 00112: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0301 - acc: 0.9914 - val_loss: 0.2684 - val_acc: 0.9390\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9949\n",
      "Epoch 00113: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0181 - acc: 0.9949 - val_loss: 0.2195 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9963\n",
      "Epoch 00114: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.2775 - val_acc: 0.9369\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9943\n",
      "Epoch 00115: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0214 - acc: 0.9943 - val_loss: 0.2133 - val_acc: 0.9497\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9933\n",
      "Epoch 00116: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0207 - acc: 0.9933 - val_loss: 0.3233 - val_acc: 0.9238\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9958\n",
      "Epoch 00117: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0165 - acc: 0.9957 - val_loss: 0.3630 - val_acc: 0.9196\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9909\n",
      "Epoch 00118: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0306 - acc: 0.9909 - val_loss: 0.2425 - val_acc: 0.9448\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 00119: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0120 - acc: 0.9965 - val_loss: 0.2433 - val_acc: 0.9415\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9961\n",
      "Epoch 00120: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.3366 - val_acc: 0.9224\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9924\n",
      "Epoch 00121: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0253 - acc: 0.9924 - val_loss: 0.2514 - val_acc: 0.9390\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9952\n",
      "Epoch 00122: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 0.3180 - val_acc: 0.9306\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9948\n",
      "Epoch 00123: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0168 - acc: 0.9948 - val_loss: 0.2710 - val_acc: 0.9378\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9940\n",
      "Epoch 00124: val_loss did not improve from 0.20926\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0205 - acc: 0.9940 - val_loss: 0.2639 - val_acc: 0.9399\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2758 - acc: 0.9304\n",
      "Loss: 0.2758367461687184 Accuracy: 0.93042576\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7369 - acc: 0.4610\n",
      "Epoch 00001: val_loss improved from inf to 1.30898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/001-1.3090.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 1.7369 - acc: 0.4610 - val_loss: 1.3090 - val_acc: 0.6047\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.7071\n",
      "Epoch 00002: val_loss improved from 1.30898 to 0.93975, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/002-0.9398.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.9631 - acc: 0.7071 - val_loss: 0.9398 - val_acc: 0.7072\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.8077\n",
      "Epoch 00003: val_loss improved from 0.93975 to 0.54506, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/003-0.5451.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.6514 - acc: 0.8077 - val_loss: 0.5451 - val_acc: 0.8407\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8539\n",
      "Epoch 00004: val_loss improved from 0.54506 to 0.43440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/004-0.4344.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.4923 - acc: 0.8539 - val_loss: 0.4344 - val_acc: 0.8686\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8814\n",
      "Epoch 00005: val_loss improved from 0.43440 to 0.34273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/005-0.3427.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.3980 - acc: 0.8813 - val_loss: 0.3427 - val_acc: 0.8973\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.9009\n",
      "Epoch 00006: val_loss improved from 0.34273 to 0.29039, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/006-0.2904.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.3341 - acc: 0.9009 - val_loss: 0.2904 - val_acc: 0.9126\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9127\n",
      "Epoch 00007: val_loss did not improve from 0.29039\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2930 - acc: 0.9127 - val_loss: 0.3143 - val_acc: 0.9045\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9246\n",
      "Epoch 00008: val_loss improved from 0.29039 to 0.26495, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/008-0.2649.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2521 - acc: 0.9246 - val_loss: 0.2649 - val_acc: 0.9213\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9328\n",
      "Epoch 00009: val_loss did not improve from 0.26495\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2261 - acc: 0.9328 - val_loss: 0.3063 - val_acc: 0.9024\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9367\n",
      "Epoch 00010: val_loss did not improve from 0.26495\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2110 - acc: 0.9367 - val_loss: 0.3106 - val_acc: 0.9015\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9463\n",
      "Epoch 00011: val_loss did not improve from 0.26495\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1796 - acc: 0.9463 - val_loss: 0.2721 - val_acc: 0.9206\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9487\n",
      "Epoch 00012: val_loss improved from 0.26495 to 0.23509, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/012-0.2351.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1731 - acc: 0.9486 - val_loss: 0.2351 - val_acc: 0.9301\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9500\n",
      "Epoch 00013: val_loss did not improve from 0.23509\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1639 - acc: 0.9500 - val_loss: 0.2480 - val_acc: 0.9250\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9576\n",
      "Epoch 00014: val_loss improved from 0.23509 to 0.21507, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/014-0.2151.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1418 - acc: 0.9575 - val_loss: 0.2151 - val_acc: 0.9366\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9595\n",
      "Epoch 00015: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1328 - acc: 0.9595 - val_loss: 0.2373 - val_acc: 0.9252\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9608\n",
      "Epoch 00016: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1287 - acc: 0.9608 - val_loss: 0.2329 - val_acc: 0.9294\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9668\n",
      "Epoch 00017: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1107 - acc: 0.9669 - val_loss: 0.2274 - val_acc: 0.9373\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9702\n",
      "Epoch 00018: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1014 - acc: 0.9702 - val_loss: 0.4671 - val_acc: 0.8542\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9721\n",
      "Epoch 00019: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0936 - acc: 0.9721 - val_loss: 0.2978 - val_acc: 0.9073\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9727\n",
      "Epoch 00020: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0907 - acc: 0.9727 - val_loss: 0.2584 - val_acc: 0.9217\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9744\n",
      "Epoch 00021: val_loss did not improve from 0.21507\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0882 - acc: 0.9744 - val_loss: 0.2175 - val_acc: 0.9362\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9783\n",
      "Epoch 00022: val_loss improved from 0.21507 to 0.19483, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/022-0.1948.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0752 - acc: 0.9783 - val_loss: 0.1948 - val_acc: 0.9443\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9785\n",
      "Epoch 00023: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0755 - acc: 0.9785 - val_loss: 0.2678 - val_acc: 0.9171\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9812\n",
      "Epoch 00024: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0647 - acc: 0.9812 - val_loss: 0.2109 - val_acc: 0.9383\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9804\n",
      "Epoch 00025: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0673 - acc: 0.9804 - val_loss: 0.7505 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9844\n",
      "Epoch 00026: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0543 - acc: 0.9844 - val_loss: 0.2248 - val_acc: 0.9369\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9826\n",
      "Epoch 00027: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0599 - acc: 0.9826 - val_loss: 0.2042 - val_acc: 0.9443\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9846\n",
      "Epoch 00028: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0528 - acc: 0.9846 - val_loss: 0.1958 - val_acc: 0.9446\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9849\n",
      "Epoch 00029: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0508 - acc: 0.9849 - val_loss: 0.4844 - val_acc: 0.8549\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9862\n",
      "Epoch 00030: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0482 - acc: 0.9862 - val_loss: 0.2390 - val_acc: 0.9350\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9871\n",
      "Epoch 00031: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0433 - acc: 0.9871 - val_loss: 0.2934 - val_acc: 0.9171\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9877\n",
      "Epoch 00032: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0417 - acc: 0.9877 - val_loss: 0.2095 - val_acc: 0.9434\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9914\n",
      "Epoch 00033: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0316 - acc: 0.9914 - val_loss: 0.2678 - val_acc: 0.9255\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9878\n",
      "Epoch 00034: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0416 - acc: 0.9878 - val_loss: 0.2967 - val_acc: 0.9189\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9864\n",
      "Epoch 00035: val_loss did not improve from 0.19483\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0469 - acc: 0.9864 - val_loss: 0.2540 - val_acc: 0.9301\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9867\n",
      "Epoch 00036: val_loss improved from 0.19483 to 0.17613, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/036-0.1761.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0447 - acc: 0.9867 - val_loss: 0.1761 - val_acc: 0.9518\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9921\n",
      "Epoch 00037: val_loss improved from 0.17613 to 0.17379, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/037-0.1738.hdf5\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0281 - acc: 0.9921 - val_loss: 0.1738 - val_acc: 0.9527\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9917\n",
      "Epoch 00038: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0309 - acc: 0.9917 - val_loss: 0.2071 - val_acc: 0.9443\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9923\n",
      "Epoch 00039: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0273 - acc: 0.9923 - val_loss: 0.2123 - val_acc: 0.9422\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9914\n",
      "Epoch 00040: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0310 - acc: 0.9914 - val_loss: 0.2158 - val_acc: 0.9425\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9933\n",
      "Epoch 00041: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0238 - acc: 0.9933 - val_loss: 0.2204 - val_acc: 0.9453\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9864\n",
      "Epoch 00042: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0437 - acc: 0.9864 - val_loss: 0.1774 - val_acc: 0.9527\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9942\n",
      "Epoch 00043: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0219 - acc: 0.9942 - val_loss: 0.2123 - val_acc: 0.9474\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9935\n",
      "Epoch 00044: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0227 - acc: 0.9935 - val_loss: 0.5399 - val_acc: 0.8640\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9881\n",
      "Epoch 00045: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0391 - acc: 0.9881 - val_loss: 0.1934 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9946\n",
      "Epoch 00046: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0198 - acc: 0.9946 - val_loss: 0.2334 - val_acc: 0.9413\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9933\n",
      "Epoch 00047: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0238 - acc: 0.9933 - val_loss: 0.2257 - val_acc: 0.9469\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9924\n",
      "Epoch 00048: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0270 - acc: 0.9924 - val_loss: 0.2181 - val_acc: 0.9471\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9942\n",
      "Epoch 00049: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0221 - acc: 0.9941 - val_loss: 0.2296 - val_acc: 0.9425\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9922\n",
      "Epoch 00050: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0266 - acc: 0.9922 - val_loss: 0.1986 - val_acc: 0.9497\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9962\n",
      "Epoch 00051: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0142 - acc: 0.9962 - val_loss: 0.2439 - val_acc: 0.9427\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 00052: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0241 - acc: 0.9930 - val_loss: 0.2089 - val_acc: 0.9471\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9933\n",
      "Epoch 00053: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.2098 - val_acc: 0.9467\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 00054: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0179 - acc: 0.9946 - val_loss: 0.2160 - val_acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9907\n",
      "Epoch 00055: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0312 - acc: 0.9907 - val_loss: 0.2099 - val_acc: 0.9483\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 00056: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0163 - acc: 0.9955 - val_loss: 0.1912 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.2311 - val_acc: 0.9439\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.2268 - val_acc: 0.9443\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9910\n",
      "Epoch 00059: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0287 - acc: 0.9909 - val_loss: 0.2588 - val_acc: 0.9390\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9876\n",
      "Epoch 00060: val_loss did not improve from 0.17379\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0421 - acc: 0.9876 - val_loss: 0.1875 - val_acc: 0.9534\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9957\n",
      "Epoch 00061: val_loss improved from 0.17379 to 0.17024, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/061-0.1702.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.1702 - val_acc: 0.9574\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00062: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0096 - acc: 0.9975 - val_loss: 0.1915 - val_acc: 0.9534\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9925\n",
      "Epoch 00063: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0244 - acc: 0.9925 - val_loss: 0.1960 - val_acc: 0.9513\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9974\n",
      "Epoch 00064: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0097 - acc: 0.9974 - val_loss: 0.1880 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00065: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0161 - acc: 0.9951 - val_loss: 0.2388 - val_acc: 0.9415\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9915\n",
      "Epoch 00066: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0304 - acc: 0.9915 - val_loss: 0.2135 - val_acc: 0.9492\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9981\n",
      "Epoch 00067: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0086 - acc: 0.9981 - val_loss: 0.1799 - val_acc: 0.9583\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9963\n",
      "Epoch 00068: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0132 - acc: 0.9963 - val_loss: 0.2458 - val_acc: 0.9420\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9941\n",
      "Epoch 00069: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0182 - acc: 0.9941 - val_loss: 0.1836 - val_acc: 0.9578\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9933\n",
      "Epoch 00070: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0232 - acc: 0.9933 - val_loss: 0.1754 - val_acc: 0.9597\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9972\n",
      "Epoch 00071: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.2232 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9951\n",
      "Epoch 00072: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0165 - acc: 0.9951 - val_loss: 0.1958 - val_acc: 0.9539\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9962\n",
      "Epoch 00073: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0136 - acc: 0.9962 - val_loss: 0.2034 - val_acc: 0.9515\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9946\n",
      "Epoch 00074: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0188 - acc: 0.9946 - val_loss: 0.1958 - val_acc: 0.9571\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9980\n",
      "Epoch 00075: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0076 - acc: 0.9980 - val_loss: 0.2009 - val_acc: 0.9555\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00076: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.2035 - val_acc: 0.9525\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9985\n",
      "Epoch 00077: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0067 - acc: 0.9985 - val_loss: 0.1962 - val_acc: 0.9574\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 00078: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0171 - acc: 0.9949 - val_loss: 0.3375 - val_acc: 0.9264\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9953\n",
      "Epoch 00079: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0142 - acc: 0.9953 - val_loss: 0.1751 - val_acc: 0.9599\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9984\n",
      "Epoch 00080: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.2487 - val_acc: 0.9434\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9935\n",
      "Epoch 00081: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0207 - acc: 0.9935 - val_loss: 0.2822 - val_acc: 0.9317\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 00082: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0128 - acc: 0.9962 - val_loss: 0.2028 - val_acc: 0.9527\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 00083: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.3027 - val_acc: 0.9345\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9960\n",
      "Epoch 00084: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0145 - acc: 0.9960 - val_loss: 0.2615 - val_acc: 0.9427\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 00085: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0133 - acc: 0.9962 - val_loss: 0.1935 - val_acc: 0.9581\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 00086: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0219 - acc: 0.9935 - val_loss: 0.1839 - val_acc: 0.9613\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00087: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1943 - val_acc: 0.9571\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9923\n",
      "Epoch 00088: val_loss did not improve from 0.17024\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0247 - acc: 0.9923 - val_loss: 0.1831 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00089: val_loss improved from 0.17024 to 0.16799, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/089-0.1680.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1680 - val_acc: 0.9627\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 00090: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0038 - acc: 0.9994 - val_loss: 0.2046 - val_acc: 0.9560\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9903\n",
      "Epoch 00091: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0337 - acc: 0.9903 - val_loss: 0.2160 - val_acc: 0.9543\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9930\n",
      "Epoch 00092: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0231 - acc: 0.9930 - val_loss: 0.1760 - val_acc: 0.9574\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 00093: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1750 - val_acc: 0.9602\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00094: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.2081 - val_acc: 0.9539\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9932\n",
      "Epoch 00095: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0217 - acc: 0.9932 - val_loss: 0.1954 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9955\n",
      "Epoch 00096: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.1972 - val_acc: 0.9543\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979\n",
      "Epoch 00097: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 0.1961 - val_acc: 0.9571\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9952\n",
      "Epoch 00098: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0142 - acc: 0.9952 - val_loss: 0.2040 - val_acc: 0.9539\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 00099: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1858 - val_acc: 0.9590\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9967\n",
      "Epoch 00100: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.3378 - val_acc: 0.9257\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9941\n",
      "Epoch 00101: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0199 - acc: 0.9941 - val_loss: 0.1683 - val_acc: 0.9625\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00102: val_loss did not improve from 0.16799\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 0.1846 - val_acc: 0.9597\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00103: val_loss improved from 0.16799 to 0.16320, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/103-0.1632.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1632 - val_acc: 0.9639\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9955\n",
      "Epoch 00104: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0146 - acc: 0.9954 - val_loss: 0.2488 - val_acc: 0.9434\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9920\n",
      "Epoch 00105: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0268 - acc: 0.9920 - val_loss: 0.1688 - val_acc: 0.9627\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00106: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1718 - val_acc: 0.9616\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00107: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.2342 - val_acc: 0.9481\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9936\n",
      "Epoch 00108: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0210 - acc: 0.9936 - val_loss: 0.1929 - val_acc: 0.9567\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00109: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0043 - acc: 0.9990 - val_loss: 0.1916 - val_acc: 0.9613\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 00110: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0106 - acc: 0.9965 - val_loss: 0.2704 - val_acc: 0.9383\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00111: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.1998 - val_acc: 0.9553\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978\n",
      "Epoch 00112: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 0.4143 - val_acc: 0.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 00113: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0168 - acc: 0.9950 - val_loss: 0.1931 - val_acc: 0.9597\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9989\n",
      "Epoch 00114: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2044 - val_acc: 0.9581\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989\n",
      "Epoch 00115: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.2863 - val_acc: 0.9355\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9945\n",
      "Epoch 00116: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0177 - acc: 0.9945 - val_loss: 0.1979 - val_acc: 0.9571\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00117: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.2140 - val_acc: 0.9550\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 00118: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.1987 - val_acc: 0.9541\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9964\n",
      "Epoch 00119: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0105 - acc: 0.9964 - val_loss: 0.2756 - val_acc: 0.9376\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9940\n",
      "Epoch 00120: val_loss did not improve from 0.16320\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0189 - acc: 0.9940 - val_loss: 0.1806 - val_acc: 0.9606\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00121: val_loss improved from 0.16320 to 0.16287, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv_checkpoint/121-0.1629.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1629 - val_acc: 0.9630\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9972\n",
      "Epoch 00122: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.4959 - val_acc: 0.8863\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00123: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1962 - val_acc: 0.9616\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00124: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.2171 - val_acc: 0.9555\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9953\n",
      "Epoch 00125: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0157 - acc: 0.9952 - val_loss: 0.2850 - val_acc: 0.9352\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00126: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.1756 - val_acc: 0.9627\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 00127: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 0.1942 - val_acc: 0.9599\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00128: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1911 - val_acc: 0.9625\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9957\n",
      "Epoch 00129: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.2029 - val_acc: 0.9546\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968\n",
      "Epoch 00130: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0110 - acc: 0.9968 - val_loss: 0.1887 - val_acc: 0.9595\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00131: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1968 - val_acc: 0.9574\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00132: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1886 - val_acc: 0.9611\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9979\n",
      "Epoch 00133: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.2207 - val_acc: 0.9534\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9963\n",
      "Epoch 00134: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0122 - acc: 0.9963 - val_loss: 0.2124 - val_acc: 0.9515\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9964\n",
      "Epoch 00135: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0116 - acc: 0.9964 - val_loss: 0.2251 - val_acc: 0.9548\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 00136: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0060 - acc: 0.9985 - val_loss: 0.2306 - val_acc: 0.9476\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9966\n",
      "Epoch 00137: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0119 - acc: 0.9966 - val_loss: 0.1885 - val_acc: 0.9543\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00138: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0043 - acc: 0.9990 - val_loss: 0.2124 - val_acc: 0.9557\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 00139: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.1838 - val_acc: 0.9606\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00140: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0040 - acc: 0.9990 - val_loss: 0.2019 - val_acc: 0.9555\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9958\n",
      "Epoch 00141: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.1945 - val_acc: 0.9604\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9985\n",
      "Epoch 00142: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.2579 - val_acc: 0.9408\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9964\n",
      "Epoch 00143: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0112 - acc: 0.9964 - val_loss: 0.2365 - val_acc: 0.9502\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 00144: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1948 - val_acc: 0.9599\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9988\n",
      "Epoch 00145: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.1858 - val_acc: 0.9604\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 00146: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1898 - val_acc: 0.9613\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 00147: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0109 - acc: 0.9968 - val_loss: 0.2008 - val_acc: 0.9574\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9970\n",
      "Epoch 00148: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0106 - acc: 0.9970 - val_loss: 0.1993 - val_acc: 0.9585\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00149: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1781 - val_acc: 0.9611\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 00150: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.2857 - val_acc: 0.9429\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9944\n",
      "Epoch 00151: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0185 - acc: 0.9944 - val_loss: 0.1916 - val_acc: 0.9606\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9989\n",
      "Epoch 00152: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.1865 - val_acc: 0.9644\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 00153: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 0.1799 - val_acc: 0.9611\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 00154: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.2277 - val_acc: 0.9539\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 00155: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0062 - acc: 0.9982 - val_loss: 0.1727 - val_acc: 0.9667\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00156: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 0.1897 - val_acc: 0.9611\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00157: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.2125 - val_acc: 0.9553\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00158: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1964 - val_acc: 0.9560\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 00159: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0087 - acc: 0.9975 - val_loss: 0.1880 - val_acc: 0.9604\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 00160: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1662 - val_acc: 0.9646\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9965\n",
      "Epoch 00161: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 0.2031 - val_acc: 0.9581\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 00162: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0063 - acc: 0.9981 - val_loss: 0.2166 - val_acc: 0.9562\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 00163: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.2191 - val_acc: 0.9520\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 00164: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0103 - acc: 0.9966 - val_loss: 0.2233 - val_acc: 0.9557\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9991\n",
      "Epoch 00165: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0036 - acc: 0.9991 - val_loss: 0.1870 - val_acc: 0.9632\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00166: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.2578 - val_acc: 0.9509\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9969\n",
      "Epoch 00167: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0095 - acc: 0.9969 - val_loss: 0.2191 - val_acc: 0.9581\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9971\n",
      "Epoch 00168: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0086 - acc: 0.9971 - val_loss: 0.2350 - val_acc: 0.9488\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00169: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.2046 - val_acc: 0.9583\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 00170: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0049 - acc: 0.9985 - val_loss: 0.2271 - val_acc: 0.9518\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00171: val_loss did not improve from 0.16287\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.1788 - val_acc: 0.9602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2248 - acc: 0.9489\n",
      "Loss: 0.22481658297899418 Accuracy: 0.94890964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN'\n",
    "\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 958us/sample - loss: 6.5852 - acc: 0.0868\n",
      "Loss: 6.585207887651516 Accuracy: 0.08681205\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 3.4299 - acc: 0.2478\n",
      "Loss: 3.429908851274944 Accuracy: 0.24776739\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 2.6929 - acc: 0.4347\n",
      "Loss: 2.692902524944159 Accuracy: 0.4346833\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.9698 - acc: 0.6565\n",
      "Loss: 1.9697624433325336 Accuracy: 0.65649015\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.5334 - acc: 0.7248\n",
      "Loss: 1.5333699085011785 Accuracy: 0.7248183\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.9171 - acc: 0.7780\n",
      "Loss: 0.9171340922824072 Accuracy: 0.77798545\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.4533 - acc: 0.9001\n",
      "Loss: 0.45326784061246694 Accuracy: 0.90010387\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2758 - acc: 0.9304\n",
      "Loss: 0.2758367461687184 Accuracy: 0.93042576\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2248 - acc: 0.9489\n",
      "Loss: 0.22481658297899418 Accuracy: 0.94890964\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 12.8734 - acc: 0.1412\n",
      "Loss: 12.873383958366924 Accuracy: 0.14122534\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,499,344\n",
      "Trainable params: 5,498,832\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 7.3874 - acc: 0.3481\n",
      "Loss: 7.387371147607346 Accuracy: 0.3480789\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,883,216\n",
      "Trainable params: 1,882,448\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 4.5169 - acc: 0.4592\n",
      "Loss: 4.516891249936936 Accuracy: 0.45919004\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 694,992\n",
      "Trainable params: 693,968\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 2.5005 - acc: 0.6393\n",
      "Loss: 2.5005284367320693 Accuracy: 0.63925236\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 567,248\n",
      "Trainable params: 565,712\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 3.0340 - acc: 0.5840\n",
      "Loss: 3.0340272793517307 Accuracy: 0.58400834\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 396,496\n",
      "Trainable params: 394,448\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.3571 - acc: 0.7524\n",
      "Loss: 1.3571312575325416 Accuracy: 0.7524403\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 405,968\n",
      "Trainable params: 403,408\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.6489 - acc: 0.8795\n",
      "Loss: 0.6489072121811309 Accuracy: 0.87954307\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 476,880\n",
      "Trainable params: 473,808\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.3322 - acc: 0.9250\n",
      "Loss: 0.33215706567526604 Accuracy: 0.92502594\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_025_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 768,208\n",
      "Trainable params: 764,112\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2706 - acc: 0.9443\n",
      "Loss: 0.2705540340819308 Accuracy: 0.9443406\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
