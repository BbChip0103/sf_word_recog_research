{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 341312)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 492928)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 492928)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           7886864     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,928,336\n",
      "Trainable params: 7,928,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 113728)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 164224)       0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 164224)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2627600     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,689,616\n",
      "Trainable params: 2,689,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 37888)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 58816)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 58816)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           941072      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,044,176\n",
      "Trainable params: 1,044,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12608)        0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 23616)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 23616)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           377872      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 563,024\n",
      "Trainable params: 563,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 8320)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 11904)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 11904)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           190480      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 457,680\n",
      "Trainable params: 457,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 2688)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3840)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 3840)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           61456       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 410,704\n",
      "Trainable params: 410,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0336 - acc: 0.3658\n",
      "Epoch 00001: val_loss improved from inf to 1.64171, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_3_conv_checkpoint/001-1.6417.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 2.0336 - acc: 0.3658 - val_loss: 1.6417 - val_acc: 0.4915\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4254 - acc: 0.5569\n",
      "Epoch 00002: val_loss improved from 1.64171 to 1.46824, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_3_conv_checkpoint/002-1.4682.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.4255 - acc: 0.5569 - val_loss: 1.4682 - val_acc: 0.5323\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1987 - acc: 0.6301\n",
      "Epoch 00003: val_loss improved from 1.46824 to 1.42731, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_3_conv_checkpoint/003-1.4273.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.1986 - acc: 0.6301 - val_loss: 1.4273 - val_acc: 0.5593\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0315 - acc: 0.6830\n",
      "Epoch 00004: val_loss improved from 1.42731 to 1.35939, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_3_conv_checkpoint/004-1.3594.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.0314 - acc: 0.6831 - val_loss: 1.3594 - val_acc: 0.5614\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8982 - acc: 0.7287\n",
      "Epoch 00005: val_loss improved from 1.35939 to 1.35805, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_3_conv_checkpoint/005-1.3580.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.8981 - acc: 0.7288 - val_loss: 1.3580 - val_acc: 0.5702\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7827 - acc: 0.7633\n",
      "Epoch 00006: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.7826 - acc: 0.7633 - val_loss: 1.3800 - val_acc: 0.5809\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.7933\n",
      "Epoch 00007: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.6893 - acc: 0.7932 - val_loss: 1.3755 - val_acc: 0.5870\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6100 - acc: 0.8209\n",
      "Epoch 00008: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.6100 - acc: 0.8209 - val_loss: 1.3980 - val_acc: 0.5928\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8439\n",
      "Epoch 00009: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.5339 - acc: 0.8439 - val_loss: 1.4086 - val_acc: 0.5982\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8672\n",
      "Epoch 00010: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.4648 - acc: 0.8672 - val_loss: 1.4321 - val_acc: 0.6012\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8821\n",
      "Epoch 00011: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.4094 - acc: 0.8821 - val_loss: 1.4792 - val_acc: 0.5942\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.9016\n",
      "Epoch 00012: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3565 - acc: 0.9016 - val_loss: 1.5283 - val_acc: 0.5900\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.9115\n",
      "Epoch 00013: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.3182 - acc: 0.9115 - val_loss: 1.5308 - val_acc: 0.6059\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9222\n",
      "Epoch 00014: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.2859 - acc: 0.9222 - val_loss: 1.5607 - val_acc: 0.6042\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9328\n",
      "Epoch 00015: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2530 - acc: 0.9328 - val_loss: 1.6789 - val_acc: 0.5980\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9398\n",
      "Epoch 00016: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.2290 - acc: 0.9398 - val_loss: 1.6334 - val_acc: 0.6117\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9486\n",
      "Epoch 00017: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1997 - acc: 0.9486 - val_loss: 1.6828 - val_acc: 0.6161\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9535\n",
      "Epoch 00018: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 0.1812 - acc: 0.9535 - val_loss: 1.6908 - val_acc: 0.6236\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9579\n",
      "Epoch 00019: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1640 - acc: 0.9579 - val_loss: 1.7271 - val_acc: 0.6208\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9608\n",
      "Epoch 00020: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1524 - acc: 0.9608 - val_loss: 1.8269 - val_acc: 0.6187\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9620\n",
      "Epoch 00021: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1463 - acc: 0.9620 - val_loss: 1.8214 - val_acc: 0.6166\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9689\n",
      "Epoch 00022: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1289 - acc: 0.9689 - val_loss: 1.8841 - val_acc: 0.6238\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9699\n",
      "Epoch 00023: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1206 - acc: 0.9699 - val_loss: 1.8716 - val_acc: 0.6222\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9730\n",
      "Epoch 00024: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1139 - acc: 0.9730 - val_loss: 1.9517 - val_acc: 0.6105\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9747\n",
      "Epoch 00025: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.1083 - acc: 0.9747 - val_loss: 1.9447 - val_acc: 0.6196\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9765\n",
      "Epoch 00026: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0961 - acc: 0.9766 - val_loss: 1.9896 - val_acc: 0.6222\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9781\n",
      "Epoch 00027: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0942 - acc: 0.9781 - val_loss: 1.9962 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9794\n",
      "Epoch 00028: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0863 - acc: 0.9794 - val_loss: 1.9797 - val_acc: 0.6310\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9780\n",
      "Epoch 00029: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0915 - acc: 0.9780 - val_loss: 2.0243 - val_acc: 0.6322\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9804\n",
      "Epoch 00030: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0816 - acc: 0.9804 - val_loss: 2.1325 - val_acc: 0.6173\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9814\n",
      "Epoch 00031: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0799 - acc: 0.9814 - val_loss: 2.0355 - val_acc: 0.6392\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9821\n",
      "Epoch 00032: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0774 - acc: 0.9821 - val_loss: 2.0874 - val_acc: 0.6287\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9853\n",
      "Epoch 00033: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0658 - acc: 0.9853 - val_loss: 2.0551 - val_acc: 0.6375\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9843\n",
      "Epoch 00034: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0695 - acc: 0.9843 - val_loss: 2.1481 - val_acc: 0.6324\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9858\n",
      "Epoch 00035: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0651 - acc: 0.9858 - val_loss: 2.1249 - val_acc: 0.6429\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9844\n",
      "Epoch 00036: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0705 - acc: 0.9844 - val_loss: 2.1216 - val_acc: 0.6338\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9865\n",
      "Epoch 00037: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0606 - acc: 0.9866 - val_loss: 2.1218 - val_acc: 0.6450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9877\n",
      "Epoch 00038: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0561 - acc: 0.9877 - val_loss: 2.2542 - val_acc: 0.6422\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9871\n",
      "Epoch 00039: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0590 - acc: 0.9871 - val_loss: 2.1241 - val_acc: 0.6434\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9879\n",
      "Epoch 00040: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0567 - acc: 0.9879 - val_loss: 2.2460 - val_acc: 0.6317\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9862\n",
      "Epoch 00041: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0599 - acc: 0.9862 - val_loss: 2.1661 - val_acc: 0.6438\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9873\n",
      "Epoch 00042: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0558 - acc: 0.9873 - val_loss: 2.2386 - val_acc: 0.6331\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9873\n",
      "Epoch 00043: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0561 - acc: 0.9873 - val_loss: 2.2253 - val_acc: 0.6385\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9879\n",
      "Epoch 00044: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0532 - acc: 0.9879 - val_loss: 2.1961 - val_acc: 0.6455\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9898\n",
      "Epoch 00045: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0477 - acc: 0.9898 - val_loss: 2.1576 - val_acc: 0.6497\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9902\n",
      "Epoch 00046: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0453 - acc: 0.9902 - val_loss: 2.1990 - val_acc: 0.6466\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9905\n",
      "Epoch 00047: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0459 - acc: 0.9905 - val_loss: 2.2314 - val_acc: 0.6457\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9908\n",
      "Epoch 00048: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0438 - acc: 0.9908 - val_loss: 2.2934 - val_acc: 0.6334\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9892\n",
      "Epoch 00049: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0504 - acc: 0.9892 - val_loss: 2.2801 - val_acc: 0.6424\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9889\n",
      "Epoch 00050: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0493 - acc: 0.9889 - val_loss: 2.2200 - val_acc: 0.6427\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9924\n",
      "Epoch 00051: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0389 - acc: 0.9924 - val_loss: 2.2781 - val_acc: 0.6413\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9914\n",
      "Epoch 00052: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0423 - acc: 0.9914 - val_loss: 2.3072 - val_acc: 0.6483\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9917\n",
      "Epoch 00053: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0420 - acc: 0.9917 - val_loss: 2.3238 - val_acc: 0.6431\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9901\n",
      "Epoch 00054: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0454 - acc: 0.9901 - val_loss: 2.2984 - val_acc: 0.6492\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9924\n",
      "Epoch 00055: val_loss did not improve from 1.35805\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 0.0385 - acc: 0.9924 - val_loss: 2.2912 - val_acc: 0.6445\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 449us/sample - loss: 1.4146 - acc: 0.5499\n",
      "Loss: 1.4145817289223677 Accuracy: 0.5499481\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9694 - acc: 0.3770\n",
      "Epoch 00001: val_loss improved from inf to 1.52738, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/001-1.5274.hdf5\n",
      "36805/36805 [==============================] - 35s 940us/sample - loss: 1.9693 - acc: 0.3770 - val_loss: 1.5274 - val_acc: 0.5211\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4023 - acc: 0.5630\n",
      "Epoch 00002: val_loss improved from 1.52738 to 1.34616, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/002-1.3462.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 1.4023 - acc: 0.5630 - val_loss: 1.3462 - val_acc: 0.5837\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1949 - acc: 0.6373\n",
      "Epoch 00003: val_loss improved from 1.34616 to 1.26180, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/003-1.2618.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 1.1949 - acc: 0.6373 - val_loss: 1.2618 - val_acc: 0.6000\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0379 - acc: 0.6882\n",
      "Epoch 00004: val_loss improved from 1.26180 to 1.21439, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/004-1.2144.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 1.0378 - acc: 0.6882 - val_loss: 1.2144 - val_acc: 0.6191\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9155 - acc: 0.7245\n",
      "Epoch 00005: val_loss improved from 1.21439 to 1.16553, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/005-1.1655.hdf5\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.9156 - acc: 0.7245 - val_loss: 1.1655 - val_acc: 0.6434\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7581\n",
      "Epoch 00006: val_loss improved from 1.16553 to 1.13674, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/006-1.1367.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.8057 - acc: 0.7581 - val_loss: 1.1367 - val_acc: 0.6525\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7839\n",
      "Epoch 00007: val_loss improved from 1.13674 to 1.11597, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_4_conv_checkpoint/007-1.1160.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.7234 - acc: 0.7839 - val_loss: 1.1160 - val_acc: 0.6594\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.8081\n",
      "Epoch 00008: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.6395 - acc: 0.8081 - val_loss: 1.1172 - val_acc: 0.6660\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.8256\n",
      "Epoch 00009: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.5756 - acc: 0.8256 - val_loss: 1.1352 - val_acc: 0.6657\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8446\n",
      "Epoch 00010: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.5143 - acc: 0.8446 - val_loss: 1.1240 - val_acc: 0.6711\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8620\n",
      "Epoch 00011: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.4608 - acc: 0.8619 - val_loss: 1.1294 - val_acc: 0.6839\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8758\n",
      "Epoch 00012: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.4164 - acc: 0.8758 - val_loss: 1.1333 - val_acc: 0.6860\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8878\n",
      "Epoch 00013: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.3725 - acc: 0.8878 - val_loss: 1.1902 - val_acc: 0.6839\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8977\n",
      "Epoch 00014: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.3375 - acc: 0.8977 - val_loss: 1.1855 - val_acc: 0.6879\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9100\n",
      "Epoch 00015: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.3047 - acc: 0.9100 - val_loss: 1.1650 - val_acc: 0.6907\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9197\n",
      "Epoch 00016: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.2672 - acc: 0.9197 - val_loss: 1.2202 - val_acc: 0.6937\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9275\n",
      "Epoch 00017: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.2493 - acc: 0.9275 - val_loss: 1.2180 - val_acc: 0.6990\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9316\n",
      "Epoch 00018: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.2283 - acc: 0.9316 - val_loss: 1.2715 - val_acc: 0.6895\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9390\n",
      "Epoch 00019: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.2045 - acc: 0.9391 - val_loss: 1.2903 - val_acc: 0.6960\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9442\n",
      "Epoch 00020: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1921 - acc: 0.9442 - val_loss: 1.3225 - val_acc: 0.6918\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9467\n",
      "Epoch 00021: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1800 - acc: 0.9467 - val_loss: 1.3697 - val_acc: 0.6969\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9522\n",
      "Epoch 00022: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1624 - acc: 0.9522 - val_loss: 1.3112 - val_acc: 0.7070\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9556\n",
      "Epoch 00023: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1515 - acc: 0.9556 - val_loss: 1.4734 - val_acc: 0.6918\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9579\n",
      "Epoch 00024: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1457 - acc: 0.9579 - val_loss: 1.3507 - val_acc: 0.7063\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9598\n",
      "Epoch 00025: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1340 - acc: 0.9598 - val_loss: 1.3632 - val_acc: 0.7070\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9610\n",
      "Epoch 00026: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1285 - acc: 0.9610 - val_loss: 1.4186 - val_acc: 0.7067\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9659\n",
      "Epoch 00027: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1183 - acc: 0.9659 - val_loss: 1.4181 - val_acc: 0.7084\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9677\n",
      "Epoch 00028: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1108 - acc: 0.9676 - val_loss: 1.4198 - val_acc: 0.7128\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9664\n",
      "Epoch 00029: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1138 - acc: 0.9664 - val_loss: 1.4259 - val_acc: 0.7144\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9716\n",
      "Epoch 00030: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0981 - acc: 0.9716 - val_loss: 1.4575 - val_acc: 0.7123\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9717\n",
      "Epoch 00031: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0944 - acc: 0.9717 - val_loss: 1.4585 - val_acc: 0.7186\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9744\n",
      "Epoch 00032: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0924 - acc: 0.9744 - val_loss: 1.4805 - val_acc: 0.7230\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9759\n",
      "Epoch 00033: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0865 - acc: 0.9759 - val_loss: 1.4927 - val_acc: 0.7174\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9761\n",
      "Epoch 00034: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0833 - acc: 0.9761 - val_loss: 1.5261 - val_acc: 0.7179\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9782\n",
      "Epoch 00035: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0806 - acc: 0.9782 - val_loss: 1.5211 - val_acc: 0.7214\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9778\n",
      "Epoch 00036: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0787 - acc: 0.9778 - val_loss: 1.5158 - val_acc: 0.7254\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9779\n",
      "Epoch 00037: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0791 - acc: 0.9779 - val_loss: 1.5408 - val_acc: 0.7179\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9805\n",
      "Epoch 00038: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0688 - acc: 0.9805 - val_loss: 1.5969 - val_acc: 0.7158\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9814\n",
      "Epoch 00039: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0703 - acc: 0.9814 - val_loss: 1.5254 - val_acc: 0.7230\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9804\n",
      "Epoch 00040: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0698 - acc: 0.9804 - val_loss: 1.5089 - val_acc: 0.7284\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9839\n",
      "Epoch 00041: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0619 - acc: 0.9839 - val_loss: 1.6071 - val_acc: 0.7154\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9827\n",
      "Epoch 00042: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0642 - acc: 0.9827 - val_loss: 1.6207 - val_acc: 0.7193\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9831\n",
      "Epoch 00043: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0610 - acc: 0.9831 - val_loss: 1.5530 - val_acc: 0.7289\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9832\n",
      "Epoch 00044: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.0626 - acc: 0.9832 - val_loss: 1.6089 - val_acc: 0.7167\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9844\n",
      "Epoch 00045: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0585 - acc: 0.9844 - val_loss: 1.6139 - val_acc: 0.7254\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9833\n",
      "Epoch 00046: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0596 - acc: 0.9833 - val_loss: 1.6867 - val_acc: 0.7170\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9866\n",
      "Epoch 00047: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0510 - acc: 0.9866 - val_loss: 1.6213 - val_acc: 0.7307\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9861\n",
      "Epoch 00048: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0527 - acc: 0.9860 - val_loss: 1.5615 - val_acc: 0.7424\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9856\n",
      "Epoch 00049: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0526 - acc: 0.9856 - val_loss: 1.5666 - val_acc: 0.7370\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9860\n",
      "Epoch 00050: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0530 - acc: 0.9860 - val_loss: 1.5552 - val_acc: 0.7396\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9861\n",
      "Epoch 00051: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0520 - acc: 0.9861 - val_loss: 1.5947 - val_acc: 0.7382\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9881\n",
      "Epoch 00052: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0463 - acc: 0.9881 - val_loss: 1.6248 - val_acc: 0.7368\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9867\n",
      "Epoch 00053: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0493 - acc: 0.9867 - val_loss: 1.6431 - val_acc: 0.7310\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9863\n",
      "Epoch 00054: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0505 - acc: 0.9863 - val_loss: 1.6432 - val_acc: 0.7326\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9871\n",
      "Epoch 00055: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0493 - acc: 0.9871 - val_loss: 1.6920 - val_acc: 0.7251\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9874\n",
      "Epoch 00056: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0481 - acc: 0.9874 - val_loss: 1.7170 - val_acc: 0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9889\n",
      "Epoch 00057: val_loss did not improve from 1.11597\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0442 - acc: 0.9889 - val_loss: 1.5859 - val_acc: 0.7405\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 413us/sample - loss: 1.1846 - acc: 0.6318\n",
      "Loss: 1.1846418001941432 Accuracy: 0.6317757\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9855 - acc: 0.3670\n",
      "Epoch 00001: val_loss improved from inf to 1.49204, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/001-1.4920.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 1.9854 - acc: 0.3670 - val_loss: 1.4920 - val_acc: 0.5260\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3997 - acc: 0.5687\n",
      "Epoch 00002: val_loss improved from 1.49204 to 1.27015, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/002-1.2702.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 1.3997 - acc: 0.5687 - val_loss: 1.2702 - val_acc: 0.6075\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2152 - acc: 0.6283\n",
      "Epoch 00003: val_loss improved from 1.27015 to 1.12291, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/003-1.1229.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 1.2152 - acc: 0.6283 - val_loss: 1.1229 - val_acc: 0.6653\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0942 - acc: 0.6690\n",
      "Epoch 00004: val_loss improved from 1.12291 to 1.06236, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/004-1.0624.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 1.0941 - acc: 0.6690 - val_loss: 1.0624 - val_acc: 0.6839\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9953 - acc: 0.7026\n",
      "Epoch 00005: val_loss improved from 1.06236 to 1.03982, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/005-1.0398.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.9952 - acc: 0.7026 - val_loss: 1.0398 - val_acc: 0.6841\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9071 - acc: 0.7271\n",
      "Epoch 00006: val_loss improved from 1.03982 to 0.92690, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/006-0.9269.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.9071 - acc: 0.7270 - val_loss: 0.9269 - val_acc: 0.7240\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8330 - acc: 0.7501\n",
      "Epoch 00007: val_loss did not improve from 0.92690\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.8330 - acc: 0.7501 - val_loss: 0.9448 - val_acc: 0.7128\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7731\n",
      "Epoch 00008: val_loss improved from 0.92690 to 0.92057, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/008-0.9206.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.7661 - acc: 0.7731 - val_loss: 0.9206 - val_acc: 0.7142\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7889\n",
      "Epoch 00009: val_loss improved from 0.92057 to 0.81216, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/009-0.8122.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.7061 - acc: 0.7889 - val_loss: 0.8122 - val_acc: 0.7601\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6391 - acc: 0.8099\n",
      "Epoch 00010: val_loss did not improve from 0.81216\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.6390 - acc: 0.8099 - val_loss: 0.8268 - val_acc: 0.7510\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5858 - acc: 0.8262\n",
      "Epoch 00011: val_loss improved from 0.81216 to 0.77435, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/011-0.7744.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5858 - acc: 0.8262 - val_loss: 0.7744 - val_acc: 0.7671\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8382\n",
      "Epoch 00012: val_loss improved from 0.77435 to 0.76585, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/012-0.7659.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.5432 - acc: 0.8382 - val_loss: 0.7659 - val_acc: 0.7743\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4933 - acc: 0.8532\n",
      "Epoch 00013: val_loss did not improve from 0.76585\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.4932 - acc: 0.8532 - val_loss: 0.7692 - val_acc: 0.7741\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8646\n",
      "Epoch 00014: val_loss improved from 0.76585 to 0.76235, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/014-0.7623.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.4497 - acc: 0.8647 - val_loss: 0.7623 - val_acc: 0.7701\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8748\n",
      "Epoch 00015: val_loss improved from 0.76235 to 0.75420, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/015-0.7542.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.4145 - acc: 0.8748 - val_loss: 0.7542 - val_acc: 0.7789\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8854\n",
      "Epoch 00016: val_loss improved from 0.75420 to 0.71459, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_5_conv_checkpoint/016-0.7146.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.3855 - acc: 0.8853 - val_loss: 0.7146 - val_acc: 0.7929\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8956\n",
      "Epoch 00017: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.3471 - acc: 0.8956 - val_loss: 0.7903 - val_acc: 0.7834\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9023\n",
      "Epoch 00018: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.3196 - acc: 0.9023 - val_loss: 0.7170 - val_acc: 0.7934\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9104\n",
      "Epoch 00019: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.2986 - acc: 0.9104 - val_loss: 0.7410 - val_acc: 0.7957\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9161\n",
      "Epoch 00020: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2782 - acc: 0.9161 - val_loss: 0.7212 - val_acc: 0.8055\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9236\n",
      "Epoch 00021: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2501 - acc: 0.9236 - val_loss: 0.7407 - val_acc: 0.7959\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9295\n",
      "Epoch 00022: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2342 - acc: 0.9294 - val_loss: 0.7291 - val_acc: 0.8069\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9342\n",
      "Epoch 00023: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2152 - acc: 0.9342 - val_loss: 0.7229 - val_acc: 0.8092\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9402\n",
      "Epoch 00024: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1993 - acc: 0.9402 - val_loss: 0.7896 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9417\n",
      "Epoch 00025: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1911 - acc: 0.9417 - val_loss: 0.7217 - val_acc: 0.8181\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9465\n",
      "Epoch 00026: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1761 - acc: 0.9465 - val_loss: 0.7640 - val_acc: 0.8032\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9496\n",
      "Epoch 00027: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1638 - acc: 0.9496 - val_loss: 0.7623 - val_acc: 0.8120\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9507\n",
      "Epoch 00028: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1590 - acc: 0.9507 - val_loss: 0.7531 - val_acc: 0.8150\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9541\n",
      "Epoch 00029: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1468 - acc: 0.9541 - val_loss: 0.7572 - val_acc: 0.8255\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9560\n",
      "Epoch 00030: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1413 - acc: 0.9560 - val_loss: 0.8165 - val_acc: 0.8027\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9608\n",
      "Epoch 00031: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1293 - acc: 0.9607 - val_loss: 0.8105 - val_acc: 0.8157\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9615\n",
      "Epoch 00032: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1264 - acc: 0.9615 - val_loss: 0.7947 - val_acc: 0.8185\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9644\n",
      "Epoch 00033: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1158 - acc: 0.9644 - val_loss: 0.8008 - val_acc: 0.8132\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9635\n",
      "Epoch 00034: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1182 - acc: 0.9635 - val_loss: 0.8261 - val_acc: 0.8102\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9671\n",
      "Epoch 00035: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1096 - acc: 0.9671 - val_loss: 0.7988 - val_acc: 0.8192\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9690\n",
      "Epoch 00036: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1024 - acc: 0.9690 - val_loss: 0.8061 - val_acc: 0.8225\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9690\n",
      "Epoch 00037: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1013 - acc: 0.9691 - val_loss: 0.8157 - val_acc: 0.8232\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9689\n",
      "Epoch 00038: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1018 - acc: 0.9689 - val_loss: 0.7794 - val_acc: 0.8348\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9735\n",
      "Epoch 00039: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0894 - acc: 0.9735 - val_loss: 0.8247 - val_acc: 0.8202\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9736\n",
      "Epoch 00040: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0897 - acc: 0.9736 - val_loss: 0.8104 - val_acc: 0.8307\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9731\n",
      "Epoch 00041: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0897 - acc: 0.9731 - val_loss: 0.8080 - val_acc: 0.8276\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9764\n",
      "Epoch 00042: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0814 - acc: 0.9764 - val_loss: 0.8274 - val_acc: 0.8318\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9767\n",
      "Epoch 00043: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0794 - acc: 0.9767 - val_loss: 0.8264 - val_acc: 0.8323\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9772\n",
      "Epoch 00044: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0770 - acc: 0.9772 - val_loss: 0.8093 - val_acc: 0.8276\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9771\n",
      "Epoch 00045: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0773 - acc: 0.9771 - val_loss: 0.8260 - val_acc: 0.8276\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9762\n",
      "Epoch 00046: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0792 - acc: 0.9763 - val_loss: 0.8315 - val_acc: 0.8237\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9802\n",
      "Epoch 00047: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0684 - acc: 0.9802 - val_loss: 0.7892 - val_acc: 0.8374\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9780\n",
      "Epoch 00048: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0747 - acc: 0.9780 - val_loss: 0.8032 - val_acc: 0.8348\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9803\n",
      "Epoch 00049: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0693 - acc: 0.9803 - val_loss: 0.8600 - val_acc: 0.8337\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9804\n",
      "Epoch 00050: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0681 - acc: 0.9804 - val_loss: 0.7977 - val_acc: 0.8423\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9821\n",
      "Epoch 00051: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0636 - acc: 0.9821 - val_loss: 0.8771 - val_acc: 0.8253\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9836\n",
      "Epoch 00052: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0601 - acc: 0.9836 - val_loss: 0.8777 - val_acc: 0.8330\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9825\n",
      "Epoch 00053: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0605 - acc: 0.9825 - val_loss: 0.8466 - val_acc: 0.8430\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9818\n",
      "Epoch 00054: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0620 - acc: 0.9818 - val_loss: 0.8439 - val_acc: 0.8386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9832\n",
      "Epoch 00055: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0596 - acc: 0.9832 - val_loss: 0.8144 - val_acc: 0.8393\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9853\n",
      "Epoch 00056: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0542 - acc: 0.9853 - val_loss: 0.8586 - val_acc: 0.8337\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9848\n",
      "Epoch 00057: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0546 - acc: 0.9848 - val_loss: 0.8257 - val_acc: 0.8439\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9847\n",
      "Epoch 00058: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0558 - acc: 0.9847 - val_loss: 0.8523 - val_acc: 0.8372\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9849\n",
      "Epoch 00059: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0546 - acc: 0.9849 - val_loss: 0.8352 - val_acc: 0.8446\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9866\n",
      "Epoch 00060: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0501 - acc: 0.9866 - val_loss: 0.8577 - val_acc: 0.8404\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9866\n",
      "Epoch 00061: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0518 - acc: 0.9866 - val_loss: 0.8462 - val_acc: 0.8453\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9862\n",
      "Epoch 00062: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0504 - acc: 0.9862 - val_loss: 0.8833 - val_acc: 0.8367\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9865\n",
      "Epoch 00063: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0497 - acc: 0.9865 - val_loss: 0.8992 - val_acc: 0.8355\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9864\n",
      "Epoch 00064: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0495 - acc: 0.9864 - val_loss: 0.8393 - val_acc: 0.8512\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9863\n",
      "Epoch 00065: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0502 - acc: 0.9863 - val_loss: 0.8334 - val_acc: 0.8444\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9868\n",
      "Epoch 00066: val_loss did not improve from 0.71459\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0499 - acc: 0.9868 - val_loss: 0.9019 - val_acc: 0.8379\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 440us/sample - loss: 0.8217 - acc: 0.7626\n",
      "Loss: 0.8217454534082026 Accuracy: 0.7626168\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0351 - acc: 0.3443\n",
      "Epoch 00001: val_loss improved from inf to 1.51962, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/001-1.5196.hdf5\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 2.0350 - acc: 0.3443 - val_loss: 1.5196 - val_acc: 0.5278\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4453 - acc: 0.5493\n",
      "Epoch 00002: val_loss improved from 1.51962 to 1.25396, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/002-1.2540.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 1.4454 - acc: 0.5493 - val_loss: 1.2540 - val_acc: 0.6240\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2260 - acc: 0.6228\n",
      "Epoch 00003: val_loss improved from 1.25396 to 1.08355, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/003-1.0836.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 1.2259 - acc: 0.6228 - val_loss: 1.0836 - val_acc: 0.6515\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.6748\n",
      "Epoch 00004: val_loss improved from 1.08355 to 0.91676, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/004-0.9168.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 1.0670 - acc: 0.6748 - val_loss: 0.9168 - val_acc: 0.7377\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9300 - acc: 0.7187\n",
      "Epoch 00005: val_loss improved from 0.91676 to 0.81299, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/005-0.8130.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.9299 - acc: 0.7187 - val_loss: 0.8130 - val_acc: 0.7659\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7511\n",
      "Epoch 00006: val_loss improved from 0.81299 to 0.69790, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/006-0.6979.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.8287 - acc: 0.7510 - val_loss: 0.6979 - val_acc: 0.7939\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7403 - acc: 0.7753\n",
      "Epoch 00007: val_loss improved from 0.69790 to 0.65171, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/007-0.6517.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.7404 - acc: 0.7753 - val_loss: 0.6517 - val_acc: 0.8155\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6606 - acc: 0.8004\n",
      "Epoch 00008: val_loss improved from 0.65171 to 0.61847, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/008-0.6185.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.6605 - acc: 0.8004 - val_loss: 0.6185 - val_acc: 0.8279\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8195\n",
      "Epoch 00009: val_loss improved from 0.61847 to 0.55747, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/009-0.5575.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5978 - acc: 0.8196 - val_loss: 0.5575 - val_acc: 0.8388\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8328\n",
      "Epoch 00010: val_loss improved from 0.55747 to 0.49461, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/010-0.4946.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5496 - acc: 0.8328 - val_loss: 0.4946 - val_acc: 0.8577\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.8475\n",
      "Epoch 00011: val_loss improved from 0.49461 to 0.47001, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/011-0.4700.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.5066 - acc: 0.8474 - val_loss: 0.4700 - val_acc: 0.8630\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8580\n",
      "Epoch 00012: val_loss improved from 0.47001 to 0.44328, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/012-0.4433.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.4686 - acc: 0.8580 - val_loss: 0.4433 - val_acc: 0.8698\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4377 - acc: 0.8678\n",
      "Epoch 00013: val_loss improved from 0.44328 to 0.43641, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/013-0.4364.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.4377 - acc: 0.8678 - val_loss: 0.4364 - val_acc: 0.8772\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8735\n",
      "Epoch 00014: val_loss improved from 0.43641 to 0.38724, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/014-0.3872.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.4131 - acc: 0.8735 - val_loss: 0.3872 - val_acc: 0.8896\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8856\n",
      "Epoch 00015: val_loss improved from 0.38724 to 0.38372, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/015-0.3837.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3775 - acc: 0.8856 - val_loss: 0.3837 - val_acc: 0.8903\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8899\n",
      "Epoch 00016: val_loss improved from 0.38372 to 0.36708, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/016-0.3671.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3584 - acc: 0.8899 - val_loss: 0.3671 - val_acc: 0.8921\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8970\n",
      "Epoch 00017: val_loss improved from 0.36708 to 0.36271, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/017-0.3627.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.3381 - acc: 0.8970 - val_loss: 0.3627 - val_acc: 0.8975\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.9005\n",
      "Epoch 00018: val_loss improved from 0.36271 to 0.34536, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/018-0.3454.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3182 - acc: 0.9005 - val_loss: 0.3454 - val_acc: 0.8984\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9060\n",
      "Epoch 00019: val_loss did not improve from 0.34536\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.3060 - acc: 0.9060 - val_loss: 0.3700 - val_acc: 0.8873\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9117\n",
      "Epoch 00020: val_loss did not improve from 0.34536\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2825 - acc: 0.9118 - val_loss: 0.3484 - val_acc: 0.8989\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9181\n",
      "Epoch 00021: val_loss did not improve from 0.34536\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2675 - acc: 0.9181 - val_loss: 0.3680 - val_acc: 0.8942\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9211\n",
      "Epoch 00022: val_loss improved from 0.34536 to 0.32533, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/022-0.3253.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2549 - acc: 0.9211 - val_loss: 0.3253 - val_acc: 0.9126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9225\n",
      "Epoch 00023: val_loss improved from 0.32533 to 0.31005, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/023-0.3100.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2430 - acc: 0.9225 - val_loss: 0.3100 - val_acc: 0.9129\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9291\n",
      "Epoch 00024: val_loss did not improve from 0.31005\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2283 - acc: 0.9291 - val_loss: 0.3240 - val_acc: 0.9054\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9311\n",
      "Epoch 00025: val_loss did not improve from 0.31005\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2177 - acc: 0.9311 - val_loss: 0.3396 - val_acc: 0.9061\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9324\n",
      "Epoch 00026: val_loss did not improve from 0.31005\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2128 - acc: 0.9324 - val_loss: 0.3192 - val_acc: 0.9115\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9363\n",
      "Epoch 00027: val_loss improved from 0.31005 to 0.30809, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/027-0.3081.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1969 - acc: 0.9363 - val_loss: 0.3081 - val_acc: 0.9152\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9411\n",
      "Epoch 00028: val_loss improved from 0.30809 to 0.30357, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/028-0.3036.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1853 - acc: 0.9411 - val_loss: 0.3036 - val_acc: 0.9194\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9429\n",
      "Epoch 00029: val_loss improved from 0.30357 to 0.29777, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/029-0.2978.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1777 - acc: 0.9429 - val_loss: 0.2978 - val_acc: 0.9210\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9447\n",
      "Epoch 00030: val_loss did not improve from 0.29777\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1705 - acc: 0.9447 - val_loss: 0.3071 - val_acc: 0.9194\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9482\n",
      "Epoch 00031: val_loss did not improve from 0.29777\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1590 - acc: 0.9482 - val_loss: 0.3075 - val_acc: 0.9166\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9514\n",
      "Epoch 00032: val_loss did not improve from 0.29777\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1513 - acc: 0.9514 - val_loss: 0.3209 - val_acc: 0.9152\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9536\n",
      "Epoch 00033: val_loss improved from 0.29777 to 0.29666, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_6_conv_checkpoint/033-0.2967.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1449 - acc: 0.9536 - val_loss: 0.2967 - val_acc: 0.9231\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9553\n",
      "Epoch 00034: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1372 - acc: 0.9553 - val_loss: 0.3006 - val_acc: 0.9208\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9565\n",
      "Epoch 00035: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1334 - acc: 0.9565 - val_loss: 0.3011 - val_acc: 0.9222\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9609\n",
      "Epoch 00036: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1233 - acc: 0.9609 - val_loss: 0.3366 - val_acc: 0.9117\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9605\n",
      "Epoch 00037: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1208 - acc: 0.9605 - val_loss: 0.3262 - val_acc: 0.9126\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9621\n",
      "Epoch 00038: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1149 - acc: 0.9622 - val_loss: 0.3134 - val_acc: 0.9208\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9642\n",
      "Epoch 00039: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1112 - acc: 0.9642 - val_loss: 0.3000 - val_acc: 0.9280\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9652\n",
      "Epoch 00040: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1053 - acc: 0.9652 - val_loss: 0.3270 - val_acc: 0.9173\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9676\n",
      "Epoch 00041: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0985 - acc: 0.9676 - val_loss: 0.2985 - val_acc: 0.9227\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9689\n",
      "Epoch 00042: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0944 - acc: 0.9689 - val_loss: 0.3105 - val_acc: 0.9280\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9705\n",
      "Epoch 00043: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0915 - acc: 0.9705 - val_loss: 0.3299 - val_acc: 0.9175\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9701\n",
      "Epoch 00044: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0919 - acc: 0.9701 - val_loss: 0.3968 - val_acc: 0.9026\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9702\n",
      "Epoch 00045: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0911 - acc: 0.9702 - val_loss: 0.3849 - val_acc: 0.9031\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9733\n",
      "Epoch 00046: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0809 - acc: 0.9733 - val_loss: 0.3335 - val_acc: 0.9199\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9727\n",
      "Epoch 00047: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0831 - acc: 0.9727 - val_loss: 0.3278 - val_acc: 0.9257\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9751\n",
      "Epoch 00048: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0747 - acc: 0.9751 - val_loss: 0.3195 - val_acc: 0.9245\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9747\n",
      "Epoch 00049: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.0753 - acc: 0.9747 - val_loss: 0.3418 - val_acc: 0.9168\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9786\n",
      "Epoch 00050: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.0691 - acc: 0.9786 - val_loss: 0.3292 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9776\n",
      "Epoch 00051: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0692 - acc: 0.9776 - val_loss: 0.3313 - val_acc: 0.9280\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0702 - acc: 0.9779 - val_loss: 0.3263 - val_acc: 0.9248\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9793\n",
      "Epoch 00053: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0630 - acc: 0.9793 - val_loss: 0.3330 - val_acc: 0.9271\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9782\n",
      "Epoch 00054: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0658 - acc: 0.9782 - val_loss: 0.3777 - val_acc: 0.9171\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9777\n",
      "Epoch 00055: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0672 - acc: 0.9777 - val_loss: 0.3636 - val_acc: 0.9194\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9804\n",
      "Epoch 00056: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0612 - acc: 0.9804 - val_loss: 0.3382 - val_acc: 0.9269\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9806\n",
      "Epoch 00057: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0602 - acc: 0.9806 - val_loss: 0.3149 - val_acc: 0.9297\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9822\n",
      "Epoch 00058: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0561 - acc: 0.9822 - val_loss: 0.3429 - val_acc: 0.9231\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9818\n",
      "Epoch 00059: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0562 - acc: 0.9817 - val_loss: 0.5042 - val_acc: 0.8931\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9768\n",
      "Epoch 00060: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0774 - acc: 0.9768 - val_loss: 0.3276 - val_acc: 0.9238\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9836\n",
      "Epoch 00061: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0511 - acc: 0.9836 - val_loss: 0.3582 - val_acc: 0.9276\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9833\n",
      "Epoch 00062: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0525 - acc: 0.9833 - val_loss: 0.3606 - val_acc: 0.9259\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9835\n",
      "Epoch 00063: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0529 - acc: 0.9835 - val_loss: 0.3510 - val_acc: 0.9250\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9842\n",
      "Epoch 00064: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0509 - acc: 0.9842 - val_loss: 0.3600 - val_acc: 0.9257\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9830\n",
      "Epoch 00065: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0517 - acc: 0.9830 - val_loss: 0.3374 - val_acc: 0.9317\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9850\n",
      "Epoch 00066: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0468 - acc: 0.9850 - val_loss: 0.3612 - val_acc: 0.9262\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9843\n",
      "Epoch 00067: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0481 - acc: 0.9843 - val_loss: 0.3775 - val_acc: 0.9243\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9869\n",
      "Epoch 00068: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0428 - acc: 0.9869 - val_loss: 0.3671 - val_acc: 0.9255\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9856\n",
      "Epoch 00069: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0455 - acc: 0.9856 - val_loss: 0.3912 - val_acc: 0.9196\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9860\n",
      "Epoch 00070: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0437 - acc: 0.9860 - val_loss: 0.3606 - val_acc: 0.9271\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9820\n",
      "Epoch 00071: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0546 - acc: 0.9820 - val_loss: 0.3510 - val_acc: 0.9236\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9879\n",
      "Epoch 00072: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0401 - acc: 0.9879 - val_loss: 0.3545 - val_acc: 0.9285\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9875\n",
      "Epoch 00073: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0412 - acc: 0.9875 - val_loss: 0.3640 - val_acc: 0.9271\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9865\n",
      "Epoch 00074: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0443 - acc: 0.9865 - val_loss: 0.3526 - val_acc: 0.9301\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9877\n",
      "Epoch 00075: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0411 - acc: 0.9877 - val_loss: 0.3335 - val_acc: 0.9320\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9868\n",
      "Epoch 00076: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0419 - acc: 0.9868 - val_loss: 0.3315 - val_acc: 0.9345\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9874\n",
      "Epoch 00077: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0398 - acc: 0.9874 - val_loss: 0.3696 - val_acc: 0.9290\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9865\n",
      "Epoch 00078: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0434 - acc: 0.9865 - val_loss: 0.3568 - val_acc: 0.9308\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9870\n",
      "Epoch 00079: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0407 - acc: 0.9870 - val_loss: 0.3969 - val_acc: 0.9257\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9886\n",
      "Epoch 00080: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0360 - acc: 0.9886 - val_loss: 0.3443 - val_acc: 0.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9879\n",
      "Epoch 00081: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0397 - acc: 0.9879 - val_loss: 0.3686 - val_acc: 0.9248\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9881\n",
      "Epoch 00082: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0388 - acc: 0.9881 - val_loss: 0.3666 - val_acc: 0.9259\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9887\n",
      "Epoch 00083: val_loss did not improve from 0.29666\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0363 - acc: 0.9887 - val_loss: 0.3515 - val_acc: 0.9313\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 464us/sample - loss: 0.3787 - acc: 0.8941\n",
      "Loss: 0.3787354461871946 Accuracy: 0.894081\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0560 - acc: 0.3267\n",
      "Epoch 00001: val_loss improved from inf to 1.38711, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/001-1.3871.hdf5\n",
      "36805/36805 [==============================] - 36s 985us/sample - loss: 2.0558 - acc: 0.3268 - val_loss: 1.3871 - val_acc: 0.5693\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2973 - acc: 0.5862\n",
      "Epoch 00002: val_loss improved from 1.38711 to 0.97257, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/002-0.9726.hdf5\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 1.2973 - acc: 0.5863 - val_loss: 0.9726 - val_acc: 0.7100\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0050 - acc: 0.6843\n",
      "Epoch 00003: val_loss improved from 0.97257 to 0.78024, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/003-0.7802.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 1.0050 - acc: 0.6842 - val_loss: 0.7802 - val_acc: 0.7657\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8381 - acc: 0.7374\n",
      "Epoch 00004: val_loss improved from 0.78024 to 0.63704, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/004-0.6370.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.8381 - acc: 0.7374 - val_loss: 0.6370 - val_acc: 0.8155\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7214 - acc: 0.7734\n",
      "Epoch 00005: val_loss improved from 0.63704 to 0.57166, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/005-0.5717.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.7213 - acc: 0.7734 - val_loss: 0.5717 - val_acc: 0.8337\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6337 - acc: 0.8037\n",
      "Epoch 00006: val_loss improved from 0.57166 to 0.49362, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/006-0.4936.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.6338 - acc: 0.8037 - val_loss: 0.4936 - val_acc: 0.8549\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.8228\n",
      "Epoch 00007: val_loss improved from 0.49362 to 0.46317, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/007-0.4632.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.5646 - acc: 0.8228 - val_loss: 0.4632 - val_acc: 0.8591\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.8401\n",
      "Epoch 00008: val_loss improved from 0.46317 to 0.44788, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/008-0.4479.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.5105 - acc: 0.8401 - val_loss: 0.4479 - val_acc: 0.8663\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8564\n",
      "Epoch 00009: val_loss improved from 0.44788 to 0.38032, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/009-0.3803.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.4604 - acc: 0.8564 - val_loss: 0.3803 - val_acc: 0.8859\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8693\n",
      "Epoch 00010: val_loss improved from 0.38032 to 0.32754, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/010-0.3275.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.4213 - acc: 0.8693 - val_loss: 0.3275 - val_acc: 0.9043\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8805\n",
      "Epoch 00011: val_loss improved from 0.32754 to 0.30879, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/011-0.3088.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.3876 - acc: 0.8805 - val_loss: 0.3088 - val_acc: 0.9126\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8887\n",
      "Epoch 00012: val_loss improved from 0.30879 to 0.28052, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/012-0.2805.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.3579 - acc: 0.8887 - val_loss: 0.2805 - val_acc: 0.9248\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8960\n",
      "Epoch 00013: val_loss improved from 0.28052 to 0.27489, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/013-0.2749.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.3329 - acc: 0.8960 - val_loss: 0.2749 - val_acc: 0.9227\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9052\n",
      "Epoch 00014: val_loss improved from 0.27489 to 0.26667, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/014-0.2667.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.3083 - acc: 0.9052 - val_loss: 0.2667 - val_acc: 0.9201\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9074\n",
      "Epoch 00015: val_loss improved from 0.26667 to 0.25996, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/015-0.2600.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.2924 - acc: 0.9075 - val_loss: 0.2600 - val_acc: 0.9215\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9127\n",
      "Epoch 00016: val_loss improved from 0.25996 to 0.23572, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/016-0.2357.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2799 - acc: 0.9127 - val_loss: 0.2357 - val_acc: 0.9283\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9196\n",
      "Epoch 00017: val_loss improved from 0.23572 to 0.22413, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/017-0.2241.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.2602 - acc: 0.9196 - val_loss: 0.2241 - val_acc: 0.9364\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9232\n",
      "Epoch 00018: val_loss did not improve from 0.22413\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.2446 - acc: 0.9232 - val_loss: 0.2249 - val_acc: 0.9334\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9247\n",
      "Epoch 00019: val_loss improved from 0.22413 to 0.20996, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/019-0.2100.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.2373 - acc: 0.9247 - val_loss: 0.2100 - val_acc: 0.9399\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9294\n",
      "Epoch 00020: val_loss did not improve from 0.20996\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.2214 - acc: 0.9294 - val_loss: 0.2141 - val_acc: 0.9378\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9325\n",
      "Epoch 00021: val_loss improved from 0.20996 to 0.19749, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/021-0.1975.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.2114 - acc: 0.9325 - val_loss: 0.1975 - val_acc: 0.9429\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9357\n",
      "Epoch 00022: val_loss did not improve from 0.19749\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.2016 - acc: 0.9357 - val_loss: 0.2041 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9404\n",
      "Epoch 00023: val_loss did not improve from 0.19749\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1884 - acc: 0.9404 - val_loss: 0.1994 - val_acc: 0.9439\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9390\n",
      "Epoch 00024: val_loss did not improve from 0.19749\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1883 - acc: 0.9390 - val_loss: 0.1986 - val_acc: 0.9397\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9448\n",
      "Epoch 00025: val_loss improved from 0.19749 to 0.19609, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/025-0.1961.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1743 - acc: 0.9447 - val_loss: 0.1961 - val_acc: 0.9415\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9454\n",
      "Epoch 00026: val_loss improved from 0.19609 to 0.17969, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/026-0.1797.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1720 - acc: 0.9454 - val_loss: 0.1797 - val_acc: 0.9467\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9483\n",
      "Epoch 00027: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1607 - acc: 0.9483 - val_loss: 0.1846 - val_acc: 0.9474\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9504\n",
      "Epoch 00028: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1539 - acc: 0.9504 - val_loss: 0.1908 - val_acc: 0.9460\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9514\n",
      "Epoch 00029: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1488 - acc: 0.9514 - val_loss: 0.1926 - val_acc: 0.9467\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9546\n",
      "Epoch 00030: val_loss improved from 0.17969 to 0.17754, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/030-0.1775.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1402 - acc: 0.9546 - val_loss: 0.1775 - val_acc: 0.9488\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9561\n",
      "Epoch 00031: val_loss did not improve from 0.17754\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1348 - acc: 0.9561 - val_loss: 0.1789 - val_acc: 0.9506\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9586\n",
      "Epoch 00032: val_loss improved from 0.17754 to 0.17562, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/032-0.1756.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1276 - acc: 0.9586 - val_loss: 0.1756 - val_acc: 0.9497\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9589\n",
      "Epoch 00033: val_loss improved from 0.17562 to 0.17499, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/033-0.1750.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1240 - acc: 0.9589 - val_loss: 0.1750 - val_acc: 0.9513\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9595\n",
      "Epoch 00034: val_loss improved from 0.17499 to 0.17393, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/034-0.1739.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1211 - acc: 0.9595 - val_loss: 0.1739 - val_acc: 0.9483\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9602\n",
      "Epoch 00035: val_loss improved from 0.17393 to 0.16902, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_7_conv_checkpoint/035-0.1690.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1190 - acc: 0.9602 - val_loss: 0.1690 - val_acc: 0.9541\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9628\n",
      "Epoch 00036: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1120 - acc: 0.9628 - val_loss: 0.1744 - val_acc: 0.9532\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9646\n",
      "Epoch 00037: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1073 - acc: 0.9646 - val_loss: 0.1704 - val_acc: 0.9520\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9663\n",
      "Epoch 00038: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1014 - acc: 0.9663 - val_loss: 0.1730 - val_acc: 0.9522\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9678\n",
      "Epoch 00039: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0979 - acc: 0.9678 - val_loss: 0.1796 - val_acc: 0.9515\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9685\n",
      "Epoch 00040: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0958 - acc: 0.9685 - val_loss: 0.1711 - val_acc: 0.9515\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9685\n",
      "Epoch 00041: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0938 - acc: 0.9685 - val_loss: 0.1897 - val_acc: 0.9467\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9691\n",
      "Epoch 00042: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0934 - acc: 0.9691 - val_loss: 0.1752 - val_acc: 0.9513\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9691\n",
      "Epoch 00043: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0886 - acc: 0.9691 - val_loss: 0.1842 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9709\n",
      "Epoch 00044: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0842 - acc: 0.9709 - val_loss: 0.1768 - val_acc: 0.9534\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9720\n",
      "Epoch 00045: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0827 - acc: 0.9720 - val_loss: 0.1774 - val_acc: 0.9527\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9730\n",
      "Epoch 00046: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0784 - acc: 0.9730 - val_loss: 0.2067 - val_acc: 0.9490\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9742\n",
      "Epoch 00047: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0749 - acc: 0.9742 - val_loss: 0.1743 - val_acc: 0.9550\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9760\n",
      "Epoch 00048: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0711 - acc: 0.9760 - val_loss: 0.1933 - val_acc: 0.9497\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9750\n",
      "Epoch 00049: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0737 - acc: 0.9750 - val_loss: 0.1823 - val_acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9771\n",
      "Epoch 00050: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0666 - acc: 0.9771 - val_loss: 0.1932 - val_acc: 0.9520\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9778\n",
      "Epoch 00051: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0649 - acc: 0.9778 - val_loss: 0.1701 - val_acc: 0.9574\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9798\n",
      "Epoch 00052: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0594 - acc: 0.9798 - val_loss: 0.1778 - val_acc: 0.9562\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9793\n",
      "Epoch 00053: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0610 - acc: 0.9793 - val_loss: 0.1948 - val_acc: 0.9546\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9792\n",
      "Epoch 00054: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0598 - acc: 0.9792 - val_loss: 0.1950 - val_acc: 0.9529\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9788\n",
      "Epoch 00055: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0606 - acc: 0.9788 - val_loss: 0.1792 - val_acc: 0.9555\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9802\n",
      "Epoch 00056: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0571 - acc: 0.9802 - val_loss: 0.1909 - val_acc: 0.9567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9824\n",
      "Epoch 00057: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0525 - acc: 0.9824 - val_loss: 0.1993 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9828\n",
      "Epoch 00058: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0515 - acc: 0.9828 - val_loss: 0.1931 - val_acc: 0.9560\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9814\n",
      "Epoch 00059: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0551 - acc: 0.9814 - val_loss: 0.1870 - val_acc: 0.9560\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9833\n",
      "Epoch 00060: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0485 - acc: 0.9833 - val_loss: 0.1826 - val_acc: 0.9536\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9830\n",
      "Epoch 00061: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0478 - acc: 0.9830 - val_loss: 0.2040 - val_acc: 0.9515\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9845\n",
      "Epoch 00062: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0453 - acc: 0.9845 - val_loss: 0.1889 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9848\n",
      "Epoch 00063: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0437 - acc: 0.9848 - val_loss: 0.2101 - val_acc: 0.9511\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9858\n",
      "Epoch 00064: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0409 - acc: 0.9858 - val_loss: 0.1901 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9852\n",
      "Epoch 00065: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0412 - acc: 0.9852 - val_loss: 0.2062 - val_acc: 0.9536\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9857\n",
      "Epoch 00066: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0414 - acc: 0.9857 - val_loss: 0.1894 - val_acc: 0.9548\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9852\n",
      "Epoch 00067: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0424 - acc: 0.9852 - val_loss: 0.2134 - val_acc: 0.9511\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9864\n",
      "Epoch 00068: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0401 - acc: 0.9864 - val_loss: 0.2087 - val_acc: 0.9548\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9871\n",
      "Epoch 00069: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0387 - acc: 0.9871 - val_loss: 0.2002 - val_acc: 0.9550\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9894\n",
      "Epoch 00070: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0333 - acc: 0.9894 - val_loss: 0.2230 - val_acc: 0.9555\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9866\n",
      "Epoch 00071: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0383 - acc: 0.9866 - val_loss: 0.2167 - val_acc: 0.9520\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9870\n",
      "Epoch 00072: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0363 - acc: 0.9870 - val_loss: 0.2310 - val_acc: 0.9534\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9888\n",
      "Epoch 00073: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0322 - acc: 0.9888 - val_loss: 0.2383 - val_acc: 0.9488\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9874\n",
      "Epoch 00074: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0363 - acc: 0.9874 - val_loss: 0.2267 - val_acc: 0.9520\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9881\n",
      "Epoch 00075: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0354 - acc: 0.9881 - val_loss: 0.2207 - val_acc: 0.9539\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9891\n",
      "Epoch 00076: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0313 - acc: 0.9891 - val_loss: 0.2047 - val_acc: 0.9555\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 00077: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0291 - acc: 0.9905 - val_loss: 0.2090 - val_acc: 0.9509\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9884\n",
      "Epoch 00078: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0334 - acc: 0.9884 - val_loss: 0.2190 - val_acc: 0.9536\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9897\n",
      "Epoch 00079: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0300 - acc: 0.9897 - val_loss: 0.1937 - val_acc: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9896\n",
      "Epoch 00080: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0315 - acc: 0.9895 - val_loss: 0.2119 - val_acc: 0.9525\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9895\n",
      "Epoch 00081: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0295 - acc: 0.9895 - val_loss: 0.2161 - val_acc: 0.9564\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9908\n",
      "Epoch 00082: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0275 - acc: 0.9908 - val_loss: 0.2153 - val_acc: 0.9555\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9906\n",
      "Epoch 00083: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0263 - acc: 0.9906 - val_loss: 0.2048 - val_acc: 0.9574\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9900\n",
      "Epoch 00084: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.2163 - val_acc: 0.9555\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9906\n",
      "Epoch 00085: val_loss did not improve from 0.16902\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0257 - acc: 0.9906 - val_loss: 0.2371 - val_acc: 0.9541\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 509us/sample - loss: 0.2445 - acc: 0.9317\n",
      "Loss: 0.2444613045683036 Accuracy: 0.93167186\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0791 - acc: 0.3160\n",
      "Epoch 00001: val_loss improved from inf to 1.27871, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/001-1.2787.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 2.0791 - acc: 0.3160 - val_loss: 1.2787 - val_acc: 0.6105\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1976 - acc: 0.6108\n",
      "Epoch 00002: val_loss improved from 1.27871 to 1.05020, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/002-1.0502.hdf5\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 1.1975 - acc: 0.6108 - val_loss: 1.0502 - val_acc: 0.6641\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9181 - acc: 0.7028\n",
      "Epoch 00003: val_loss improved from 1.05020 to 0.66314, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/003-0.6631.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.9180 - acc: 0.7028 - val_loss: 0.6631 - val_acc: 0.7978\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7565 - acc: 0.7552\n",
      "Epoch 00004: val_loss improved from 0.66314 to 0.56808, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/004-0.5681.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.7564 - acc: 0.7553 - val_loss: 0.5681 - val_acc: 0.8251\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.7927\n",
      "Epoch 00005: val_loss improved from 0.56808 to 0.46642, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/005-0.4664.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.6403 - acc: 0.7927 - val_loss: 0.4664 - val_acc: 0.8509\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8218\n",
      "Epoch 00006: val_loss improved from 0.46642 to 0.42495, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/006-0.4250.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.5589 - acc: 0.8217 - val_loss: 0.4250 - val_acc: 0.8668\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8424\n",
      "Epoch 00007: val_loss improved from 0.42495 to 0.36179, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/007-0.3618.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.4954 - acc: 0.8424 - val_loss: 0.3618 - val_acc: 0.8917\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8577\n",
      "Epoch 00008: val_loss improved from 0.36179 to 0.32163, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/008-0.3216.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.4459 - acc: 0.8577 - val_loss: 0.3216 - val_acc: 0.8994\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8719\n",
      "Epoch 00009: val_loss improved from 0.32163 to 0.29992, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/009-0.2999.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.3977 - acc: 0.8719 - val_loss: 0.2999 - val_acc: 0.9101\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8841\n",
      "Epoch 00010: val_loss improved from 0.29992 to 0.27655, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/010-0.2766.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.3628 - acc: 0.8841 - val_loss: 0.2766 - val_acc: 0.9131\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8923\n",
      "Epoch 00011: val_loss improved from 0.27655 to 0.25103, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/011-0.2510.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3354 - acc: 0.8922 - val_loss: 0.2510 - val_acc: 0.9227\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9037\n",
      "Epoch 00012: val_loss did not improve from 0.25103\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.3055 - acc: 0.9037 - val_loss: 0.2624 - val_acc: 0.9208\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9102\n",
      "Epoch 00013: val_loss improved from 0.25103 to 0.21772, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/013-0.2177.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.2840 - acc: 0.9102 - val_loss: 0.2177 - val_acc: 0.9299\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9157\n",
      "Epoch 00014: val_loss improved from 0.21772 to 0.19763, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/014-0.1976.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.2621 - acc: 0.9157 - val_loss: 0.1976 - val_acc: 0.9380\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9229\n",
      "Epoch 00015: val_loss improved from 0.19763 to 0.19503, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/015-0.1950.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.2441 - acc: 0.9229 - val_loss: 0.1950 - val_acc: 0.9383\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9255\n",
      "Epoch 00016: val_loss improved from 0.19503 to 0.18416, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/016-0.1842.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.2293 - acc: 0.9255 - val_loss: 0.1842 - val_acc: 0.9373\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9297\n",
      "Epoch 00017: val_loss improved from 0.18416 to 0.17434, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/017-0.1743.hdf5\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.2183 - acc: 0.9297 - val_loss: 0.1743 - val_acc: 0.9413\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9345\n",
      "Epoch 00018: val_loss did not improve from 0.17434\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.2054 - acc: 0.9345 - val_loss: 0.2098 - val_acc: 0.9352\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9347\n",
      "Epoch 00019: val_loss improved from 0.17434 to 0.16751, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/019-0.1675.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1989 - acc: 0.9347 - val_loss: 0.1675 - val_acc: 0.9483\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9396\n",
      "Epoch 00020: val_loss improved from 0.16751 to 0.16367, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/020-0.1637.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1864 - acc: 0.9396 - val_loss: 0.1637 - val_acc: 0.9476\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9426\n",
      "Epoch 00021: val_loss improved from 0.16367 to 0.15103, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/021-0.1510.hdf5\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.1767 - acc: 0.9426 - val_loss: 0.1510 - val_acc: 0.9522\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9441\n",
      "Epoch 00022: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1720 - acc: 0.9441 - val_loss: 0.1611 - val_acc: 0.9481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9463\n",
      "Epoch 00023: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1610 - acc: 0.9463 - val_loss: 0.1558 - val_acc: 0.9509\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9512\n",
      "Epoch 00024: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1513 - acc: 0.9511 - val_loss: 0.1707 - val_acc: 0.9495\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9505\n",
      "Epoch 00025: val_loss improved from 0.15103 to 0.14289, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/025-0.1429.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.1481 - acc: 0.9505 - val_loss: 0.1429 - val_acc: 0.9539\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9536\n",
      "Epoch 00026: val_loss did not improve from 0.14289\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1399 - acc: 0.9536 - val_loss: 0.1603 - val_acc: 0.9534\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9553\n",
      "Epoch 00027: val_loss did not improve from 0.14289\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.1342 - acc: 0.9553 - val_loss: 0.1551 - val_acc: 0.9483\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9569\n",
      "Epoch 00028: val_loss did not improve from 0.14289\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.1289 - acc: 0.9569 - val_loss: 0.1429 - val_acc: 0.9583\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9589\n",
      "Epoch 00029: val_loss did not improve from 0.14289\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1211 - acc: 0.9589 - val_loss: 0.1500 - val_acc: 0.9515\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9596\n",
      "Epoch 00030: val_loss did not improve from 0.14289\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1201 - acc: 0.9596 - val_loss: 0.1632 - val_acc: 0.9506\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9615\n",
      "Epoch 00031: val_loss improved from 0.14289 to 0.13279, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_8_conv_checkpoint/031-0.1328.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1123 - acc: 0.9616 - val_loss: 0.1328 - val_acc: 0.9581\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9638\n",
      "Epoch 00032: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1073 - acc: 0.9638 - val_loss: 0.1456 - val_acc: 0.9569\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9647\n",
      "Epoch 00033: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1034 - acc: 0.9647 - val_loss: 0.1395 - val_acc: 0.9585\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9662\n",
      "Epoch 00034: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1009 - acc: 0.9662 - val_loss: 0.1473 - val_acc: 0.9567\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9685\n",
      "Epoch 00035: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0938 - acc: 0.9685 - val_loss: 0.1396 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9682\n",
      "Epoch 00036: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0923 - acc: 0.9682 - val_loss: 0.1566 - val_acc: 0.9543\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9698\n",
      "Epoch 00037: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0879 - acc: 0.9698 - val_loss: 0.1348 - val_acc: 0.9597\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9706\n",
      "Epoch 00038: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0860 - acc: 0.9706 - val_loss: 0.1696 - val_acc: 0.9520\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9724\n",
      "Epoch 00039: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0806 - acc: 0.9724 - val_loss: 0.1490 - val_acc: 0.9592\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9736\n",
      "Epoch 00040: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0770 - acc: 0.9736 - val_loss: 0.1425 - val_acc: 0.9567\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9743\n",
      "Epoch 00041: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0765 - acc: 0.9743 - val_loss: 0.1365 - val_acc: 0.9578\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9770\n",
      "Epoch 00042: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0683 - acc: 0.9770 - val_loss: 0.1363 - val_acc: 0.9604\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9760\n",
      "Epoch 00043: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0697 - acc: 0.9760 - val_loss: 0.1344 - val_acc: 0.9637\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9765\n",
      "Epoch 00044: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0679 - acc: 0.9765 - val_loss: 0.1407 - val_acc: 0.9583\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9769\n",
      "Epoch 00045: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0661 - acc: 0.9769 - val_loss: 0.1428 - val_acc: 0.9602\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9779\n",
      "Epoch 00046: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0652 - acc: 0.9779 - val_loss: 0.1677 - val_acc: 0.9525\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9806\n",
      "Epoch 00047: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0581 - acc: 0.9806 - val_loss: 0.1498 - val_acc: 0.9574\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9796\n",
      "Epoch 00048: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0572 - acc: 0.9796 - val_loss: 0.1505 - val_acc: 0.9595\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9796\n",
      "Epoch 00049: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0582 - acc: 0.9796 - val_loss: 0.1406 - val_acc: 0.9623\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9821\n",
      "Epoch 00050: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0530 - acc: 0.9821 - val_loss: 0.1336 - val_acc: 0.9632\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9827\n",
      "Epoch 00051: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0518 - acc: 0.9827 - val_loss: 0.1556 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9805\n",
      "Epoch 00052: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0556 - acc: 0.9805 - val_loss: 0.1433 - val_acc: 0.9618\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9829\n",
      "Epoch 00053: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0483 - acc: 0.9829 - val_loss: 0.1671 - val_acc: 0.9574\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9832\n",
      "Epoch 00054: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0481 - acc: 0.9832 - val_loss: 0.1535 - val_acc: 0.9595\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9848\n",
      "Epoch 00055: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0447 - acc: 0.9848 - val_loss: 0.1464 - val_acc: 0.9627\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9836\n",
      "Epoch 00056: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0473 - acc: 0.9836 - val_loss: 0.1886 - val_acc: 0.9504\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9853\n",
      "Epoch 00057: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0419 - acc: 0.9853 - val_loss: 0.1552 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9850\n",
      "Epoch 00058: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0436 - acc: 0.9850 - val_loss: 0.1432 - val_acc: 0.9639\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9861\n",
      "Epoch 00059: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0402 - acc: 0.9861 - val_loss: 0.1544 - val_acc: 0.9597\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9871\n",
      "Epoch 00060: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0395 - acc: 0.9871 - val_loss: 0.1655 - val_acc: 0.9627\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9865\n",
      "Epoch 00061: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0386 - acc: 0.9866 - val_loss: 0.1477 - val_acc: 0.9625\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9876\n",
      "Epoch 00062: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0359 - acc: 0.9876 - val_loss: 0.1494 - val_acc: 0.9641\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9879\n",
      "Epoch 00063: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0357 - acc: 0.9879 - val_loss: 0.1700 - val_acc: 0.9634\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9873\n",
      "Epoch 00064: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0379 - acc: 0.9873 - val_loss: 0.1579 - val_acc: 0.9590\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9885\n",
      "Epoch 00065: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0333 - acc: 0.9885 - val_loss: 0.1542 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9894\n",
      "Epoch 00066: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0315 - acc: 0.9894 - val_loss: 0.1711 - val_acc: 0.9611\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9887\n",
      "Epoch 00067: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0326 - acc: 0.9887 - val_loss: 0.1890 - val_acc: 0.9613\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9901\n",
      "Epoch 00068: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0291 - acc: 0.9901 - val_loss: 0.1650 - val_acc: 0.9641\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9888\n",
      "Epoch 00069: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0313 - acc: 0.9888 - val_loss: 0.1600 - val_acc: 0.9637\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9907\n",
      "Epoch 00070: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0272 - acc: 0.9907 - val_loss: 0.1771 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9896\n",
      "Epoch 00071: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0293 - acc: 0.9896 - val_loss: 0.1853 - val_acc: 0.9585\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9898\n",
      "Epoch 00072: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0293 - acc: 0.9898 - val_loss: 0.1728 - val_acc: 0.9641\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9907\n",
      "Epoch 00073: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.1647 - val_acc: 0.9613\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9904\n",
      "Epoch 00074: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0270 - acc: 0.9904 - val_loss: 0.1670 - val_acc: 0.9630\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9918\n",
      "Epoch 00075: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0230 - acc: 0.9918 - val_loss: 0.1807 - val_acc: 0.9639\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9897\n",
      "Epoch 00076: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0303 - acc: 0.9897 - val_loss: 0.1672 - val_acc: 0.9632\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9914\n",
      "Epoch 00077: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0268 - acc: 0.9914 - val_loss: 0.1813 - val_acc: 0.9634\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9911\n",
      "Epoch 00078: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0269 - acc: 0.9911 - val_loss: 0.1547 - val_acc: 0.9655\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9919\n",
      "Epoch 00079: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0252 - acc: 0.9919 - val_loss: 0.1671 - val_acc: 0.9632\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9917\n",
      "Epoch 00080: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0249 - acc: 0.9917 - val_loss: 0.1482 - val_acc: 0.9674\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9928\n",
      "Epoch 00081: val_loss did not improve from 0.13279\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0220 - acc: 0.9928 - val_loss: 0.1750 - val_acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 507us/sample - loss: 0.2161 - acc: 0.9344\n",
      "Loss: 0.21605762022925562 Accuracy: 0.93437177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 492928)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,928,336\n",
      "Trainable params: 7,928,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 520us/sample - loss: 1.4146 - acc: 0.5499\n",
      "Loss: 1.4145817289223677 Accuracy: 0.5499481\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 164224)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,689,616\n",
      "Trainable params: 2,689,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 499us/sample - loss: 1.1846 - acc: 0.6318\n",
      "Loss: 1.1846418001941432 Accuracy: 0.6317757\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 58816)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,044,176\n",
      "Trainable params: 1,044,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 529us/sample - loss: 0.8217 - acc: 0.7626\n",
      "Loss: 0.8217454534082026 Accuracy: 0.7626168\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23616)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 563,024\n",
      "Trainable params: 563,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 528us/sample - loss: 0.3787 - acc: 0.8941\n",
      "Loss: 0.3787354461871946 Accuracy: 0.894081\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 11904)        0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 457,680\n",
      "Trainable params: 457,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 509us/sample - loss: 0.2445 - acc: 0.9317\n",
      "Loss: 0.2444613045683036 Accuracy: 0.93167186\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3840)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 410,704\n",
      "Trainable params: 410,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 559us/sample - loss: 0.2161 - acc: 0.9344\n",
      "Loss: 0.21605762022925562 Accuracy: 0.93437177\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 492928)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,928,336\n",
      "Trainable params: 7,928,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 540us/sample - loss: 2.5151 - acc: 0.6015\n",
      "Loss: 2.515087713160371 Accuracy: 0.6014538\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 164224)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,689,616\n",
      "Trainable params: 2,689,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 537us/sample - loss: 1.8426 - acc: 0.7020\n",
      "Loss: 1.842565804552809 Accuracy: 0.701973\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 58816)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,044,176\n",
      "Trainable params: 1,044,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 567us/sample - loss: 1.0927 - acc: 0.8066\n",
      "Loss: 1.0927247780753322 Accuracy: 0.8066459\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23616)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 563,024\n",
      "Trainable params: 563,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 574us/sample - loss: 0.4611 - acc: 0.9051\n",
      "Loss: 0.4610851303346432 Accuracy: 0.90508825\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 11904)        0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 457,680\n",
      "Trainable params: 457,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 578us/sample - loss: 0.3443 - acc: 0.9360\n",
      "Loss: 0.3442751756965 Accuracy: 0.93603325\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3840)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 410,704\n",
      "Trainable params: 410,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 602us/sample - loss: 0.2777 - acc: 0.9475\n",
      "Loss: 0.277709924445709 Accuracy: 0.9474559\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
