{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3532 - acc: 0.2358\n",
      "Epoch 00001: val_loss improved from inf to 2.32846, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/001-2.3285.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 2.3532 - acc: 0.2358 - val_loss: 2.3285 - val_acc: 0.2269\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7470 - acc: 0.4318\n",
      "Epoch 00002: val_loss improved from 2.32846 to 1.63503, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/002-1.6350.hdf5\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 1.7471 - acc: 0.4318 - val_loss: 1.6350 - val_acc: 0.4715\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4462 - acc: 0.5325\n",
      "Epoch 00003: val_loss improved from 1.63503 to 1.61928, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/003-1.6193.hdf5\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 1.4463 - acc: 0.5325 - val_loss: 1.6193 - val_acc: 0.4752\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.5982\n",
      "Epoch 00004: val_loss did not improve from 1.61928\n",
      "36805/36805 [==============================] - 23s 626us/sample - loss: 1.2515 - acc: 0.5980 - val_loss: 1.6196 - val_acc: 0.5013\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1122 - acc: 0.6446\n",
      "Epoch 00005: val_loss improved from 1.61928 to 1.17157, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/005-1.1716.hdf5\n",
      "36805/36805 [==============================] - 23s 615us/sample - loss: 1.1122 - acc: 0.6447 - val_loss: 1.1716 - val_acc: 0.6294\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0114 - acc: 0.6745\n",
      "Epoch 00006: val_loss did not improve from 1.17157\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 1.0114 - acc: 0.6745 - val_loss: 1.1848 - val_acc: 0.6359\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7002\n",
      "Epoch 00007: val_loss did not improve from 1.17157\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.9344 - acc: 0.7001 - val_loss: 2.2432 - val_acc: 0.4216\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8961 - acc: 0.7066\n",
      "Epoch 00008: val_loss improved from 1.17157 to 1.11769, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/008-1.1177.hdf5\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.8960 - acc: 0.7066 - val_loss: 1.1177 - val_acc: 0.6592\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8397 - acc: 0.7310\n",
      "Epoch 00009: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 591us/sample - loss: 0.8398 - acc: 0.7310 - val_loss: 1.2607 - val_acc: 0.6045\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7911 - acc: 0.7440\n",
      "Epoch 00010: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.7912 - acc: 0.7440 - val_loss: 1.1468 - val_acc: 0.6455\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7234 - acc: 0.7643\n",
      "Epoch 00011: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.7236 - acc: 0.7643 - val_loss: 1.1470 - val_acc: 0.6536\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.7743\n",
      "Epoch 00012: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 602us/sample - loss: 0.6978 - acc: 0.7743 - val_loss: 1.1971 - val_acc: 0.6497\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.7836\n",
      "Epoch 00013: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.6619 - acc: 0.7836 - val_loss: 1.4167 - val_acc: 0.5826\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.7948\n",
      "Epoch 00014: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.6307 - acc: 0.7948 - val_loss: 1.3632 - val_acc: 0.6247\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6150 - acc: 0.7990\n",
      "Epoch 00015: val_loss did not improve from 1.11769\n",
      "36805/36805 [==============================] - 22s 603us/sample - loss: 0.6150 - acc: 0.7990 - val_loss: 1.2167 - val_acc: 0.6562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5952 - acc: 0.8047\n",
      "Epoch 00016: val_loss improved from 1.11769 to 1.09039, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/016-1.0904.hdf5\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.5952 - acc: 0.8047 - val_loss: 1.0904 - val_acc: 0.6720\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8136\n",
      "Epoch 00017: val_loss improved from 1.09039 to 1.08949, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/017-1.0895.hdf5\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.5696 - acc: 0.8136 - val_loss: 1.0895 - val_acc: 0.6837\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.8217\n",
      "Epoch 00018: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.5487 - acc: 0.8217 - val_loss: 1.1557 - val_acc: 0.6713\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.8223\n",
      "Epoch 00019: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.5411 - acc: 0.8223 - val_loss: 1.1480 - val_acc: 0.6657\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8340\n",
      "Epoch 00020: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 603us/sample - loss: 0.5115 - acc: 0.8341 - val_loss: 1.0960 - val_acc: 0.6830\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4963 - acc: 0.8357\n",
      "Epoch 00021: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.4963 - acc: 0.8357 - val_loss: 1.1258 - val_acc: 0.7046\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.8368\n",
      "Epoch 00022: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 602us/sample - loss: 0.4928 - acc: 0.8368 - val_loss: 1.2270 - val_acc: 0.6781\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8455\n",
      "Epoch 00023: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.4705 - acc: 0.8455 - val_loss: 1.2027 - val_acc: 0.6993\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8457\n",
      "Epoch 00024: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 602us/sample - loss: 0.4686 - acc: 0.8456 - val_loss: 1.5889 - val_acc: 0.5171\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8372\n",
      "Epoch 00025: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.5019 - acc: 0.8372 - val_loss: 1.4041 - val_acc: 0.6613\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8498\n",
      "Epoch 00026: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.4630 - acc: 0.8498 - val_loss: 1.0996 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8537\n",
      "Epoch 00027: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.4398 - acc: 0.8537 - val_loss: 1.2047 - val_acc: 0.7016\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8544\n",
      "Epoch 00028: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.4417 - acc: 0.8544 - val_loss: 1.2149 - val_acc: 0.6890\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8629\n",
      "Epoch 00029: val_loss did not improve from 1.08949\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.4182 - acc: 0.8628 - val_loss: 1.3526 - val_acc: 0.6583\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8621\n",
      "Epoch 00030: val_loss improved from 1.08949 to 1.08253, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv_checkpoint/030-1.0825.hdf5\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.4219 - acc: 0.8621 - val_loss: 1.0825 - val_acc: 0.6974\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8669\n",
      "Epoch 00031: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.4001 - acc: 0.8669 - val_loss: 1.1001 - val_acc: 0.7014\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8665\n",
      "Epoch 00032: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.4126 - acc: 0.8665 - val_loss: 1.2133 - val_acc: 0.6923\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8680\n",
      "Epoch 00033: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.4026 - acc: 0.8680 - val_loss: 1.1769 - val_acc: 0.7133\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8703\n",
      "Epoch 00034: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.3893 - acc: 0.8703 - val_loss: 1.1900 - val_acc: 0.6872\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8727\n",
      "Epoch 00035: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.3930 - acc: 0.8727 - val_loss: 1.1864 - val_acc: 0.6942\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8739\n",
      "Epoch 00036: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.3855 - acc: 0.8739 - val_loss: 1.1846 - val_acc: 0.6876\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8755\n",
      "Epoch 00037: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.3760 - acc: 0.8755 - val_loss: 1.4545 - val_acc: 0.6657\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8737\n",
      "Epoch 00038: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 594us/sample - loss: 0.3809 - acc: 0.8737 - val_loss: 1.0834 - val_acc: 0.7160\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8789\n",
      "Epoch 00039: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.3702 - acc: 0.8789 - val_loss: 1.5127 - val_acc: 0.6273\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8062\n",
      "Epoch 00040: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.6036 - acc: 0.8062 - val_loss: 1.1835 - val_acc: 0.6942\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8627\n",
      "Epoch 00041: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.4248 - acc: 0.8626 - val_loss: 1.4192 - val_acc: 0.6424\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8657\n",
      "Epoch 00042: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.4088 - acc: 0.8656 - val_loss: 1.4176 - val_acc: 0.6555\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8660\n",
      "Epoch 00043: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.4067 - acc: 0.8660 - val_loss: 1.3071 - val_acc: 0.6797\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8714\n",
      "Epoch 00044: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.3888 - acc: 0.8714 - val_loss: 2.2544 - val_acc: 0.5034\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8627\n",
      "Epoch 00045: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.4233 - acc: 0.8627 - val_loss: 1.1593 - val_acc: 0.7042\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8759\n",
      "Epoch 00046: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.3745 - acc: 0.8759 - val_loss: 1.1023 - val_acc: 0.7116\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8816\n",
      "Epoch 00047: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 0.3593 - acc: 0.8816 - val_loss: 1.1734 - val_acc: 0.7060\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8806\n",
      "Epoch 00048: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 603us/sample - loss: 0.3662 - acc: 0.8806 - val_loss: 1.2055 - val_acc: 0.7165\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8826\n",
      "Epoch 00049: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.3529 - acc: 0.8826 - val_loss: 1.1609 - val_acc: 0.6981\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8838\n",
      "Epoch 00050: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.3545 - acc: 0.8838 - val_loss: 1.2103 - val_acc: 0.7093\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8849\n",
      "Epoch 00051: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.3471 - acc: 0.8849 - val_loss: 1.1223 - val_acc: 0.7158\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8865\n",
      "Epoch 00052: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.3492 - acc: 0.8865 - val_loss: 1.3000 - val_acc: 0.6997\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8930\n",
      "Epoch 00053: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.3309 - acc: 0.8931 - val_loss: 1.2206 - val_acc: 0.7084\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8907\n",
      "Epoch 00054: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.3336 - acc: 0.8907 - val_loss: 1.2828 - val_acc: 0.7128\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.8890\n",
      "Epoch 00055: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.3353 - acc: 0.8890 - val_loss: 1.6268 - val_acc: 0.6499\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8906\n",
      "Epoch 00056: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 596us/sample - loss: 0.3357 - acc: 0.8906 - val_loss: 1.3299 - val_acc: 0.6606\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8804\n",
      "Epoch 00057: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 597us/sample - loss: 0.3656 - acc: 0.8804 - val_loss: 1.1738 - val_acc: 0.7091\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8923\n",
      "Epoch 00058: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.3332 - acc: 0.8922 - val_loss: 1.2036 - val_acc: 0.7077\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8919\n",
      "Epoch 00059: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 610us/sample - loss: 0.3289 - acc: 0.8919 - val_loss: 1.3390 - val_acc: 0.6760\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8934\n",
      "Epoch 00060: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 607us/sample - loss: 0.3298 - acc: 0.8934 - val_loss: 1.4675 - val_acc: 0.6760\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8913\n",
      "Epoch 00061: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 605us/sample - loss: 0.3316 - acc: 0.8913 - val_loss: 1.1936 - val_acc: 0.7119\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8938\n",
      "Epoch 00062: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 604us/sample - loss: 0.3218 - acc: 0.8937 - val_loss: 1.2905 - val_acc: 0.6930\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8934\n",
      "Epoch 00063: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 603us/sample - loss: 0.3253 - acc: 0.8934 - val_loss: 1.2906 - val_acc: 0.7070\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.9003\n",
      "Epoch 00064: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.3078 - acc: 0.9003 - val_loss: 1.4486 - val_acc: 0.6928\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9040\n",
      "Epoch 00065: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.2940 - acc: 0.9040 - val_loss: 1.6098 - val_acc: 0.6546\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8914\n",
      "Epoch 00066: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 604us/sample - loss: 0.3315 - acc: 0.8914 - val_loss: 1.2372 - val_acc: 0.7149\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9010\n",
      "Epoch 00067: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 604us/sample - loss: 0.3115 - acc: 0.9010 - val_loss: 1.1999 - val_acc: 0.7107\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9007\n",
      "Epoch 00068: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 0.2957 - acc: 0.9007 - val_loss: 1.1798 - val_acc: 0.6981\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9001\n",
      "Epoch 00069: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 603us/sample - loss: 0.3058 - acc: 0.9001 - val_loss: 1.1408 - val_acc: 0.7130\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9034\n",
      "Epoch 00070: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.2931 - acc: 0.9034 - val_loss: 1.2640 - val_acc: 0.6825\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9009\n",
      "Epoch 00071: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.3067 - acc: 0.9009 - val_loss: 1.3818 - val_acc: 0.6771\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8992\n",
      "Epoch 00072: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.3103 - acc: 0.8993 - val_loss: 1.2764 - val_acc: 0.7079\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9054\n",
      "Epoch 00073: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 604us/sample - loss: 0.2900 - acc: 0.9054 - val_loss: 1.2579 - val_acc: 0.7133\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9000\n",
      "Epoch 00074: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.3042 - acc: 0.9000 - val_loss: 1.2016 - val_acc: 0.7228\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9098\n",
      "Epoch 00075: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 598us/sample - loss: 0.2764 - acc: 0.9097 - val_loss: 1.2655 - val_acc: 0.7112\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9047\n",
      "Epoch 00076: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 600us/sample - loss: 0.2932 - acc: 0.9047 - val_loss: 1.1966 - val_acc: 0.7121\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9054\n",
      "Epoch 00077: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.2869 - acc: 0.9054 - val_loss: 1.2564 - val_acc: 0.7174\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9077\n",
      "Epoch 00078: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.2791 - acc: 0.9078 - val_loss: 1.2509 - val_acc: 0.7233\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9070\n",
      "Epoch 00079: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 601us/sample - loss: 0.2838 - acc: 0.9070 - val_loss: 1.2735 - val_acc: 0.7160\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9085\n",
      "Epoch 00080: val_loss did not improve from 1.08253\n",
      "36805/36805 [==============================] - 22s 599us/sample - loss: 0.2831 - acc: 0.9085 - val_loss: 1.2950 - val_acc: 0.6939\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 318us/sample - loss: 1.1766 - acc: 0.6762\n",
      "Loss: 1.1766346916355201 Accuracy: 0.6762201\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2654 - acc: 0.2511\n",
      "Epoch 00001: val_loss improved from inf to 1.71526, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv_checkpoint/001-1.7153.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 2.2653 - acc: 0.2511 - val_loss: 1.7153 - val_acc: 0.4372\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6855 - acc: 0.4487\n",
      "Epoch 00002: val_loss did not improve from 1.71526\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 1.6855 - acc: 0.4487 - val_loss: 1.9489 - val_acc: 0.4018\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3383 - acc: 0.5700\n",
      "Epoch 00003: val_loss improved from 1.71526 to 1.20569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv_checkpoint/003-1.2057.hdf5\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 1.3383 - acc: 0.5700 - val_loss: 1.2057 - val_acc: 0.6219\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1052 - acc: 0.6543\n",
      "Epoch 00004: val_loss did not improve from 1.20569\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 1.1052 - acc: 0.6543 - val_loss: 1.2718 - val_acc: 0.6024\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0105 - acc: 0.6808\n",
      "Epoch 00005: val_loss improved from 1.20569 to 0.87211, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv_checkpoint/005-0.8721.hdf5\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 1.0104 - acc: 0.6808 - val_loss: 0.8721 - val_acc: 0.7410\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7151\n",
      "Epoch 00006: val_loss did not improve from 0.87211\n",
      "36805/36805 [==============================] - 23s 626us/sample - loss: 0.9079 - acc: 0.7151 - val_loss: 0.9735 - val_acc: 0.7060\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9006 - acc: 0.7166\n",
      "Epoch 00007: val_loss improved from 0.87211 to 0.78380, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv_checkpoint/007-0.7838.hdf5\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 0.9006 - acc: 0.7166 - val_loss: 0.7838 - val_acc: 0.7741\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8471 - acc: 0.7359\n",
      "Epoch 00008: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.8470 - acc: 0.7359 - val_loss: 1.3881 - val_acc: 0.5756\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9001 - acc: 0.7152\n",
      "Epoch 00009: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 626us/sample - loss: 0.9000 - acc: 0.7152 - val_loss: 0.9113 - val_acc: 0.7372\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8151 - acc: 0.7481\n",
      "Epoch 00010: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 0.8152 - acc: 0.7481 - val_loss: 1.1316 - val_acc: 0.6639\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6107 - acc: 0.4827\n",
      "Epoch 00011: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 1.6108 - acc: 0.4827 - val_loss: 2.7572 - val_acc: 0.0801\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7240 - acc: 0.0766\n",
      "Epoch 00012: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7240 - acc: 0.0766 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0814\n",
      "Epoch 00013: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0814 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00014: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7186 - val_acc: 0.0776\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0780\n",
      "Epoch 00015: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7206 - acc: 0.0781 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0822\n",
      "Epoch 00016: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0822 - val_loss: 2.7188 - val_acc: 0.0811\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00017: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7186 - val_acc: 0.0785\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00018: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 626us/sample - loss: 2.7205 - acc: 0.0809 - val_loss: 2.7189 - val_acc: 0.0776\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0797\n",
      "Epoch 00019: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0797 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00020: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7187 - val_acc: 0.0776\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0805\n",
      "Epoch 00021: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0805 - val_loss: 2.7185 - val_acc: 0.0818\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00022: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7184 - val_acc: 0.0818\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0803\n",
      "Epoch 00023: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0803 - val_loss: 2.7185 - val_acc: 0.0818\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0831\n",
      "Epoch 00024: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 2.7205 - acc: 0.0831 - val_loss: 2.7189 - val_acc: 0.0818\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00025: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 620us/sample - loss: 2.7205 - acc: 0.0812 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00026: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 628us/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7190 - val_acc: 0.0785\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0782\n",
      "Epoch 00027: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7205 - acc: 0.0782 - val_loss: 2.7187 - val_acc: 0.0785\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0808\n",
      "Epoch 00028: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7205 - acc: 0.0807 - val_loss: 2.7185 - val_acc: 0.0776\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0811\n",
      "Epoch 00029: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0811 - val_loss: 2.7189 - val_acc: 0.0818\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0804\n",
      "Epoch 00030: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7206 - acc: 0.0803 - val_loss: 2.7182 - val_acc: 0.0820\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00031: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00032: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7205 - acc: 0.0798 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0815\n",
      "Epoch 00033: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7204 - acc: 0.0815 - val_loss: 2.7190 - val_acc: 0.0818\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0808\n",
      "Epoch 00034: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7204 - acc: 0.0808 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0821\n",
      "Epoch 00035: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7205 - acc: 0.0821 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0795\n",
      "Epoch 00036: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0795 - val_loss: 2.7185 - val_acc: 0.0785\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0810\n",
      "Epoch 00037: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00038: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0773\n",
      "Epoch 00039: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 2.7204 - acc: 0.0773 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0814\n",
      "Epoch 00040: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0813 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0817\n",
      "Epoch 00041: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7205 - acc: 0.0817 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0806\n",
      "Epoch 00042: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7204 - acc: 0.0806 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0811\n",
      "Epoch 00043: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7206 - acc: 0.0811 - val_loss: 2.7181 - val_acc: 0.0820\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0815\n",
      "Epoch 00044: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0815 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0816\n",
      "Epoch 00045: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 621us/sample - loss: 2.7205 - acc: 0.0816 - val_loss: 2.7186 - val_acc: 0.0776\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0807\n",
      "Epoch 00046: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0806 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0801\n",
      "Epoch 00047: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 2.7205 - acc: 0.0801 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0798\n",
      "Epoch 00048: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7187 - val_acc: 0.0820\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0802\n",
      "Epoch 00049: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0802 - val_loss: 2.7188 - val_acc: 0.0820\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0822\n",
      "Epoch 00050: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0822 - val_loss: 2.7186 - val_acc: 0.0818\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0804\n",
      "Epoch 00051: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 2.7206 - acc: 0.0804 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0799\n",
      "Epoch 00052: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 2.7205 - acc: 0.0799 - val_loss: 2.7184 - val_acc: 0.0820\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0815\n",
      "Epoch 00053: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7205 - acc: 0.0816 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0792\n",
      "Epoch 00054: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7205 - acc: 0.0792 - val_loss: 2.7185 - val_acc: 0.0820\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7204 - acc: 0.0798\n",
      "Epoch 00055: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 622us/sample - loss: 2.7204 - acc: 0.0798 - val_loss: 2.7186 - val_acc: 0.0820\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7205 - acc: 0.0810\n",
      "Epoch 00056: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.7205 - acc: 0.0810 - val_loss: 2.7183 - val_acc: 0.0820\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7206 - acc: 0.0790\n",
      "Epoch 00057: val_loss did not improve from 0.78380\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 2.7206 - acc: 0.0790 - val_loss: 2.7183 - val_acc: 0.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 387us/sample - loss: 0.8981 - acc: 0.7304\n",
      "Loss: 0.8980849098193683 Accuracy: 0.7304258\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3061 - acc: 0.2391\n",
      "Epoch 00001: val_loss improved from inf to 1.73793, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/001-1.7379.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 2.3061 - acc: 0.2391 - val_loss: 1.7379 - val_acc: 0.4482\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6669 - acc: 0.4488\n",
      "Epoch 00002: val_loss did not improve from 1.73793\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 1.6672 - acc: 0.4488 - val_loss: 2.1400 - val_acc: 0.2483\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3728 - acc: 0.5587\n",
      "Epoch 00003: val_loss did not improve from 1.73793\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 1.3730 - acc: 0.5587 - val_loss: 1.7801 - val_acc: 0.4561\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0571 - acc: 0.6676\n",
      "Epoch 00004: val_loss improved from 1.73793 to 0.83241, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/004-0.8324.hdf5\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 1.0570 - acc: 0.6676 - val_loss: 0.8324 - val_acc: 0.7454\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.7149\n",
      "Epoch 00005: val_loss did not improve from 0.83241\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.9170 - acc: 0.7149 - val_loss: 1.0093 - val_acc: 0.7088\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0890 - acc: 0.6576\n",
      "Epoch 00006: val_loss improved from 0.83241 to 0.74593, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/006-0.7459.hdf5\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 1.0889 - acc: 0.6576 - val_loss: 0.7459 - val_acc: 0.7873\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8152 - acc: 0.7491\n",
      "Epoch 00007: val_loss did not improve from 0.74593\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.8151 - acc: 0.7491 - val_loss: 0.7766 - val_acc: 0.7752\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7607 - acc: 0.7684\n",
      "Epoch 00008: val_loss did not improve from 0.74593\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.7607 - acc: 0.7683 - val_loss: 0.7513 - val_acc: 0.7782\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7604 - acc: 0.7664\n",
      "Epoch 00009: val_loss did not improve from 0.74593\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.7603 - acc: 0.7664 - val_loss: 0.9125 - val_acc: 0.7349\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7440 - acc: 0.7693\n",
      "Epoch 00010: val_loss improved from 0.74593 to 0.63997, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/010-0.6400.hdf5\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.7440 - acc: 0.7693 - val_loss: 0.6400 - val_acc: 0.8216\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6764 - acc: 0.7909\n",
      "Epoch 00011: val_loss improved from 0.63997 to 0.60518, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/011-0.6052.hdf5\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.6764 - acc: 0.7909 - val_loss: 0.6052 - val_acc: 0.8300\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.7994\n",
      "Epoch 00012: val_loss did not improve from 0.60518\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.6537 - acc: 0.7994 - val_loss: 0.6216 - val_acc: 0.8265\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.8056\n",
      "Epoch 00013: val_loss did not improve from 0.60518\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.6334 - acc: 0.8056 - val_loss: 0.7328 - val_acc: 0.7871\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6559 - acc: 0.7970\n",
      "Epoch 00014: val_loss did not improve from 0.60518\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.6558 - acc: 0.7971 - val_loss: 0.7142 - val_acc: 0.8067\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5987 - acc: 0.8145\n",
      "Epoch 00015: val_loss improved from 0.60518 to 0.57799, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/015-0.5780.hdf5\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.5987 - acc: 0.8145 - val_loss: 0.5780 - val_acc: 0.8367\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.8147\n",
      "Epoch 00016: val_loss did not improve from 0.57799\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.5958 - acc: 0.8147 - val_loss: 0.7970 - val_acc: 0.7699\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8224\n",
      "Epoch 00017: val_loss did not improve from 0.57799\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.5694 - acc: 0.8224 - val_loss: 0.7147 - val_acc: 0.7936\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8286\n",
      "Epoch 00018: val_loss did not improve from 0.57799\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.5584 - acc: 0.8286 - val_loss: 0.7444 - val_acc: 0.8034\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.8064\n",
      "Epoch 00019: val_loss did not improve from 0.57799\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.6191 - acc: 0.8063 - val_loss: 1.2274 - val_acc: 0.6140\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8122\n",
      "Epoch 00020: val_loss improved from 0.57799 to 0.52188, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/020-0.5219.hdf5\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.5991 - acc: 0.8123 - val_loss: 0.5219 - val_acc: 0.8553\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5367 - acc: 0.8354\n",
      "Epoch 00021: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.5368 - acc: 0.8353 - val_loss: 1.0722 - val_acc: 0.6727\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.8136\n",
      "Epoch 00022: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.5927 - acc: 0.8136 - val_loss: 0.6121 - val_acc: 0.8262\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.8365\n",
      "Epoch 00023: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.5247 - acc: 0.8365 - val_loss: 0.5261 - val_acc: 0.8584\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8349\n",
      "Epoch 00024: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.5271 - acc: 0.8349 - val_loss: 0.5682 - val_acc: 0.8605\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8366\n",
      "Epoch 00025: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.5153 - acc: 0.8366 - val_loss: 0.5611 - val_acc: 0.8397\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8458\n",
      "Epoch 00026: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.4969 - acc: 0.8458 - val_loss: 0.5587 - val_acc: 0.8544\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8452\n",
      "Epoch 00027: val_loss did not improve from 0.52188\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4882 - acc: 0.8452 - val_loss: 0.6605 - val_acc: 0.8358\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.8482\n",
      "Epoch 00028: val_loss improved from 0.52188 to 0.51137, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/028-0.5114.hdf5\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4794 - acc: 0.8482 - val_loss: 0.5114 - val_acc: 0.8626\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8476\n",
      "Epoch 00029: val_loss did not improve from 0.51137\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4829 - acc: 0.8475 - val_loss: 0.5644 - val_acc: 0.8418\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8487\n",
      "Epoch 00030: val_loss did not improve from 0.51137\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4784 - acc: 0.8487 - val_loss: 0.5407 - val_acc: 0.8595\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4604 - acc: 0.8543\n",
      "Epoch 00031: val_loss did not improve from 0.51137\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4603 - acc: 0.8543 - val_loss: 0.5115 - val_acc: 0.8658\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8537\n",
      "Epoch 00032: val_loss did not improve from 0.51137\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4572 - acc: 0.8537 - val_loss: 0.5367 - val_acc: 0.8535\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8554\n",
      "Epoch 00033: val_loss improved from 0.51137 to 0.47559, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/033-0.4756.hdf5\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4499 - acc: 0.8554 - val_loss: 0.4756 - val_acc: 0.8705\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8569\n",
      "Epoch 00034: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4489 - acc: 0.8569 - val_loss: 0.4930 - val_acc: 0.8712\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.8549\n",
      "Epoch 00035: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.4558 - acc: 0.8549 - val_loss: 0.6081 - val_acc: 0.8444\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8580\n",
      "Epoch 00036: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4473 - acc: 0.8580 - val_loss: 0.5289 - val_acc: 0.8640\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.8561\n",
      "Epoch 00037: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4514 - acc: 0.8561 - val_loss: 0.7034 - val_acc: 0.7983\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8592\n",
      "Epoch 00038: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4461 - acc: 0.8593 - val_loss: 0.5348 - val_acc: 0.8717\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8640\n",
      "Epoch 00039: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4281 - acc: 0.8640 - val_loss: 0.5150 - val_acc: 0.8633\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8640\n",
      "Epoch 00040: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4273 - acc: 0.8641 - val_loss: 0.5013 - val_acc: 0.8647\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8671\n",
      "Epoch 00041: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4175 - acc: 0.8671 - val_loss: 0.4948 - val_acc: 0.8717\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8663\n",
      "Epoch 00042: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4190 - acc: 0.8663 - val_loss: 0.6313 - val_acc: 0.8379\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8648\n",
      "Epoch 00043: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4197 - acc: 0.8648 - val_loss: 0.5472 - val_acc: 0.8637\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8655\n",
      "Epoch 00044: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4185 - acc: 0.8655 - val_loss: 0.5264 - val_acc: 0.8724\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8687\n",
      "Epoch 00045: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4045 - acc: 0.8688 - val_loss: 0.5052 - val_acc: 0.8691\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8690\n",
      "Epoch 00046: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4112 - acc: 0.8690 - val_loss: 0.4889 - val_acc: 0.8726\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8702\n",
      "Epoch 00047: val_loss did not improve from 0.47559\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4045 - acc: 0.8702 - val_loss: 0.4936 - val_acc: 0.8742\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8709\n",
      "Epoch 00048: val_loss improved from 0.47559 to 0.47403, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv_checkpoint/048-0.4740.hdf5\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4043 - acc: 0.8709 - val_loss: 0.4740 - val_acc: 0.8677\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8711\n",
      "Epoch 00049: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3983 - acc: 0.8710 - val_loss: 0.5002 - val_acc: 0.8579\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8714\n",
      "Epoch 00050: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.4028 - acc: 0.8713 - val_loss: 0.6553 - val_acc: 0.8199\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8611\n",
      "Epoch 00051: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.4314 - acc: 0.8612 - val_loss: 0.4781 - val_acc: 0.8710\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8709\n",
      "Epoch 00052: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4047 - acc: 0.8709 - val_loss: 0.5216 - val_acc: 0.8765\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8724\n",
      "Epoch 00053: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.4004 - acc: 0.8724 - val_loss: 0.5343 - val_acc: 0.8586\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8751\n",
      "Epoch 00054: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3942 - acc: 0.8752 - val_loss: 0.4781 - val_acc: 0.8737\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8767\n",
      "Epoch 00055: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3843 - acc: 0.8768 - val_loss: 0.5323 - val_acc: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8722\n",
      "Epoch 00056: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3982 - acc: 0.8722 - val_loss: 0.5947 - val_acc: 0.8404\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8715\n",
      "Epoch 00057: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.4046 - acc: 0.8715 - val_loss: 0.5293 - val_acc: 0.8551\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8723\n",
      "Epoch 00058: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3931 - acc: 0.8722 - val_loss: 0.5263 - val_acc: 0.8532\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8726\n",
      "Epoch 00059: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4020 - acc: 0.8726 - val_loss: 0.6722 - val_acc: 0.8283\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8738\n",
      "Epoch 00060: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3954 - acc: 0.8738 - val_loss: 0.5091 - val_acc: 0.8700\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8762\n",
      "Epoch 00061: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3845 - acc: 0.8762 - val_loss: 0.4988 - val_acc: 0.8682\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8777\n",
      "Epoch 00062: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3776 - acc: 0.8777 - val_loss: 0.5252 - val_acc: 0.8621\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8780\n",
      "Epoch 00063: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3798 - acc: 0.8780 - val_loss: 0.5159 - val_acc: 0.8682\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8819\n",
      "Epoch 00064: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3673 - acc: 0.8819 - val_loss: 0.4797 - val_acc: 0.8768\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8822\n",
      "Epoch 00065: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3663 - acc: 0.8822 - val_loss: 0.5139 - val_acc: 0.8735\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8795\n",
      "Epoch 00066: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3704 - acc: 0.8795 - val_loss: 0.5781 - val_acc: 0.8509\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8820\n",
      "Epoch 00067: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3677 - acc: 0.8820 - val_loss: 0.6058 - val_acc: 0.8444\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3782 - acc: 0.8781\n",
      "Epoch 00068: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3782 - acc: 0.8781 - val_loss: 0.4932 - val_acc: 0.8721\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8818\n",
      "Epoch 00069: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3711 - acc: 0.8818 - val_loss: 0.5498 - val_acc: 0.8605\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8813\n",
      "Epoch 00070: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3703 - acc: 0.8812 - val_loss: 0.5529 - val_acc: 0.8633\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8794\n",
      "Epoch 00071: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3729 - acc: 0.8794 - val_loss: 0.4767 - val_acc: 0.8728\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8798\n",
      "Epoch 00072: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 645us/sample - loss: 0.3672 - acc: 0.8798 - val_loss: 0.4937 - val_acc: 0.8803\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8813\n",
      "Epoch 00073: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3650 - acc: 0.8812 - val_loss: 0.6385 - val_acc: 0.8463\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8832\n",
      "Epoch 00074: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3648 - acc: 0.8832 - val_loss: 0.5250 - val_acc: 0.8754\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8842\n",
      "Epoch 00075: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3632 - acc: 0.8841 - val_loss: 0.6269 - val_acc: 0.8383\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8751\n",
      "Epoch 00076: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3863 - acc: 0.8750 - val_loss: 0.7008 - val_acc: 0.8400\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8537\n",
      "Epoch 00077: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.4531 - acc: 0.8537 - val_loss: 0.5211 - val_acc: 0.8654\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8776\n",
      "Epoch 00078: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3815 - acc: 0.8775 - val_loss: 0.4786 - val_acc: 0.8726\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8789\n",
      "Epoch 00079: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3783 - acc: 0.8789 - val_loss: 0.5112 - val_acc: 0.8705\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8779\n",
      "Epoch 00080: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3772 - acc: 0.8779 - val_loss: 0.5188 - val_acc: 0.8742\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8833\n",
      "Epoch 00081: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.3600 - acc: 0.8833 - val_loss: 0.6416 - val_acc: 0.8430\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8823\n",
      "Epoch 00082: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3654 - acc: 0.8823 - val_loss: 0.4996 - val_acc: 0.8707\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8825\n",
      "Epoch 00083: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3614 - acc: 0.8825 - val_loss: 0.4748 - val_acc: 0.8870\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8859\n",
      "Epoch 00084: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3581 - acc: 0.8859 - val_loss: 0.5385 - val_acc: 0.8651\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8838\n",
      "Epoch 00085: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3639 - acc: 0.8838 - val_loss: 0.4757 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8877\n",
      "Epoch 00086: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 646us/sample - loss: 0.3517 - acc: 0.8877 - val_loss: 0.5060 - val_acc: 0.8691\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8852\n",
      "Epoch 00087: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3525 - acc: 0.8852 - val_loss: 0.5559 - val_acc: 0.8721\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8871\n",
      "Epoch 00088: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.3476 - acc: 0.8871 - val_loss: 0.7151 - val_acc: 0.8132\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8879\n",
      "Epoch 00089: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3442 - acc: 0.8879 - val_loss: 0.5653 - val_acc: 0.8516\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8835\n",
      "Epoch 00090: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3591 - acc: 0.8834 - val_loss: 0.4824 - val_acc: 0.8721\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8869\n",
      "Epoch 00091: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3470 - acc: 0.8869 - val_loss: 0.5021 - val_acc: 0.8807\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8899\n",
      "Epoch 00092: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.3420 - acc: 0.8899 - val_loss: 0.4756 - val_acc: 0.8840\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8904\n",
      "Epoch 00093: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3369 - acc: 0.8904 - val_loss: 0.5231 - val_acc: 0.8761\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8905\n",
      "Epoch 00094: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 641us/sample - loss: 0.3431 - acc: 0.8905 - val_loss: 0.5013 - val_acc: 0.8630\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8858\n",
      "Epoch 00095: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 642us/sample - loss: 0.3524 - acc: 0.8858 - val_loss: 0.5208 - val_acc: 0.8705\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8893\n",
      "Epoch 00096: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3426 - acc: 0.8893 - val_loss: 0.4789 - val_acc: 0.8800\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8907\n",
      "Epoch 00097: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 643us/sample - loss: 0.3384 - acc: 0.8907 - val_loss: 0.4910 - val_acc: 0.8798\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8922\n",
      "Epoch 00098: val_loss did not improve from 0.47403\n",
      "36805/36805 [==============================] - 24s 644us/sample - loss: 0.3361 - acc: 0.8922 - val_loss: 0.5328 - val_acc: 0.8782\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 357us/sample - loss: 0.5932 - acc: 0.8395\n",
      "Loss: 0.5932148873125157 Accuracy: 0.83946\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5915 - acc: 0.1421\n",
      "Epoch 00001: val_loss improved from inf to 1.88919, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/001-1.8892.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 2.5914 - acc: 0.1422 - val_loss: 1.8892 - val_acc: 0.3986\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8192 - acc: 0.3962\n",
      "Epoch 00002: val_loss improved from 1.88919 to 1.84569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/002-1.8457.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.8191 - acc: 0.3962 - val_loss: 1.8457 - val_acc: 0.4167\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5360 - acc: 0.4933\n",
      "Epoch 00003: val_loss improved from 1.84569 to 1.75358, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/003-1.7536.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.5361 - acc: 0.4934 - val_loss: 1.7536 - val_acc: 0.4023\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2493 - acc: 0.6023\n",
      "Epoch 00004: val_loss improved from 1.75358 to 1.10506, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/004-1.1051.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.2494 - acc: 0.6023 - val_loss: 1.1051 - val_acc: 0.6660\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0785 - acc: 0.6629\n",
      "Epoch 00005: val_loss did not improve from 1.10506\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.0785 - acc: 0.6629 - val_loss: 1.1790 - val_acc: 0.6529\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8580 - acc: 0.7395\n",
      "Epoch 00006: val_loss improved from 1.10506 to 0.72652, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/006-0.7265.hdf5\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.8579 - acc: 0.7395 - val_loss: 0.7265 - val_acc: 0.7936\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8120 - acc: 0.7547\n",
      "Epoch 00007: val_loss improved from 0.72652 to 0.52809, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/007-0.5281.hdf5\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.8119 - acc: 0.7547 - val_loss: 0.5281 - val_acc: 0.8605\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7385 - acc: 0.7760\n",
      "Epoch 00008: val_loss did not improve from 0.52809\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.7385 - acc: 0.7760 - val_loss: 0.8138 - val_acc: 0.7796\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7312 - acc: 0.7778\n",
      "Epoch 00009: val_loss did not improve from 0.52809\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.7312 - acc: 0.7779 - val_loss: 0.5397 - val_acc: 0.8553\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6343 - acc: 0.8070\n",
      "Epoch 00010: val_loss did not improve from 0.52809\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.6343 - acc: 0.8070 - val_loss: 1.0539 - val_acc: 0.6911\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6524 - acc: 0.8040\n",
      "Epoch 00011: val_loss improved from 0.52809 to 0.43529, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/011-0.4353.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6524 - acc: 0.8040 - val_loss: 0.4353 - val_acc: 0.8887\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8196\n",
      "Epoch 00012: val_loss did not improve from 0.43529\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.5972 - acc: 0.8196 - val_loss: 0.4458 - val_acc: 0.8730\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8315\n",
      "Epoch 00013: val_loss improved from 0.43529 to 0.42330, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/013-0.4233.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5628 - acc: 0.8315 - val_loss: 0.4233 - val_acc: 0.8861\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8353\n",
      "Epoch 00014: val_loss did not improve from 0.42330\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5379 - acc: 0.8353 - val_loss: 0.4489 - val_acc: 0.8654\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.8399\n",
      "Epoch 00015: val_loss improved from 0.42330 to 0.39900, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/015-0.3990.hdf5\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.5289 - acc: 0.8399 - val_loss: 0.3990 - val_acc: 0.8873\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8473\n",
      "Epoch 00016: val_loss did not improve from 0.39900\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.5064 - acc: 0.8473 - val_loss: 0.4597 - val_acc: 0.8754\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.8493\n",
      "Epoch 00017: val_loss improved from 0.39900 to 0.35794, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/017-0.3579.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4944 - acc: 0.8493 - val_loss: 0.3579 - val_acc: 0.9012\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8534\n",
      "Epoch 00018: val_loss did not improve from 0.35794\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.4841 - acc: 0.8534 - val_loss: 0.3715 - val_acc: 0.9017\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8582\n",
      "Epoch 00019: val_loss did not improve from 0.35794\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.4621 - acc: 0.8582 - val_loss: 0.5313 - val_acc: 0.8537\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8531\n",
      "Epoch 00020: val_loss did not improve from 0.35794\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4764 - acc: 0.8531 - val_loss: 0.5147 - val_acc: 0.8588\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8622\n",
      "Epoch 00021: val_loss did not improve from 0.35794\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4534 - acc: 0.8622 - val_loss: 0.4122 - val_acc: 0.8945\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8614\n",
      "Epoch 00022: val_loss did not improve from 0.35794\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.4464 - acc: 0.8614 - val_loss: 0.3601 - val_acc: 0.8921\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8663\n",
      "Epoch 00023: val_loss improved from 0.35794 to 0.33844, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/023-0.3384.hdf5\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.4369 - acc: 0.8663 - val_loss: 0.3384 - val_acc: 0.9073\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8682\n",
      "Epoch 00024: val_loss improved from 0.33844 to 0.32155, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/024-0.3215.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.4337 - acc: 0.8681 - val_loss: 0.3215 - val_acc: 0.9115\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8711\n",
      "Epoch 00025: val_loss improved from 0.32155 to 0.31315, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/025-0.3132.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4181 - acc: 0.8711 - val_loss: 0.3132 - val_acc: 0.9143\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8715\n",
      "Epoch 00026: val_loss did not improve from 0.31315\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.4171 - acc: 0.8715 - val_loss: 0.3986 - val_acc: 0.8938\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8734\n",
      "Epoch 00027: val_loss did not improve from 0.31315\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.4116 - acc: 0.8734 - val_loss: 0.4206 - val_acc: 0.8852\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8748\n",
      "Epoch 00028: val_loss did not improve from 0.31315\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.4092 - acc: 0.8747 - val_loss: 0.3311 - val_acc: 0.9073\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8730\n",
      "Epoch 00029: val_loss improved from 0.31315 to 0.31033, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/029-0.3103.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4153 - acc: 0.8730 - val_loss: 0.3103 - val_acc: 0.9143\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8758\n",
      "Epoch 00030: val_loss did not improve from 0.31033\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3931 - acc: 0.8758 - val_loss: 0.3838 - val_acc: 0.9071\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8731\n",
      "Epoch 00031: val_loss improved from 0.31033 to 0.29154, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/031-0.2915.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4097 - acc: 0.8730 - val_loss: 0.2915 - val_acc: 0.9168\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8829\n",
      "Epoch 00032: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3797 - acc: 0.8828 - val_loss: 0.3544 - val_acc: 0.9082\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8795\n",
      "Epoch 00033: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3898 - acc: 0.8796 - val_loss: 0.3377 - val_acc: 0.9082\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8836\n",
      "Epoch 00034: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3713 - acc: 0.8836 - val_loss: 0.3491 - val_acc: 0.9143\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8835\n",
      "Epoch 00035: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3729 - acc: 0.8835 - val_loss: 0.3058 - val_acc: 0.9129\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8858\n",
      "Epoch 00036: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3729 - acc: 0.8859 - val_loss: 0.3253 - val_acc: 0.9096\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8858\n",
      "Epoch 00037: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3624 - acc: 0.8858 - val_loss: 0.3006 - val_acc: 0.9164\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8890\n",
      "Epoch 00038: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3545 - acc: 0.8891 - val_loss: 0.3135 - val_acc: 0.9161\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8880\n",
      "Epoch 00039: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3612 - acc: 0.8880 - val_loss: 0.2975 - val_acc: 0.9157\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8910\n",
      "Epoch 00040: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3498 - acc: 0.8910 - val_loss: 0.3194 - val_acc: 0.9106\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8908\n",
      "Epoch 00041: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3530 - acc: 0.8909 - val_loss: 0.3128 - val_acc: 0.9182\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8929\n",
      "Epoch 00042: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3470 - acc: 0.8929 - val_loss: 0.3035 - val_acc: 0.9196\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8926\n",
      "Epoch 00043: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3422 - acc: 0.8926 - val_loss: 0.3331 - val_acc: 0.9157\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8921\n",
      "Epoch 00044: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3470 - acc: 0.8921 - val_loss: 0.7930 - val_acc: 0.8013\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8892\n",
      "Epoch 00045: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3550 - acc: 0.8892 - val_loss: 0.3660 - val_acc: 0.8991\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8938\n",
      "Epoch 00046: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3424 - acc: 0.8937 - val_loss: 0.4038 - val_acc: 0.8910\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8895\n",
      "Epoch 00047: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.3480 - acc: 0.8896 - val_loss: 0.2937 - val_acc: 0.9222\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8976\n",
      "Epoch 00048: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3289 - acc: 0.8976 - val_loss: 0.3044 - val_acc: 0.9276\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8932\n",
      "Epoch 00049: val_loss did not improve from 0.29154\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3356 - acc: 0.8932 - val_loss: 0.2970 - val_acc: 0.9203\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8950\n",
      "Epoch 00050: val_loss improved from 0.29154 to 0.28671, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/050-0.2867.hdf5\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3284 - acc: 0.8950 - val_loss: 0.2867 - val_acc: 0.9252\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8941\n",
      "Epoch 00051: val_loss did not improve from 0.28671\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3325 - acc: 0.8941 - val_loss: 0.3535 - val_acc: 0.8961\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8955\n",
      "Epoch 00052: val_loss did not improve from 0.28671\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3308 - acc: 0.8955 - val_loss: 0.3232 - val_acc: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8967\n",
      "Epoch 00053: val_loss improved from 0.28671 to 0.28265, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/053-0.2827.hdf5\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3256 - acc: 0.8966 - val_loss: 0.2827 - val_acc: 0.9227\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8975\n",
      "Epoch 00054: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3312 - acc: 0.8975 - val_loss: 0.3135 - val_acc: 0.9103\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8963\n",
      "Epoch 00055: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3288 - acc: 0.8963 - val_loss: 0.2975 - val_acc: 0.9217\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8985\n",
      "Epoch 00056: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3177 - acc: 0.8985 - val_loss: 0.4430 - val_acc: 0.8717\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8846\n",
      "Epoch 00057: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3647 - acc: 0.8846 - val_loss: 0.2836 - val_acc: 0.9229\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8974\n",
      "Epoch 00058: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3291 - acc: 0.8975 - val_loss: 0.2974 - val_acc: 0.9196\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8992\n",
      "Epoch 00059: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3195 - acc: 0.8992 - val_loss: 0.3028 - val_acc: 0.9178\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.9007\n",
      "Epoch 00060: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3167 - acc: 0.9006 - val_loss: 0.3561 - val_acc: 0.9052\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8984\n",
      "Epoch 00061: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3211 - acc: 0.8984 - val_loss: 0.2991 - val_acc: 0.9294\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8980\n",
      "Epoch 00062: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3221 - acc: 0.8980 - val_loss: 0.2969 - val_acc: 0.9259\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9023\n",
      "Epoch 00063: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3104 - acc: 0.9023 - val_loss: 0.3019 - val_acc: 0.9264\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9023\n",
      "Epoch 00064: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3098 - acc: 0.9023 - val_loss: 0.3540 - val_acc: 0.9178\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9010\n",
      "Epoch 00065: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3137 - acc: 0.9009 - val_loss: 0.2950 - val_acc: 0.9250\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8973\n",
      "Epoch 00066: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3240 - acc: 0.8974 - val_loss: 0.2886 - val_acc: 0.9231\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9031\n",
      "Epoch 00067: val_loss did not improve from 0.28265\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3036 - acc: 0.9031 - val_loss: 0.3066 - val_acc: 0.9206\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9032\n",
      "Epoch 00068: val_loss improved from 0.28265 to 0.27887, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/068-0.2789.hdf5\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3111 - acc: 0.9032 - val_loss: 0.2789 - val_acc: 0.9294\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9040\n",
      "Epoch 00069: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3050 - acc: 0.9040 - val_loss: 0.4606 - val_acc: 0.8798\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9008\n",
      "Epoch 00070: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3074 - acc: 0.9008 - val_loss: 0.4016 - val_acc: 0.8982\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9001\n",
      "Epoch 00071: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3146 - acc: 0.9001 - val_loss: 0.3848 - val_acc: 0.9050\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9045\n",
      "Epoch 00072: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3011 - acc: 0.9045 - val_loss: 0.3034 - val_acc: 0.9241\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9066\n",
      "Epoch 00073: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2962 - acc: 0.9066 - val_loss: 0.3016 - val_acc: 0.9213\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9049\n",
      "Epoch 00074: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2958 - acc: 0.9048 - val_loss: 0.3339 - val_acc: 0.9089\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9014\n",
      "Epoch 00075: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3085 - acc: 0.9014 - val_loss: 0.2944 - val_acc: 0.9220\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9076\n",
      "Epoch 00076: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2945 - acc: 0.9076 - val_loss: 0.3134 - val_acc: 0.9252\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9036\n",
      "Epoch 00077: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.3011 - acc: 0.9036 - val_loss: 0.3377 - val_acc: 0.9161\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9021\n",
      "Epoch 00078: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3099 - acc: 0.9021 - val_loss: 0.3006 - val_acc: 0.9208\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9041\n",
      "Epoch 00079: val_loss did not improve from 0.27887\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3037 - acc: 0.9041 - val_loss: 0.2867 - val_acc: 0.9292\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9068\n",
      "Epoch 00080: val_loss improved from 0.27887 to 0.27018, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/080-0.2702.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2959 - acc: 0.9068 - val_loss: 0.2702 - val_acc: 0.9287\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9074\n",
      "Epoch 00081: val_loss did not improve from 0.27018\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2915 - acc: 0.9074 - val_loss: 0.3164 - val_acc: 0.9213\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9071\n",
      "Epoch 00082: val_loss did not improve from 0.27018\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2925 - acc: 0.9071 - val_loss: 0.2912 - val_acc: 0.9220\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9061\n",
      "Epoch 00083: val_loss did not improve from 0.27018\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2935 - acc: 0.9061 - val_loss: 0.2956 - val_acc: 0.9266\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9052\n",
      "Epoch 00084: val_loss did not improve from 0.27018\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2936 - acc: 0.9052 - val_loss: 0.2821 - val_acc: 0.9227\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9059\n",
      "Epoch 00085: val_loss did not improve from 0.27018\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3005 - acc: 0.9059 - val_loss: 0.2882 - val_acc: 0.9192\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9065\n",
      "Epoch 00086: val_loss improved from 0.27018 to 0.25900, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv_checkpoint/086-0.2590.hdf5\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2894 - acc: 0.9065 - val_loss: 0.2590 - val_acc: 0.9273\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9081\n",
      "Epoch 00087: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2934 - acc: 0.9081 - val_loss: 0.2904 - val_acc: 0.9248\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9065\n",
      "Epoch 00088: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2955 - acc: 0.9065 - val_loss: 0.3712 - val_acc: 0.9082\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9016\n",
      "Epoch 00089: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.3129 - acc: 0.9016 - val_loss: 0.3506 - val_acc: 0.9152\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9080\n",
      "Epoch 00090: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2953 - acc: 0.9080 - val_loss: 0.2837 - val_acc: 0.9280\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9078\n",
      "Epoch 00091: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2900 - acc: 0.9078 - val_loss: 0.3061 - val_acc: 0.9213\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9093\n",
      "Epoch 00092: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2880 - acc: 0.9093 - val_loss: 0.3142 - val_acc: 0.9238\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9076\n",
      "Epoch 00093: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2908 - acc: 0.9076 - val_loss: 0.3163 - val_acc: 0.9215\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9090\n",
      "Epoch 00094: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2880 - acc: 0.9090 - val_loss: 0.3151 - val_acc: 0.9206\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9082\n",
      "Epoch 00095: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2916 - acc: 0.9082 - val_loss: 0.2842 - val_acc: 0.9248\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9098\n",
      "Epoch 00096: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2831 - acc: 0.9098 - val_loss: 0.2973 - val_acc: 0.9241\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9095\n",
      "Epoch 00097: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2832 - acc: 0.9095 - val_loss: 0.2761 - val_acc: 0.9262\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9114\n",
      "Epoch 00098: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2820 - acc: 0.9114 - val_loss: 0.2825 - val_acc: 0.9336\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9105\n",
      "Epoch 00099: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2789 - acc: 0.9104 - val_loss: 0.2946 - val_acc: 0.9201\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9112\n",
      "Epoch 00100: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2811 - acc: 0.9112 - val_loss: 0.3174 - val_acc: 0.9150\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9088\n",
      "Epoch 00101: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2878 - acc: 0.9088 - val_loss: 0.2756 - val_acc: 0.9257\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9097\n",
      "Epoch 00102: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 653us/sample - loss: 0.2874 - acc: 0.9097 - val_loss: 0.2894 - val_acc: 0.9278\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9082\n",
      "Epoch 00103: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2876 - acc: 0.9082 - val_loss: 0.3111 - val_acc: 0.9189\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9101\n",
      "Epoch 00104: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2791 - acc: 0.9101 - val_loss: 0.2963 - val_acc: 0.9248\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9086\n",
      "Epoch 00105: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2860 - acc: 0.9085 - val_loss: 0.3766 - val_acc: 0.9015\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9040\n",
      "Epoch 00106: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2989 - acc: 0.9040 - val_loss: 0.3408 - val_acc: 0.9231\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9089\n",
      "Epoch 00107: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2910 - acc: 0.9089 - val_loss: 0.3076 - val_acc: 0.9257\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9083\n",
      "Epoch 00108: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2844 - acc: 0.9083 - val_loss: 0.3429 - val_acc: 0.9164\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9080\n",
      "Epoch 00109: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2912 - acc: 0.9080 - val_loss: 0.2853 - val_acc: 0.9248\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9066\n",
      "Epoch 00110: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2898 - acc: 0.9065 - val_loss: 0.2948 - val_acc: 0.9245\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9093\n",
      "Epoch 00111: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2873 - acc: 0.9093 - val_loss: 0.3137 - val_acc: 0.9201\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9109\n",
      "Epoch 00112: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2801 - acc: 0.9109 - val_loss: 0.2921 - val_acc: 0.9229\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9059\n",
      "Epoch 00113: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2949 - acc: 0.9059 - val_loss: 0.2900 - val_acc: 0.9231\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9133\n",
      "Epoch 00114: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2707 - acc: 0.9133 - val_loss: 0.2947 - val_acc: 0.9278\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9076\n",
      "Epoch 00115: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2873 - acc: 0.9076 - val_loss: 0.2980 - val_acc: 0.9231\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9111\n",
      "Epoch 00116: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2816 - acc: 0.9110 - val_loss: 0.3130 - val_acc: 0.9224\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9107\n",
      "Epoch 00117: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2811 - acc: 0.9107 - val_loss: 0.2879 - val_acc: 0.9278\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9120\n",
      "Epoch 00118: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2720 - acc: 0.9120 - val_loss: 0.3233 - val_acc: 0.9122\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9082\n",
      "Epoch 00119: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2901 - acc: 0.9082 - val_loss: 0.2761 - val_acc: 0.9301\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9104\n",
      "Epoch 00120: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.2794 - acc: 0.9104 - val_loss: 0.3064 - val_acc: 0.9306\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9089\n",
      "Epoch 00121: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2838 - acc: 0.9089 - val_loss: 0.3282 - val_acc: 0.9185\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9110\n",
      "Epoch 00122: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 647us/sample - loss: 0.2779 - acc: 0.9110 - val_loss: 0.2899 - val_acc: 0.9257\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9129\n",
      "Epoch 00123: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2720 - acc: 0.9129 - val_loss: 0.3276 - val_acc: 0.9266\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9133\n",
      "Epoch 00124: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2689 - acc: 0.9134 - val_loss: 0.2908 - val_acc: 0.9259\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9143\n",
      "Epoch 00125: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2720 - acc: 0.9143 - val_loss: 0.2983 - val_acc: 0.9224\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9126\n",
      "Epoch 00126: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.2724 - acc: 0.9126 - val_loss: 0.2930 - val_acc: 0.9280\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9128\n",
      "Epoch 00127: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 648us/sample - loss: 0.2718 - acc: 0.9128 - val_loss: 0.2977 - val_acc: 0.9187\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9139\n",
      "Epoch 00128: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2695 - acc: 0.9139 - val_loss: 0.3022 - val_acc: 0.9220\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9119\n",
      "Epoch 00129: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2741 - acc: 0.9118 - val_loss: 0.3214 - val_acc: 0.9236\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9055\n",
      "Epoch 00130: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2961 - acc: 0.9056 - val_loss: 0.3196 - val_acc: 0.9243\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9133\n",
      "Epoch 00131: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 649us/sample - loss: 0.2742 - acc: 0.9134 - val_loss: 0.2949 - val_acc: 0.9276\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9118\n",
      "Epoch 00132: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2752 - acc: 0.9118 - val_loss: 0.3135 - val_acc: 0.9236\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9125\n",
      "Epoch 00133: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2757 - acc: 0.9125 - val_loss: 0.3259 - val_acc: 0.9164\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9111\n",
      "Epoch 00134: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2770 - acc: 0.9111 - val_loss: 0.3349 - val_acc: 0.9182\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9121\n",
      "Epoch 00135: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2726 - acc: 0.9121 - val_loss: 0.2942 - val_acc: 0.9262\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9122\n",
      "Epoch 00136: val_loss did not improve from 0.25900\n",
      "36805/36805 [==============================] - 24s 650us/sample - loss: 0.2780 - acc: 0.9122 - val_loss: 0.2997 - val_acc: 0.9299\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8FVX6+PHPuclNryQhlAQSeidAQBQBK6IoFhbRta9119X1q+vqV93VXddddXXXtf/U1cWKfO0KdoOggPQqJZQAKZDe2y3P74+ThBZCgFwSkuf9et3XLXPuzDOT3HnmnJk5x4gISiml1L4crR2AUkqptkeTg1JKqYNoclBKKXUQTQ5KKaUOoslBKaXUQTQ5KKWUOogmB6WUUgfR5KCUUuogmhyUUkodxL+1AzhSsbGxkpSU1NphKKXUCWX58uX5IhLX3PInXHJISkpi2bJlrR2GUkqdUIwxO46kvDYrKaWUOogmB6WUUgfR5KCUUuogJ9w5h8a4XC4yMzOprq5u7VBOWEFBQSQkJOB0Ols7FKVUG9AukkNmZibh4eEkJSVhjGntcE44IkJBQQGZmZkkJye3djhKqTagXTQrVVdXExMTo4nhKBljiImJ0ZqXUqpBu0gOgCaGY6TbTym1r3aTHA7H46mipiYLr9fV2qEopVSb12GSg9dbRW1tDiItnxyKi4t5/vnnj+q75513HsXFxc0u/9BDD/HEE08c1bKUUqq5OkxyMKZ+Vb0tPu+mkoPb7W7yu3PnziUqKqrFY1JKqWPRYZJD/aqKSIvP+d5772Xr1q2kpKRw9913M2/ePMaPH8/UqVMZNGgQABdddBGjRo1i8ODBvPTSSw3fTUpKIj8/n4yMDAYOHMiNN97I4MGDmTRpElVVVU0ud9WqVYwdO5Zhw4Zx8cUXU1RUBMDTTz/NoEGDGDZsGJdddhkA33//PSkpKaSkpDBixAjKyspafDsopdqPdnEp677S0++gvHzVQZ+LePB6K3E4gjHmyFY7LCyFvn2fOuT0Rx99lHXr1rFqlV3uvHnzWLFiBevWrWu4NPTVV1+lU6dOVFVVMXr0aKZNm0ZMTMwBsafzzjvv8PLLL3PppZfy/vvvc+WVVx5yuVdffTXPPPMMEydO5E9/+hN//vOfeeqpp3j00UfZvn07gYGBDU1WTzzxBM899xzjxo2jvLycoKCgI9oGSqmOxWc1B2NMojEmzRjzszFmvTHmd42UOc0YU2KMWVX3+JMP4/HVrBs1ZsyY/e4ZePrppxk+fDhjx45l165dpKenH/Sd5ORkUlJSABg1ahQZGRmHnH9JSQnFxcVMnDgRgGuuuYb58+cDMGzYMK644grefPNN/P1tIhw3bhx33nknTz/9NMXFxQ2fK6VUY3y5h3ADd4nICmNMOLDcGPO1iPx8QLkFInJ+Sy30UEf4Hk81lZXrCApKxumMabRMSwoNDW14PW/ePL755hsWLVpESEgIp512WqP3FAQGBja89vPzO2yz0qHMmTOH+fPn8+mnn/LII4+wdu1a7r33XqZMmcLcuXMZN24cX375JQMGDDiq+Sul2j+f1RxEJEdEVtS9LgM2AN19tbzDqT8hLdLyJ6TDw8ObbMMvKSkhOjqakJAQNm7cyOLFi495mZGRkURHR7NgwQIA3njjDSZOnIjX62XXrl2cfvrpPPbYY5SUlFBeXs7WrVsZOnQo99xzD6NHj2bjxo3HHINSqv06Lm0LxpgkYATwUyOTTzbGrAaygd+LyHrfROG7q5ViYmIYN24cQ4YM4dxzz2XKlCn7TZ88eTIvvvgiAwcOpH///owdO7ZFljtz5kxuueUWKisr6dWrF6+99hoej4crr7ySkpISRITbb7+dqKgo/vjHP5KWlobD4WDw4MGce+65LRKDUqp9Mr64eme/BRgTBnwPPCIiHxwwLQLwiki5MeY84N8i0reRedwE3ATQo0ePUTt27D9mxYYNGxg4cGCTcYh4KC9fSUBAdwIDux7TOrVXzdmOSqkTkzFmuYikNre8Ty9lNcY4gfeBtw5MDAAiUioi5XWv5wJOY0xsI+VeEpFUEUmNi2v2KHcH8F3NQSml2htfXq1kgP8AG0Tkn4co06WuHMaYMXXxFPgoHsD45D4HpZRqb3x5zmEccBWw1hhTf+PBfUAPABF5EfgF8GtjjBuoAi4Tn+69HWjNQSmlDs9nyUFEfgCavLlARJ4FnvVVDAcyxuGTq5WUUqq96UDdZ4DWHJRSqnk6VHKw9zpoclBKqcPpUMkB2k6zUlhY2BF9rpRSx1OHSg5ac1BKqebpUMnBVzWHe++9l+eee67hff2APOXl5Zx55pmMHDmSoUOH8vHHHzd7niLC3XffzZAhQxg6dCjvvvsuADk5OUyYMIGUlBSGDBnCggUL8Hg8XHvttQ1l//Wvf7X4OiqlOpb21zXnHXfAqoO77AYI9FaBeMEvtNHph5SSAk8dusvuGTNmcMcdd3DrrbcCMHv2bL788kuCgoL48MMPiYiIID8/n7FjxzJ16tRm9RD7wQcfsGrVKlavXk1+fj6jR49mwoQJvP3225xzzjncf//9eDweKisrWbVqFVlZWaxbtw7giEaWU0qpxrS/5NAk33TbPWLECHJzc8nOziYvL4/o6GgSExNxuVzcd999zJ8/H4fDQVZWFnv27KFLly6HnecPP/zA5Zdfjp+fH/Hx8UycOJGlS5cyevRofvWrX+FyubjoootISUmhV69ebNu2jdtuu40pU6YwadIkn6ynUqrjaH/JoYkjfFd1Bm53CWFhw1t8sdOnT+e9995j9+7dzJgxA4C33nqLvLw8li9fjtPpJCkpqdGuuo/EhAkTmD9/PnPmzOHaa6/lzjvv5Oqrr2b16tV8+eWXvPjii8yePZtXX321JVZLKdVB6TmHFjJjxgxmzZrFe++9x/Tp0wHbVXfnzp1xOp2kpaVxYIeBTRk/fjzvvvsuHo+HvLw85s+fz5gxY9ixYwfx8fHceOON3HDDDaxYsYL8/Hy8Xi/Tpk3jr3/9KytWrPDJOiqlOo72V3Noku+uVho8eDBlZWV0796drl1tr69XXHEFF1xwAUOHDiU1NfWIBte5+OKLWbRoEcOHD8cYw+OPP06XLl2YOXMm//jHP3A6nYSFhfH666+TlZXFddddh9dr1+3vf/+7T9ZRKdVx+LzL7paWmpoqy5Yt2++z5nY1XVOTTW1tNmFho477sKEnAu2yW6n2q0112d32aLfdSinVHB0qOfhyqFCllGpPOlRy2HspqyYHpZRqSodKDlpzUEqp5ulQyWHv6p5YJ+GVUup461DJQWsOSinVPB0qOfjqaqXi4mKef/75o/rueeedp30hKaXanA6VHHxVc2gqObjd7ia/O3fuXKKiolo0HqWUOlYdKjn4quZw7733snXrVlJSUrj77ruZN28e48ePZ+rUqQwaNAiAiy66iFGjRjF48GBeeumlhu8mJSWRn59PRkYGAwcO5MYbb2Tw4MFMmjSJqqqqg5b16aefctJJJzFixAjOOuss9uzZA0B5eTnXXXcdQ4cOZdiwYbz//vsAfPHFF4wcOZLhw4dz5plntuh6K6Xar3bXfUYTPXYDgXg8/XE4gjiSG6QP02M3jz76KOvWrWNV3YLnzZvHihUrWLduHcnJyQC8+uqrdOrUiaqqKkaPHs20adOIiYnZbz7p6em88847vPzyy1x66aW8//77XHnllfuVOfXUU1m8eDHGGF555RUef/xxnnzySR5++GEiIyNZu3YtAEVFReTl5XHjjTcyf/58kpOTKSwsbP5KK6U6tHaXHJrH91crjRkzpiExADz99NN8+OGHAOzatYv09PSDkkNycjIpKSkAjBo1ioyMjIPmm5mZyYwZM8jJyaG2trZhGd988w2zZs1qKBcdHc2nn37KhAkTGsp06tSpRddRKdV+tbvk0NQRvoiX8vJNBAYmEBBw+DEVjkVo6N4BhebNm8c333zDokWLCAkJ4bTTTmu06+7AwMCG135+fo02K912223ceeedTJ06lXnz5vHQQw/5JH6lVMfWIc85tHRng+Hh4ZSVlR1yeklJCdHR0YSEhLBx40YWL1581MsqKSmhe/fuAMycObPh87PPPnu/oUqLiooYO3Ys8+fPZ/v27QDarKSUarYOlhx8031GTEwM48aNY8iQIdx9990HTZ88eTJut5uBAwdy7733Mnbs2KNe1kMPPcT06dMZNWoUsbGxDZ8/8MADFBUVMWTIEIYPH05aWhpxcXG89NJLXHLJJQwfPrxhECKllDqcDtVlN0BZ2QqczjiCghJ9Ed4JTbvsVqr90i67D8Pe66B3SCulVFM6XHLw5VChSinVXnTI5KA1B6WUalqHSw7GaM1BKaUOp8MlB605KKXU4fksORhjEo0xacaYn40x640xv2ukjDHGPG2M2WKMWWOMGemrePZZZovf56CUUu2NL2sObuAuERkEjAVuNcYMOqDMuUDfusdNwAs+jKdO26g5hIWFtXYISil1SD5LDiKSIyIr6l6XARuA7gcUuxB4XazFQJQxpquvYgK9lFUppZrjuJxzMMYkASOAnw6Y1B3Ytc/7TA5OIC2s5U9I33vvvft1XfHQQw/xxBNPUF5ezplnnsnIkSMZOnQoH3/88WHndaiuvRvrevtQ3XQrpdSx8nnHe8aYMOB94A4RKT3KedyEbXaiR48eTZa944s7WLX7kH124/VWI+LGz6/5zTopXVJ4avKhe/SbMWMGd9xxB7feeisAs2fP5ssvvyQoKIgPP/yQiIgI8vPzGTt2LFOnTsU00V94Y117e73eRrvebqybbqWUagk+TQ7GGCc2MbwlIh80UiQL2Lcfi4S6z/YjIi8BL4HtPuMYozq2rzdixIgR5Obmkp2dTV5eHtHR0SQmJuJyubjvvvuYP38+DoeDrKws9uzZQ5cuh+4RtrGuvfPy8hrteruxbrqVUqol+Cw5GHt4/B9gg4j88xDFPgF+a4yZBZwElIhIzrEst6kjfICamixqa3MICxvV5BH8kZo+fTrvvfceu3fvbujg7q233iIvL4/ly5fjdDpJSkpqtKvues3t2lsppXzNl+ccxgFXAWcYY1bVPc4zxtxijLmlrsxcYBuwBXgZ+I0P46lTv8oteznrjBkzmDVrFu+99x7Tp08HbPfanTt3xul0kpaWxo4dO5qcx6G69j5U19uNddOtlFItwWc1BxH5gcO04Yi94eBWX8XQmL21BS8tmRsHDx5MWVkZ3bt3p2tXe8HVFVdcwQUXXMDQoUNJTU1lwIABTc5j8uTJvPjiiwwcOJD+/fs3dO29b9fbXq+Xzp078/XXX/PAAw9w6623MmTIEPz8/HjwwQe55JJLWmydlFIdV4frsru2Npeamp2Ehg7H4XD6IsQTlnbZrVT7pV12H4a9zwH0XgellDq0jpUcvHubkrTzPaWUOrR2kxwO2zxWUgLr1mFc9UlBk8O+TrTmRaWUb7WL5BAUFERBQUHTO7jAQHC5cGTlg2jNYV8iQkFBAUFBQa0dilKqjfD5HdLHQ0JCApmZmeTl5TVd0OWCrRnU5oN/hAOHI/j4BHgCCAoKIiEhobXDUEq1Ee0iOTidzoa7h5vkduNJHYp710bKFr9BbN8rfR+cUkqdgNpFs1Kz+ftT+/wjBBZC4GuftnY0SinVZnWs5ACYkamUDoDAL5cdvrBSSnVQHS45OBwhFJwCzpXbYffu1g5HKaXapA6XHJzOTuSf7MCIwJw5rR2OUkq1SR0uORjjwDUwHlfXUPjkk9YORyml2qQOlxwAAgK7UTIxBr7+GqqqWjscpZRqczpkcggM7ErBOH+bGL79trXDUUqpNqdDJoeAgK4UDCmD8HA976CUUo3ooMmhG7XkI/36wq5drR2OUkq1OR0yOQQGdgUECXZCRUVrh6OUUm1Oh0wOAQF2pDZPsJ8mB6WUakSHTg7eIDQ5KKVUIzp0cvAECZSXt3I0SinV9nTQ5BAPGNyBLq05KKVUIzpkcnA4nDidsbgDazU5KKVUIzpkcgDbtOQKqIbqavB4WjscpZRqUzp4cqi0byorWzcYpZRqYzpscggM7EqNs8y+0aYlpZTaT4dNDgEBXXFpclBKqUZ14OTQDXeg177R5KCUUvvpsMkhMLAr3uC6N5oclFJqPx02OQQEdMUTVPdGk4NSSu1HkwNoclBKqQNocgBNDkopdYAOmxz8/IIwYRH2jSYHpZTaj8+SgzHmVWNMrjFm3SGmn2aMKTHGrKp7/MlXsRyKX0QX+0KTg1JK7ceXNYf/ApMPU2aBiKTUPf7iw1ga5R/Zzb7Q5KCUUvvxWXIQkflAoa/m3xKcod3x+qHJQSmlDtDa5xxONsasNsZ8bowZfKhCxpibjDHLjDHL8vLyWmzhgYFd8QaB6JgOSim1n9ZMDiuAniIyHHgG+OhQBUXkJRFJFZHUuLi4FgsgIKArnmCQ8qIWm6dSSrUHrZYcRKRURMrrXs8FnMaY2OMZQ0BANzxB4C0rOJ6LVUqpNq/VkoMxposxxtS9HlMXy3HdSwcG2nsdvOXFx3OxSinV5jUrORhjfmeMiTDWf4wxK4wxkw7znXeARUB/Y0ymMeZ6Y8wtxphb6or8AlhnjFkNPA1cJiJyLCtzpAIC6s85lBzPxSqlVJvn38xyvxKRfxtjzgGigauAN4CvDvUFEbm8qRmKyLPAs80N1BcCArpSHQRUlLVmGEop1eY0t1nJ1D2fB7whIuv3+eyE5e8fjjfYDyp0JDillNpXc5PDcmPMV9jk8KUxJhzw+i6s4yg0BFNZ3dpRKKVUm9LcZqXrgRRgm4hUGmM6Adf5LqzjKDQUU6lXKyml1L6aW3M4GdgkIsXGmCuBB4D2cRY3LAJHlbu1o1BKqTalucnhBaDSGDMcuAvYCrzus6iOIxMWhaNa4PheKKWUUm1ac5ODu+4y0wuBZ0XkOSDcd2EdP47wThgBd1nLdcuhlFInuuYmhzJjzP9iL2GdY4xxAE7fhXX8+IXbm7Jri7a3ciRKKdV2NDc5zABqsPc77AYSgH/4LKrjyBEeD4C7JKN1A1FKqTakWcmhLiG8BUQaY84HqkWkXZxz8I/sCoCreFcrR6KUUm1Hc7vPuBRYAkwHLgV+Msb8wpeBHS/+UQkAuEsyWzkSpZRqO5p7n8P9wGgRyQUwxsQB3wDv+Sqw48Uvor5ZaXcrR6KUUm1Hc885OOoTQ52CI/hum2bCwgDwlu5p5UiUUqrtaG7N4QtjzJfAO3XvZwBzfRPScRYaCoCnVC9lVUqpes1KDiJytzFmGjCu7qOXRORD34V1HNUlB295mx7uWimljqvm1hwQkfeB930YS+uoSw5SVoiIUDf+kFJKdWhNJgdjTBnQWL8SBhARifBJVMdTXXIwlTXU1u4hMLBLKweklFKtr8nkICLtoouMJgUGIg4HftVeKit/1uSglFK0kyuOjokxEBqCoxoqKn5u7WiUUqpN0OQAEBqGsyaAykpNDkopBZocADChoQS4IqjKXwv/7/+Bt30McqeUUkdLkwNAaChOVwiRr6+EW26BVataOyKllGpVmhzAJoeaQDrPrbDvi4paNx6llGplmhwAQkMJWJVJSH3HrMXFrRqOUkq1Nk0OAKGhOMqrkPqtoclBKdXBaXKAhhvhCsb52feaHJRSHZwmB9ibHK7oY2sPmhyUUh1cs/tWatdOOQUyMvCe2hV36GacJSWtHZFSSrUqrTkAXHstfPUVoeGDcYcK3sLcw35FKaXaM00O+wgJGYA7DDyF2a0dilJKtSpNDvsICkrGHQZS1MyBf+bNA22CUkq1Q5oc9hEUlIQ7jOadkK6ogLPOgldf9XlcSil1vPksORhjXjXG5Bpj1h1iujHGPG2M2WKMWWOMGemrWJrL3z8cb3ggprT88IWLi8HjgUIdQU4p1f74subwX2ByE9PPBfrWPW4CXvBhLM0XFYmjtPrw5UpL7XNFhW/jUUqpVuCz5CAi84GmDqsvBF4XazEQZYzp6qt4mstExeBX7ra1gqbUJ4fyZtQylFLqBNOa5xy6A7v2eZ9Z91mrcnSyI8FJyWHOO2jNQSnVjp0QJ6SNMTcZY5YZY5bl5TXzSqKj5Bdj81Nt7pamC2rNQSnVjrVmcsgCEvd5n1D32UFE5CURSRWR1Li4OJ8G5RebBEBt7oamC2pyUEq1Y62ZHD4Brq67amksUCIiOa0YDwDO2N4AuPKbWXPQZiWlVDvks76VjDHvAKcBscaYTOBBwAkgIi8Cc4HzgC1AJXCdr2I5EgFx/QBw52c0XVBrDkqpdsxnyUFELj/MdAFu9dXyj5ZfjL1gylOwq+mCWnPwicKqQrJKsxgYNxB/h/33FBGMMT5bpojw3fbvcHldJEclEx4YjsvjIiIwgujg6P3KNRZHZmkmq3ev5ty+5+Iwx1YZ94r3oHmICJsKNrEyZyV+Dj+C/YMZ030M8WHx+5WrdFWSX5lPt/BuDduuXl5FHrkVuXQJ60J0cHSjce4p30N+ZT4D4wYe8Xq4PC4KqgpYnLmYb7Z9g8M4uP2k2+nTqc9+5WrcNeSU51BQWUB4YDh9O/U97N/W4/VQ46khxBlyRDHVf3d78XbKasroF9OP0IDQhmlur5vNBZuJD40nJiQGgOLqYmatm0VFbQVe8XJmrzMZ2dXegiUi5FfmU+WuosZdg9PPSaBfILEhsTj9nIhIw7KGxQ875HrVuGsoqi6iqKqIClcFiRGJdA7tfMjyWaVZbCncwtairQyMHcjJiScf8XY4Gtor64GiogDwFu5uulwbqDmICGW1ZVTUVtA1/NBXAYsI2WXZdA3visM4EBGWZS+j0lXJKYmn4PRz7ld+T/keqt3VhAeGExkYiZ/DjnORW5HL0qyluLwu/B3++Bk//Bx+xATHMChuEIH+gazMWcn8HfPJr8ynvLac6OBo+nTqw7jEcSRHJx8yxueWPMfjCx9nZ8lOAMIDwhmbMJb8ynw25G9gSt8pvD3tbQL8AgDIKM7gnbXv8OXWLzkl8RRuHX0rVe4q/t+y/4dXvDwx6QmMMZTVlHHhrAtxe930iOxBfGg80cHR9Ivpx0UDLrI7sc9v54VlB99mYzCckngK4xLHsWL3Cn7c+SPGGGJDYokNiSUuJI7CqkKWZi8F4MUpL3Jz6s0Hzccr3ob1C3GG0C28G+f1OY8bR92I2+vm2SXP8smmT9hZspOi6iJO7XEqFw+4GID1uetJy0hja9HWg+Y7pPMQOod2ptZTS3ZZNtuLtiMI/g5/ekb2pFd0L3pE9mBt7lqWZi1FEABigmP4y+l/4eZRN2OMYfXu1Tyz5BneXPMmLq+LqKAohnQeQlFVEQVVBUQERhAXEkdEYARB/kGU1ZaxuWAzmaWZOB1O/B3+VLj2HiSFOEPweD08t/Q5pg2cxqk9TiUpKonPNn/GrHWzKKstayjbL6Yf4xLHsSF/A5sLNvPb0b/lwdMexO11c9+39/HW2rfIrchFRJjQcwIX9LuADfkbmJM+hz3le3AYB+GB4QyIHUCPyB4UVBaQU55DracWr3jJLsum2l3d8PdMiEggNCAUh3GwrWgb1e5qwgLCuGfcPfSL6ccdX9xBTvn+rdun9jiV7uHdSctII7fi4E45/YwfXUISKHeVUlJrhxhODO/BmT3Po7y2gszSnRTX5lPqsgmhyl110DzCA8IZ3X00pyedQc/InhRXF5FemM4XW74gvTC9odzvTvrdcUsOxh7AnzhSU1Nl2bJlvluAxwP+/mReH03CK03cpnH55TBrFgQHQ2XlUS2qqKqIstoyDIbEyMTDlnd5XLy26jW+3vY1m/I3sa1oW8OP8rqU63jm3Gf2OzJanr2cx358jHkZ88irzKNTcCdO7XEq63LXsa1oGwBRQVFM6DmBIP8gqt3VrMxZya7SvbUmP+NH94juBPgFsKXw0OdhHMZBeEA4JTUlDd8LCwijtKYUQUiOSmbr7VsbPTqauWom1358LRN6TmBK3yl0DevKwl0LWZS5iC5hXegc2pk31rzBtIHTePrcp7nv2/uYuXomAIPjBrMhfwMO48DtdTfM85PLPuGC/hfwp7Q/8fD8hxmXOI6ssixyK3KpdNm/V7fwbvSMSGZR1o/cfcrdTO0/lYziDCpqK6ms8Gdjzg6+3/0Jm0pW0T9qKOMSJhIVFkh+VR75FflkFefjrvVnTNT5LCr4lJyarTzXP50e8REkJkJoZDWr8n/ir/P/wvzM7xgQNJFAd2f2uLay26wggFCMEWqkktSY0wmuHEBZQSjZIZ+Ty3r79wmIZVDkaFLDL6CnGU9ZmSGvtISfK+azxZOGiwocBBDijaOTZzBhdKHMsYNSv22U+G2jxGwnSnrR2zuFCFdfCqpzyQj6mD0h3xEtfXD7FVPmzSfABDO49lcEFqSSH/wjJc5N+NXEYqpjMIFleIJycZlyXFKNwxNMeG0/Qtw9qKn1UOt24XBHEuCKwZE/hJqtJ1PtKKB25D+p6DsTT5C9wtC4Q4jPu5TosvFIRQy1QVmUdPuQ0tAVhFQMRmpCKO38JfEF0yAshz2BCxkROA2/ooEUFXvYE/0R5cEbcHoiSKiZTIz0x8/fSyUF5Ho2UebYhamKg7IuhAUGExVlkPJ4slcNprIogtiBGwjutpXK2ioqql04SpMJKh5GecInVCd/AEBwSQqpOS8ieYMoLKmmNPkN8ns9j9dUEbz7DLyZqVQWh+GpDQSHG/yrITwboraDOxiyR4EnAAa9D0nzoDIGShOhojNURxMZEE1Sl2g6BUeTvjaazG0hELkTR/wmJOEHJH71Pj/4YJxZp+PYPgnZMxhPfi/uujGRx/62/8FccxljlotIarPLa3I4mCc8kJzJbrrPrsUYv8YLnX8+zJkDgLhcGP8jq4T948d/cM839zQczd0//n7+esZf9ytTWlPKs0ueBSDYP5gXlr1AemE6vaJ7MShuEL2je9M9vDs55Tk8tfgp+sX04+9n/p1z+pzDrHWz+M2c3xARGMHkPpMZ0WUEa3LXsGDHApKikvjl0F8SFRTFp5s/ZUnWErzixd/hz9DOQxnTfQwRgRGU1ZSRW5HLrtJdVLgqOKn7SZySeAoh/qEUFnuodXnwD/CQWZx6aKopAAAgAElEQVTD4ozVZBXvYUjEeEZEnoWjMp7CQkOVq4Z5tU/ySfn9PN5tOwlhSYTEFHL3+tOIMolEVY3k29q/kxJ1Og8kz+HrLwJYvRqSk6FPH5uri4thZdC/+DHsToz4YXAwqPR/6Jn7ayhOosC7nR2xL+HvDaNn8VWsGDwJjwd6zv+KjCkDmTrgAn6fNIt//QvWroWS8hoKo76jKuVJ6Pk9/l8/Q+edtxAUBH5+kJt7QH+KfrX2Bw8EBEDPnjam/a6q7rYMbhoNC/4XfroNzv0d9P8Y/GuhJgy+/CesuAEwOBzg7bwSxjwL4oDF/wN5gwAIC6urjEZlgCvE7lQa4XBAZCQ4nSACXq991L9u7DOn034nIFAo6Pw+ZYP/DYW9IeM02Hw+TlcsiXXHKMZAeDiEhNhjn5ISu23Cw+18amrA7YaICBuzMXYZISEQHQ2BgVBdDTW1gic4h8rgdPxyR1K0Oxyv1x5TidgKeFWVjSsiUtgS9yRbkv9g1/3jV2H9pYSF2f8FEShz7MJTEk9tVQA1NTYOYyA2du8jMhIyMmDdOhvfWWdBYiKsXAnp6dClC/ToYWN0uezftDTqR4olA8fGS9md5SQiAjp1sutYVmbXPSbGfhYTYxsYgoLsPAIC7DZxuex2qqqyn9V/7u9vW58LCuDnn2HhQtvrzvjxcNppNv7SUvvsDsin3FOIpzwaqYom0OmP02nnExAAEyfCpElHtKtpoMmhBbgTYsgbWkj0h7sICkpovNCECbBgAQCXvHk+q/LX8+hZj3JG8hk8u+RZvt3+LX845Q9c0P8Cskqz+NUnv6KgsoBbUm8hvSCdxxc+zsUDLmZK3yl8uvlT5qTPYd2v19E/tj9gm4IumX0JH238qGGRg+MG8+hZjzKl75SDjsDTtqdx1YdXkVWWRYBfALWeWs5MPpNZv5hFbEhsQ7naWvvP6nDYf+QPP4SlS6FbN4iPh6ws2LLF7hCMsf/oeXlQZGvLeDyQk2N/+M3WeS38Zhh8+F9YfQ0MfQumXQml3SEiC3JS4L/fQ00EoaEwYgTs3Gkfxtgfu58flA1+Cne3H4hY+gjhtf0JDbU7o5CQvTubigooiP2EzaMuJLAyiZrALEJf20BFZm+iouD00+386h/+gTWUlwSSn293NC6X/fEPGAAJCXY7eb12viUlkJkJ27bZwQNHj7blAgPtNn143dV8nT2bABNMjaeasc6bSZIz6RswnpQBUfTrZ7dxRMTepJeTY3dkVVV2fsnJdttv22Z3JiUldhtER9sdUv2jfod8LNxu+7devx46d4bUVLsdW9uSrCXEhsQS7u5FRYXdkTuO4lRO/a7Nh6erjopIQwPFcaXJoQW4BydT1CkD56fziYoa33ihlBRYbauAnR+Noai2BLfXjcM48IqXbuHdyC7L5rqU65iTPoeK2gqSo5NZl2v7Ibxl1C08e96z+Dn8yK3Ipe8zfRmXOI65V8wF4NEfHuV/v/1f/jnpn/x69K8prCok3MSzeZMfu3bZnUZx8f7PhcUudrCArLBP8RR3wSy+i5oqfyIi7M6soMA+nE579JSba3eIDUer2B9SYqI9QhSxR0edO9sdksNhp3fpYssEBNidmr+/3ZF27myni9jynTrZMrUuL4NnxnF24oX8eeSr3JZ2BUsLv+aTCbsJ7radCEdnsjPCARg3zu5sYf9EdiREhDNeP4N5GfO4pPvtBKX9m9Gj4frr7Xr5SmZpJkOeH8LQ+KH8Z+p/6BfTz3cLU+oIaXJoAZ5TR1NasYyaz2fy/IYtxIfGc+sYe2GVx+vhq61fMfmc32B27qLE30PU/8LfzvgbXcK6sC53HdePvJ7e0b25++u7eWbJM/SP6c+HMz5kQOwAFuxcwK6SXfxy6C8xxpCfDxs2wNNL/8l7ZXcxtvaPRMSW8HXpsySUTKfyjXdADE6n3Zl7vQfHGxpqd8aRkQc/BwbaanFZma1yd+lid+jZ2XbnfdllMGaMrQns2WOnBwW1/Da9+N2LWbtnLZt+u4n4J+I5r+95vH7x6y2/oDob8jbwtx/+xr/O+dd+NSdfq6itIMQZ4tOrq5Q6GkeaHPRqpUY4ouNx5vqzcMtTPLJgNZ2CO3HTqJtw+jn5z8r/cPNnN/NjSASnxMezVeyocf1j+3PJwEv2m8/T5z7N9SOup1d0byqLw/joI/j22wmsWgV/LbA7+4Yev/1+i/n1KyyOfRgKgmD7ZKrmvcL5UwyhofYIv3t3GDbMNj3U7/wjI1umehocDElJxz6fQ5nQYwIfbfyIjzd9TEFVAef2Odd3CwMGxg3kjYvf8OkyGrPvBQFKncg0OTTCRHciqCaatzevxCuQX5nPd9u/45w+5zRcJbM6rJxTgvqypdomh32v5962Dd5915582rRpOJs37z3BGRJi23aHDLFH8v36wcCBMGBAACGx88ktz8eb34fKcn9Gv27b2tuDiUkTAXjguwdwGAeTeh/lWTWl1HGhyaExUVG4K118vsfJSTGwoTyIWetn0Su6Fwt3LQRgfYwX/LqRXrwcgN7RvVm0CP72N3sRk4htl+/fH664wj6PGAEnnWTb4RsXS+ewWOhyfFbzeBoeP5yIwAg25G9gbMLYhpuOlFJtkyaHxkRG8l73EopqhV90c7CovDsfrJ5F5/e+wPQxJEf0YF3nHbyecTGfddpMJ28Bv7oqlNmzIS4O7r8fbrnFNgMpy8/hx6k9TmVu+lyfNykppY7dCdFl93EXFcXzqUK/6D6cN/AqTonIoFSq+Wfybs5KOoMz4sbyU+dQrpl3LYs7xVG4cxCffQYPPmiblB5+WBNDY05LOBVAk4NSJwBNDo3IChcWJcINvafTresNpERWE1frh9sPLo48n+1LBlMdUsGNk7+nc6eFTE3wY/t2eOghe1moatwt2zrx7v9BKt1aOxSl1GFocmjEMqe99fXUkIFERo7DSArRayZBZSd+c+lNfPvZKQCc98tvyA33MjYokM6N38iq9hGeW8Kl68H4eMAmpdSx03MOjVghOTi8MNx0Yd06w403fsP2HcFcn/ZPhl+xkbCutfwK+KR2DQB9arW60Cz1l2zV326tlGqzNDk0YnnNdvrl+XPnY3155WuIi43kO89ZnFbxPQwLReLiuHstfJr3AwB9K31w11h7VFw3LrcmB6XaPG1WasSykk1sz7mE/3zdg1//GtbMyeY0vgcgf+1LlGWlMTgX8mvsTq53+dH1ktjhaM1BqROGJocDZJdls6cmj5qccXz3m/d55hmIq907tLVkZpC37VWG1HXrHl/lR3hZbStFe4KprznUPyul2ixNDgdYnm1vakvK6cSp/ovthzl1g3+EhxNbOwb/Sj8G5duP+lQE6lChzaU1B6VOGJocDvDNz8tBDNd5sjBb6wa3qU8OI0ZgcnYT6u7OoDLbsVqfmlAdKrS59JyDUicMTQ4H+GLNcsgfwHVDttjO7sEmB4cDhg+H7GyC3fH0rxD8jB+DXFFac2guTQ5KnTA0OWD7/3d5XIjA1soVxHtHkTgsGrZutX1k5+TYUVoSE6G8nKB8JxH+8PEFd3Jr2QCtOTRXfbOSnnNQqs3T5IAdWCfy0UimvnIDnpBszhgwyo5LWFNjh/7KyYGuXe1waYBf+g4kLIhEv3WEBkdqzaE5PB47qARozUGpE4AmB2Dl7pUIwpxMO/jMtZNSbXIA27R0QHIgKwtHVDzFxWnUOEtwlexiw4ZrWyf4E0Vp6d7XmhyUavM0OWCHd0yJOQV5ahuXuGdzdv9xTScHwD8mCa+3mtyKT3FUutmzZyZlZStaaQ1OAPVNSQEBmhyUOgFocgCyyrIo2N6dwJoEnv/tdDvEY0KCHWNz40Y7ZNsBycEZ05uIiJMJ6Twav2rwd0SRkfFQ661EW1d/vqFnT5soTrDhaZXqaDp8cvCKl+yybLat6s6119rzzoC9OqlXL1i40O7Iuna1o9PXdbvqiIxm5MiFxPSYDkCP2NsoKPiU0tKlrbMibV19zSE5GWpr7UDWSqk2q8Mnh7yKPNxeN57i7tx11wET+/SB5famOLp2tc/1tYeICPtclyy6RVyNv38ntm//I6JHxQerrznUD1StTUtKtWkdPjlkldmuMfp3TaBv3wMm9ukDbrd9fajkEGoHlPevMfTs+QBFRV+SmflvH0d9AqqvOWhyUOqE0OGTQ/pumxzGDW1k6Lb6k9KwNznUD/EWHm6f60f3KS8nIeF3xMZezNatv6ew8BsfRXyCOjA56L0OSrVpHT45zF9lk8M5pxwmOXTpYp8P0axEeTnGOBgwYCahoQP5+edLKS9f7aOoT0DarKTUCaXDJ4fl6Zng9eO8CfEHT6xPDjEx9hJMOGSzUv1d0v7+4QwZ8jEORwgrV46nqOhbH0Z/AikuttsqLs6+1+SgVJvm0+RgjJlsjNlkjNlijLm3kenXGmPyjDGr6h43+DKexqTvySLA1YWwUL+DJ/boAf7+e5uUYG9yiIy0z/vUHOoFB/di5MhFBAb2YM2ac0lPv53i4u8R8TQdTFERFBQcw9q0YSUlEBUF0dH2vSYHpdo0nyUHY4wf8BxwLjAIuNwYM6iRou+KSErd4xVfxdOYggIodGURF9hIkxLYxNCr197zDABTpsBjj0Fqqn1/QM2hXlBQIiNG/EBc3DRycl5m1arTWL48laqqjEMHdPnlNiH9/e+26472pLjYJtT6pKrnHJRq03xZcxgDbBGRbSJSC8wCLvTh8o7Y998D4Vn0jjtEcgB4+WW7s64XGgp/+INNHNBozaGe0xnFoEHvcMopeQwYMJOqqu0sX57a+MlqEVi8GIKD4b774OSTbX9E7UV9zcHf357M15qDUm2aL5NDd2DXPu8z6z470DRjzBpjzHvGmEQfxnOQtDQgIovBPZpIDhMmwIgRh55+iJrDvvz9w+jS5WpGjVpCQEBn1qw5m5UrJ5Cb+x5eb90ocpmZdgf68MPw73/DypWwoh11x1FfcwDbtKTJQak2rbVPSH8KJInIMOBrYGZjhYwxNxljlhljluXl5bXYwr+eVwFBJfSMSjj6mYSE2Odm9MwaEtKPkSOX0Lv3k9TU7OLnn6ezcGE30tNvo3Z53YnrIUPgssvqAvz66ONqa+prDqDJQakTgC+TQxawb00goe6zBiJSICL1jeuvAKMam5GIvCQiqSKSGld/tcsxysiATTk2nO4RTdQcDsfPzzYFNbPbbn//MBIT7+Skk7YwdOgcoqPPIjv7ZbK+vBUAV//ubCt/iqoBkXi/+PTo42pr9q05REXpOQel2jhfJoelQF9jTLIxJgC4DPhk3wLGmH0uA2IqsMGH8ezn88+BiEwAuocfQ3IAe96huQP+ZGXBZ59hjB8xMecxePAsRo9eS/gOJ9VxsHhTCjt3PkreyHJYtJiSzHZwKayITQYdoebw3//Czp2tHYVSx8xnyUFE3MBvgS+xO/3ZIrLeGPMXY8zUumK3G2PWG2NWA7cD1/oqngPNnQuxvVqg5gA2OZSXw/btsHbt3s9ra+HJJyE9fe9nv/kNTJ0K+zSPhYT0JSarB+4BCXTqNJnU1NXE/fIlHG7Y+cY5rFp1FhkZf6WsbNWJ2W9TVZXthuR4n3OorfX9MvaVnw/XXQe33XZ8l6uUD/j0nIOIzBWRfiLSW0QeqfvsTyLySd3r/xWRwSIyXEROF5GNvoynXnU1fPcd9BlRlxyOteYQGgoffGAvex02DO6/3456duGF8Pvfw9VX26PnjRvhk0/s62/2uWLJ7cZs3ETY2MsZPHg2YWFDCT7rCiQ4mMQNQ3G58sjI+CPLl49gyZIBbN78W3bufJyCgrmIeI8t9uOhvgnpeNYc0tJsMjqeR/Eb6/59P/kE1q07fstVygf8WzuA1jB/PlRW2ppDZEkkoQGhxzbDk06yR6lXX21PZvztb/DcczZBTJsG778P//d/8NVXEBRkx4n46it7XwPYmkVtLQwduneegYGYiROJWrKd0aM3UlubR37+B+TmziY39y3cbrvDDQ8fTa9ejxMWNgyHIwQ/vyDYvduOfb3P+BOtqr7rjH1rDpWVdp3r7zxvad9+a48Cliyx944cD5s22Wd/f3svzBtvHJ/lKuUDHSo5iAgzV8/kjh8exjFjFHn+mcfepATwygH37qWmwiOPwGuv2SakkSPhrrvsoEE33GCbH776ytYgjNnbFLVvcgCYNAnuvBN27CCgZ0+6dbuZbt1uBsDtLiM//wO2bbuP1atPb/hKl8XR9P1rGe7ukez5+g+AUFq6lOrq7fTocS+dO9vxJ0pLlyLiITJy7LGv/+EcWHOofy4uhs6dfbPMlSvt89q18Itf+GYZB9q0ySa7X/8ann0W/vIXO36FUieg1r6U9bjJKM7gnDfP4bqPr6O6JBLT73N+yl507E1Kjbn5ZtuccfHF9mqmJ5+09zG43XZnP2kSZGfD+vW2/Nq1ttyAAfvPZ+pUO+jQv/510CL8/cPp0uUaTjppMwMGvE6fPv8m5bOzGPC/RYjDS+DmAnLm3cO2bfdSXr4Cr7eSn3++lC1bfs+GDdewYsUYVq2aSGHhcbhctrGaA9gazllnwRNPtPwy900Ox8vGjdC3L9x9t/273X47FBYev+Ur1YI6THJYu2ctizIX8dj456l5Zhl/itjGgxMf5M6T7/T9ws86C669Fm69FXr3tskBbO0BbPt03762yWlfvXvbE5wvvGCbqxrh5xdKly5XkWCmEfXkN3Dppfgvt23fo7MfZty4QsaO3UZq6iq6dbuFogVPUrD1HRIT7yEkZCDr1l1EXt4HZGQ8zKpVZ7Fnz1stf9K7sXMOYI+wv/0WZjZ6e8vR273bjvu9b63seNi0Cfr3t92tPPKIveqhf394++3jF4NSLUVETqjHqFGj5GjlVeTJ0qUiIPLxx0c9m5YxcKDIOeeIeDwivXqJTJ/eeLldu0SCgkSuuqrp+T39tF2xjRvt+1GjRE4+ef8yBQXiDQqQ2l/NEBGRmprdsnhxH0lLQ9LSkIULEyQtDVmzZqrk5PxXdu36t+zY8Zjs2PGYZGa+IDU1u49uXV980caWmWnfL1xo34NIQoJ9zs09unk3Zu5cO88zzhAxRqS8vOXmfSi1tSL+/iL33bf3s9WrRcaOFXE4RLKyfB/DkVq+XKSmprWjONjf/y7y5putHUW7AyyTI9jXdpiaA0BsSCy76jr0SDiGm6JbxKRJtnOn4cNh2zY49dTGyyUk2OaJN9/c21TSmPfes3dX9+9v3190ke2rKSdnb5l33sFU1+KcMw+8XgIC4hk+PI2+fZ9j7NgdjB2bQe/eT1BU9BUbN17Lli2/Y9u2e9i27R7S03/NwoXdWbt2Kjk5r1JdnUlFxQays18mJ+c/uN1lh47tUDWH0aPhrbfs63nzDrfFmq9+O115JYiw7bNLKCpKa7xsS41lvXWrbTas3/5gr1x79VV7ccB777XMclrK/PkwapT9v9u2rbWj2Wv3bnu139VX1/Vvo1pLh0oOYJv+oQ0khwsusFfTeDx2x3/rrYcue++9dkyJM8+su3vvALt3w4IF+594vfBCe2z+6T53Wb/6KjidsGcPLF0KQNDcJXQ/7V8EvfMtRgyJgVcxbsWDjA2dy7hx+YwfX8748RWMHr2exMS7KCtbwaZN17N4cSJLlw5i8+ab2LTpBhYt6sa6db9g1aqzWLp0KGvXTmX79ofIz/8Yd8EuxM+P4toV7Nr1L1ZX3UbGDQFsfCSK4oEeJCyUyrkvU1DwRdNNWi+/bM/jlDWRiMAmh169GhJu7fKvWL9+GlVV2/cv9+GHNmE11YfVkiUwaJDd+Tel/kqlfZMDwMCB9gBg1qymv3+8vfmm7folPd32Hfbdd60dkTV7tk2m3bvDjBl7f7D1TsT7fE5UR1LNaAuPY2lWEhH5wx9EAgJsa06r27Ch+YGkp4sMG2abSe69V6SiYu+0F16wzSjr1u39zOu1zVXnnmvfr1plyzz0kIifn8j999sygwfb9yDSv7+I02lf9+69/zIaZuuVsrLVsnPnPyU7+xWpqNgkJSWLZcOGa2XRoiRZvnysrFlzgfz000BJS3NIWhqSORWpjaCh+eqnnwbJhg3XyoIFMZKWhuSfhJT3sNPWrp0m1dU5Ulq6TLKyXpTCwu/E7a4Sb1WleONiREDcZ5wq7oqC/eJyuyvF7a5rPurdW2TaNKko/VncgUjulUmyYEGULF2aIm533Tq5XCL9+tl1vfDCxre51ysybpwtc/31Tf99HnvMlisqOnja3/9up23fvv/nWVki2dkHl6+pEXnkEdvs4ws1NSLR0SJXXGFj6tVLZORIu76tbexYkeHD7W8jLEzkpJNEqqrstNWrRbp3F5k9u3Vj9DWv167rN9+06Gw5wmalVt/ZH+njWJPDL38pkpx8TLNoPRUVdicFIklJ9kdSXS1y5pkiAwYc/OO+6y7bDv7aayK/+53Nivn5IhMnigwdKvLll3Zer71mH6mpIrffbl+DyN132/ksXCjywANNt917vSKFhfvF4HaXS0n6Z+JKipOa/l0kP/8zqaratc/0CsnO/o+U3H+JCMiuJffJvHkBDUmk/vH990Gy4f4AEZCsKYiA7JmILPoxSVavniJLlgxpSEQ/zAkWASn+wxRZtepsKR3gEM8ZEyQ//zNJS0OWLk2RXbv+LTUv2B22d/x4EZCS+a9IXt5Hsnv3O1JWtsoG+PHHdjskJ9ukuWuXuFzFkpPzXyks/G7/9f/Vr0Ti4xvfNlu32vk89tj+n8XFiXTrJlKwT6IrLBQ57TRbvls3kT17Dr3Nj9Znn9n5f/aZff/88/b94sUtv6wjUb+dHn3Uvv/gA/v+l7+026hXL/u+Tx+b3FvSzp2N74yLi0Wuvtr+/9dbv17kkktsvEdixw6Rr74SWbKk8XNsu3eLPPigXb/6c3JffHFky2iCJofDmDDBPk5o8+aJDBpk/3yhofaE5/33H1wuL2/vjsbPT+QXv7CfP/GE/Wz4cJGuXRs/KXnjjXa+N91kn0Fk9Gi7s9qyReSee0ReflnE7RYpKRG5+GJbpnt3kRkzRN56y/7gUlJEgoNFvv/+0OuzZIn97ttvS1nZGtm69T7ZvfsdqahIl7y8TyQ9/X+kKqWruJI7y56cd6XkT9NFQHL+MFyWLBkiq1edK/n3nS0FD10gmTN/IQKy+lGbWMqnj2nYaefkvC5LlgyTeV8iVZ2RkgHIj58FiSvUJpt9E9LyJWOltl8XcfWKl90LHhavn5G8q3rL998HNZTZvOk2cZXlSVHRAqlOTZbaccPE5SoWl6tMKiu3yJ49s2XTpltk/frLxTVqkD06F7E7uv79RaKiRJxO8Uy7UCrKN9mdzqBBNhH9+c/iDQyU2jNGS37uZ1Jbu39NSaqqbK3wL3858iP+K66wNYeaGnG5yiRj7QPiCQsW774XPWzaJPLPf4qcf769uCE52e6chw4V+e1v7d+9uT791B7MfPDBwdN277b/R8XFtrYEIhkZe6fXf5aQYLfLPffY92+9tf98fvhB5Oab9//uoezYYWvfK1bY90VFtrZpjMjSpXvLrVmz/4569mx7gFT/2xsxYm+tpikej8hTT9kLS+rnFRQk8v77dnp+vv29BQTYGM4+217EMXjwwQcPx0CTw2H06mUPRE54tbUic+aI3HKL3Wlv2dJ4ObfbHo0EBoqkpdnPNm/e+0/6yCONf6+oyCYOELnmGpG337Y7+dhYmyyMsdOGDbM7Oj8/kf/5H5HLLhPp0mXv/J1Okc8/b3pdXC6RiAj7o6iv2l16qchLL9lktHy5nddTT9nyXq/I1Kl2ndatE/nrX/cuL9jWHMrSv5asrBfF88Tj0nA11MKFIn/5i7gnjLHJ5Y1rJD39Tim5/RzxGiPVD94q1S//TQr/dJEUTAwVAVn3kE0EOWcbcQcZ2bLwOikuXigZc66UkoFIbTiy7AXbbJZ1PgfVeubPD5MFC6Il/Tc2vuph3cSVFC/eAKeUfvaU5N89QQRk9xmIJ8AhnpgIyf+/38vPP18tm++2Mey6GJk/B/npp4GyefPvJD9/jrj+9Pu963zDDXYbZmWJd0u6uN2VUl2dI7m578uWLb+XrKyXxOUqtZuuvFy8YWHiuvZS2bPnXVm4MNE2/V2EeJwOqdiUJt7rrts77/79RSZPtgnliivsFXZgDxq8XltznTu30SZI8Xptk5ox9n8mJmb/mtCCBXv/xzp1sv83p5xS91WvFBZ+K1mZL4jrsgtsmRdesDvaIUPsDtrjEamstDXc+v/HTp0ajrZdrmJxu6vsb2XTJvt7ueMO+39Tf2D1+eciU6bYGnanTrZZy+MRWbTITu/a1f5uxowRiYy0B0HG2PZpsL+/vDybON580/4/5uXZZPXSS3Z5o0bZslOmiKSlifuj98Q9JsXO5/bb7cGLv7/Ib35jf5v1Vqywn0+fLvLRRyJ//KPd1kdJk0MTvF6bnP/wh6OexYnrwGp4//52R5qff+jvrF27f1V78WJ79HvPPba9fPZskZ49RTp3trWZeh6P/eHfdZf9QTbH1Kn23zEqyr7u1s2+9/cXSUwUCQnZvz1/9+69zTIgcuWVtpmkf397mXC9r76y0wcMsM/G2HMN9edcROw2qG+yqHt44+PFff2VUlWZIVVVGeJZs9J+1+m0zW9Op3g6RUhtt3DxhNmEVPHwTZKR8Yjs2PGYZGe/JsXFi8TjqRW3u1wy1zwsuVMipWA0UjwEWfvnugTyjZHy1DgRkPzxAfLD+/bzH36IlfXrfimV10wSAXFHBUv2bX1lwVeBsuhtxB2A5J7uLzuvCREB8QTsjb1gNLL6MeTn+5CdlxlZ/qxNUkuXjpJN99hYVz5pl7NkyRApKvpesr+xyaY2zM4j55rusvHzs+Xnn6+W1aunyNKlI2X58pNl7dpLJPf6/nY5p4eJK84u3z2wt5Qv+1jKyzdIZc5KcR+zsNQAAA0ZSURBVD37D/HWHWF7L7tMaheniTfAX8om95fNP10r+b8eKV5/h9T0jJScZy6U2tPtDrTmmUckO/s/snTpyIYEO+9rZNVrdntkZj4vu586XwSkeEyYuEPt+bLCGf1l1yfXimtwkniNEVfXCCnr45Cq7k7x1p9TA/E6jFReNlGqvv8/8Q4bsvdv/uyzIjNnioDk3tBP3FEh4u2dLK4dm6W0dJlUrP1cvBERtmx9Lb0+QdQnpsYeISE2ObzyiojXKxUVm+SnnwbL958jFWfX/U+mpNhzgo15+OG983I4bII4SkeaHIz9zokjNTVVli1bdlTfzc2F+Hh4+mntOJOvv7ad31166bHNx+Wyj/pBj47W9u2wZg2cc469GVDE3hz45pv2Sp/LL4dHH93/Ox99ZK9emjzZdnbndNorXVwu238V2N5vExLs1V733GNvKoyIOHj5IvZO7rw8Oz0+/uAyP/5or/5atAj69LHxVFXx/9u79+C4zvKO49/frlaXtWSvb7FiSyD5QkLsIcEBT4gNJA5tk8AQhqHThJSkhSGdDkyhwzUkLbgtM6VhmrYpEDLQxrSZwDg1jetJGcBO0zIkFrEdO4mTgB0bLEdBxpIsWZfVXp7+8R7HG61kSzbWHmWfz8yO9lx29ejRnn32vPue9+Xqq0N30K1bwxzjp1EoDDE09DyFwgnMcjQ0LKd+eDbs3k3hHWvo7dtOOn0RDQ1vQFJ4UEcHbNgAjzyCtbdRmJ8msW8/v/zBzeQW1TF760Hq93STX3oByYEcs+/fQfLYqSHkLV3Pr+5bz2jhKMtu20XukiX0/tcGahsuJJNZTyKRCrH9zjvR4zvo+rur6H7bCPn8MfL546RS86mtvZBiMcvoaBdWzLPib4eY9/ARei9PcHRtkbaNkMzC8GKYdRBkMLBCdN00m5fXj1C0LK0PwrL7oFAPyRE4ek0t+z89i9GGQcxGaeiqYbg5D4KGhjfQ2voZMpm309f3f/T1baO3dzu5XDcqpljzJylS3TmOXz2f7nen6Vs1yuhoNxoepXUTNByB9PAisjU9jLSmSL3xSroa/4fBljz56EL95CBc/BXINac58de3kBs9RstNm5jzLGTnw+57EoxceGpgy/k762neOZ/+L95E09w11CdbSX95I8WmOoavXEqh3qh55iCJnn4Ky1soXtxOou0iUnULGR19iYGBnRw6tAEpxZw56zjW/TAtBy9H666mmBylWByJbsMUiyNAgnTdcjKP9jI0b4BjSzpZ8LoP0tLy8dO+xiYiaaeZvWXS+1dTcdi1K3Tt3rw5vKe414Ddu0N30bFXl5c6eBCam8OkTOfD4cNwzz3wpS+de5E8nW3bwjUv+/bBXXeFEX/HMzQURv1ta4MFC2D9+jBcSyoF8+bBT38K402a1d8fBkNcsODMsRSL0NlJYclC+vsfxzoPMetz98LgMLm3LmdoXRsDq1Lk8j2kUvNIpRbS1HApmdv+GdXUwZ13hi6+hILZ27udvr7t1NcvJZN5J7NmrUR6dU97M2Nk5EVqaxeTLCTDFfCp1CvbC4UR+vsfZ2Cgg7lz30VT0+UMDb3A3r3XMzLyIosW3UJ7+9+Qz/cwMLCbQmGAYjFLf/8T9PQ8glmB5cMfZfFXn2P4y3/Kywt3k0w2kk5fRKFwgv7+DgYGOjhx4inMcpP9r71KU9NbWblyE3V1r+Oll77OgQOfxSxPItFAIlFPMhl+JhL1FItZhocPYDZKIjGLxsbLWLz4ozQ333pWv9uLw2ls2RK6/3d0hOuvnJtxcrlw5rJuXRi/aTI6O8Nc6IODoTAsW3Z+Y4yZXK6PbLaTxsZVE+5TKAxRLGZJpeae8fmKxSyDg8+SzR4mmz1CMjmb+vpWksk5gL1yM8uTyx0jlztKbW0z6fRK6uqWnDojJBS80uXy35Unl/s1tbXNSMkp/NXlplocqmpU1thcAOfc2Uqlwhv9VLS0wJ49kM1O7qzgNSaVypBKZU67TzKZJpmc3FlfIlFHU9NqmppWn3NspysM4XfVUFd3HgYHnYSqKw41NedvlGjnYqupKdycm6SqGj6jszNclZ88t7Mz55x7zau64uBNSs45d2ZVVRwOH/bi4Jxzk1E1xcHMzxycc26yqqY49PSEEbK9ODjn3JlVTXHwbqzOOTd5Xhycc86VqZrikMnA+98P7e2VjsQ55+Kvai6CW7s23Jxzzp1Z1Zw5OOecmzwvDs4558p4cXDOOVfGi4NzzrkyXhycc86V8eLgnHOujBcH55xzZbw4OOecKzPj5pCWdBT45Vk+fAHwm99iONNlJsbtMU8Pj3l6vBZifr2ZLZzsg2dccTgXkp6cygTbcTET4/aYp4fHPD2qMWZvVnLOOVfGi4Nzzrky1VYc7qt0AGdpJsbtMU8Pj3l6VF3MVfWdg3POucmptjMH55xzk1A1xUHStZJekLRf0ucrHc94JLVKelTSPknPSvpEtH6epB9J+kX0c26lYx1LUlLSbklbo+V2STuifH9PUm2lYywlKSPpIUnPS3pO0tvinmdJfx69Lp6R9KCk+jjmWdK/SOqW9EzJunFzq+Cfovj3Slodo5jvil4feyV9X1KmZNvtUcwvSPq9uMRcsu1TkkzSgmh5ynmuiuIgKQl8DbgOuAS4SdIllY1qXHngU2Z2CXAF8LEozs8D28xsBbAtWo6bTwDPlSx/BbjbzJYDvcBHKhLVxP4R+IGZXQxcSog9tnmWtAT4M+AtZrYKSAI3Es883w9cO2bdRLm9DlgR3W4DvjFNMY51P+Ux/whYZWZvAn4O3A4QHZM3Aiujx3w9eo+ZbvdTHjOSWoHfBX5VsnrKea6K4gCsAfab2YtmNgp8F7ihwjGVMbMuM9sV3R8gvGEtIcS6MdptI/C+ykQ4PkktwLuBb0XLAtYDD0W7xCpmSXOAdwDfBjCzUTPrI+Z5Jszc2CCpBkgDXcQwz2b2v0DPmNUT5fYG4DsWPAFkJF04PZGeMl7MZvZDM8tHi08AJ2egvwH4rpllzewgsJ/wHjOtJsgzwN3AZ4HSL5SnnOdqKQ5LgMMly53RutiS1Aa8GdgBLDKzrmjTy8CiCoU1kX8gvBiL0fJ8oK/kwIpbvtuBo8C/Rk1h35I0ixjn2cyOAF8lfBrsAo4DO4l3nktNlNuZcmx+GPjv6H5sY5Z0A3DEzPaM2TTlmKulOMwokhqB/wA+aWb9pdssdC+LTRczSe8Bus1sZ6VjmYIaYDXwDTN7MzDImCakGOZ5LuHTXzuwGJjFOE0KM0Hccnsmku4gNPk+UOlYTkdSGvgC8Je/jeerluJwBGgtWW6J1sWOpBShMDxgZpuj1b8+eQoY/eyuVHzjWAu8V9IhQnPdekJ7fiZq/oD45bsT6DSzHdHyQ4RiEec8vws4aGZHzSwHbCbkPs55LjVRbmN9bEr6I+A9wM12qt9/XGNeRvjwsCc6HluAXZKaOYuYq6U4/AxYEfXsqCV8mbSlwjGVidrqvw08Z2Z/X7JpC3BrdP9W4OHpjm0iZna7mbWYWRshr9vN7GbgUeAD0W5xi/ll4LCki6JV1wD7iHGeCc1JV0hKR6+TkzHHNs9jTJTbLcAtUW+aK4DjJc1PFSXpWkJz6XvNbKhk0xbgRkl1ktoJX/J2VCLGUmb2tJldYGZt0fHYCayOXu9Tz7OZVcUNuJ7Q4+AAcEel45kgxnWE0+29wFPR7XpCG/424BfAj4F5lY51gvivArZG95cSDpj9wCagrtLxjYn1MuDJKNf/CcyNe56BDcDzwDPAvwF1ccwz8CDhe5Fc9Ab1kYlyC4jQk/AA8DShN1ZcYt5PaKc/eSzeW7L/HVHMLwDXxSXmMdsPAQvONs9+hbRzzrky1dKs5Jxzbgq8ODjnnCvjxcE551wZLw7OOefKeHFwzjlXxouDc9NI0lWKRq51Ls68ODjnnCvjxcG5cUj6Q0kdkp6S9E2F+SpOSLo7mlNhm6SF0b6XSXqiZNz/k3MVLJf0Y0l7JO2StCx6+kadmkvigeiKZ+dixYuDc2NIeiPwB8BaM7sMKAA3Ewa7e9LMVgKPAV+MHvId4HMWxv1/umT9A8DXzOxS4ErC1awQRtv9JGFukaWEMZKci5WaM+/iXNW5Brgc+Fn0ob6BMFBcEfhetM+/A5ujuSEyZvZYtH4jsElSE7DEzL4PYGYjANHzdZhZZ7T8FNAG/OT8/1nOTZ4XB+fKCdhoZre/aqX0F2P2O9uxZ7Il9wv4cehiyJuVnCu3DfiApAvglfmPX084Xk6OgPpB4CdmdhzolfT2aP2HgMcszOTXKel90XPURePtOzcj+CcW58Yws32S7gR+KClBGPXyY4RJgdZE27oJ30tAGIL63ujN/0Xgj6P1HwK+Kemvouf4/Wn8M5w7Jz4qq3OTJOmEmTVWOg7npoM3KznnnCvjZw7OOefK+JmDc865Ml4cnHPOlfHi4JxzrowXB+ecc2W8ODjnnCvjxcE551yZ/wdkPu2n6aTmngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 351us/sample - loss: 0.3395 - acc: 0.9063\n",
      "Loss: 0.33951914914548087 Accuracy: 0.9063344\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6926 - acc: 0.0986\n",
      "Epoch 00001: val_loss improved from inf to 2.49706, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/001-2.4971.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 2.6925 - acc: 0.0987 - val_loss: 2.4971 - val_acc: 0.1915\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1334 - acc: 0.2959\n",
      "Epoch 00002: val_loss improved from 2.49706 to 2.43781, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/002-2.4378.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 2.1334 - acc: 0.2959 - val_loss: 2.4378 - val_acc: 0.2113\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7012 - acc: 0.4332\n",
      "Epoch 00003: val_loss improved from 2.43781 to 1.39850, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/003-1.3985.hdf5\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 1.7013 - acc: 0.4332 - val_loss: 1.3985 - val_acc: 0.5649\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4533 - acc: 0.5210\n",
      "Epoch 00004: val_loss improved from 1.39850 to 1.23393, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/004-1.2339.hdf5\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 1.4532 - acc: 0.5210 - val_loss: 1.2339 - val_acc: 0.6124\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1743 - acc: 0.6208\n",
      "Epoch 00005: val_loss improved from 1.23393 to 0.86572, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/005-0.8657.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 1.1743 - acc: 0.6208 - val_loss: 0.8657 - val_acc: 0.7431\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9157 - acc: 0.7150\n",
      "Epoch 00006: val_loss improved from 0.86572 to 0.83996, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/006-0.8400.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.9157 - acc: 0.7150 - val_loss: 0.8400 - val_acc: 0.7421\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7639\n",
      "Epoch 00007: val_loss improved from 0.83996 to 0.47860, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/007-0.4786.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.7660 - acc: 0.7639 - val_loss: 0.4786 - val_acc: 0.8649\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6653 - acc: 0.7961\n",
      "Epoch 00008: val_loss improved from 0.47860 to 0.44578, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/008-0.4458.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.6653 - acc: 0.7961 - val_loss: 0.4458 - val_acc: 0.8628\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6099 - acc: 0.8144\n",
      "Epoch 00009: val_loss did not improve from 0.44578\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.6101 - acc: 0.8144 - val_loss: 0.9829 - val_acc: 0.7547\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8102\n",
      "Epoch 00010: val_loss improved from 0.44578 to 0.35607, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/010-0.3561.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.6191 - acc: 0.8102 - val_loss: 0.3561 - val_acc: 0.9029\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.8356\n",
      "Epoch 00011: val_loss improved from 0.35607 to 0.34136, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/011-0.3414.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.5361 - acc: 0.8355 - val_loss: 0.3414 - val_acc: 0.9057\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8396\n",
      "Epoch 00012: val_loss did not improve from 0.34136\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5282 - acc: 0.8396 - val_loss: 0.3727 - val_acc: 0.8880\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.8539\n",
      "Epoch 00013: val_loss improved from 0.34136 to 0.29843, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/013-0.2984.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.4846 - acc: 0.8539 - val_loss: 0.2984 - val_acc: 0.9087\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8620\n",
      "Epoch 00014: val_loss improved from 0.29843 to 0.29454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/014-0.2945.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.4608 - acc: 0.8620 - val_loss: 0.2945 - val_acc: 0.9206\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8653\n",
      "Epoch 00015: val_loss improved from 0.29454 to 0.26160, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/015-0.2616.hdf5\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.4458 - acc: 0.8653 - val_loss: 0.2616 - val_acc: 0.9269\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.8691\n",
      "Epoch 00016: val_loss did not improve from 0.26160\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.4331 - acc: 0.8691 - val_loss: 0.3430 - val_acc: 0.9003\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8689\n",
      "Epoch 00017: val_loss improved from 0.26160 to 0.25598, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/017-0.2560.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4237 - acc: 0.8689 - val_loss: 0.2560 - val_acc: 0.9287\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8754\n",
      "Epoch 00018: val_loss improved from 0.25598 to 0.25554, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/018-0.2555.hdf5\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.4056 - acc: 0.8754 - val_loss: 0.2555 - val_acc: 0.9250\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8804\n",
      "Epoch 00019: val_loss improved from 0.25554 to 0.24664, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/019-0.2466.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.3900 - acc: 0.8804 - val_loss: 0.2466 - val_acc: 0.9266\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8826\n",
      "Epoch 00020: val_loss improved from 0.24664 to 0.21945, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/020-0.2194.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3831 - acc: 0.8826 - val_loss: 0.2194 - val_acc: 0.9436\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8834\n",
      "Epoch 00021: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3759 - acc: 0.8834 - val_loss: 0.2566 - val_acc: 0.9248\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8873\n",
      "Epoch 00022: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3622 - acc: 0.8874 - val_loss: 0.2452 - val_acc: 0.9394\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8910\n",
      "Epoch 00023: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3530 - acc: 0.8910 - val_loss: 0.2327 - val_acc: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8910\n",
      "Epoch 00024: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3564 - acc: 0.8910 - val_loss: 0.2207 - val_acc: 0.9380\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8929\n",
      "Epoch 00025: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3420 - acc: 0.8929 - val_loss: 0.2239 - val_acc: 0.9378\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8954\n",
      "Epoch 00026: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3351 - acc: 0.8954 - val_loss: 0.2889 - val_acc: 0.9248\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8994\n",
      "Epoch 00027: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.3256 - acc: 0.8994 - val_loss: 0.2352 - val_acc: 0.9378\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8981\n",
      "Epoch 00028: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3252 - acc: 0.8981 - val_loss: 0.2248 - val_acc: 0.9387\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8982\n",
      "Epoch 00029: val_loss did not improve from 0.21945\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3281 - acc: 0.8982 - val_loss: 0.2244 - val_acc: 0.9420\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9032\n",
      "Epoch 00030: val_loss improved from 0.21945 to 0.21090, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/030-0.2109.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3105 - acc: 0.9032 - val_loss: 0.2109 - val_acc: 0.9399\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9031\n",
      "Epoch 00031: val_loss did not improve from 0.21090\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3128 - acc: 0.9031 - val_loss: 0.2279 - val_acc: 0.9411\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9037\n",
      "Epoch 00032: val_loss improved from 0.21090 to 0.20490, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/032-0.2049.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3050 - acc: 0.9037 - val_loss: 0.2049 - val_acc: 0.9434\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9055\n",
      "Epoch 00033: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3025 - acc: 0.9055 - val_loss: 0.2180 - val_acc: 0.9404\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9062\n",
      "Epoch 00034: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3026 - acc: 0.9062 - val_loss: 0.2389 - val_acc: 0.9378\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9055\n",
      "Epoch 00035: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3019 - acc: 0.9055 - val_loss: 0.2399 - val_acc: 0.9457\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9068\n",
      "Epoch 00036: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2966 - acc: 0.9068 - val_loss: 0.2374 - val_acc: 0.9404\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9081\n",
      "Epoch 00037: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2920 - acc: 0.9080 - val_loss: 0.2648 - val_acc: 0.9273\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9053\n",
      "Epoch 00038: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3049 - acc: 0.9053 - val_loss: 0.2304 - val_acc: 0.9387\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9117\n",
      "Epoch 00039: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2840 - acc: 0.9118 - val_loss: 0.2254 - val_acc: 0.9446\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9089\n",
      "Epoch 00040: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2905 - acc: 0.9089 - val_loss: 0.2516 - val_acc: 0.9348\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9085\n",
      "Epoch 00041: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2912 - acc: 0.9085 - val_loss: 0.2089 - val_acc: 0.9425\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9131\n",
      "Epoch 00042: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2781 - acc: 0.9131 - val_loss: 0.2275 - val_acc: 0.9441\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9104\n",
      "Epoch 00043: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2844 - acc: 0.9104 - val_loss: 0.3147 - val_acc: 0.9138\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9076\n",
      "Epoch 00044: val_loss did not improve from 0.20490\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2982 - acc: 0.9076 - val_loss: 0.2166 - val_acc: 0.9420\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9125\n",
      "Epoch 00045: val_loss improved from 0.20490 to 0.19373, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/045-0.1937.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2802 - acc: 0.9125 - val_loss: 0.1937 - val_acc: 0.9474\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9155\n",
      "Epoch 00046: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2682 - acc: 0.9155 - val_loss: 0.2073 - val_acc: 0.9453\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9166\n",
      "Epoch 00047: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2688 - acc: 0.9166 - val_loss: 0.2129 - val_acc: 0.9432\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9169\n",
      "Epoch 00048: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2657 - acc: 0.9169 - val_loss: 0.2288 - val_acc: 0.9415\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9151\n",
      "Epoch 00049: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2696 - acc: 0.9151 - val_loss: 0.2133 - val_acc: 0.9460\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9155\n",
      "Epoch 00050: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2731 - acc: 0.9154 - val_loss: 0.2277 - val_acc: 0.9404\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9160\n",
      "Epoch 00051: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2715 - acc: 0.9160 - val_loss: 0.2227 - val_acc: 0.9397\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9153\n",
      "Epoch 00052: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2700 - acc: 0.9153 - val_loss: 0.2238 - val_acc: 0.9453\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9208\n",
      "Epoch 00053: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2542 - acc: 0.9208 - val_loss: 0.2772 - val_acc: 0.9301\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9152\n",
      "Epoch 00054: val_loss did not improve from 0.19373\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2718 - acc: 0.9152 - val_loss: 0.2253 - val_acc: 0.9432\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9177\n",
      "Epoch 00055: val_loss improved from 0.19373 to 0.18857, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv_checkpoint/055-0.1886.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2643 - acc: 0.9177 - val_loss: 0.1886 - val_acc: 0.9471\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9201\n",
      "Epoch 00056: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2556 - acc: 0.9201 - val_loss: 0.2805 - val_acc: 0.9269\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9145\n",
      "Epoch 00057: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2742 - acc: 0.9144 - val_loss: 0.2191 - val_acc: 0.9476\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9180\n",
      "Epoch 00058: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2558 - acc: 0.9180 - val_loss: 0.2105 - val_acc: 0.9443\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9204\n",
      "Epoch 00059: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2591 - acc: 0.9204 - val_loss: 0.2356 - val_acc: 0.9390\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9205\n",
      "Epoch 00060: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2559 - acc: 0.9205 - val_loss: 0.2357 - val_acc: 0.9383\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9234\n",
      "Epoch 00061: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2497 - acc: 0.9234 - val_loss: 0.2301 - val_acc: 0.9404\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9212\n",
      "Epoch 00062: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2501 - acc: 0.9212 - val_loss: 0.2221 - val_acc: 0.9429\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9204\n",
      "Epoch 00063: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2540 - acc: 0.9204 - val_loss: 0.2486 - val_acc: 0.9392\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9227\n",
      "Epoch 00064: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2520 - acc: 0.9228 - val_loss: 0.2225 - val_acc: 0.9394\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9244\n",
      "Epoch 00065: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2427 - acc: 0.9244 - val_loss: 0.2747 - val_acc: 0.9327\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9193\n",
      "Epoch 00066: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2564 - acc: 0.9193 - val_loss: 0.2543 - val_acc: 0.9392\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9234\n",
      "Epoch 00067: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2430 - acc: 0.9234 - val_loss: 0.2266 - val_acc: 0.9429\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9223\n",
      "Epoch 00068: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2477 - acc: 0.9223 - val_loss: 0.2041 - val_acc: 0.9448\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9271\n",
      "Epoch 00069: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2377 - acc: 0.9271 - val_loss: 0.2310 - val_acc: 0.9422\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9230\n",
      "Epoch 00070: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2384 - acc: 0.9231 - val_loss: 0.2098 - val_acc: 0.9443\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9232\n",
      "Epoch 00071: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2413 - acc: 0.9232 - val_loss: 0.2304 - val_acc: 0.9404\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9219\n",
      "Epoch 00072: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2428 - acc: 0.9219 - val_loss: 0.2016 - val_acc: 0.9481\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9200\n",
      "Epoch 00073: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2550 - acc: 0.9200 - val_loss: 0.2333 - val_acc: 0.9425\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9239\n",
      "Epoch 00074: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2429 - acc: 0.9239 - val_loss: 0.2714 - val_acc: 0.9271\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9138\n",
      "Epoch 00075: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2742 - acc: 0.9138 - val_loss: 0.2074 - val_acc: 0.9436\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9243\n",
      "Epoch 00076: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2412 - acc: 0.9243 - val_loss: 0.2458 - val_acc: 0.9380\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9249\n",
      "Epoch 00077: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2382 - acc: 0.9249 - val_loss: 0.2983 - val_acc: 0.9269\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9224\n",
      "Epoch 00078: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2452 - acc: 0.9224 - val_loss: 0.2304 - val_acc: 0.9385\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9230\n",
      "Epoch 00079: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2437 - acc: 0.9229 - val_loss: 0.2179 - val_acc: 0.9376\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9178\n",
      "Epoch 00080: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2582 - acc: 0.9178 - val_loss: 0.2443 - val_acc: 0.9373\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9178\n",
      "Epoch 00081: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2635 - acc: 0.9178 - val_loss: 0.2230 - val_acc: 0.9411\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9236\n",
      "Epoch 00082: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2422 - acc: 0.9236 - val_loss: 0.2437 - val_acc: 0.9439\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9232\n",
      "Epoch 00083: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2448 - acc: 0.9232 - val_loss: 0.2088 - val_acc: 0.9485\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9258\n",
      "Epoch 00084: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2302 - acc: 0.9257 - val_loss: 0.2318 - val_acc: 0.9471\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9256\n",
      "Epoch 00085: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2418 - acc: 0.9256 - val_loss: 0.1997 - val_acc: 0.9492\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9254\n",
      "Epoch 00086: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2347 - acc: 0.9254 - val_loss: 0.1990 - val_acc: 0.9513\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9254\n",
      "Epoch 00087: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2333 - acc: 0.9254 - val_loss: 0.1980 - val_acc: 0.9490\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9260\n",
      "Epoch 00088: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.2388 - acc: 0.9260 - val_loss: 0.2248 - val_acc: 0.9434\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9280\n",
      "Epoch 00089: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2297 - acc: 0.9280 - val_loss: 0.2256 - val_acc: 0.9448\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9262\n",
      "Epoch 00090: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2380 - acc: 0.9263 - val_loss: 0.1961 - val_acc: 0.9460\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9275\n",
      "Epoch 00091: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2329 - acc: 0.9275 - val_loss: 0.2012 - val_acc: 0.9441\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9280\n",
      "Epoch 00092: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2269 - acc: 0.9281 - val_loss: 0.1946 - val_acc: 0.9474\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9290\n",
      "Epoch 00093: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2263 - acc: 0.9291 - val_loss: 0.2184 - val_acc: 0.9441\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9279\n",
      "Epoch 00094: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2240 - acc: 0.9279 - val_loss: 0.2201 - val_acc: 0.9492\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9285\n",
      "Epoch 00095: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2287 - acc: 0.9285 - val_loss: 0.1912 - val_acc: 0.9511\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9295\n",
      "Epoch 00096: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2259 - acc: 0.9295 - val_loss: 0.2190 - val_acc: 0.9434\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9291\n",
      "Epoch 00097: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2255 - acc: 0.9291 - val_loss: 0.2785 - val_acc: 0.9257\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9168\n",
      "Epoch 00098: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2690 - acc: 0.9168 - val_loss: 0.2260 - val_acc: 0.9455\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9249\n",
      "Epoch 00099: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2392 - acc: 0.9249 - val_loss: 0.2404 - val_acc: 0.9411\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9264\n",
      "Epoch 00100: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2343 - acc: 0.9264 - val_loss: 0.2398 - val_acc: 0.9401\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9289\n",
      "Epoch 00101: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2270 - acc: 0.9288 - val_loss: 0.2343 - val_acc: 0.9404\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9261\n",
      "Epoch 00102: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2335 - acc: 0.9261 - val_loss: 0.2347 - val_acc: 0.9439\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9264\n",
      "Epoch 00103: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2361 - acc: 0.9264 - val_loss: 0.2202 - val_acc: 0.9411\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9299\n",
      "Epoch 00104: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2214 - acc: 0.9298 - val_loss: 0.2389 - val_acc: 0.9352\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9248\n",
      "Epoch 00105: val_loss did not improve from 0.18857\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2457 - acc: 0.9248 - val_loss: 0.2030 - val_acc: 0.9457\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 350us/sample - loss: 0.2312 - acc: 0.9352\n",
      "Loss: 0.23115783570835277 Accuracy: 0.9352025\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7053 - acc: 0.0954\n",
      "Epoch 00001: val_loss improved from inf to 2.47142, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/001-2.4714.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.7052 - acc: 0.0954 - val_loss: 2.4714 - val_acc: 0.1824\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2151 - acc: 0.2448\n",
      "Epoch 00002: val_loss improved from 2.47142 to 2.02286, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/002-2.0229.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 2.2152 - acc: 0.2448 - val_loss: 2.0229 - val_acc: 0.3312\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5676 - acc: 0.4722\n",
      "Epoch 00003: val_loss improved from 2.02286 to 1.45736, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/003-1.4574.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 1.5677 - acc: 0.4721 - val_loss: 1.4574 - val_acc: 0.5399\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1964 - acc: 0.6039\n",
      "Epoch 00004: val_loss improved from 1.45736 to 0.62176, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/004-0.6218.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 1.1963 - acc: 0.6040 - val_loss: 0.6218 - val_acc: 0.8202\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7183\n",
      "Epoch 00005: val_loss did not improve from 0.62176\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.8963 - acc: 0.7183 - val_loss: 0.6572 - val_acc: 0.8127\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7154 - acc: 0.7770\n",
      "Epoch 00006: val_loss improved from 0.62176 to 0.40986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/006-0.4099.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.7153 - acc: 0.7771 - val_loss: 0.4099 - val_acc: 0.8791\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.8139\n",
      "Epoch 00007: val_loss improved from 0.40986 to 0.35169, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/007-0.3517.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.6041 - acc: 0.8139 - val_loss: 0.3517 - val_acc: 0.9017\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8390\n",
      "Epoch 00008: val_loss did not improve from 0.35169\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.5262 - acc: 0.8389 - val_loss: 0.4148 - val_acc: 0.8668\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8491\n",
      "Epoch 00009: val_loss did not improve from 0.35169\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.4951 - acc: 0.8490 - val_loss: 0.4931 - val_acc: 0.8556\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8613\n",
      "Epoch 00010: val_loss improved from 0.35169 to 0.25704, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/010-0.2570.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.4572 - acc: 0.8613 - val_loss: 0.2570 - val_acc: 0.9203\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8682\n",
      "Epoch 00011: val_loss did not improve from 0.25704\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.4264 - acc: 0.8682 - val_loss: 0.2992 - val_acc: 0.9143\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8799\n",
      "Epoch 00012: val_loss improved from 0.25704 to 0.23080, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/012-0.2308.hdf5\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.3948 - acc: 0.8799 - val_loss: 0.2308 - val_acc: 0.9352\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8882\n",
      "Epoch 00013: val_loss improved from 0.23080 to 0.17761, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/013-0.1776.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3662 - acc: 0.8882 - val_loss: 0.1776 - val_acc: 0.9488\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8954\n",
      "Epoch 00014: val_loss did not improve from 0.17761\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3439 - acc: 0.8954 - val_loss: 0.1807 - val_acc: 0.9499\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9001\n",
      "Epoch 00015: val_loss did not improve from 0.17761\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3321 - acc: 0.9001 - val_loss: 0.2020 - val_acc: 0.9443\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8997\n",
      "Epoch 00016: val_loss did not improve from 0.17761\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3254 - acc: 0.8997 - val_loss: 0.1882 - val_acc: 0.9483\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9051\n",
      "Epoch 00017: val_loss improved from 0.17761 to 0.17124, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/017-0.1712.hdf5\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.3157 - acc: 0.9051 - val_loss: 0.1712 - val_acc: 0.9506\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9099\n",
      "Epoch 00018: val_loss did not improve from 0.17124\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2957 - acc: 0.9099 - val_loss: 0.2123 - val_acc: 0.9443\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9137\n",
      "Epoch 00019: val_loss did not improve from 0.17124\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2825 - acc: 0.9137 - val_loss: 0.1998 - val_acc: 0.9441\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9127\n",
      "Epoch 00020: val_loss did not improve from 0.17124\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2800 - acc: 0.9127 - val_loss: 0.2523 - val_acc: 0.9408\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9174\n",
      "Epoch 00021: val_loss did not improve from 0.17124\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2687 - acc: 0.9174 - val_loss: 0.2179 - val_acc: 0.9392\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9181\n",
      "Epoch 00022: val_loss improved from 0.17124 to 0.16788, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/022-0.1679.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2647 - acc: 0.9181 - val_loss: 0.1679 - val_acc: 0.9506\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9231\n",
      "Epoch 00023: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2524 - acc: 0.9231 - val_loss: 0.1879 - val_acc: 0.9478\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9233\n",
      "Epoch 00024: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2507 - acc: 0.9233 - val_loss: 0.1788 - val_acc: 0.9515\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9215\n",
      "Epoch 00025: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2492 - acc: 0.9215 - val_loss: 0.2273 - val_acc: 0.9413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9209\n",
      "Epoch 00026: val_loss improved from 0.16788 to 0.16628, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/026-0.1663.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2569 - acc: 0.9209 - val_loss: 0.1663 - val_acc: 0.9511\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9287\n",
      "Epoch 00027: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2316 - acc: 0.9287 - val_loss: 0.1714 - val_acc: 0.9529\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9276\n",
      "Epoch 00028: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.2323 - acc: 0.9276 - val_loss: 0.1681 - val_acc: 0.9483\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9331\n",
      "Epoch 00029: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2206 - acc: 0.9331 - val_loss: 0.1959 - val_acc: 0.9478\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9288\n",
      "Epoch 00030: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.2291 - acc: 0.9288 - val_loss: 0.1781 - val_acc: 0.9490\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9340\n",
      "Epoch 00031: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2129 - acc: 0.9340 - val_loss: 0.1718 - val_acc: 0.9539\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9332\n",
      "Epoch 00032: val_loss did not improve from 0.16628\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2120 - acc: 0.9332 - val_loss: 0.1825 - val_acc: 0.9522\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9332\n",
      "Epoch 00033: val_loss improved from 0.16628 to 0.14794, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv_checkpoint/033-0.1479.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2152 - acc: 0.9332 - val_loss: 0.1479 - val_acc: 0.9576\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9354\n",
      "Epoch 00034: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2083 - acc: 0.9354 - val_loss: 0.1805 - val_acc: 0.9499\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9350\n",
      "Epoch 00035: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2059 - acc: 0.9350 - val_loss: 0.1671 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9370\n",
      "Epoch 00036: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2034 - acc: 0.9370 - val_loss: 0.1790 - val_acc: 0.9513\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9375\n",
      "Epoch 00037: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2034 - acc: 0.9375 - val_loss: 0.1577 - val_acc: 0.9569\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9388\n",
      "Epoch 00038: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2019 - acc: 0.9388 - val_loss: 0.2372 - val_acc: 0.9418\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9369\n",
      "Epoch 00039: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.2015 - acc: 0.9369 - val_loss: 0.1734 - val_acc: 0.9548\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9393\n",
      "Epoch 00040: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1944 - acc: 0.9393 - val_loss: 0.1799 - val_acc: 0.9513\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9404\n",
      "Epoch 00041: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1915 - acc: 0.9404 - val_loss: 0.1705 - val_acc: 0.9536\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9415\n",
      "Epoch 00042: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1895 - acc: 0.9415 - val_loss: 0.3800 - val_acc: 0.9050\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9283\n",
      "Epoch 00043: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2323 - acc: 0.9283 - val_loss: 0.1956 - val_acc: 0.9469\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9379\n",
      "Epoch 00044: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1967 - acc: 0.9379 - val_loss: 0.1887 - val_acc: 0.9474\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9402\n",
      "Epoch 00045: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1910 - acc: 0.9402 - val_loss: 0.2106 - val_acc: 0.9441\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9407\n",
      "Epoch 00046: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1953 - acc: 0.9407 - val_loss: 0.1868 - val_acc: 0.9509\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9417\n",
      "Epoch 00047: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1919 - acc: 0.9417 - val_loss: 0.1878 - val_acc: 0.9474\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9435\n",
      "Epoch 00048: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1865 - acc: 0.9435 - val_loss: 0.1747 - val_acc: 0.9546\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9396\n",
      "Epoch 00049: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1939 - acc: 0.9397 - val_loss: 0.1778 - val_acc: 0.9567\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9428\n",
      "Epoch 00050: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1820 - acc: 0.9428 - val_loss: 0.1819 - val_acc: 0.9553\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9439\n",
      "Epoch 00051: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1789 - acc: 0.9439 - val_loss: 0.1842 - val_acc: 0.9511\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9458\n",
      "Epoch 00052: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1782 - acc: 0.9458 - val_loss: 0.1913 - val_acc: 0.9539\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9471\n",
      "Epoch 00053: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1747 - acc: 0.9471 - val_loss: 0.1985 - val_acc: 0.9525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9438\n",
      "Epoch 00054: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.1801 - acc: 0.9438 - val_loss: 0.2749 - val_acc: 0.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9359\n",
      "Epoch 00055: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2088 - acc: 0.9359 - val_loss: 0.1821 - val_acc: 0.9532\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9442\n",
      "Epoch 00056: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1841 - acc: 0.9442 - val_loss: 0.2040 - val_acc: 0.9522\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9427\n",
      "Epoch 00057: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1852 - acc: 0.9427 - val_loss: 0.1862 - val_acc: 0.9522\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9435\n",
      "Epoch 00058: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1850 - acc: 0.9434 - val_loss: 0.2225 - val_acc: 0.9439\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9397\n",
      "Epoch 00059: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.2004 - acc: 0.9397 - val_loss: 0.1905 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9433\n",
      "Epoch 00060: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1846 - acc: 0.9433 - val_loss: 0.1780 - val_acc: 0.9527\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9438\n",
      "Epoch 00061: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1848 - acc: 0.9438 - val_loss: 0.2102 - val_acc: 0.9413\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9470\n",
      "Epoch 00062: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1714 - acc: 0.9470 - val_loss: 0.1864 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9461\n",
      "Epoch 00063: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1753 - acc: 0.9461 - val_loss: 0.1974 - val_acc: 0.9525\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9456\n",
      "Epoch 00064: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1786 - acc: 0.9456 - val_loss: 0.1882 - val_acc: 0.9553\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9460\n",
      "Epoch 00065: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1750 - acc: 0.9460 - val_loss: 0.2301 - val_acc: 0.9408\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9469\n",
      "Epoch 00066: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1773 - acc: 0.9469 - val_loss: 0.1943 - val_acc: 0.9543\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9482\n",
      "Epoch 00067: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1668 - acc: 0.9482 - val_loss: 0.2035 - val_acc: 0.9492\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9449\n",
      "Epoch 00068: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1804 - acc: 0.9450 - val_loss: 0.2198 - val_acc: 0.9492\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9459\n",
      "Epoch 00069: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1795 - acc: 0.9459 - val_loss: 0.1810 - val_acc: 0.9539\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9452\n",
      "Epoch 00070: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1730 - acc: 0.9452 - val_loss: 0.1652 - val_acc: 0.9578\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9508\n",
      "Epoch 00071: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1586 - acc: 0.9508 - val_loss: 0.1689 - val_acc: 0.9576\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9486\n",
      "Epoch 00072: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1653 - acc: 0.9486 - val_loss: 0.1916 - val_acc: 0.9522\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9492\n",
      "Epoch 00073: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.1664 - acc: 0.9492 - val_loss: 0.2152 - val_acc: 0.9460\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9448\n",
      "Epoch 00074: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1824 - acc: 0.9448 - val_loss: 0.2103 - val_acc: 0.9481\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9466\n",
      "Epoch 00075: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1718 - acc: 0.9466 - val_loss: 0.1924 - val_acc: 0.9529\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9456\n",
      "Epoch 00076: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1776 - acc: 0.9456 - val_loss: 0.2034 - val_acc: 0.9509\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9493\n",
      "Epoch 00077: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1666 - acc: 0.9493 - val_loss: 0.2169 - val_acc: 0.9467\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9456\n",
      "Epoch 00078: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.1749 - acc: 0.9456 - val_loss: 0.1888 - val_acc: 0.9502\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9504\n",
      "Epoch 00079: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.1675 - acc: 0.9504 - val_loss: 0.2539 - val_acc: 0.9462\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9449\n",
      "Epoch 00080: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1785 - acc: 0.9449 - val_loss: 0.1865 - val_acc: 0.9539\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9470\n",
      "Epoch 00081: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1759 - acc: 0.9470 - val_loss: 0.2082 - val_acc: 0.9541\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9465\n",
      "Epoch 00082: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.1756 - acc: 0.9464 - val_loss: 0.2518 - val_acc: 0.9411\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9454\n",
      "Epoch 00083: val_loss did not improve from 0.14794\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.1819 - acc: 0.9454 - val_loss: 0.2102 - val_acc: 0.9506\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 353us/sample - loss: 0.2026 - acc: 0.9435\n",
      "Loss: 0.20264555334982967 Accuracy: 0.9435099\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7088 - acc: 0.0950\n",
      "Epoch 00001: val_loss improved from inf to 2.49728, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/001-2.4973.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 2.7087 - acc: 0.0950 - val_loss: 2.4973 - val_acc: 0.2080\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9313 - acc: 0.3667\n",
      "Epoch 00002: val_loss improved from 2.49728 to 1.56255, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/002-1.5625.hdf5\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 1.9313 - acc: 0.3667 - val_loss: 1.5625 - val_acc: 0.5008\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2426 - acc: 0.5937\n",
      "Epoch 00003: val_loss improved from 1.56255 to 0.97577, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/003-0.9758.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 1.2425 - acc: 0.5937 - val_loss: 0.9758 - val_acc: 0.6956\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9820 - acc: 0.6857\n",
      "Epoch 00004: val_loss improved from 0.97577 to 0.82088, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/004-0.8209.hdf5\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.9819 - acc: 0.6857 - val_loss: 0.8209 - val_acc: 0.7400\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7418 - acc: 0.7678\n",
      "Epoch 00005: val_loss did not improve from 0.82088\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.7418 - acc: 0.7678 - val_loss: 1.3204 - val_acc: 0.6252\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8363\n",
      "Epoch 00006: val_loss improved from 0.82088 to 0.30292, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/006-0.3029.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5421 - acc: 0.8362 - val_loss: 0.3029 - val_acc: 0.9064\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.8742\n",
      "Epoch 00007: val_loss improved from 0.30292 to 0.24912, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/007-0.2491.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4308 - acc: 0.8742 - val_loss: 0.2491 - val_acc: 0.9266\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8927\n",
      "Epoch 00008: val_loss did not improve from 0.24912\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.3631 - acc: 0.8927 - val_loss: 0.2552 - val_acc: 0.9269\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.9053\n",
      "Epoch 00009: val_loss did not improve from 0.24912\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.3291 - acc: 0.9053 - val_loss: 0.2613 - val_acc: 0.9231\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9140\n",
      "Epoch 00010: val_loss improved from 0.24912 to 0.18741, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/010-0.1874.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2934 - acc: 0.9140 - val_loss: 0.1874 - val_acc: 0.9441\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9216\n",
      "Epoch 00011: val_loss did not improve from 0.18741\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2608 - acc: 0.9216 - val_loss: 0.2407 - val_acc: 0.9283\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9271\n",
      "Epoch 00012: val_loss improved from 0.18741 to 0.17006, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/012-0.1701.hdf5\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.2457 - acc: 0.9271 - val_loss: 0.1701 - val_acc: 0.9474\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9308\n",
      "Epoch 00013: val_loss did not improve from 0.17006\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2316 - acc: 0.9309 - val_loss: 0.2350 - val_acc: 0.9345\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9358\n",
      "Epoch 00014: val_loss improved from 0.17006 to 0.16024, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/014-0.1602.hdf5\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2138 - acc: 0.9358 - val_loss: 0.1602 - val_acc: 0.9518\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9388\n",
      "Epoch 00015: val_loss did not improve from 0.16024\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.2030 - acc: 0.9388 - val_loss: 0.1820 - val_acc: 0.9499\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9423\n",
      "Epoch 00016: val_loss did not improve from 0.16024\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1929 - acc: 0.9423 - val_loss: 0.1680 - val_acc: 0.9548\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9453\n",
      "Epoch 00017: val_loss did not improve from 0.16024\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1830 - acc: 0.9453 - val_loss: 0.1830 - val_acc: 0.9485\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9458\n",
      "Epoch 00018: val_loss did not improve from 0.16024\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1805 - acc: 0.9458 - val_loss: 0.1729 - val_acc: 0.9536\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9460\n",
      "Epoch 00019: val_loss did not improve from 0.16024\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1808 - acc: 0.9460 - val_loss: 0.1751 - val_acc: 0.9481\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9493\n",
      "Epoch 00020: val_loss improved from 0.16024 to 0.15706, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/020-0.1571.hdf5\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1690 - acc: 0.9493 - val_loss: 0.1571 - val_acc: 0.9539\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9511\n",
      "Epoch 00021: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1621 - acc: 0.9511 - val_loss: 0.1770 - val_acc: 0.9485\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9514\n",
      "Epoch 00022: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1594 - acc: 0.9514 - val_loss: 0.1734 - val_acc: 0.9557\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9541\n",
      "Epoch 00023: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1507 - acc: 0.9541 - val_loss: 0.1771 - val_acc: 0.9592\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9548\n",
      "Epoch 00024: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1525 - acc: 0.9548 - val_loss: 0.1583 - val_acc: 0.9562\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9562\n",
      "Epoch 00025: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1452 - acc: 0.9562 - val_loss: 0.1910 - val_acc: 0.9536\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9549\n",
      "Epoch 00026: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1501 - acc: 0.9549 - val_loss: 0.1958 - val_acc: 0.9404\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9547\n",
      "Epoch 00027: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1522 - acc: 0.9547 - val_loss: 0.2075 - val_acc: 0.9483\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9561\n",
      "Epoch 00028: val_loss improved from 0.15706 to 0.14665, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv_checkpoint/028-0.1467.hdf5\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1463 - acc: 0.9561 - val_loss: 0.1467 - val_acc: 0.9597\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9595\n",
      "Epoch 00029: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1325 - acc: 0.9595 - val_loss: 0.1958 - val_acc: 0.9548\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9586\n",
      "Epoch 00030: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1391 - acc: 0.9585 - val_loss: 0.1646 - val_acc: 0.9539\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9601\n",
      "Epoch 00031: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1369 - acc: 0.9601 - val_loss: 0.1691 - val_acc: 0.9555\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9602\n",
      "Epoch 00032: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1343 - acc: 0.9602 - val_loss: 0.1873 - val_acc: 0.9585\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9588\n",
      "Epoch 00033: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1382 - acc: 0.9588 - val_loss: 0.1791 - val_acc: 0.9590\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9595\n",
      "Epoch 00034: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1318 - acc: 0.9595 - val_loss: 0.1742 - val_acc: 0.9578\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9621\n",
      "Epoch 00035: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1301 - acc: 0.9621 - val_loss: 0.1813 - val_acc: 0.9560\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9630\n",
      "Epoch 00036: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1202 - acc: 0.9630 - val_loss: 0.1906 - val_acc: 0.9509\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9620\n",
      "Epoch 00037: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.1264 - acc: 0.9620 - val_loss: 0.2239 - val_acc: 0.9495\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9611\n",
      "Epoch 00038: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1285 - acc: 0.9611 - val_loss: 0.2114 - val_acc: 0.9534\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9650\n",
      "Epoch 00039: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1202 - acc: 0.9650 - val_loss: 0.1706 - val_acc: 0.9590\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9635\n",
      "Epoch 00040: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1228 - acc: 0.9635 - val_loss: 0.1828 - val_acc: 0.9557\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9662\n",
      "Epoch 00041: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1135 - acc: 0.9662 - val_loss: 0.2278 - val_acc: 0.9478\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9629\n",
      "Epoch 00042: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.1251 - acc: 0.9629 - val_loss: 0.1983 - val_acc: 0.9532\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9632\n",
      "Epoch 00043: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1195 - acc: 0.9632 - val_loss: 0.2194 - val_acc: 0.9555\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9667\n",
      "Epoch 00044: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1139 - acc: 0.9667 - val_loss: 0.1821 - val_acc: 0.9576\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9655\n",
      "Epoch 00045: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1164 - acc: 0.9655 - val_loss: 0.2438 - val_acc: 0.9390\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9624\n",
      "Epoch 00046: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1236 - acc: 0.9623 - val_loss: 0.1889 - val_acc: 0.9539\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9622\n",
      "Epoch 00047: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1280 - acc: 0.9622 - val_loss: 0.1975 - val_acc: 0.9504\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9644\n",
      "Epoch 00048: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.1244 - acc: 0.9644 - val_loss: 0.2095 - val_acc: 0.9553\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9643\n",
      "Epoch 00049: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1221 - acc: 0.9643 - val_loss: 0.2704 - val_acc: 0.9427\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9654\n",
      "Epoch 00050: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1186 - acc: 0.9654 - val_loss: 0.2336 - val_acc: 0.9511\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9634\n",
      "Epoch 00051: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1237 - acc: 0.9634 - val_loss: 0.1985 - val_acc: 0.9527\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9630\n",
      "Epoch 00052: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1281 - acc: 0.9630 - val_loss: 0.5069 - val_acc: 0.8975\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9416\n",
      "Epoch 00053: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.2076 - acc: 0.9416 - val_loss: 0.2184 - val_acc: 0.9522\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9579\n",
      "Epoch 00054: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1441 - acc: 0.9579 - val_loss: 0.2170 - val_acc: 0.9474\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9642\n",
      "Epoch 00055: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1264 - acc: 0.9642 - val_loss: 0.2117 - val_acc: 0.9534\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9579\n",
      "Epoch 00056: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1442 - acc: 0.9579 - val_loss: 0.2260 - val_acc: 0.9497\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9607\n",
      "Epoch 00057: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1342 - acc: 0.9607 - val_loss: 0.2497 - val_acc: 0.9497\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9637\n",
      "Epoch 00058: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1212 - acc: 0.9637 - val_loss: 0.2151 - val_acc: 0.9497\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9634\n",
      "Epoch 00059: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1248 - acc: 0.9634 - val_loss: 0.2569 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9664\n",
      "Epoch 00060: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1114 - acc: 0.9664 - val_loss: 0.1815 - val_acc: 0.9618\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9636\n",
      "Epoch 00061: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1257 - acc: 0.9636 - val_loss: 0.2335 - val_acc: 0.9502\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9640\n",
      "Epoch 00062: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1196 - acc: 0.9641 - val_loss: 0.2009 - val_acc: 0.9569\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9651\n",
      "Epoch 00063: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 687us/sample - loss: 0.1189 - acc: 0.9651 - val_loss: 0.2313 - val_acc: 0.9499\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9639\n",
      "Epoch 00064: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1201 - acc: 0.9639 - val_loss: 0.2081 - val_acc: 0.9536\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9639\n",
      "Epoch 00065: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1282 - acc: 0.9639 - val_loss: 0.2699 - val_acc: 0.9434\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9655\n",
      "Epoch 00066: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1163 - acc: 0.9655 - val_loss: 0.2230 - val_acc: 0.9522\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9658\n",
      "Epoch 00067: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1211 - acc: 0.9658 - val_loss: 0.2206 - val_acc: 0.9532\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9669\n",
      "Epoch 00068: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1144 - acc: 0.9669 - val_loss: 0.2372 - val_acc: 0.9495\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9673\n",
      "Epoch 00069: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1131 - acc: 0.9673 - val_loss: 0.2669 - val_acc: 0.9471\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9626\n",
      "Epoch 00070: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1298 - acc: 0.9626 - val_loss: 0.8073 - val_acc: 0.8633\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9405\n",
      "Epoch 00071: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2101 - acc: 0.9406 - val_loss: 0.1810 - val_acc: 0.9555\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9583\n",
      "Epoch 00072: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1421 - acc: 0.9583 - val_loss: 0.2314 - val_acc: 0.9464\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9608\n",
      "Epoch 00073: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1331 - acc: 0.9608 - val_loss: 0.2218 - val_acc: 0.9550\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9613\n",
      "Epoch 00074: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1305 - acc: 0.9613 - val_loss: 0.1823 - val_acc: 0.9555\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9636\n",
      "Epoch 00075: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.1235 - acc: 0.9636 - val_loss: 0.2322 - val_acc: 0.9511\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9670\n",
      "Epoch 00076: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 685us/sample - loss: 0.1161 - acc: 0.9670 - val_loss: 0.1735 - val_acc: 0.9574\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9635\n",
      "Epoch 00077: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.1271 - acc: 0.9635 - val_loss: 0.2918 - val_acc: 0.9425\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9628\n",
      "Epoch 00078: val_loss did not improve from 0.14665\n",
      "36805/36805 [==============================] - 25s 684us/sample - loss: 0.1283 - acc: 0.9628 - val_loss: 0.1973 - val_acc: 0.9515\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 378us/sample - loss: 0.2057 - acc: 0.9466\n",
      "Loss: 0.20568351803128904 Accuracy: 0.9466251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO_075_DO_SGD'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 330us/sample - loss: 1.1766 - acc: 0.6762\n",
      "Loss: 1.1766346916355201 Accuracy: 0.6762201\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 351us/sample - loss: 0.8981 - acc: 0.7304\n",
      "Loss: 0.8980849098193683 Accuracy: 0.7304258\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 377us/sample - loss: 0.5932 - acc: 0.8395\n",
      "Loss: 0.5932148873125157 Accuracy: 0.83946\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 365us/sample - loss: 0.3395 - acc: 0.9063\n",
      "Loss: 0.33951914914548087 Accuracy: 0.9063344\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 383us/sample - loss: 0.2312 - acc: 0.9352\n",
      "Loss: 0.23115783570835277 Accuracy: 0.9352025\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 394us/sample - loss: 0.2026 - acc: 0.9435\n",
      "Loss: 0.20264555334982967 Accuracy: 0.9435099\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 413us/sample - loss: 0.2057 - acc: 0.9466\n",
      "Loss: 0.20568351803128904 Accuracy: 0.9466251\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO_075_DO_SGD'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 391us/sample - loss: 1.5153 - acc: 0.6440\n",
      "Loss: 1.5152981949248665 Accuracy: 0.6440291\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 380us/sample - loss: 2.7127 - acc: 0.0800\n",
      "Loss: 2.712656770490288 Accuracy: 0.07995846\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 410us/sample - loss: 0.6096 - acc: 0.8444\n",
      "Loss: 0.6095959447873592 Accuracy: 0.84444445\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 418us/sample - loss: 0.3510 - acc: 0.9059\n",
      "Loss: 0.3510391135701014 Accuracy: 0.905919\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 423us/sample - loss: 0.2527 - acc: 0.9306\n",
      "Loss: 0.2526769327628402 Accuracy: 0.9306334\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 456us/sample - loss: 0.2428 - acc: 0.9404\n",
      "Loss: 0.24284628251816995 Accuracy: 0.9403946\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_SGD_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 444us/sample - loss: 0.2891 - acc: 0.9360\n",
      "Loss: 0.28911270132657774 Accuracy: 0.93603325\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
