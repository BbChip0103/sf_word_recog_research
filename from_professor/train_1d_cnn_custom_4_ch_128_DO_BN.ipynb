{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 32,769,296\n",
      "Trainable params: 32,769,040\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 11,005,840\n",
      "Trainable params: 11,005,328\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 813,904\n",
      "Trainable params: 813,008\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 430,224\n",
      "Trainable params: 429,200\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 315,856\n",
      "Trainable params: 314,704\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 270,448\n",
      "Trainable params: 269,232\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 268,560\n",
      "Trainable params: 267,280\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 271,280\n",
      "Trainable params: 269,936\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0458 - acc: 0.3794\n",
      "Epoch 00001: val_loss improved from inf to 1.50821, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/001-1.5082.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 2.0458 - acc: 0.3794 - val_loss: 1.5082 - val_acc: 0.5306\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3492 - acc: 0.5758\n",
      "Epoch 00002: val_loss improved from 1.50821 to 1.12909, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/002-1.1291.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 1.3495 - acc: 0.5758 - val_loss: 1.1291 - val_acc: 0.6671\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1416 - acc: 0.6451\n",
      "Epoch 00003: val_loss improved from 1.12909 to 1.04662, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/003-1.0466.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 1.1416 - acc: 0.6450 - val_loss: 1.0466 - val_acc: 0.6748\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0039 - acc: 0.6889\n",
      "Epoch 00004: val_loss improved from 1.04662 to 0.96214, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/004-0.9621.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 1.0040 - acc: 0.6888 - val_loss: 0.9621 - val_acc: 0.7142\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9063 - acc: 0.7176\n",
      "Epoch 00005: val_loss did not improve from 0.96214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.9063 - acc: 0.7175 - val_loss: 0.9672 - val_acc: 0.7102\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8171 - acc: 0.7486\n",
      "Epoch 00006: val_loss did not improve from 0.96214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.8171 - acc: 0.7486 - val_loss: 1.0150 - val_acc: 0.7002\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7412 - acc: 0.7721\n",
      "Epoch 00007: val_loss did not improve from 0.96214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.7413 - acc: 0.7720 - val_loss: 1.1505 - val_acc: 0.6613\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.7896\n",
      "Epoch 00008: val_loss improved from 0.96214 to 0.90298, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/008-0.9030.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.6860 - acc: 0.7896 - val_loss: 0.9030 - val_acc: 0.7417\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8070\n",
      "Epoch 00009: val_loss improved from 0.90298 to 0.89796, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/009-0.8980.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.6206 - acc: 0.8070 - val_loss: 0.8980 - val_acc: 0.7400\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.8184\n",
      "Epoch 00010: val_loss improved from 0.89796 to 0.87091, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/010-0.8709.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5719 - acc: 0.8184 - val_loss: 0.8709 - val_acc: 0.7552\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.8315\n",
      "Epoch 00011: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5331 - acc: 0.8315 - val_loss: 1.0088 - val_acc: 0.7130\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8430\n",
      "Epoch 00012: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5003 - acc: 0.8430 - val_loss: 0.8766 - val_acc: 0.7489\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8586\n",
      "Epoch 00013: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4544 - acc: 0.8586 - val_loss: 0.9031 - val_acc: 0.7456\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8692\n",
      "Epoch 00014: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4201 - acc: 0.8692 - val_loss: 1.0930 - val_acc: 0.7063\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8738\n",
      "Epoch 00015: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3978 - acc: 0.8738 - val_loss: 0.8954 - val_acc: 0.7510\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8834\n",
      "Epoch 00016: val_loss did not improve from 0.87091\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3672 - acc: 0.8834 - val_loss: 0.9612 - val_acc: 0.7407\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8893\n",
      "Epoch 00017: val_loss improved from 0.87091 to 0.85214, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_4_conv_checkpoint/017-0.8521.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3457 - acc: 0.8893 - val_loss: 0.8521 - val_acc: 0.7699\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8969\n",
      "Epoch 00018: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3266 - acc: 0.8969 - val_loss: 0.9618 - val_acc: 0.7419\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9008\n",
      "Epoch 00019: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3092 - acc: 0.9009 - val_loss: 0.8607 - val_acc: 0.7813\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9041\n",
      "Epoch 00020: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2967 - acc: 0.9041 - val_loss: 0.8896 - val_acc: 0.7734\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9125\n",
      "Epoch 00021: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2744 - acc: 0.9125 - val_loss: 0.9537 - val_acc: 0.7603\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9132\n",
      "Epoch 00022: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2699 - acc: 0.9132 - val_loss: 0.9788 - val_acc: 0.7496\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9185\n",
      "Epoch 00023: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2506 - acc: 0.9184 - val_loss: 0.9037 - val_acc: 0.7741\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9244\n",
      "Epoch 00024: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2375 - acc: 0.9244 - val_loss: 0.9521 - val_acc: 0.7624\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9267\n",
      "Epoch 00025: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2277 - acc: 0.9267 - val_loss: 0.9302 - val_acc: 0.7750\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9302\n",
      "Epoch 00026: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2159 - acc: 0.9302 - val_loss: 1.0859 - val_acc: 0.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9315\n",
      "Epoch 00027: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2102 - acc: 0.9316 - val_loss: 0.9263 - val_acc: 0.7717\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9324\n",
      "Epoch 00028: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2058 - acc: 0.9324 - val_loss: 1.0032 - val_acc: 0.7615\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9349\n",
      "Epoch 00029: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2001 - acc: 0.9349 - val_loss: 0.9478 - val_acc: 0.7729\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9369\n",
      "Epoch 00030: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1932 - acc: 0.9369 - val_loss: 1.0328 - val_acc: 0.7615\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9423\n",
      "Epoch 00031: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1815 - acc: 0.9422 - val_loss: 0.8928 - val_acc: 0.7871\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9423\n",
      "Epoch 00032: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1753 - acc: 0.9423 - val_loss: 0.9974 - val_acc: 0.7699\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9451\n",
      "Epoch 00033: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1715 - acc: 0.9451 - val_loss: 1.1504 - val_acc: 0.7466\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9473\n",
      "Epoch 00034: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1662 - acc: 0.9473 - val_loss: 0.9524 - val_acc: 0.7787\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9482\n",
      "Epoch 00035: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1592 - acc: 0.9482 - val_loss: 1.1026 - val_acc: 0.7505\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9483\n",
      "Epoch 00036: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1591 - acc: 0.9483 - val_loss: 0.9992 - val_acc: 0.7782\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9510\n",
      "Epoch 00037: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1527 - acc: 0.9510 - val_loss: 1.1363 - val_acc: 0.7491\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9524\n",
      "Epoch 00038: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1484 - acc: 0.9524 - val_loss: 1.0767 - val_acc: 0.7538\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9525\n",
      "Epoch 00039: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1500 - acc: 0.9525 - val_loss: 1.0460 - val_acc: 0.7629\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9547\n",
      "Epoch 00040: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1425 - acc: 0.9547 - val_loss: 1.0976 - val_acc: 0.7598\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9547\n",
      "Epoch 00041: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1418 - acc: 0.9547 - val_loss: 1.0484 - val_acc: 0.7638\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9574\n",
      "Epoch 00042: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1340 - acc: 0.9575 - val_loss: 1.0228 - val_acc: 0.7734\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9566\n",
      "Epoch 00043: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1342 - acc: 0.9566 - val_loss: 1.0299 - val_acc: 0.7736\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9590\n",
      "Epoch 00044: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1311 - acc: 0.9590 - val_loss: 0.9871 - val_acc: 0.7822\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9599\n",
      "Epoch 00045: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1267 - acc: 0.9598 - val_loss: 1.0184 - val_acc: 0.7794\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9604\n",
      "Epoch 00046: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1236 - acc: 0.9603 - val_loss: 1.5706 - val_acc: 0.6883\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9601\n",
      "Epoch 00047: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1249 - acc: 0.9601 - val_loss: 0.9798 - val_acc: 0.7841\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9628\n",
      "Epoch 00048: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1142 - acc: 0.9628 - val_loss: 1.0845 - val_acc: 0.7661\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9632\n",
      "Epoch 00049: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1181 - acc: 0.9631 - val_loss: 1.0435 - val_acc: 0.7745\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9607\n",
      "Epoch 00050: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1181 - acc: 0.9607 - val_loss: 1.0380 - val_acc: 0.7820\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9646\n",
      "Epoch 00051: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1102 - acc: 0.9647 - val_loss: 0.9907 - val_acc: 0.7908\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9654\n",
      "Epoch 00052: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1115 - acc: 0.9654 - val_loss: 0.9320 - val_acc: 0.7964\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9666\n",
      "Epoch 00053: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1062 - acc: 0.9666 - val_loss: 1.1236 - val_acc: 0.7608\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9681\n",
      "Epoch 00054: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1041 - acc: 0.9681 - val_loss: 1.1043 - val_acc: 0.7722\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9666\n",
      "Epoch 00055: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1057 - acc: 0.9666 - val_loss: 1.0649 - val_acc: 0.7848\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9691\n",
      "Epoch 00056: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0996 - acc: 0.9691 - val_loss: 1.1183 - val_acc: 0.7629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9674\n",
      "Epoch 00057: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1019 - acc: 0.9675 - val_loss: 1.0744 - val_acc: 0.7766\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9701\n",
      "Epoch 00058: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0967 - acc: 0.9701 - val_loss: 1.0334 - val_acc: 0.7897\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9694\n",
      "Epoch 00059: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0973 - acc: 0.9694 - val_loss: 1.1214 - val_acc: 0.7692\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9712\n",
      "Epoch 00060: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0943 - acc: 0.9712 - val_loss: 1.0008 - val_acc: 0.7920\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9707\n",
      "Epoch 00061: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0928 - acc: 0.9707 - val_loss: 1.0991 - val_acc: 0.7799\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9708\n",
      "Epoch 00062: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0941 - acc: 0.9708 - val_loss: 1.0274 - val_acc: 0.7885\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9719\n",
      "Epoch 00063: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0906 - acc: 0.9719 - val_loss: 1.0445 - val_acc: 0.7880\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9717\n",
      "Epoch 00064: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0914 - acc: 0.9717 - val_loss: 1.1085 - val_acc: 0.7734\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9727\n",
      "Epoch 00065: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0869 - acc: 0.9727 - val_loss: 1.2386 - val_acc: 0.7643\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9726\n",
      "Epoch 00066: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0864 - acc: 0.9726 - val_loss: 1.0910 - val_acc: 0.7796\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9719\n",
      "Epoch 00067: val_loss did not improve from 0.85214\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0893 - acc: 0.9719 - val_loss: 1.2394 - val_acc: 0.7582\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.9989 - acc: 0.7242\n",
      "Loss: 0.9988764359324644 Accuracy: 0.72419524\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1012 - acc: 0.3551\n",
      "Epoch 00001: val_loss improved from inf to 1.50765, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/001-1.5076.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 2.1013 - acc: 0.3551 - val_loss: 1.5076 - val_acc: 0.5201\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3343 - acc: 0.5761\n",
      "Epoch 00002: val_loss improved from 1.50765 to 0.99844, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/002-0.9984.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 1.3346 - acc: 0.5761 - val_loss: 0.9984 - val_acc: 0.6979\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1068 - acc: 0.6549\n",
      "Epoch 00003: val_loss improved from 0.99844 to 0.95646, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/003-0.9565.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 1.1069 - acc: 0.6548 - val_loss: 0.9565 - val_acc: 0.7107\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9721 - acc: 0.7034\n",
      "Epoch 00004: val_loss improved from 0.95646 to 0.81201, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/004-0.8120.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.9721 - acc: 0.7034 - val_loss: 0.8120 - val_acc: 0.7638\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8868 - acc: 0.7332\n",
      "Epoch 00005: val_loss did not improve from 0.81201\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.8868 - acc: 0.7332 - val_loss: 0.8279 - val_acc: 0.7508\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8184 - acc: 0.7524\n",
      "Epoch 00006: val_loss did not improve from 0.81201\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.8186 - acc: 0.7523 - val_loss: 0.8219 - val_acc: 0.7554\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7682 - acc: 0.7660\n",
      "Epoch 00007: val_loss improved from 0.81201 to 0.74989, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/007-0.7499.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.7682 - acc: 0.7659 - val_loss: 0.7499 - val_acc: 0.7720\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7243 - acc: 0.7801\n",
      "Epoch 00008: val_loss improved from 0.74989 to 0.72889, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/008-0.7289.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.7242 - acc: 0.7802 - val_loss: 0.7289 - val_acc: 0.7799\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6814 - acc: 0.7942\n",
      "Epoch 00009: val_loss improved from 0.72889 to 0.68406, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/009-0.6841.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.6815 - acc: 0.7942 - val_loss: 0.6841 - val_acc: 0.7920\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.8055\n",
      "Epoch 00010: val_loss did not improve from 0.68406\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.6409 - acc: 0.8055 - val_loss: 0.6955 - val_acc: 0.7936\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.8150\n",
      "Epoch 00011: val_loss did not improve from 0.68406\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.6126 - acc: 0.8150 - val_loss: 0.7001 - val_acc: 0.7901\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.8204\n",
      "Epoch 00012: val_loss improved from 0.68406 to 0.65173, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/012-0.6517.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5950 - acc: 0.8204 - val_loss: 0.6517 - val_acc: 0.8116\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8274\n",
      "Epoch 00013: val_loss improved from 0.65173 to 0.62821, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/013-0.6282.hdf5\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5694 - acc: 0.8274 - val_loss: 0.6282 - val_acc: 0.8318\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.8344\n",
      "Epoch 00014: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5433 - acc: 0.8343 - val_loss: 0.6366 - val_acc: 0.8134\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.8389\n",
      "Epoch 00015: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5267 - acc: 0.8389 - val_loss: 0.6554 - val_acc: 0.8099\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8487\n",
      "Epoch 00016: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.5029 - acc: 0.8486 - val_loss: 0.8332 - val_acc: 0.7573\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8540\n",
      "Epoch 00017: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4862 - acc: 0.8540 - val_loss: 0.6819 - val_acc: 0.8015\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.8583\n",
      "Epoch 00018: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4653 - acc: 0.8583 - val_loss: 0.7086 - val_acc: 0.8011\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.8623\n",
      "Epoch 00019: val_loss did not improve from 0.62821\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4568 - acc: 0.8622 - val_loss: 0.6635 - val_acc: 0.8018\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.8667\n",
      "Epoch 00020: val_loss improved from 0.62821 to 0.58542, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_5_conv_checkpoint/020-0.5854.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.4382 - acc: 0.8667 - val_loss: 0.5854 - val_acc: 0.8307\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8721\n",
      "Epoch 00021: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4233 - acc: 0.8721 - val_loss: 0.6144 - val_acc: 0.8253\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8749\n",
      "Epoch 00022: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.4068 - acc: 0.8750 - val_loss: 0.6752 - val_acc: 0.8085\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8795\n",
      "Epoch 00023: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3982 - acc: 0.8795 - val_loss: 0.6196 - val_acc: 0.8316\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8810\n",
      "Epoch 00024: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3861 - acc: 0.8810 - val_loss: 0.5944 - val_acc: 0.8297\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8851\n",
      "Epoch 00025: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3739 - acc: 0.8851 - val_loss: 0.6429 - val_acc: 0.8209\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8877\n",
      "Epoch 00026: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3650 - acc: 0.8876 - val_loss: 0.6736 - val_acc: 0.8157\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8893\n",
      "Epoch 00027: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3558 - acc: 0.8892 - val_loss: 0.6782 - val_acc: 0.8134\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8948\n",
      "Epoch 00028: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3420 - acc: 0.8949 - val_loss: 0.5957 - val_acc: 0.8369\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8983\n",
      "Epoch 00029: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3274 - acc: 0.8982 - val_loss: 0.6299 - val_acc: 0.8239\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8985\n",
      "Epoch 00030: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3252 - acc: 0.8985 - val_loss: 0.7692 - val_acc: 0.7955\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9034\n",
      "Epoch 00031: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3124 - acc: 0.9034 - val_loss: 0.6141 - val_acc: 0.8325\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9063\n",
      "Epoch 00032: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3065 - acc: 0.9063 - val_loss: 0.6225 - val_acc: 0.8304\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9040\n",
      "Epoch 00033: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.3053 - acc: 0.9040 - val_loss: 0.6070 - val_acc: 0.8362\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9077\n",
      "Epoch 00034: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2933 - acc: 0.9077 - val_loss: 0.6667 - val_acc: 0.8323\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9123\n",
      "Epoch 00035: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2856 - acc: 0.9123 - val_loss: 0.6373 - val_acc: 0.8311\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9115\n",
      "Epoch 00036: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2822 - acc: 0.9115 - val_loss: 0.6211 - val_acc: 0.8272\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9146\n",
      "Epoch 00037: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2711 - acc: 0.9146 - val_loss: 0.5966 - val_acc: 0.8330\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9176\n",
      "Epoch 00038: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2592 - acc: 0.9176 - val_loss: 0.6435 - val_acc: 0.8269\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9165\n",
      "Epoch 00039: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2613 - acc: 0.9165 - val_loss: 0.6273 - val_acc: 0.8297\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9211\n",
      "Epoch 00040: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2542 - acc: 0.9211 - val_loss: 0.6642 - val_acc: 0.8367\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9193\n",
      "Epoch 00041: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2479 - acc: 0.9193 - val_loss: 0.6251 - val_acc: 0.8453\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9224\n",
      "Epoch 00042: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2447 - acc: 0.9223 - val_loss: 0.6278 - val_acc: 0.8321\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9263\n",
      "Epoch 00043: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2360 - acc: 0.9263 - val_loss: 0.6711 - val_acc: 0.8281\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9272\n",
      "Epoch 00044: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2268 - acc: 0.9272 - val_loss: 0.6254 - val_acc: 0.8369\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9260\n",
      "Epoch 00045: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2291 - acc: 0.9259 - val_loss: 0.6044 - val_acc: 0.8397\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9276\n",
      "Epoch 00046: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2251 - acc: 0.9276 - val_loss: 0.6345 - val_acc: 0.8318\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9326\n",
      "Epoch 00047: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2104 - acc: 0.9326 - val_loss: 0.6166 - val_acc: 0.8463\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9337\n",
      "Epoch 00048: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2073 - acc: 0.9337 - val_loss: 0.6139 - val_acc: 0.8435\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9332\n",
      "Epoch 00049: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2093 - acc: 0.9332 - val_loss: 0.6199 - val_acc: 0.8428\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9351\n",
      "Epoch 00050: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2039 - acc: 0.9351 - val_loss: 0.6320 - val_acc: 0.8374\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9347\n",
      "Epoch 00051: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.2031 - acc: 0.9347 - val_loss: 0.6282 - val_acc: 0.8409\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9403\n",
      "Epoch 00052: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1910 - acc: 0.9403 - val_loss: 0.6378 - val_acc: 0.8463\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9384\n",
      "Epoch 00053: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1944 - acc: 0.9384 - val_loss: 0.6418 - val_acc: 0.8383\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9410\n",
      "Epoch 00054: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1827 - acc: 0.9410 - val_loss: 0.7078 - val_acc: 0.8286\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9409\n",
      "Epoch 00055: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1863 - acc: 0.9409 - val_loss: 0.6080 - val_acc: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9433\n",
      "Epoch 00056: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1796 - acc: 0.9433 - val_loss: 0.7361 - val_acc: 0.8106\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9442\n",
      "Epoch 00057: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1753 - acc: 0.9442 - val_loss: 0.6127 - val_acc: 0.8442\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9458\n",
      "Epoch 00058: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1712 - acc: 0.9458 - val_loss: 0.7041 - val_acc: 0.8281\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9452\n",
      "Epoch 00059: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1716 - acc: 0.9452 - val_loss: 0.7097 - val_acc: 0.8185\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9466\n",
      "Epoch 00060: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1701 - acc: 0.9466 - val_loss: 0.6528 - val_acc: 0.8397\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9487\n",
      "Epoch 00061: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1636 - acc: 0.9486 - val_loss: 0.5986 - val_acc: 0.8532\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9470\n",
      "Epoch 00062: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1632 - acc: 0.9470 - val_loss: 0.6322 - val_acc: 0.8458\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9463\n",
      "Epoch 00063: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1675 - acc: 0.9462 - val_loss: 0.6388 - val_acc: 0.8491\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9498\n",
      "Epoch 00064: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1611 - acc: 0.9498 - val_loss: 0.6831 - val_acc: 0.8346\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9509\n",
      "Epoch 00065: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1553 - acc: 0.9509 - val_loss: 0.6401 - val_acc: 0.8493\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9491\n",
      "Epoch 00066: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1556 - acc: 0.9491 - val_loss: 0.6590 - val_acc: 0.8367\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9537\n",
      "Epoch 00067: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1500 - acc: 0.9537 - val_loss: 0.6814 - val_acc: 0.8379\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9535\n",
      "Epoch 00068: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1431 - acc: 0.9535 - val_loss: 0.6116 - val_acc: 0.8507\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9534\n",
      "Epoch 00069: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1458 - acc: 0.9534 - val_loss: 0.6529 - val_acc: 0.8425\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9552\n",
      "Epoch 00070: val_loss did not improve from 0.58542\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.1429 - acc: 0.9552 - val_loss: 0.6353 - val_acc: 0.8477\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.6696 - acc: 0.8010\n",
      "Loss: 0.6695837328367145 Accuracy: 0.80103844\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3584 - acc: 0.2899\n",
      "Epoch 00001: val_loss improved from inf to 1.60045, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/001-1.6004.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 2.3582 - acc: 0.2900 - val_loss: 1.6004 - val_acc: 0.4822\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4838 - acc: 0.5242\n",
      "Epoch 00002: val_loss improved from 1.60045 to 1.01453, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/002-1.0145.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.4839 - acc: 0.5242 - val_loss: 1.0145 - val_acc: 0.6904\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1769 - acc: 0.6273\n",
      "Epoch 00003: val_loss improved from 1.01453 to 0.98990, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/003-0.9899.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.1770 - acc: 0.6273 - val_loss: 0.9899 - val_acc: 0.6914\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0101 - acc: 0.6829\n",
      "Epoch 00004: val_loss improved from 0.98990 to 0.94404, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/004-0.9440.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.0102 - acc: 0.6828 - val_loss: 0.9440 - val_acc: 0.6956\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8974 - acc: 0.7232\n",
      "Epoch 00005: val_loss improved from 0.94404 to 0.77930, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/005-0.7793.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.8975 - acc: 0.7232 - val_loss: 0.7793 - val_acc: 0.7717\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8147 - acc: 0.7509\n",
      "Epoch 00006: val_loss improved from 0.77930 to 0.72107, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/006-0.7211.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.8147 - acc: 0.7509 - val_loss: 0.7211 - val_acc: 0.7871\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7697\n",
      "Epoch 00007: val_loss improved from 0.72107 to 0.70583, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/007-0.7058.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.7515 - acc: 0.7697 - val_loss: 0.7058 - val_acc: 0.8020\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.7895\n",
      "Epoch 00008: val_loss improved from 0.70583 to 0.63914, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/008-0.6391.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.6996 - acc: 0.7895 - val_loss: 0.6391 - val_acc: 0.8169\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6578 - acc: 0.8013\n",
      "Epoch 00009: val_loss improved from 0.63914 to 0.56805, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/009-0.5681.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.6579 - acc: 0.8013 - val_loss: 0.5681 - val_acc: 0.8362\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.8177\n",
      "Epoch 00010: val_loss did not improve from 0.56805\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.6147 - acc: 0.8177 - val_loss: 0.5783 - val_acc: 0.8407\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.8243\n",
      "Epoch 00011: val_loss did not improve from 0.56805\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.5902 - acc: 0.8243 - val_loss: 0.6207 - val_acc: 0.8351\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5589 - acc: 0.8332\n",
      "Epoch 00012: val_loss did not improve from 0.56805\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.5588 - acc: 0.8332 - val_loss: 0.7126 - val_acc: 0.7978\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8460\n",
      "Epoch 00013: val_loss improved from 0.56805 to 0.54240, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/013-0.5424.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.5264 - acc: 0.8460 - val_loss: 0.5424 - val_acc: 0.8514\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8510\n",
      "Epoch 00014: val_loss improved from 0.54240 to 0.51335, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/014-0.5133.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.5043 - acc: 0.8510 - val_loss: 0.5133 - val_acc: 0.8595\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8548\n",
      "Epoch 00015: val_loss did not improve from 0.51335\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4912 - acc: 0.8547 - val_loss: 0.5589 - val_acc: 0.8421\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8622\n",
      "Epoch 00016: val_loss did not improve from 0.51335\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4629 - acc: 0.8621 - val_loss: 0.5422 - val_acc: 0.8425\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8693\n",
      "Epoch 00017: val_loss improved from 0.51335 to 0.46605, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/017-0.4661.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4476 - acc: 0.8693 - val_loss: 0.4661 - val_acc: 0.8686\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8707\n",
      "Epoch 00018: val_loss did not improve from 0.46605\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4331 - acc: 0.8707 - val_loss: 0.5403 - val_acc: 0.8509\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8742\n",
      "Epoch 00019: val_loss improved from 0.46605 to 0.44155, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/019-0.4416.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4199 - acc: 0.8743 - val_loss: 0.4416 - val_acc: 0.8770\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8807\n",
      "Epoch 00020: val_loss did not improve from 0.44155\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4012 - acc: 0.8807 - val_loss: 0.5591 - val_acc: 0.8444\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8849\n",
      "Epoch 00021: val_loss did not improve from 0.44155\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3845 - acc: 0.8849 - val_loss: 0.4467 - val_acc: 0.8749\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8868\n",
      "Epoch 00022: val_loss did not improve from 0.44155\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3767 - acc: 0.8868 - val_loss: 0.5317 - val_acc: 0.8553\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8885\n",
      "Epoch 00023: val_loss improved from 0.44155 to 0.41535, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/023-0.4154.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3683 - acc: 0.8885 - val_loss: 0.4154 - val_acc: 0.8817\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8933\n",
      "Epoch 00024: val_loss did not improve from 0.41535\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3549 - acc: 0.8933 - val_loss: 0.4346 - val_acc: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8991\n",
      "Epoch 00025: val_loss did not improve from 0.41535\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.3402 - acc: 0.8990 - val_loss: 0.4535 - val_acc: 0.8763\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8967\n",
      "Epoch 00026: val_loss improved from 0.41535 to 0.41069, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/026-0.4107.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3439 - acc: 0.8967 - val_loss: 0.4107 - val_acc: 0.8854\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.9003\n",
      "Epoch 00027: val_loss did not improve from 0.41069\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3256 - acc: 0.9003 - val_loss: 0.4938 - val_acc: 0.8726\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.9033\n",
      "Epoch 00028: val_loss improved from 0.41069 to 0.40947, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/028-0.4095.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3234 - acc: 0.9032 - val_loss: 0.4095 - val_acc: 0.8847\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9046\n",
      "Epoch 00029: val_loss did not improve from 0.40947\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3114 - acc: 0.9046 - val_loss: 0.4169 - val_acc: 0.8912\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9086\n",
      "Epoch 00030: val_loss did not improve from 0.40947\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2993 - acc: 0.9086 - val_loss: 0.4759 - val_acc: 0.8735\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9099\n",
      "Epoch 00031: val_loss improved from 0.40947 to 0.38856, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/031-0.3886.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2972 - acc: 0.9099 - val_loss: 0.3886 - val_acc: 0.8963\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9121\n",
      "Epoch 00032: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2834 - acc: 0.9121 - val_loss: 0.4976 - val_acc: 0.8765\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9142\n",
      "Epoch 00033: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2837 - acc: 0.9142 - val_loss: 0.4095 - val_acc: 0.8928\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9185\n",
      "Epoch 00034: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2683 - acc: 0.9185 - val_loss: 0.4918 - val_acc: 0.8640\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9190\n",
      "Epoch 00035: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2644 - acc: 0.9190 - val_loss: 0.4318 - val_acc: 0.8847\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9205\n",
      "Epoch 00036: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2611 - acc: 0.9205 - val_loss: 0.4029 - val_acc: 0.8942\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9223\n",
      "Epoch 00037: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2539 - acc: 0.9223 - val_loss: 0.4142 - val_acc: 0.8859\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9231\n",
      "Epoch 00038: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2462 - acc: 0.9231 - val_loss: 0.4313 - val_acc: 0.8831\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9262\n",
      "Epoch 00039: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2430 - acc: 0.9262 - val_loss: 0.4376 - val_acc: 0.8901\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9253\n",
      "Epoch 00040: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2399 - acc: 0.9253 - val_loss: 0.4785 - val_acc: 0.8770\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9290\n",
      "Epoch 00041: val_loss did not improve from 0.38856\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2319 - acc: 0.9290 - val_loss: 0.3924 - val_acc: 0.8977\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9303\n",
      "Epoch 00042: val_loss improved from 0.38856 to 0.35677, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/042-0.3568.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2257 - acc: 0.9303 - val_loss: 0.3568 - val_acc: 0.9019\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9315\n",
      "Epoch 00043: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2225 - acc: 0.9314 - val_loss: 0.4221 - val_acc: 0.8921\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9295\n",
      "Epoch 00044: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2268 - acc: 0.9294 - val_loss: 0.3806 - val_acc: 0.8954\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9310\n",
      "Epoch 00045: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2205 - acc: 0.9310 - val_loss: 0.4210 - val_acc: 0.8945\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9336\n",
      "Epoch 00046: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2113 - acc: 0.9336 - val_loss: 0.4272 - val_acc: 0.8931\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9384\n",
      "Epoch 00047: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2008 - acc: 0.9384 - val_loss: 0.4441 - val_acc: 0.8870\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9369\n",
      "Epoch 00048: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2044 - acc: 0.9369 - val_loss: 0.3985 - val_acc: 0.8970\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9390\n",
      "Epoch 00049: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1954 - acc: 0.9390 - val_loss: 0.3884 - val_acc: 0.8994\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9382\n",
      "Epoch 00050: val_loss did not improve from 0.35677\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1985 - acc: 0.9382 - val_loss: 0.4763 - val_acc: 0.8719\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9371\n",
      "Epoch 00051: val_loss improved from 0.35677 to 0.35184, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_6_conv_checkpoint/051-0.3518.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1980 - acc: 0.9371 - val_loss: 0.3518 - val_acc: 0.9052\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9406\n",
      "Epoch 00052: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1874 - acc: 0.9406 - val_loss: 0.4540 - val_acc: 0.8973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9417\n",
      "Epoch 00053: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1834 - acc: 0.9417 - val_loss: 0.4394 - val_acc: 0.8880\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9420\n",
      "Epoch 00054: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1832 - acc: 0.9420 - val_loss: 0.4719 - val_acc: 0.8789\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9453\n",
      "Epoch 00055: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1752 - acc: 0.9453 - val_loss: 0.4091 - val_acc: 0.8938\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9436\n",
      "Epoch 00056: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1786 - acc: 0.9435 - val_loss: 0.3793 - val_acc: 0.8959\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9439\n",
      "Epoch 00057: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1771 - acc: 0.9439 - val_loss: 0.4163 - val_acc: 0.9012\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9483\n",
      "Epoch 00058: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1658 - acc: 0.9482 - val_loss: 0.3653 - val_acc: 0.9092\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9464\n",
      "Epoch 00059: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1659 - acc: 0.9464 - val_loss: 0.3652 - val_acc: 0.9115\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9478\n",
      "Epoch 00060: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1626 - acc: 0.9477 - val_loss: 0.4008 - val_acc: 0.8935\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9493\n",
      "Epoch 00061: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1598 - acc: 0.9493 - val_loss: 0.3918 - val_acc: 0.8977\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9498\n",
      "Epoch 00062: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1548 - acc: 0.9498 - val_loss: 0.3621 - val_acc: 0.9085\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9503\n",
      "Epoch 00063: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1554 - acc: 0.9503 - val_loss: 0.3658 - val_acc: 0.9033\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9516\n",
      "Epoch 00064: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1550 - acc: 0.9516 - val_loss: 0.3890 - val_acc: 0.9024\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9524\n",
      "Epoch 00065: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1548 - acc: 0.9524 - val_loss: 0.3996 - val_acc: 0.8996\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9540\n",
      "Epoch 00066: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1440 - acc: 0.9540 - val_loss: 0.4309 - val_acc: 0.8938\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9568\n",
      "Epoch 00067: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1392 - acc: 0.9568 - val_loss: 0.3888 - val_acc: 0.9040\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9549\n",
      "Epoch 00068: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1426 - acc: 0.9550 - val_loss: 0.4492 - val_acc: 0.8901\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9562\n",
      "Epoch 00069: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1386 - acc: 0.9561 - val_loss: 0.4490 - val_acc: 0.8868\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9558\n",
      "Epoch 00070: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1381 - acc: 0.9558 - val_loss: 0.4139 - val_acc: 0.8994\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9571\n",
      "Epoch 00071: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1354 - acc: 0.9572 - val_loss: 0.3857 - val_acc: 0.9117\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9571\n",
      "Epoch 00072: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1349 - acc: 0.9570 - val_loss: 0.4087 - val_acc: 0.9038\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9601\n",
      "Epoch 00073: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1285 - acc: 0.9601 - val_loss: 0.4644 - val_acc: 0.8856\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9602\n",
      "Epoch 00074: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1259 - acc: 0.9602 - val_loss: 0.3905 - val_acc: 0.8998\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9589\n",
      "Epoch 00075: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1260 - acc: 0.9588 - val_loss: 0.3937 - val_acc: 0.9031\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9605\n",
      "Epoch 00076: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1275 - acc: 0.9605 - val_loss: 0.3934 - val_acc: 0.9008\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9601\n",
      "Epoch 00077: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1278 - acc: 0.9601 - val_loss: 0.4108 - val_acc: 0.9019\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9598\n",
      "Epoch 00078: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1220 - acc: 0.9598 - val_loss: 0.4315 - val_acc: 0.9029\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9616\n",
      "Epoch 00079: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1189 - acc: 0.9616 - val_loss: 0.3935 - val_acc: 0.9108\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9630\n",
      "Epoch 00080: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1171 - acc: 0.9630 - val_loss: 0.3853 - val_acc: 0.9059\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9636\n",
      "Epoch 00081: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1147 - acc: 0.9635 - val_loss: 0.4750 - val_acc: 0.8833\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9626\n",
      "Epoch 00082: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1184 - acc: 0.9626 - val_loss: 0.3983 - val_acc: 0.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9645\n",
      "Epoch 00083: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1130 - acc: 0.9645 - val_loss: 0.4183 - val_acc: 0.8980\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9632\n",
      "Epoch 00084: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1159 - acc: 0.9632 - val_loss: 0.3846 - val_acc: 0.9087\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9663\n",
      "Epoch 00085: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1078 - acc: 0.9662 - val_loss: 0.5011 - val_acc: 0.8852\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9622\n",
      "Epoch 00086: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1166 - acc: 0.9622 - val_loss: 0.4063 - val_acc: 0.9096\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9657\n",
      "Epoch 00087: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1095 - acc: 0.9657 - val_loss: 0.4702 - val_acc: 0.8975\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9694\n",
      "Epoch 00088: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0975 - acc: 0.9694 - val_loss: 0.4301 - val_acc: 0.8984\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9636\n",
      "Epoch 00089: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.1174 - acc: 0.9636 - val_loss: 0.4128 - val_acc: 0.9129\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9701\n",
      "Epoch 00090: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0954 - acc: 0.9701 - val_loss: 0.4229 - val_acc: 0.8959\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9686\n",
      "Epoch 00091: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0997 - acc: 0.9686 - val_loss: 0.5168 - val_acc: 0.8770\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9641\n",
      "Epoch 00092: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1110 - acc: 0.9641 - val_loss: 0.4302 - val_acc: 0.8977\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9641\n",
      "Epoch 00093: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1125 - acc: 0.9641 - val_loss: 0.4375 - val_acc: 0.8952\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9727\n",
      "Epoch 00094: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0886 - acc: 0.9727 - val_loss: 0.4143 - val_acc: 0.9071\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9683\n",
      "Epoch 00095: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0986 - acc: 0.9683 - val_loss: 0.4102 - val_acc: 0.8959\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9678\n",
      "Epoch 00096: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0979 - acc: 0.9678 - val_loss: 0.4158 - val_acc: 0.9015\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9697\n",
      "Epoch 00097: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0964 - acc: 0.9697 - val_loss: 0.4414 - val_acc: 0.9024\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9702\n",
      "Epoch 00098: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.0946 - acc: 0.9702 - val_loss: 0.4320 - val_acc: 0.8949\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9677\n",
      "Epoch 00099: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0982 - acc: 0.9677 - val_loss: 0.4583 - val_acc: 0.8984\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9690\n",
      "Epoch 00100: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0974 - acc: 0.9690 - val_loss: 0.3795 - val_acc: 0.9113\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9723\n",
      "Epoch 00101: val_loss did not improve from 0.35184\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0864 - acc: 0.9723 - val_loss: 0.3796 - val_acc: 0.9033\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.4538 - acc: 0.8725\n",
      "Loss: 0.4538032439267524 Accuracy: 0.8724818\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7025 - acc: 0.1864\n",
      "Epoch 00001: val_loss improved from inf to 2.01884, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/001-2.0188.hdf5\n",
      "36805/36805 [==============================] - 190s 5ms/sample - loss: 2.7025 - acc: 0.1864 - val_loss: 2.0188 - val_acc: 0.3685\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9123 - acc: 0.3789\n",
      "Epoch 00002: val_loss improved from 2.01884 to 1.36297, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/002-1.3630.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 1.9122 - acc: 0.3789 - val_loss: 1.3630 - val_acc: 0.5998\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5331 - acc: 0.4974\n",
      "Epoch 00003: val_loss improved from 1.36297 to 1.11868, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/003-1.1187.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 1.5331 - acc: 0.4974 - val_loss: 1.1187 - val_acc: 0.6608\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2954 - acc: 0.5833\n",
      "Epoch 00004: val_loss improved from 1.11868 to 0.93841, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/004-0.9384.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 1.2956 - acc: 0.5833 - val_loss: 0.9384 - val_acc: 0.7426\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1234 - acc: 0.6442\n",
      "Epoch 00005: val_loss improved from 0.93841 to 0.79720, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/005-0.7972.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 1.1233 - acc: 0.6442 - val_loss: 0.7972 - val_acc: 0.7785\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9901 - acc: 0.6889\n",
      "Epoch 00006: val_loss improved from 0.79720 to 0.72601, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/006-0.7260.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.9901 - acc: 0.6888 - val_loss: 0.7260 - val_acc: 0.8022\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8860 - acc: 0.7266\n",
      "Epoch 00007: val_loss improved from 0.72601 to 0.64759, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/007-0.6476.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.8862 - acc: 0.7266 - val_loss: 0.6476 - val_acc: 0.8234\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8155 - acc: 0.7514\n",
      "Epoch 00008: val_loss improved from 0.64759 to 0.59877, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/008-0.5988.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.8156 - acc: 0.7514 - val_loss: 0.5988 - val_acc: 0.8409\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7506 - acc: 0.7750\n",
      "Epoch 00009: val_loss did not improve from 0.59877\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.7505 - acc: 0.7750 - val_loss: 0.6052 - val_acc: 0.8255\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7933\n",
      "Epoch 00010: val_loss did not improve from 0.59877\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.6918 - acc: 0.7933 - val_loss: 0.6176 - val_acc: 0.8246\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6574 - acc: 0.8043\n",
      "Epoch 00011: val_loss improved from 0.59877 to 0.52672, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/011-0.5267.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.6573 - acc: 0.8043 - val_loss: 0.5267 - val_acc: 0.8526\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6145 - acc: 0.8183\n",
      "Epoch 00012: val_loss did not improve from 0.52672\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.6144 - acc: 0.8183 - val_loss: 0.5602 - val_acc: 0.8435\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8281\n",
      "Epoch 00013: val_loss improved from 0.52672 to 0.45461, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/013-0.4546.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.5769 - acc: 0.8281 - val_loss: 0.4546 - val_acc: 0.8793\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.8396\n",
      "Epoch 00014: val_loss did not improve from 0.45461\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.5421 - acc: 0.8396 - val_loss: 0.4778 - val_acc: 0.8586\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8447\n",
      "Epoch 00015: val_loss did not improve from 0.45461\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.5232 - acc: 0.8447 - val_loss: 0.4789 - val_acc: 0.8677\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.8534\n",
      "Epoch 00016: val_loss did not improve from 0.45461\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4997 - acc: 0.8534 - val_loss: 0.5078 - val_acc: 0.8549\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.8566\n",
      "Epoch 00017: val_loss did not improve from 0.45461\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4834 - acc: 0.8566 - val_loss: 0.4700 - val_acc: 0.8654\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8630\n",
      "Epoch 00018: val_loss improved from 0.45461 to 0.38893, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/018-0.3889.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4585 - acc: 0.8630 - val_loss: 0.3889 - val_acc: 0.8921\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8665\n",
      "Epoch 00019: val_loss improved from 0.38893 to 0.38391, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/019-0.3839.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4409 - acc: 0.8665 - val_loss: 0.3839 - val_acc: 0.8961\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8715\n",
      "Epoch 00020: val_loss did not improve from 0.38391\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4240 - acc: 0.8715 - val_loss: 0.3936 - val_acc: 0.8945\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8764\n",
      "Epoch 00021: val_loss improved from 0.38391 to 0.37159, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/021-0.3716.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4092 - acc: 0.8764 - val_loss: 0.3716 - val_acc: 0.9012\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8782\n",
      "Epoch 00022: val_loss improved from 0.37159 to 0.32780, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/022-0.3278.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.4032 - acc: 0.8782 - val_loss: 0.3278 - val_acc: 0.9161\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8847\n",
      "Epoch 00023: val_loss improved from 0.32780 to 0.32671, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/023-0.3267.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3861 - acc: 0.8847 - val_loss: 0.3267 - val_acc: 0.9147\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8870\n",
      "Epoch 00024: val_loss did not improve from 0.32671\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3746 - acc: 0.8869 - val_loss: 0.3752 - val_acc: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8883\n",
      "Epoch 00025: val_loss did not improve from 0.32671\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3692 - acc: 0.8883 - val_loss: 0.3438 - val_acc: 0.9085\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8929\n",
      "Epoch 00026: val_loss improved from 0.32671 to 0.30270, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/026-0.3027.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3557 - acc: 0.8929 - val_loss: 0.3027 - val_acc: 0.9133\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8961\n",
      "Epoch 00027: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3367 - acc: 0.8961 - val_loss: 0.3076 - val_acc: 0.9150\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8978\n",
      "Epoch 00028: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3364 - acc: 0.8978 - val_loss: 0.3690 - val_acc: 0.8947\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8992\n",
      "Epoch 00029: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3274 - acc: 0.8993 - val_loss: 0.3150 - val_acc: 0.9166\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9043\n",
      "Epoch 00030: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3149 - acc: 0.9043 - val_loss: 0.3035 - val_acc: 0.9189\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9066\n",
      "Epoch 00031: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.3091 - acc: 0.9066 - val_loss: 0.3778 - val_acc: 0.8991\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9092\n",
      "Epoch 00032: val_loss did not improve from 0.30270\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2995 - acc: 0.9092 - val_loss: 0.3177 - val_acc: 0.9166\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9089\n",
      "Epoch 00033: val_loss improved from 0.30270 to 0.28953, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/033-0.2895.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2968 - acc: 0.9088 - val_loss: 0.2895 - val_acc: 0.9271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9123\n",
      "Epoch 00034: val_loss did not improve from 0.28953\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2887 - acc: 0.9123 - val_loss: 0.3110 - val_acc: 0.9189\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9123\n",
      "Epoch 00035: val_loss did not improve from 0.28953\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2857 - acc: 0.9123 - val_loss: 0.2986 - val_acc: 0.9101\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9151\n",
      "Epoch 00036: val_loss did not improve from 0.28953\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2770 - acc: 0.9150 - val_loss: 0.2984 - val_acc: 0.9203\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9159\n",
      "Epoch 00037: val_loss improved from 0.28953 to 0.28442, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/037-0.2844.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2714 - acc: 0.9159 - val_loss: 0.2844 - val_acc: 0.9252\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9189\n",
      "Epoch 00038: val_loss improved from 0.28442 to 0.26033, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/038-0.2603.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2653 - acc: 0.9189 - val_loss: 0.2603 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9201\n",
      "Epoch 00039: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2562 - acc: 0.9201 - val_loss: 0.2689 - val_acc: 0.9292\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9232\n",
      "Epoch 00040: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2518 - acc: 0.9232 - val_loss: 0.2984 - val_acc: 0.9236\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9236\n",
      "Epoch 00041: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2448 - acc: 0.9236 - val_loss: 0.3036 - val_acc: 0.9201\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9256\n",
      "Epoch 00042: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2427 - acc: 0.9256 - val_loss: 0.4043 - val_acc: 0.8887\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9265\n",
      "Epoch 00043: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2367 - acc: 0.9265 - val_loss: 0.4191 - val_acc: 0.8856\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9290\n",
      "Epoch 00044: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2304 - acc: 0.9290 - val_loss: 0.3139 - val_acc: 0.9178\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9295\n",
      "Epoch 00045: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2283 - acc: 0.9295 - val_loss: 0.2634 - val_acc: 0.9271\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9283\n",
      "Epoch 00046: val_loss did not improve from 0.26033\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2265 - acc: 0.9283 - val_loss: 0.2680 - val_acc: 0.9285\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9320\n",
      "Epoch 00047: val_loss improved from 0.26033 to 0.24896, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/047-0.2490.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2215 - acc: 0.9320 - val_loss: 0.2490 - val_acc: 0.9348\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9333\n",
      "Epoch 00048: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2141 - acc: 0.9333 - val_loss: 0.2955 - val_acc: 0.9341\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9350\n",
      "Epoch 00049: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2090 - acc: 0.9350 - val_loss: 0.2542 - val_acc: 0.9306\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9350\n",
      "Epoch 00050: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2085 - acc: 0.9350 - val_loss: 0.2709 - val_acc: 0.9355\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9375\n",
      "Epoch 00051: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2061 - acc: 0.9375 - val_loss: 0.2964 - val_acc: 0.9257\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9370\n",
      "Epoch 00052: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.2011 - acc: 0.9369 - val_loss: 0.3552 - val_acc: 0.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9376\n",
      "Epoch 00053: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1961 - acc: 0.9376 - val_loss: 0.2615 - val_acc: 0.9336\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9389\n",
      "Epoch 00054: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1960 - acc: 0.9388 - val_loss: 0.2684 - val_acc: 0.9334\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9391\n",
      "Epoch 00055: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1930 - acc: 0.9391 - val_loss: 0.2567 - val_acc: 0.9369\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9418\n",
      "Epoch 00056: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1844 - acc: 0.9417 - val_loss: 0.2800 - val_acc: 0.9276\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9433\n",
      "Epoch 00057: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1812 - acc: 0.9434 - val_loss: 0.2655 - val_acc: 0.9385\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9443\n",
      "Epoch 00058: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1766 - acc: 0.9442 - val_loss: 0.2811 - val_acc: 0.9304\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9436\n",
      "Epoch 00059: val_loss did not improve from 0.24896\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1807 - acc: 0.9436 - val_loss: 0.2500 - val_acc: 0.9338\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9457\n",
      "Epoch 00060: val_loss improved from 0.24896 to 0.22959, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_7_conv_checkpoint/060-0.2296.hdf5\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1741 - acc: 0.9457 - val_loss: 0.2296 - val_acc: 0.9378\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9466\n",
      "Epoch 00061: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1674 - acc: 0.9466 - val_loss: 0.2991 - val_acc: 0.9264\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9485\n",
      "Epoch 00062: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1644 - acc: 0.9485 - val_loss: 0.2828 - val_acc: 0.9290\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9468\n",
      "Epoch 00063: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1667 - acc: 0.9469 - val_loss: 0.2512 - val_acc: 0.9352\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9502\n",
      "Epoch 00064: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1589 - acc: 0.9502 - val_loss: 0.3023 - val_acc: 0.9234\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9515\n",
      "Epoch 00065: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1553 - acc: 0.9514 - val_loss: 0.2805 - val_acc: 0.9271\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9511\n",
      "Epoch 00066: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1566 - acc: 0.9511 - val_loss: 0.2841 - val_acc: 0.9299\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9502\n",
      "Epoch 00067: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1531 - acc: 0.9502 - val_loss: 0.2502 - val_acc: 0.9369\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9522\n",
      "Epoch 00068: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1526 - acc: 0.9522 - val_loss: 0.2673 - val_acc: 0.9376\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9556\n",
      "Epoch 00069: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1440 - acc: 0.9556 - val_loss: 0.2855 - val_acc: 0.9222\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9530\n",
      "Epoch 00070: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1470 - acc: 0.9530 - val_loss: 0.3153 - val_acc: 0.9271\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9525\n",
      "Epoch 00071: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1477 - acc: 0.9525 - val_loss: 0.2571 - val_acc: 0.9371\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9545\n",
      "Epoch 00072: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1430 - acc: 0.9545 - val_loss: 0.2744 - val_acc: 0.9350\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9548\n",
      "Epoch 00073: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1426 - acc: 0.9548 - val_loss: 0.2881 - val_acc: 0.9317\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9563\n",
      "Epoch 00074: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1373 - acc: 0.9563 - val_loss: 0.2645 - val_acc: 0.9385\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9569\n",
      "Epoch 00075: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1365 - acc: 0.9569 - val_loss: 0.2637 - val_acc: 0.9320\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9554\n",
      "Epoch 00076: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1407 - acc: 0.9554 - val_loss: 0.2875 - val_acc: 0.9292\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9568\n",
      "Epoch 00077: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1335 - acc: 0.9568 - val_loss: 0.2616 - val_acc: 0.9373\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9588\n",
      "Epoch 00078: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1311 - acc: 0.9588 - val_loss: 0.3007 - val_acc: 0.9308\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9600\n",
      "Epoch 00079: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1231 - acc: 0.9600 - val_loss: 0.2874 - val_acc: 0.9304\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9584\n",
      "Epoch 00080: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1309 - acc: 0.9584 - val_loss: 0.3131 - val_acc: 0.9287\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9597\n",
      "Epoch 00081: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1255 - acc: 0.9597 - val_loss: 0.2935 - val_acc: 0.9341\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9622\n",
      "Epoch 00082: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1207 - acc: 0.9622 - val_loss: 0.2598 - val_acc: 0.9369\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9615\n",
      "Epoch 00083: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1207 - acc: 0.9615 - val_loss: 0.2783 - val_acc: 0.9350\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9628\n",
      "Epoch 00084: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1173 - acc: 0.9627 - val_loss: 0.2670 - val_acc: 0.9292\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9604\n",
      "Epoch 00085: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1246 - acc: 0.9603 - val_loss: 0.2835 - val_acc: 0.9301\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9610\n",
      "Epoch 00086: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1182 - acc: 0.9610 - val_loss: 0.2546 - val_acc: 0.9406\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9649\n",
      "Epoch 00087: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1100 - acc: 0.9649 - val_loss: 0.2521 - val_acc: 0.9383\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9647\n",
      "Epoch 00088: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1098 - acc: 0.9647 - val_loss: 0.2779 - val_acc: 0.9359\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9649\n",
      "Epoch 00089: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1102 - acc: 0.9650 - val_loss: 0.3235 - val_acc: 0.9229\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9655\n",
      "Epoch 00090: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1082 - acc: 0.9655 - val_loss: 0.2855 - val_acc: 0.9352\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9624\n",
      "Epoch 00091: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1163 - acc: 0.9624 - val_loss: 0.2616 - val_acc: 0.9359\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9651\n",
      "Epoch 00092: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1040 - acc: 0.9651 - val_loss: 0.2868 - val_acc: 0.9287\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9664\n",
      "Epoch 00093: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1037 - acc: 0.9664 - val_loss: 0.2939 - val_acc: 0.9283\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9659\n",
      "Epoch 00094: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1054 - acc: 0.9659 - val_loss: 0.2872 - val_acc: 0.9355\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9677\n",
      "Epoch 00095: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0998 - acc: 0.9677 - val_loss: 0.3037 - val_acc: 0.9301\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9640\n",
      "Epoch 00096: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1122 - acc: 0.9639 - val_loss: 0.2870 - val_acc: 0.9327\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9657\n",
      "Epoch 00097: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1040 - acc: 0.9657 - val_loss: 0.3076 - val_acc: 0.9317\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9693\n",
      "Epoch 00098: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0963 - acc: 0.9693 - val_loss: 0.3003 - val_acc: 0.9313\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9646\n",
      "Epoch 00099: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1091 - acc: 0.9646 - val_loss: 0.2673 - val_acc: 0.9404\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9697\n",
      "Epoch 00100: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0960 - acc: 0.9697 - val_loss: 0.3380 - val_acc: 0.9278\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9683\n",
      "Epoch 00101: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1001 - acc: 0.9683 - val_loss: 0.2819 - val_acc: 0.9341\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9697\n",
      "Epoch 00102: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0947 - acc: 0.9697 - val_loss: 0.2554 - val_acc: 0.9420\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9708\n",
      "Epoch 00103: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0914 - acc: 0.9707 - val_loss: 0.3018 - val_acc: 0.9287\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9668\n",
      "Epoch 00104: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1001 - acc: 0.9668 - val_loss: 0.3046 - val_acc: 0.9320\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9719\n",
      "Epoch 00105: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0892 - acc: 0.9719 - val_loss: 0.2862 - val_acc: 0.9357\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9715\n",
      "Epoch 00106: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0908 - acc: 0.9715 - val_loss: 0.3003 - val_acc: 0.9390\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9717\n",
      "Epoch 00107: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0894 - acc: 0.9717 - val_loss: 0.2839 - val_acc: 0.9350\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9723\n",
      "Epoch 00108: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0883 - acc: 0.9722 - val_loss: 0.3557 - val_acc: 0.9238\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9685\n",
      "Epoch 00109: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.1010 - acc: 0.9685 - val_loss: 0.2651 - val_acc: 0.9401\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9727\n",
      "Epoch 00110: val_loss did not improve from 0.22959\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0852 - acc: 0.9727 - val_loss: 0.2761 - val_acc: 0.9376\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2935 - acc: 0.9161\n",
      "Loss: 0.2935313658117629 Accuracy: 0.91609555\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8310 - acc: 0.1666\n",
      "Epoch 00001: val_loss improved from inf to 2.15548, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/001-2.1555.hdf5\n",
      "36805/36805 [==============================] - 195s 5ms/sample - loss: 2.8309 - acc: 0.1666 - val_loss: 2.1555 - val_acc: 0.3529\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0868 - acc: 0.3300\n",
      "Epoch 00002: val_loss improved from 2.15548 to 1.45581, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/002-1.4558.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 2.0868 - acc: 0.3300 - val_loss: 1.4558 - val_acc: 0.5604\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6917 - acc: 0.4484\n",
      "Epoch 00003: val_loss improved from 1.45581 to 1.23008, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/003-1.2301.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.6916 - acc: 0.4484 - val_loss: 1.2301 - val_acc: 0.6289\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4368 - acc: 0.5329\n",
      "Epoch 00004: val_loss improved from 1.23008 to 1.05137, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/004-1.0514.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 1.4367 - acc: 0.5329 - val_loss: 1.0514 - val_acc: 0.6855\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2383 - acc: 0.6026\n",
      "Epoch 00005: val_loss improved from 1.05137 to 0.96405, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/005-0.9640.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.2384 - acc: 0.6026 - val_loss: 0.9640 - val_acc: 0.7170\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.6576\n",
      "Epoch 00006: val_loss improved from 0.96405 to 0.70894, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/006-0.7089.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.0747 - acc: 0.6576 - val_loss: 0.7089 - val_acc: 0.8095\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9435 - acc: 0.7070\n",
      "Epoch 00007: val_loss improved from 0.70894 to 0.59549, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/007-0.5955.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.9436 - acc: 0.7070 - val_loss: 0.5955 - val_acc: 0.8463\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8258 - acc: 0.7441\n",
      "Epoch 00008: val_loss improved from 0.59549 to 0.52881, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/008-0.5288.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.8258 - acc: 0.7441 - val_loss: 0.5288 - val_acc: 0.8591\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7734\n",
      "Epoch 00009: val_loss improved from 0.52881 to 0.52399, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/009-0.5240.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.7404 - acc: 0.7734 - val_loss: 0.5240 - val_acc: 0.8537\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6628 - acc: 0.8015\n",
      "Epoch 00010: val_loss improved from 0.52399 to 0.43803, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/010-0.4380.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.6630 - acc: 0.8014 - val_loss: 0.4380 - val_acc: 0.8840\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8170\n",
      "Epoch 00011: val_loss improved from 0.43803 to 0.41440, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/011-0.4144.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.6067 - acc: 0.8170 - val_loss: 0.4144 - val_acc: 0.8945\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.8317\n",
      "Epoch 00012: val_loss improved from 0.41440 to 0.37295, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/012-0.3730.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.5562 - acc: 0.8317 - val_loss: 0.3730 - val_acc: 0.9043\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.8433\n",
      "Epoch 00013: val_loss improved from 0.37295 to 0.33418, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/013-0.3342.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.5227 - acc: 0.8433 - val_loss: 0.3342 - val_acc: 0.9133\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4810 - acc: 0.8577\n",
      "Epoch 00014: val_loss did not improve from 0.33418\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4810 - acc: 0.8577 - val_loss: 0.3349 - val_acc: 0.9159\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8657\n",
      "Epoch 00015: val_loss improved from 0.33418 to 0.29654, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/015-0.2965.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4496 - acc: 0.8657 - val_loss: 0.2965 - val_acc: 0.9236\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8681\n",
      "Epoch 00016: val_loss improved from 0.29654 to 0.27872, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/016-0.2787.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4340 - acc: 0.8681 - val_loss: 0.2787 - val_acc: 0.9271\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8790\n",
      "Epoch 00017: val_loss did not improve from 0.27872\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4049 - acc: 0.8789 - val_loss: 0.2791 - val_acc: 0.9250\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8838\n",
      "Epoch 00018: val_loss did not improve from 0.27872\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3892 - acc: 0.8838 - val_loss: 0.3194 - val_acc: 0.9122\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8902\n",
      "Epoch 00019: val_loss improved from 0.27872 to 0.23536, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/019-0.2354.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3639 - acc: 0.8902 - val_loss: 0.2354 - val_acc: 0.9359\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8943\n",
      "Epoch 00020: val_loss did not improve from 0.23536\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3513 - acc: 0.8943 - val_loss: 0.2416 - val_acc: 0.9331\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8987\n",
      "Epoch 00021: val_loss did not improve from 0.23536\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3396 - acc: 0.8987 - val_loss: 0.2972 - val_acc: 0.9217\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.9008\n",
      "Epoch 00022: val_loss improved from 0.23536 to 0.23228, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/022-0.2323.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3253 - acc: 0.9007 - val_loss: 0.2323 - val_acc: 0.9341\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9063\n",
      "Epoch 00023: val_loss improved from 0.23228 to 0.22776, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/023-0.2278.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3132 - acc: 0.9063 - val_loss: 0.2278 - val_acc: 0.9390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9080\n",
      "Epoch 00024: val_loss improved from 0.22776 to 0.22020, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/024-0.2202.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3034 - acc: 0.9079 - val_loss: 0.2202 - val_acc: 0.9378\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9135\n",
      "Epoch 00025: val_loss improved from 0.22020 to 0.20848, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/025-0.2085.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2867 - acc: 0.9135 - val_loss: 0.2085 - val_acc: 0.9427\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9170\n",
      "Epoch 00026: val_loss did not improve from 0.20848\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2780 - acc: 0.9169 - val_loss: 0.2286 - val_acc: 0.9373\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9189\n",
      "Epoch 00027: val_loss did not improve from 0.20848\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2692 - acc: 0.9188 - val_loss: 0.2276 - val_acc: 0.9392\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9209\n",
      "Epoch 00028: val_loss improved from 0.20848 to 0.19584, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/028-0.1958.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2608 - acc: 0.9209 - val_loss: 0.1958 - val_acc: 0.9404\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9226\n",
      "Epoch 00029: val_loss did not improve from 0.19584\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2538 - acc: 0.9226 - val_loss: 0.1988 - val_acc: 0.9441\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9245\n",
      "Epoch 00030: val_loss improved from 0.19584 to 0.18833, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/030-0.1883.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2458 - acc: 0.9245 - val_loss: 0.1883 - val_acc: 0.9485\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9250\n",
      "Epoch 00031: val_loss did not improve from 0.18833\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2447 - acc: 0.9250 - val_loss: 0.1937 - val_acc: 0.9443\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9305\n",
      "Epoch 00032: val_loss improved from 0.18833 to 0.18203, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/032-0.1820.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2319 - acc: 0.9305 - val_loss: 0.1820 - val_acc: 0.9511\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9316\n",
      "Epoch 00033: val_loss did not improve from 0.18203\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2245 - acc: 0.9316 - val_loss: 0.1837 - val_acc: 0.9495\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9324\n",
      "Epoch 00034: val_loss did not improve from 0.18203\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2197 - acc: 0.9325 - val_loss: 0.1940 - val_acc: 0.9453\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9344\n",
      "Epoch 00035: val_loss did not improve from 0.18203\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2137 - acc: 0.9343 - val_loss: 0.1895 - val_acc: 0.9469\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9339\n",
      "Epoch 00036: val_loss improved from 0.18203 to 0.17531, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/036-0.1753.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2138 - acc: 0.9339 - val_loss: 0.1753 - val_acc: 0.9520\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9374\n",
      "Epoch 00037: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2026 - acc: 0.9374 - val_loss: 0.2089 - val_acc: 0.9413\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9398\n",
      "Epoch 00038: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1976 - acc: 0.9398 - val_loss: 0.1851 - val_acc: 0.9497\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9396\n",
      "Epoch 00039: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1953 - acc: 0.9396 - val_loss: 0.1924 - val_acc: 0.9485\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9439\n",
      "Epoch 00040: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1838 - acc: 0.9438 - val_loss: 0.1879 - val_acc: 0.9455\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9406\n",
      "Epoch 00041: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1890 - acc: 0.9406 - val_loss: 0.1901 - val_acc: 0.9467\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9446\n",
      "Epoch 00042: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1783 - acc: 0.9446 - val_loss: 0.2064 - val_acc: 0.9455\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9449\n",
      "Epoch 00043: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1784 - acc: 0.9449 - val_loss: 0.1832 - val_acc: 0.9504\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9470\n",
      "Epoch 00044: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1712 - acc: 0.9469 - val_loss: 0.1770 - val_acc: 0.9485\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9476\n",
      "Epoch 00045: val_loss did not improve from 0.17531\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1674 - acc: 0.9476 - val_loss: 0.1926 - val_acc: 0.9448\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9478\n",
      "Epoch 00046: val_loss improved from 0.17531 to 0.16846, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/046-0.1685.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1710 - acc: 0.9478 - val_loss: 0.1685 - val_acc: 0.9522\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9486\n",
      "Epoch 00047: val_loss improved from 0.16846 to 0.16595, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/047-0.1659.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1646 - acc: 0.9486 - val_loss: 0.1659 - val_acc: 0.9509\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9520\n",
      "Epoch 00048: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1558 - acc: 0.9520 - val_loss: 0.1697 - val_acc: 0.9499\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9489\n",
      "Epoch 00049: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1631 - acc: 0.9489 - val_loss: 0.1736 - val_acc: 0.9515\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9533\n",
      "Epoch 00050: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1491 - acc: 0.9533 - val_loss: 0.1696 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9504\n",
      "Epoch 00051: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1551 - acc: 0.9504 - val_loss: 0.2141 - val_acc: 0.9415\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9549\n",
      "Epoch 00052: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1440 - acc: 0.9549 - val_loss: 0.2000 - val_acc: 0.9462\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9553\n",
      "Epoch 00053: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1407 - acc: 0.9553 - val_loss: 0.1888 - val_acc: 0.9460\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9542\n",
      "Epoch 00054: val_loss did not improve from 0.16595\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1471 - acc: 0.9541 - val_loss: 0.1816 - val_acc: 0.9485\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9530\n",
      "Epoch 00055: val_loss improved from 0.16595 to 0.16354, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_8_conv_checkpoint/055-0.1635.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1467 - acc: 0.9530 - val_loss: 0.1635 - val_acc: 0.9536\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9568\n",
      "Epoch 00056: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1365 - acc: 0.9569 - val_loss: 0.1719 - val_acc: 0.9525\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9597\n",
      "Epoch 00057: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1252 - acc: 0.9597 - val_loss: 0.1802 - val_acc: 0.9490\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9610\n",
      "Epoch 00058: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1257 - acc: 0.9610 - val_loss: 0.1870 - val_acc: 0.9478\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9626\n",
      "Epoch 00059: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1214 - acc: 0.9626 - val_loss: 0.1678 - val_acc: 0.9506\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9602\n",
      "Epoch 00060: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1289 - acc: 0.9602 - val_loss: 0.1832 - val_acc: 0.9476\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9583\n",
      "Epoch 00061: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1314 - acc: 0.9583 - val_loss: 0.1673 - val_acc: 0.9541\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9639\n",
      "Epoch 00062: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1170 - acc: 0.9639 - val_loss: 0.2117 - val_acc: 0.9399\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9636\n",
      "Epoch 00063: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1149 - acc: 0.9636 - val_loss: 0.1681 - val_acc: 0.9546\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9641\n",
      "Epoch 00064: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1137 - acc: 0.9641 - val_loss: 0.1711 - val_acc: 0.9497\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9643\n",
      "Epoch 00065: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1135 - acc: 0.9643 - val_loss: 0.1687 - val_acc: 0.9515\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9636\n",
      "Epoch 00066: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1125 - acc: 0.9636 - val_loss: 0.2078 - val_acc: 0.9427\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9621\n",
      "Epoch 00067: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1202 - acc: 0.9621 - val_loss: 0.1837 - val_acc: 0.9520\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9690\n",
      "Epoch 00068: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1004 - acc: 0.9690 - val_loss: 0.2034 - val_acc: 0.9497\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9641\n",
      "Epoch 00069: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1137 - acc: 0.9641 - val_loss: 0.1903 - val_acc: 0.9485\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9676\n",
      "Epoch 00070: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1018 - acc: 0.9676 - val_loss: 0.1713 - val_acc: 0.9513\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9684\n",
      "Epoch 00071: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0993 - acc: 0.9684 - val_loss: 0.1941 - val_acc: 0.9471\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9689\n",
      "Epoch 00072: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0965 - acc: 0.9689 - val_loss: 0.1703 - val_acc: 0.9529\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9675\n",
      "Epoch 00073: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1040 - acc: 0.9675 - val_loss: 0.1962 - val_acc: 0.9488\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9695\n",
      "Epoch 00074: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0949 - acc: 0.9695 - val_loss: 0.1689 - val_acc: 0.9515\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9698\n",
      "Epoch 00075: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0952 - acc: 0.9698 - val_loss: 0.2112 - val_acc: 0.9432\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9695\n",
      "Epoch 00076: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0951 - acc: 0.9695 - val_loss: 0.2310 - val_acc: 0.9418\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9701\n",
      "Epoch 00077: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0937 - acc: 0.9701 - val_loss: 0.2062 - val_acc: 0.9443\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9725\n",
      "Epoch 00078: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0867 - acc: 0.9724 - val_loss: 0.2395 - val_acc: 0.9415\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9682\n",
      "Epoch 00079: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0982 - acc: 0.9682 - val_loss: 0.1846 - val_acc: 0.9529\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9739\n",
      "Epoch 00080: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0838 - acc: 0.9739 - val_loss: 0.1855 - val_acc: 0.9515\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9745\n",
      "Epoch 00081: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0817 - acc: 0.9744 - val_loss: 0.2264 - val_acc: 0.9425\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9663\n",
      "Epoch 00082: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1022 - acc: 0.9663 - val_loss: 0.1750 - val_acc: 0.9532\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9743\n",
      "Epoch 00083: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0827 - acc: 0.9743 - val_loss: 0.1895 - val_acc: 0.9515\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9751\n",
      "Epoch 00084: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0777 - acc: 0.9751 - val_loss: 0.2540 - val_acc: 0.9369\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9700\n",
      "Epoch 00085: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0945 - acc: 0.9700 - val_loss: 0.1766 - val_acc: 0.9548\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9765\n",
      "Epoch 00086: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0759 - acc: 0.9765 - val_loss: 0.2166 - val_acc: 0.9448\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9718\n",
      "Epoch 00087: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0882 - acc: 0.9718 - val_loss: 0.2166 - val_acc: 0.9446\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9745\n",
      "Epoch 00088: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0793 - acc: 0.9745 - val_loss: 0.1960 - val_acc: 0.9490\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9775\n",
      "Epoch 00089: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0720 - acc: 0.9775 - val_loss: 0.1792 - val_acc: 0.9504\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9746\n",
      "Epoch 00090: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0814 - acc: 0.9746 - val_loss: 0.1909 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9773\n",
      "Epoch 00091: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0719 - acc: 0.9772 - val_loss: 0.2446 - val_acc: 0.9359\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9708\n",
      "Epoch 00092: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0909 - acc: 0.9707 - val_loss: 0.1825 - val_acc: 0.9509\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9752\n",
      "Epoch 00093: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0801 - acc: 0.9752 - val_loss: 0.2064 - val_acc: 0.9446\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9775\n",
      "Epoch 00094: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0699 - acc: 0.9775 - val_loss: 0.1883 - val_acc: 0.9502\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9769\n",
      "Epoch 00095: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0731 - acc: 0.9769 - val_loss: 0.1959 - val_acc: 0.9497\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9772\n",
      "Epoch 00096: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0712 - acc: 0.9772 - val_loss: 0.2052 - val_acc: 0.9485\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00097: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0699 - acc: 0.9776 - val_loss: 0.2102 - val_acc: 0.9485\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9757\n",
      "Epoch 00098: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0746 - acc: 0.9757 - val_loss: 0.1940 - val_acc: 0.9502\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9793\n",
      "Epoch 00099: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0665 - acc: 0.9792 - val_loss: 0.1944 - val_acc: 0.9509\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9772\n",
      "Epoch 00100: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0722 - acc: 0.9771 - val_loss: 0.2159 - val_acc: 0.9518\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9792\n",
      "Epoch 00101: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0638 - acc: 0.9792 - val_loss: 0.1839 - val_acc: 0.9546\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9801\n",
      "Epoch 00102: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0631 - acc: 0.9801 - val_loss: 0.2175 - val_acc: 0.9469\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9789\n",
      "Epoch 00103: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0668 - acc: 0.9789 - val_loss: 0.2133 - val_acc: 0.9448\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9771\n",
      "Epoch 00104: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0733 - acc: 0.9770 - val_loss: 0.1746 - val_acc: 0.9534\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9783\n",
      "Epoch 00105: val_loss did not improve from 0.16354\n",
      "36805/36805 [==============================] - 180s 5ms/sample - loss: 0.0691 - acc: 0.9783 - val_loss: 0.1902 - val_acc: 0.9557\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2099 - acc: 0.9373\n",
      "Loss: 0.20994856844067203 Accuracy: 0.93727934\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8115 - acc: 0.1828\n",
      "Epoch 00001: val_loss improved from inf to 2.17029, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/001-2.1703.hdf5\n",
      "36805/36805 [==============================] - 200s 5ms/sample - loss: 2.8113 - acc: 0.1829 - val_loss: 2.1703 - val_acc: 0.3692\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9617 - acc: 0.3732\n",
      "Epoch 00002: val_loss improved from 2.17029 to 1.34460, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/002-1.3446.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.9616 - acc: 0.3732 - val_loss: 1.3446 - val_acc: 0.6287\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5392 - acc: 0.5093\n",
      "Epoch 00003: val_loss improved from 1.34460 to 0.97380, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/003-0.9738.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.5392 - acc: 0.5093 - val_loss: 0.9738 - val_acc: 0.7417\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2730 - acc: 0.5992\n",
      "Epoch 00004: val_loss improved from 0.97380 to 0.80156, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/004-0.8016.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.2730 - acc: 0.5992 - val_loss: 0.8016 - val_acc: 0.7997\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0865 - acc: 0.6626\n",
      "Epoch 00005: val_loss improved from 0.80156 to 0.68357, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/005-0.6836.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.0865 - acc: 0.6625 - val_loss: 0.6836 - val_acc: 0.8290\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9543 - acc: 0.7078\n",
      "Epoch 00006: val_loss improved from 0.68357 to 0.57950, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/006-0.5795.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.9543 - acc: 0.7078 - val_loss: 0.5795 - val_acc: 0.8479\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8401 - acc: 0.7449\n",
      "Epoch 00007: val_loss improved from 0.57950 to 0.51753, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/007-0.5175.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.8402 - acc: 0.7449 - val_loss: 0.5175 - val_acc: 0.8703\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7633 - acc: 0.7705\n",
      "Epoch 00008: val_loss improved from 0.51753 to 0.49385, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/008-0.4938.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.7633 - acc: 0.7705 - val_loss: 0.4938 - val_acc: 0.8749\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.7936\n",
      "Epoch 00009: val_loss improved from 0.49385 to 0.39036, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/009-0.3904.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.6917 - acc: 0.7936 - val_loss: 0.3904 - val_acc: 0.9017\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6303 - acc: 0.8130\n",
      "Epoch 00010: val_loss did not improve from 0.39036\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.6304 - acc: 0.8130 - val_loss: 0.3926 - val_acc: 0.8949\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8244\n",
      "Epoch 00011: val_loss improved from 0.39036 to 0.33995, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/011-0.3400.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.5891 - acc: 0.8244 - val_loss: 0.3400 - val_acc: 0.9145\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8353\n",
      "Epoch 00012: val_loss improved from 0.33995 to 0.33569, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/012-0.3357.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.5553 - acc: 0.8353 - val_loss: 0.3357 - val_acc: 0.9159\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8508\n",
      "Epoch 00013: val_loss improved from 0.33569 to 0.28625, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/013-0.2863.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.5091 - acc: 0.8508 - val_loss: 0.2863 - val_acc: 0.9276\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8557\n",
      "Epoch 00014: val_loss did not improve from 0.28625\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4851 - acc: 0.8557 - val_loss: 0.3108 - val_acc: 0.9143\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8601\n",
      "Epoch 00015: val_loss improved from 0.28625 to 0.26423, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/015-0.2642.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4685 - acc: 0.8600 - val_loss: 0.2642 - val_acc: 0.9315\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4336 - acc: 0.8703\n",
      "Epoch 00016: val_loss did not improve from 0.26423\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4340 - acc: 0.8702 - val_loss: 0.2711 - val_acc: 0.9306\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8760\n",
      "Epoch 00017: val_loss improved from 0.26423 to 0.25742, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/017-0.2574.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.4191 - acc: 0.8760 - val_loss: 0.2574 - val_acc: 0.9315\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3962 - acc: 0.8814\n",
      "Epoch 00018: val_loss improved from 0.25742 to 0.23944, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/018-0.2394.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3963 - acc: 0.8814 - val_loss: 0.2394 - val_acc: 0.9376\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8862\n",
      "Epoch 00019: val_loss improved from 0.23944 to 0.23443, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/019-0.2344.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3795 - acc: 0.8862 - val_loss: 0.2344 - val_acc: 0.9341\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8910\n",
      "Epoch 00020: val_loss improved from 0.23443 to 0.23089, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/020-0.2309.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3676 - acc: 0.8909 - val_loss: 0.2309 - val_acc: 0.9364\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8970\n",
      "Epoch 00021: val_loss improved from 0.23089 to 0.22250, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/021-0.2225.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3482 - acc: 0.8969 - val_loss: 0.2225 - val_acc: 0.9397\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8994\n",
      "Epoch 00022: val_loss improved from 0.22250 to 0.21427, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/022-0.2143.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3381 - acc: 0.8994 - val_loss: 0.2143 - val_acc: 0.9387\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9042\n",
      "Epoch 00023: val_loss improved from 0.21427 to 0.20773, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/023-0.2077.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3225 - acc: 0.9041 - val_loss: 0.2077 - val_acc: 0.9446\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9064\n",
      "Epoch 00024: val_loss improved from 0.20773 to 0.19987, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/024-0.1999.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.3112 - acc: 0.9064 - val_loss: 0.1999 - val_acc: 0.9467\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9095\n",
      "Epoch 00025: val_loss improved from 0.19987 to 0.18904, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/025-0.1890.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.3010 - acc: 0.9094 - val_loss: 0.1890 - val_acc: 0.9511\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9140\n",
      "Epoch 00026: val_loss did not improve from 0.18904\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2906 - acc: 0.9140 - val_loss: 0.2481 - val_acc: 0.9324\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9170\n",
      "Epoch 00027: val_loss did not improve from 0.18904\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2825 - acc: 0.9170 - val_loss: 0.2038 - val_acc: 0.9446\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9182\n",
      "Epoch 00028: val_loss did not improve from 0.18904\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2720 - acc: 0.9182 - val_loss: 0.2055 - val_acc: 0.9439\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9206\n",
      "Epoch 00029: val_loss did not improve from 0.18904\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2675 - acc: 0.9206 - val_loss: 0.1957 - val_acc: 0.9495\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9224\n",
      "Epoch 00030: val_loss improved from 0.18904 to 0.18453, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/030-0.1845.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2571 - acc: 0.9223 - val_loss: 0.1845 - val_acc: 0.9490\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9224\n",
      "Epoch 00031: val_loss did not improve from 0.18453\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2572 - acc: 0.9224 - val_loss: 0.1878 - val_acc: 0.9478\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9259\n",
      "Epoch 00032: val_loss did not improve from 0.18453\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2449 - acc: 0.9259 - val_loss: 0.2025 - val_acc: 0.9392\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9310\n",
      "Epoch 00033: val_loss improved from 0.18453 to 0.18241, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/033-0.1824.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2316 - acc: 0.9309 - val_loss: 0.1824 - val_acc: 0.9515\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9299\n",
      "Epoch 00034: val_loss did not improve from 0.18241\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2309 - acc: 0.9299 - val_loss: 0.1858 - val_acc: 0.9471\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9337\n",
      "Epoch 00035: val_loss improved from 0.18241 to 0.16898, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/035-0.1690.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2217 - acc: 0.9337 - val_loss: 0.1690 - val_acc: 0.9574\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9336\n",
      "Epoch 00036: val_loss improved from 0.16898 to 0.16843, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/036-0.1684.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.2184 - acc: 0.9336 - val_loss: 0.1684 - val_acc: 0.9525\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9351\n",
      "Epoch 00037: val_loss did not improve from 0.16843\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2098 - acc: 0.9351 - val_loss: 0.1786 - val_acc: 0.9513\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9402\n",
      "Epoch 00038: val_loss did not improve from 0.16843\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1998 - acc: 0.9402 - val_loss: 0.1728 - val_acc: 0.9532\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9383\n",
      "Epoch 00039: val_loss improved from 0.16843 to 0.16485, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/039-0.1648.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.2015 - acc: 0.9384 - val_loss: 0.1648 - val_acc: 0.9515\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9405\n",
      "Epoch 00040: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1949 - acc: 0.9405 - val_loss: 0.1725 - val_acc: 0.9536\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9420\n",
      "Epoch 00041: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1930 - acc: 0.9419 - val_loss: 0.1871 - val_acc: 0.9515\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9417\n",
      "Epoch 00042: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1885 - acc: 0.9416 - val_loss: 0.1890 - val_acc: 0.9474\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9464\n",
      "Epoch 00043: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1791 - acc: 0.9464 - val_loss: 0.1660 - val_acc: 0.9560\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9479\n",
      "Epoch 00044: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1732 - acc: 0.9479 - val_loss: 0.1822 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9446\n",
      "Epoch 00045: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1790 - acc: 0.9446 - val_loss: 0.1652 - val_acc: 0.9553\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9459\n",
      "Epoch 00046: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1761 - acc: 0.9458 - val_loss: 0.1736 - val_acc: 0.9520\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9490\n",
      "Epoch 00047: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1641 - acc: 0.9490 - val_loss: 0.1688 - val_acc: 0.9557\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9513\n",
      "Epoch 00048: val_loss did not improve from 0.16485\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1598 - acc: 0.9513 - val_loss: 0.1814 - val_acc: 0.9460\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9521\n",
      "Epoch 00049: val_loss improved from 0.16485 to 0.16209, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/049-0.1621.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1564 - acc: 0.9521 - val_loss: 0.1621 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9555\n",
      "Epoch 00050: val_loss improved from 0.16209 to 0.15471, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/050-0.1547.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1456 - acc: 0.9554 - val_loss: 0.1547 - val_acc: 0.9574\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9521\n",
      "Epoch 00051: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1563 - acc: 0.9521 - val_loss: 0.1717 - val_acc: 0.9532\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9546\n",
      "Epoch 00052: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1456 - acc: 0.9546 - val_loss: 0.1748 - val_acc: 0.9567\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9543\n",
      "Epoch 00053: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1503 - acc: 0.9542 - val_loss: 0.1614 - val_acc: 0.9555\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9593\n",
      "Epoch 00054: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1366 - acc: 0.9593 - val_loss: 0.1651 - val_acc: 0.9539\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9576\n",
      "Epoch 00055: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1342 - acc: 0.9575 - val_loss: 0.1856 - val_acc: 0.9481\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9600\n",
      "Epoch 00056: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1299 - acc: 0.9600 - val_loss: 0.1771 - val_acc: 0.9522\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9604\n",
      "Epoch 00057: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1294 - acc: 0.9604 - val_loss: 0.1748 - val_acc: 0.9515\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9620\n",
      "Epoch 00058: val_loss did not improve from 0.15471\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.1267 - acc: 0.9619 - val_loss: 0.1824 - val_acc: 0.9511\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9491\n",
      "Epoch 00059: val_loss improved from 0.15471 to 0.15394, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/059-0.1539.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1674 - acc: 0.9491 - val_loss: 0.1539 - val_acc: 0.9590\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9611\n",
      "Epoch 00060: val_loss did not improve from 0.15394\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1256 - acc: 0.9611 - val_loss: 0.1682 - val_acc: 0.9560\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9596\n",
      "Epoch 00061: val_loss did not improve from 0.15394\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1302 - acc: 0.9596 - val_loss: 0.1678 - val_acc: 0.9581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9640\n",
      "Epoch 00062: val_loss improved from 0.15394 to 0.15264, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/062-0.1526.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1143 - acc: 0.9640 - val_loss: 0.1526 - val_acc: 0.9595\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9651\n",
      "Epoch 00063: val_loss did not improve from 0.15264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1136 - acc: 0.9650 - val_loss: 0.1649 - val_acc: 0.9590\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9614\n",
      "Epoch 00064: val_loss did not improve from 0.15264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1229 - acc: 0.9614 - val_loss: 0.1624 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9667\n",
      "Epoch 00065: val_loss did not improve from 0.15264\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1120 - acc: 0.9667 - val_loss: 0.1535 - val_acc: 0.9597\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9634\n",
      "Epoch 00066: val_loss improved from 0.15264 to 0.15216, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/066-0.1522.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1148 - acc: 0.9634 - val_loss: 0.1522 - val_acc: 0.9583\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9660\n",
      "Epoch 00067: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1078 - acc: 0.9660 - val_loss: 0.1730 - val_acc: 0.9536\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9689\n",
      "Epoch 00068: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1021 - acc: 0.9688 - val_loss: 0.1699 - val_acc: 0.9604\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9657\n",
      "Epoch 00069: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1131 - acc: 0.9657 - val_loss: 0.1842 - val_acc: 0.9488\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9680\n",
      "Epoch 00070: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1025 - acc: 0.9680 - val_loss: 0.1931 - val_acc: 0.9518\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9662\n",
      "Epoch 00071: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1071 - acc: 0.9662 - val_loss: 0.1823 - val_acc: 0.9529\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9700\n",
      "Epoch 00072: val_loss did not improve from 0.15216\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0998 - acc: 0.9699 - val_loss: 0.1703 - val_acc: 0.9602\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9657\n",
      "Epoch 00073: val_loss improved from 0.15216 to 0.15126, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/073-0.1513.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1101 - acc: 0.9657 - val_loss: 0.1513 - val_acc: 0.9606\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9700\n",
      "Epoch 00074: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0948 - acc: 0.9700 - val_loss: 0.1642 - val_acc: 0.9588\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9713\n",
      "Epoch 00075: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0915 - acc: 0.9713 - val_loss: 0.1698 - val_acc: 0.9562\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9712\n",
      "Epoch 00076: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0960 - acc: 0.9712 - val_loss: 0.2467 - val_acc: 0.9460\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9685\n",
      "Epoch 00077: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1010 - acc: 0.9685 - val_loss: 0.1721 - val_acc: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9721\n",
      "Epoch 00078: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0899 - acc: 0.9720 - val_loss: 0.1681 - val_acc: 0.9585\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9671\n",
      "Epoch 00079: val_loss improved from 0.15126 to 0.15029, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_BN_9_conv_checkpoint/079-0.1503.hdf5\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.1051 - acc: 0.9671 - val_loss: 0.1503 - val_acc: 0.9604\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9704\n",
      "Epoch 00080: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0930 - acc: 0.9704 - val_loss: 0.1574 - val_acc: 0.9623\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9735\n",
      "Epoch 00081: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0843 - acc: 0.9735 - val_loss: 0.1570 - val_acc: 0.9620\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9731\n",
      "Epoch 00082: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.0852 - acc: 0.9731 - val_loss: 0.1616 - val_acc: 0.9581\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9752\n",
      "Epoch 00083: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0793 - acc: 0.9752 - val_loss: 0.1788 - val_acc: 0.9590\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9732\n",
      "Epoch 00084: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0838 - acc: 0.9732 - val_loss: 0.2205 - val_acc: 0.9432\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9719\n",
      "Epoch 00085: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0876 - acc: 0.9719 - val_loss: 0.1650 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9744\n",
      "Epoch 00086: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0820 - acc: 0.9744 - val_loss: 0.2058 - val_acc: 0.9469\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9697\n",
      "Epoch 00087: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0957 - acc: 0.9697 - val_loss: 0.1777 - val_acc: 0.9578\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9740\n",
      "Epoch 00088: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0815 - acc: 0.9740 - val_loss: 0.1700 - val_acc: 0.9602\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9718\n",
      "Epoch 00089: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0890 - acc: 0.9719 - val_loss: 0.1792 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9782\n",
      "Epoch 00090: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0709 - acc: 0.9782 - val_loss: 0.1588 - val_acc: 0.9616\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9736\n",
      "Epoch 00091: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0843 - acc: 0.9736 - val_loss: 0.1769 - val_acc: 0.9539\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9750\n",
      "Epoch 00092: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0818 - acc: 0.9750 - val_loss: 0.1750 - val_acc: 0.9581\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9778\n",
      "Epoch 00093: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0727 - acc: 0.9778 - val_loss: 0.1580 - val_acc: 0.9623\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9786\n",
      "Epoch 00094: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0698 - acc: 0.9785 - val_loss: 0.1692 - val_acc: 0.9616\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9751\n",
      "Epoch 00095: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0765 - acc: 0.9751 - val_loss: 0.1767 - val_acc: 0.9585\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9772\n",
      "Epoch 00096: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0706 - acc: 0.9772 - val_loss: 0.1751 - val_acc: 0.9609\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9783\n",
      "Epoch 00097: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0698 - acc: 0.9783 - val_loss: 0.1907 - val_acc: 0.9550\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9770\n",
      "Epoch 00098: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0721 - acc: 0.9770 - val_loss: 0.2031 - val_acc: 0.9532\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9780\n",
      "Epoch 00099: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0704 - acc: 0.9780 - val_loss: 0.1799 - val_acc: 0.9581\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9765\n",
      "Epoch 00100: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0732 - acc: 0.9765 - val_loss: 0.2297 - val_acc: 0.9504\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9731\n",
      "Epoch 00101: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0893 - acc: 0.9730 - val_loss: 0.2023 - val_acc: 0.9543\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9760\n",
      "Epoch 00102: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0746 - acc: 0.9760 - val_loss: 0.1779 - val_acc: 0.9599\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9808\n",
      "Epoch 00103: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0602 - acc: 0.9808 - val_loss: 0.2083 - val_acc: 0.9511\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9755\n",
      "Epoch 00104: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0785 - acc: 0.9755 - val_loss: 0.1712 - val_acc: 0.9592\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9818\n",
      "Epoch 00105: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0582 - acc: 0.9818 - val_loss: 0.1953 - val_acc: 0.9571\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9819\n",
      "Epoch 00106: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0591 - acc: 0.9819 - val_loss: 0.1876 - val_acc: 0.9578\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9801\n",
      "Epoch 00107: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0624 - acc: 0.9801 - val_loss: 0.1689 - val_acc: 0.9585\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9780\n",
      "Epoch 00108: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0721 - acc: 0.9780 - val_loss: 0.2771 - val_acc: 0.9362\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9763\n",
      "Epoch 00109: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0772 - acc: 0.9763 - val_loss: 0.2050 - val_acc: 0.9529\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9766\n",
      "Epoch 00110: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0738 - acc: 0.9766 - val_loss: 0.1872 - val_acc: 0.9534\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9795\n",
      "Epoch 00111: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0639 - acc: 0.9795 - val_loss: 0.1800 - val_acc: 0.9571\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9815\n",
      "Epoch 00112: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0586 - acc: 0.9815 - val_loss: 0.1922 - val_acc: 0.9564\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9815\n",
      "Epoch 00113: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0601 - acc: 0.9815 - val_loss: 0.1777 - val_acc: 0.9588\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9818\n",
      "Epoch 00114: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0575 - acc: 0.9818 - val_loss: 0.2001 - val_acc: 0.9569\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9773\n",
      "Epoch 00115: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0746 - acc: 0.9773 - val_loss: 0.1929 - val_acc: 0.9550\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9807\n",
      "Epoch 00116: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0604 - acc: 0.9807 - val_loss: 0.2170 - val_acc: 0.9525\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9829\n",
      "Epoch 00117: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0549 - acc: 0.9829 - val_loss: 0.1739 - val_acc: 0.9590\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9821\n",
      "Epoch 00118: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0576 - acc: 0.9821 - val_loss: 0.2021 - val_acc: 0.9541\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9781\n",
      "Epoch 00119: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0694 - acc: 0.9781 - val_loss: 0.1996 - val_acc: 0.9536\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9828\n",
      "Epoch 00120: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0535 - acc: 0.9828 - val_loss: 0.1987 - val_acc: 0.9555\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9821\n",
      "Epoch 00121: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0574 - acc: 0.9821 - val_loss: 0.1854 - val_acc: 0.9602\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9807\n",
      "Epoch 00122: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0588 - acc: 0.9807 - val_loss: 0.2324 - val_acc: 0.9502\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9789\n",
      "Epoch 00123: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0679 - acc: 0.9789 - val_loss: 0.1714 - val_acc: 0.9599\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9834\n",
      "Epoch 00124: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0501 - acc: 0.9834 - val_loss: 0.2048 - val_acc: 0.9553\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9846\n",
      "Epoch 00125: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0508 - acc: 0.9846 - val_loss: 0.1975 - val_acc: 0.9536\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9805\n",
      "Epoch 00126: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0612 - acc: 0.9805 - val_loss: 0.1884 - val_acc: 0.9546\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9795\n",
      "Epoch 00127: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0640 - acc: 0.9795 - val_loss: 0.1868 - val_acc: 0.9546\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9840\n",
      "Epoch 00128: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0535 - acc: 0.9841 - val_loss: 0.1709 - val_acc: 0.9618\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9835\n",
      "Epoch 00129: val_loss did not improve from 0.15029\n",
      "36805/36805 [==============================] - 182s 5ms/sample - loss: 0.0524 - acc: 0.9835 - val_loss: 0.1673 - val_acc: 0.9553\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2100 - acc: 0.9408\n",
      "Loss: 0.21001893762278595 Accuracy: 0.94080997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_4_ch_128_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 813,904\n",
      "Trainable params: 813,008\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9989 - acc: 0.7242\n",
      "Loss: 0.9988764359324644 Accuracy: 0.72419524\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 430,224\n",
      "Trainable params: 429,200\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.6696 - acc: 0.8010\n",
      "Loss: 0.6695837328367145 Accuracy: 0.80103844\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 315,856\n",
      "Trainable params: 314,704\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.4538 - acc: 0.8725\n",
      "Loss: 0.4538032439267524 Accuracy: 0.8724818\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 270,448\n",
      "Trainable params: 269,232\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2935 - acc: 0.9161\n",
      "Loss: 0.2935313658117629 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 268,560\n",
      "Trainable params: 267,280\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2099 - acc: 0.9373\n",
      "Loss: 0.20994856844067203 Accuracy: 0.93727934\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 271,280\n",
      "Trainable params: 269,936\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2100 - acc: 0.9408\n",
      "Loss: 0.21001893762278595 Accuracy: 0.94080997\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_4_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 813,904\n",
      "Trainable params: 813,008\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.4671 - acc: 0.7186\n",
      "Loss: 1.4671491958146774 Accuracy: 0.71858776\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 430,224\n",
      "Trainable params: 429,200\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.7340 - acc: 0.8185\n",
      "Loss: 0.7339927767791233 Accuracy: 0.8184839\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 315,856\n",
      "Trainable params: 314,704\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.5076 - acc: 0.8685\n",
      "Loss: 0.5076343005969889 Accuracy: 0.8685358\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 270,448\n",
      "Trainable params: 269,232\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3353 - acc: 0.9178\n",
      "Loss: 0.33526503316833717 Accuracy: 0.91775703\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 268,560\n",
      "Trainable params: 267,280\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2479 - acc: 0.9371\n",
      "Loss: 0.24785564507465124 Accuracy: 0.9370716\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 271,280\n",
      "Trainable params: 269,936\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2458 - acc: 0.9402\n",
      "Loss: 0.2458303237161749 Accuracy: 0.9401869\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
