{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 151616)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 151616)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2425872     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,467,344\n",
      "Trainable params: 2,467,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50496)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50496)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           807952      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 869,968\n",
      "Trainable params: 869,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20928)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20928)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           334864      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 437,968\n",
      "Trainable params: 437,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11008)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 11008)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           176144      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 361,296\n",
      "Trainable params: 361,296\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3584)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3584)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           57360       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 324,560\n",
      "Trainable params: 324,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1152)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1152)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           18448       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 367,696\n",
      "Trainable params: 367,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0679 - acc: 0.3472\n",
      "Epoch 00001: val_loss improved from inf to 1.65629, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/001-1.6563.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 2.0678 - acc: 0.3472 - val_loss: 1.6563 - val_acc: 0.4938\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4961 - acc: 0.5349\n",
      "Epoch 00002: val_loss improved from 1.65629 to 1.44573, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/002-1.4457.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.4960 - acc: 0.5349 - val_loss: 1.4457 - val_acc: 0.5570\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2955 - acc: 0.6017\n",
      "Epoch 00003: val_loss improved from 1.44573 to 1.35351, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/003-1.3535.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 1.2955 - acc: 0.6018 - val_loss: 1.3535 - val_acc: 0.5714\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1494 - acc: 0.6463\n",
      "Epoch 00004: val_loss improved from 1.35351 to 1.30968, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/004-1.3097.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 1.1495 - acc: 0.6462 - val_loss: 1.3097 - val_acc: 0.5865\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0250 - acc: 0.6893\n",
      "Epoch 00005: val_loss improved from 1.30968 to 1.30267, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/005-1.3027.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 1.0251 - acc: 0.6893 - val_loss: 1.3027 - val_acc: 0.6000\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.7179\n",
      "Epoch 00006: val_loss improved from 1.30267 to 1.30167, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/006-1.3017.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.9275 - acc: 0.7180 - val_loss: 1.3017 - val_acc: 0.5917\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8403 - acc: 0.7456\n",
      "Epoch 00007: val_loss improved from 1.30167 to 1.28681, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/007-1.2868.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.8403 - acc: 0.7456 - val_loss: 1.2868 - val_acc: 0.5970\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7706\n",
      "Epoch 00008: val_loss did not improve from 1.28681\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.7582 - acc: 0.7707 - val_loss: 1.2877 - val_acc: 0.5986\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.7924\n",
      "Epoch 00009: val_loss improved from 1.28681 to 1.25977, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_3_conv_checkpoint/009-1.2598.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.6873 - acc: 0.7923 - val_loss: 1.2598 - val_acc: 0.6189\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6235 - acc: 0.8117\n",
      "Epoch 00010: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.6235 - acc: 0.8117 - val_loss: 1.2915 - val_acc: 0.6101\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.8272\n",
      "Epoch 00011: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.5705 - acc: 0.8272 - val_loss: 1.3034 - val_acc: 0.6252\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8458\n",
      "Epoch 00012: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.5157 - acc: 0.8458 - val_loss: 1.3016 - val_acc: 0.6215\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4676 - acc: 0.8595\n",
      "Epoch 00013: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.4676 - acc: 0.8595 - val_loss: 1.3293 - val_acc: 0.6338\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8703\n",
      "Epoch 00014: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.4338 - acc: 0.8703 - val_loss: 1.3869 - val_acc: 0.6259\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8803\n",
      "Epoch 00015: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.3928 - acc: 0.8803 - val_loss: 1.3796 - val_acc: 0.6280\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8923\n",
      "Epoch 00016: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.3563 - acc: 0.8924 - val_loss: 1.3800 - val_acc: 0.6382\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.9005\n",
      "Epoch 00017: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.3328 - acc: 0.9005 - val_loss: 1.3891 - val_acc: 0.6441\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9115\n",
      "Epoch 00018: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3027 - acc: 0.9115 - val_loss: 1.4523 - val_acc: 0.6375\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9155\n",
      "Epoch 00019: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2814 - acc: 0.9155 - val_loss: 1.4679 - val_acc: 0.6396\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9204\n",
      "Epoch 00020: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2639 - acc: 0.9204 - val_loss: 1.4427 - val_acc: 0.6469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9282\n",
      "Epoch 00021: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2400 - acc: 0.9282 - val_loss: 1.5213 - val_acc: 0.6464\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9321\n",
      "Epoch 00022: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2282 - acc: 0.9321 - val_loss: 1.4905 - val_acc: 0.6534\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9377\n",
      "Epoch 00023: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2086 - acc: 0.9376 - val_loss: 1.5750 - val_acc: 0.6457\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9413\n",
      "Epoch 00024: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1984 - acc: 0.9412 - val_loss: 1.5687 - val_acc: 0.6520\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9448\n",
      "Epoch 00025: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1886 - acc: 0.9448 - val_loss: 1.6155 - val_acc: 0.6427\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9501\n",
      "Epoch 00026: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1724 - acc: 0.9501 - val_loss: 1.6091 - val_acc: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9518\n",
      "Epoch 00027: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1677 - acc: 0.9518 - val_loss: 1.6505 - val_acc: 0.6573\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9546\n",
      "Epoch 00028: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1584 - acc: 0.9546 - val_loss: 1.6619 - val_acc: 0.6518\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9573\n",
      "Epoch 00029: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1484 - acc: 0.9573 - val_loss: 1.7072 - val_acc: 0.6471\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9583\n",
      "Epoch 00030: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1441 - acc: 0.9583 - val_loss: 1.6709 - val_acc: 0.6520\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9638\n",
      "Epoch 00031: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1303 - acc: 0.9638 - val_loss: 1.7191 - val_acc: 0.6499\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9630\n",
      "Epoch 00032: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1299 - acc: 0.9630 - val_loss: 1.7119 - val_acc: 0.6567\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9646\n",
      "Epoch 00033: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1259 - acc: 0.9646 - val_loss: 1.6701 - val_acc: 0.6760\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9665\n",
      "Epoch 00034: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1161 - acc: 0.9666 - val_loss: 1.6805 - val_acc: 0.6690\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9665\n",
      "Epoch 00035: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1172 - acc: 0.9665 - val_loss: 1.8064 - val_acc: 0.6546\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9707\n",
      "Epoch 00036: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1079 - acc: 0.9707 - val_loss: 1.7321 - val_acc: 0.6632\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9712\n",
      "Epoch 00037: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1024 - acc: 0.9712 - val_loss: 1.8152 - val_acc: 0.6620\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9712\n",
      "Epoch 00038: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1029 - acc: 0.9712 - val_loss: 1.8458 - val_acc: 0.6562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9725\n",
      "Epoch 00039: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0994 - acc: 0.9725 - val_loss: 1.8137 - val_acc: 0.6671\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9723\n",
      "Epoch 00040: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0992 - acc: 0.9723 - val_loss: 1.8040 - val_acc: 0.6706\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9751\n",
      "Epoch 00041: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0901 - acc: 0.9751 - val_loss: 1.7918 - val_acc: 0.6751\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9760\n",
      "Epoch 00042: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0867 - acc: 0.9760 - val_loss: 1.8202 - val_acc: 0.6776\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9773\n",
      "Epoch 00043: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0868 - acc: 0.9773 - val_loss: 1.8327 - val_acc: 0.6697\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9781\n",
      "Epoch 00044: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0838 - acc: 0.9781 - val_loss: 1.8457 - val_acc: 0.6713\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9762\n",
      "Epoch 00045: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0833 - acc: 0.9762 - val_loss: 1.8605 - val_acc: 0.6734\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9779\n",
      "Epoch 00046: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0802 - acc: 0.9779 - val_loss: 1.8550 - val_acc: 0.6725\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9774\n",
      "Epoch 00047: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0826 - acc: 0.9774 - val_loss: 1.8421 - val_acc: 0.6746\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9805\n",
      "Epoch 00048: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0744 - acc: 0.9805 - val_loss: 1.8328 - val_acc: 0.6830\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9811\n",
      "Epoch 00049: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0714 - acc: 0.9811 - val_loss: 1.8815 - val_acc: 0.6755\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9806\n",
      "Epoch 00050: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0725 - acc: 0.9806 - val_loss: 1.8661 - val_acc: 0.6748\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9833\n",
      "Epoch 00051: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0657 - acc: 0.9833 - val_loss: 1.8824 - val_acc: 0.6783\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9834\n",
      "Epoch 00052: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0689 - acc: 0.9834 - val_loss: 1.9016 - val_acc: 0.6832\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9817\n",
      "Epoch 00053: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0690 - acc: 0.9817 - val_loss: 1.8488 - val_acc: 0.6839\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9823\n",
      "Epoch 00054: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0674 - acc: 0.9823 - val_loss: 1.8858 - val_acc: 0.6774\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9812\n",
      "Epoch 00055: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0664 - acc: 0.9812 - val_loss: 1.9138 - val_acc: 0.6790\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9833\n",
      "Epoch 00056: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0633 - acc: 0.9833 - val_loss: 1.9477 - val_acc: 0.6823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9846\n",
      "Epoch 00057: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0586 - acc: 0.9846 - val_loss: 1.9329 - val_acc: 0.6844\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9832\n",
      "Epoch 00058: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0623 - acc: 0.9832 - val_loss: 1.8808 - val_acc: 0.6846\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9844\n",
      "Epoch 00059: val_loss did not improve from 1.25977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0572 - acc: 0.9844 - val_loss: 1.9059 - val_acc: 0.6911\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 412us/sample - loss: 1.3416 - acc: 0.5931\n",
      "Loss: 1.3415830714680324 Accuracy: 0.59314644\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0507 - acc: 0.3405\n",
      "Epoch 00001: val_loss improved from inf to 1.60281, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/001-1.6028.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 2.0506 - acc: 0.3405 - val_loss: 1.6028 - val_acc: 0.4924\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5035 - acc: 0.5288\n",
      "Epoch 00002: val_loss improved from 1.60281 to 1.38525, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/002-1.3853.hdf5\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 1.5035 - acc: 0.5288 - val_loss: 1.3853 - val_acc: 0.5826\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3046 - acc: 0.5999\n",
      "Epoch 00003: val_loss improved from 1.38525 to 1.23837, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/003-1.2384.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 1.3046 - acc: 0.5999 - val_loss: 1.2384 - val_acc: 0.6238\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1657 - acc: 0.6448\n",
      "Epoch 00004: val_loss improved from 1.23837 to 1.12882, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/004-1.1288.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 1.1657 - acc: 0.6448 - val_loss: 1.1288 - val_acc: 0.6632\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0620 - acc: 0.6796\n",
      "Epoch 00005: val_loss improved from 1.12882 to 1.09956, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/005-1.0996.hdf5\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 1.0622 - acc: 0.6796 - val_loss: 1.0996 - val_acc: 0.6639\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9769 - acc: 0.7021\n",
      "Epoch 00006: val_loss improved from 1.09956 to 1.04668, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/006-1.0467.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.9769 - acc: 0.7021 - val_loss: 1.0467 - val_acc: 0.6844\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9001 - acc: 0.7268\n",
      "Epoch 00007: val_loss improved from 1.04668 to 0.99262, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/007-0.9926.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.9001 - acc: 0.7268 - val_loss: 0.9926 - val_acc: 0.6928\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8250 - acc: 0.7516\n",
      "Epoch 00008: val_loss improved from 0.99262 to 0.96335, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/008-0.9634.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.8250 - acc: 0.7516 - val_loss: 0.9634 - val_acc: 0.7084\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7695\n",
      "Epoch 00009: val_loss did not improve from 0.96335\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.7614 - acc: 0.7696 - val_loss: 0.9733 - val_acc: 0.6951\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7834\n",
      "Epoch 00010: val_loss improved from 0.96335 to 0.92077, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/010-0.9208.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.7116 - acc: 0.7834 - val_loss: 0.9208 - val_acc: 0.7184\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.7984\n",
      "Epoch 00011: val_loss improved from 0.92077 to 0.90852, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_4_conv_checkpoint/011-0.9085.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.6619 - acc: 0.7984 - val_loss: 0.9085 - val_acc: 0.7244\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8139\n",
      "Epoch 00012: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.6075 - acc: 0.8139 - val_loss: 0.9508 - val_acc: 0.7151\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8274\n",
      "Epoch 00013: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.5635 - acc: 0.8274 - val_loss: 0.9172 - val_acc: 0.7307\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8420\n",
      "Epoch 00014: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.5224 - acc: 0.8420 - val_loss: 0.9365 - val_acc: 0.7233\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8477\n",
      "Epoch 00015: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.4915 - acc: 0.8477 - val_loss: 0.9090 - val_acc: 0.7333\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8609\n",
      "Epoch 00016: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.4539 - acc: 0.8609 - val_loss: 0.9103 - val_acc: 0.7303\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8674\n",
      "Epoch 00017: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.4264 - acc: 0.8674 - val_loss: 0.9429 - val_acc: 0.7275\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8776\n",
      "Epoch 00018: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.3929 - acc: 0.8777 - val_loss: 0.9215 - val_acc: 0.7342\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8861\n",
      "Epoch 00019: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.3722 - acc: 0.8861 - val_loss: 0.9539 - val_acc: 0.7410\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8924\n",
      "Epoch 00020: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.3445 - acc: 0.8924 - val_loss: 0.9533 - val_acc: 0.7370\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9010\n",
      "Epoch 00021: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.3208 - acc: 0.9010 - val_loss: 0.9756 - val_acc: 0.7363\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9051\n",
      "Epoch 00022: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.3029 - acc: 0.9051 - val_loss: 0.9757 - val_acc: 0.7370\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9095\n",
      "Epoch 00023: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.2853 - acc: 0.9096 - val_loss: 0.9892 - val_acc: 0.7349\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9167\n",
      "Epoch 00024: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.2664 - acc: 0.9167 - val_loss: 0.9811 - val_acc: 0.7466\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9190\n",
      "Epoch 00025: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.2550 - acc: 0.9190 - val_loss: 1.0113 - val_acc: 0.7393\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9245\n",
      "Epoch 00026: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.2417 - acc: 0.9244 - val_loss: 1.0176 - val_acc: 0.7456\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9295\n",
      "Epoch 00027: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.2251 - acc: 0.9295 - val_loss: 1.0249 - val_acc: 0.7445\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9311\n",
      "Epoch 00028: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.2195 - acc: 0.9311 - val_loss: 1.0438 - val_acc: 0.7503\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9351\n",
      "Epoch 00029: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.2048 - acc: 0.9351 - val_loss: 1.0370 - val_acc: 0.7463\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9368\n",
      "Epoch 00030: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1986 - acc: 0.9368 - val_loss: 1.0551 - val_acc: 0.7435\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9405\n",
      "Epoch 00031: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1867 - acc: 0.9405 - val_loss: 1.0327 - val_acc: 0.7531\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9418\n",
      "Epoch 00032: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1827 - acc: 0.9418 - val_loss: 1.0872 - val_acc: 0.7533\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9442\n",
      "Epoch 00033: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1752 - acc: 0.9442 - val_loss: 1.0170 - val_acc: 0.7610\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9483\n",
      "Epoch 00034: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1616 - acc: 0.9483 - val_loss: 1.0887 - val_acc: 0.7529\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9512\n",
      "Epoch 00035: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1571 - acc: 0.9512 - val_loss: 1.0636 - val_acc: 0.7594\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9531\n",
      "Epoch 00036: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1504 - acc: 0.9531 - val_loss: 1.1054 - val_acc: 0.7552\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9520\n",
      "Epoch 00037: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1503 - acc: 0.9520 - val_loss: 1.0916 - val_acc: 0.7582\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9565\n",
      "Epoch 00038: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1370 - acc: 0.9566 - val_loss: 1.0730 - val_acc: 0.7659\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9567\n",
      "Epoch 00039: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1354 - acc: 0.9567 - val_loss: 1.1628 - val_acc: 0.7556\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9612\n",
      "Epoch 00040: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1262 - acc: 0.9612 - val_loss: 1.1270 - val_acc: 0.7587\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9609\n",
      "Epoch 00041: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1227 - acc: 0.9609 - val_loss: 1.1234 - val_acc: 0.7650\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9605\n",
      "Epoch 00042: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1236 - acc: 0.9605 - val_loss: 1.1535 - val_acc: 0.7587\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9664\n",
      "Epoch 00043: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1114 - acc: 0.9664 - val_loss: 1.0842 - val_acc: 0.7682\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9657\n",
      "Epoch 00044: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1139 - acc: 0.9657 - val_loss: 1.1514 - val_acc: 0.7545\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9639\n",
      "Epoch 00045: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1145 - acc: 0.9639 - val_loss: 1.1303 - val_acc: 0.7668\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9648\n",
      "Epoch 00046: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1104 - acc: 0.9648 - val_loss: 1.1560 - val_acc: 0.7636\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9669\n",
      "Epoch 00047: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1043 - acc: 0.9669 - val_loss: 1.1306 - val_acc: 0.7685\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9687\n",
      "Epoch 00048: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1015 - acc: 0.9687 - val_loss: 1.1590 - val_acc: 0.7633\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9687\n",
      "Epoch 00049: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1018 - acc: 0.9687 - val_loss: 1.1589 - val_acc: 0.7699\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9693\n",
      "Epoch 00050: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1008 - acc: 0.9694 - val_loss: 1.1759 - val_acc: 0.7685\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9728\n",
      "Epoch 00051: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0908 - acc: 0.9728 - val_loss: 1.1975 - val_acc: 0.7738\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9693\n",
      "Epoch 00052: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0977 - acc: 0.9693 - val_loss: 1.1517 - val_acc: 0.7694\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9739\n",
      "Epoch 00053: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0872 - acc: 0.9739 - val_loss: 1.1775 - val_acc: 0.7682\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9740\n",
      "Epoch 00054: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0884 - acc: 0.9740 - val_loss: 1.1709 - val_acc: 0.7694\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9753\n",
      "Epoch 00055: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0850 - acc: 0.9753 - val_loss: 1.2388 - val_acc: 0.7671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9741\n",
      "Epoch 00056: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0835 - acc: 0.9741 - val_loss: 1.1722 - val_acc: 0.7724\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9751\n",
      "Epoch 00057: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0831 - acc: 0.9751 - val_loss: 1.1728 - val_acc: 0.7764\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9751\n",
      "Epoch 00058: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0803 - acc: 0.9751 - val_loss: 1.2701 - val_acc: 0.7587\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9755\n",
      "Epoch 00059: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0819 - acc: 0.9755 - val_loss: 1.1939 - val_acc: 0.7701\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9746\n",
      "Epoch 00060: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0834 - acc: 0.9746 - val_loss: 1.2235 - val_acc: 0.7612\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9760\n",
      "Epoch 00061: val_loss did not improve from 0.90852\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0785 - acc: 0.9760 - val_loss: 1.1740 - val_acc: 0.7787\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 443us/sample - loss: 1.0058 - acc: 0.6922\n",
      "Loss: 1.005834775067564 Accuracy: 0.69221187\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0700 - acc: 0.3324\n",
      "Epoch 00001: val_loss improved from inf to 1.54189, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/001-1.5419.hdf5\n",
      "36805/36805 [==============================] - 35s 937us/sample - loss: 2.0698 - acc: 0.3325 - val_loss: 1.5419 - val_acc: 0.5222\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5152 - acc: 0.5239\n",
      "Epoch 00002: val_loss improved from 1.54189 to 1.34026, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/002-1.3403.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 1.5152 - acc: 0.5238 - val_loss: 1.3403 - val_acc: 0.5875\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3381 - acc: 0.5890\n",
      "Epoch 00003: val_loss improved from 1.34026 to 1.19187, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/003-1.1919.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 1.3382 - acc: 0.5889 - val_loss: 1.1919 - val_acc: 0.6501\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2029 - acc: 0.6327\n",
      "Epoch 00004: val_loss improved from 1.19187 to 1.08871, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/004-1.0887.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 1.2030 - acc: 0.6326 - val_loss: 1.0887 - val_acc: 0.6853\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0998 - acc: 0.6668\n",
      "Epoch 00005: val_loss improved from 1.08871 to 1.00244, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/005-1.0024.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 1.0998 - acc: 0.6668 - val_loss: 1.0024 - val_acc: 0.7137\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0195 - acc: 0.6917\n",
      "Epoch 00006: val_loss improved from 1.00244 to 0.98086, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/006-0.9809.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 1.0194 - acc: 0.6918 - val_loss: 0.9809 - val_acc: 0.7023\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9483 - acc: 0.7167\n",
      "Epoch 00007: val_loss improved from 0.98086 to 0.93360, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/007-0.9336.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.9482 - acc: 0.7168 - val_loss: 0.9336 - val_acc: 0.7251\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7330\n",
      "Epoch 00008: val_loss improved from 0.93360 to 0.84613, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/008-0.8461.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.8860 - acc: 0.7330 - val_loss: 0.8461 - val_acc: 0.7556\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7480\n",
      "Epoch 00009: val_loss improved from 0.84613 to 0.81718, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/009-0.8172.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.8350 - acc: 0.7479 - val_loss: 0.8172 - val_acc: 0.7666\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7829 - acc: 0.7657\n",
      "Epoch 00010: val_loss improved from 0.81718 to 0.76737, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/010-0.7674.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.7829 - acc: 0.7657 - val_loss: 0.7674 - val_acc: 0.7789\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7349 - acc: 0.7799\n",
      "Epoch 00011: val_loss improved from 0.76737 to 0.75746, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/011-0.7575.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.7349 - acc: 0.7799 - val_loss: 0.7575 - val_acc: 0.7771\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6895 - acc: 0.7924\n",
      "Epoch 00012: val_loss improved from 0.75746 to 0.71137, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/012-0.7114.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.6896 - acc: 0.7924 - val_loss: 0.7114 - val_acc: 0.7969\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8073\n",
      "Epoch 00013: val_loss improved from 0.71137 to 0.67867, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/013-0.6787.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.6475 - acc: 0.8074 - val_loss: 0.6787 - val_acc: 0.8060\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6076 - acc: 0.8173\n",
      "Epoch 00014: val_loss improved from 0.67867 to 0.63709, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/014-0.6371.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.6076 - acc: 0.8173 - val_loss: 0.6371 - val_acc: 0.8143\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5749 - acc: 0.8284\n",
      "Epoch 00015: val_loss improved from 0.63709 to 0.62283, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/015-0.6228.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.5748 - acc: 0.8284 - val_loss: 0.6228 - val_acc: 0.8174\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8375\n",
      "Epoch 00016: val_loss improved from 0.62283 to 0.60864, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/016-0.6086.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.5448 - acc: 0.8375 - val_loss: 0.6086 - val_acc: 0.8253\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8444\n",
      "Epoch 00017: val_loss improved from 0.60864 to 0.58841, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/017-0.5884.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.5139 - acc: 0.8444 - val_loss: 0.5884 - val_acc: 0.8360\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8510\n",
      "Epoch 00018: val_loss improved from 0.58841 to 0.56990, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/018-0.5699.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.4864 - acc: 0.8509 - val_loss: 0.5699 - val_acc: 0.8360\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8593\n",
      "Epoch 00019: val_loss improved from 0.56990 to 0.56910, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/019-0.5691.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.4660 - acc: 0.8593 - val_loss: 0.5691 - val_acc: 0.8404\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8677\n",
      "Epoch 00020: val_loss improved from 0.56910 to 0.56300, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/020-0.5630.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.4418 - acc: 0.8677 - val_loss: 0.5630 - val_acc: 0.8425\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8735\n",
      "Epoch 00021: val_loss did not improve from 0.56300\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.4178 - acc: 0.8735 - val_loss: 0.5742 - val_acc: 0.8311\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8801\n",
      "Epoch 00022: val_loss improved from 0.56300 to 0.54121, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/022-0.5412.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.4002 - acc: 0.8801 - val_loss: 0.5412 - val_acc: 0.8418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8849\n",
      "Epoch 00023: val_loss did not improve from 0.54121\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.3775 - acc: 0.8849 - val_loss: 0.5784 - val_acc: 0.8348\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8890\n",
      "Epoch 00024: val_loss did not improve from 0.54121\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.3611 - acc: 0.8890 - val_loss: 0.5693 - val_acc: 0.8381\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8967\n",
      "Epoch 00025: val_loss improved from 0.54121 to 0.53672, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/025-0.5367.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.3420 - acc: 0.8967 - val_loss: 0.5367 - val_acc: 0.8549\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8988\n",
      "Epoch 00026: val_loss did not improve from 0.53672\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3277 - acc: 0.8988 - val_loss: 0.5549 - val_acc: 0.8556\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9055\n",
      "Epoch 00027: val_loss improved from 0.53672 to 0.53555, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/027-0.5356.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.3115 - acc: 0.9055 - val_loss: 0.5356 - val_acc: 0.8537\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9061\n",
      "Epoch 00028: val_loss did not improve from 0.53555\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.3011 - acc: 0.9061 - val_loss: 0.5724 - val_acc: 0.8421\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9120\n",
      "Epoch 00029: val_loss improved from 0.53555 to 0.51787, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/029-0.5179.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.2811 - acc: 0.9120 - val_loss: 0.5179 - val_acc: 0.8588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9143\n",
      "Epoch 00030: val_loss did not improve from 0.51787\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.2779 - acc: 0.9143 - val_loss: 0.5399 - val_acc: 0.8574\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9188\n",
      "Epoch 00031: val_loss improved from 0.51787 to 0.51169, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/031-0.5117.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.2613 - acc: 0.9188 - val_loss: 0.5117 - val_acc: 0.8658\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9211\n",
      "Epoch 00032: val_loss did not improve from 0.51169\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.2516 - acc: 0.9211 - val_loss: 0.5547 - val_acc: 0.8477\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9206\n",
      "Epoch 00033: val_loss did not improve from 0.51169\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.2513 - acc: 0.9206 - val_loss: 0.5280 - val_acc: 0.8612\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9269\n",
      "Epoch 00034: val_loss did not improve from 0.51169\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.2321 - acc: 0.9269 - val_loss: 0.5664 - val_acc: 0.8523\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9297\n",
      "Epoch 00035: val_loss improved from 0.51169 to 0.50926, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_5_conv_checkpoint/035-0.5093.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.2188 - acc: 0.9297 - val_loss: 0.5093 - val_acc: 0.8721\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9343\n",
      "Epoch 00036: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.2099 - acc: 0.9343 - val_loss: 0.5173 - val_acc: 0.8640\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9344\n",
      "Epoch 00037: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.2071 - acc: 0.9344 - val_loss: 0.5440 - val_acc: 0.8570\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9341\n",
      "Epoch 00038: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.2006 - acc: 0.9341 - val_loss: 0.5284 - val_acc: 0.8658\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9381\n",
      "Epoch 00039: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1925 - acc: 0.9381 - val_loss: 0.5387 - val_acc: 0.8677\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9415\n",
      "Epoch 00040: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1808 - acc: 0.9416 - val_loss: 0.5133 - val_acc: 0.8686\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9432\n",
      "Epoch 00041: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.1755 - acc: 0.9432 - val_loss: 0.5743 - val_acc: 0.8593\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9452\n",
      "Epoch 00042: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.1669 - acc: 0.9453 - val_loss: 0.5183 - val_acc: 0.8684\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9463\n",
      "Epoch 00043: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1648 - acc: 0.9463 - val_loss: 0.5454 - val_acc: 0.8675\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9485\n",
      "Epoch 00044: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1601 - acc: 0.9484 - val_loss: 0.5411 - val_acc: 0.8658\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9483\n",
      "Epoch 00045: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1561 - acc: 0.9483 - val_loss: 0.5312 - val_acc: 0.8724\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9520\n",
      "Epoch 00046: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1458 - acc: 0.9520 - val_loss: 0.5493 - val_acc: 0.8693\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9566\n",
      "Epoch 00047: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1378 - acc: 0.9566 - val_loss: 0.5327 - val_acc: 0.8777\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9534\n",
      "Epoch 00048: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1415 - acc: 0.9533 - val_loss: 0.5707 - val_acc: 0.8630\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9576\n",
      "Epoch 00049: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1324 - acc: 0.9576 - val_loss: 0.5452 - val_acc: 0.8668\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9570\n",
      "Epoch 00050: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1282 - acc: 0.9570 - val_loss: 0.5555 - val_acc: 0.8719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9592\n",
      "Epoch 00051: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1290 - acc: 0.9592 - val_loss: 0.5522 - val_acc: 0.8642\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9596\n",
      "Epoch 00052: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1236 - acc: 0.9596 - val_loss: 0.5466 - val_acc: 0.8772\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9602\n",
      "Epoch 00053: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1224 - acc: 0.9602 - val_loss: 0.5669 - val_acc: 0.8728\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9615\n",
      "Epoch 00054: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1181 - acc: 0.9616 - val_loss: 0.5405 - val_acc: 0.8761\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9624\n",
      "Epoch 00055: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1135 - acc: 0.9624 - val_loss: 0.5694 - val_acc: 0.8784\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9644\n",
      "Epoch 00056: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1129 - acc: 0.9644 - val_loss: 0.6185 - val_acc: 0.8696\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9650\n",
      "Epoch 00057: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1078 - acc: 0.9650 - val_loss: 0.5576 - val_acc: 0.8705\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9647\n",
      "Epoch 00058: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1080 - acc: 0.9647 - val_loss: 0.5467 - val_acc: 0.8754\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9667\n",
      "Epoch 00059: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1018 - acc: 0.9667 - val_loss: 0.5513 - val_acc: 0.8737\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9657\n",
      "Epoch 00060: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1045 - acc: 0.9657 - val_loss: 0.5730 - val_acc: 0.8770\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9688\n",
      "Epoch 00061: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0979 - acc: 0.9688 - val_loss: 0.5541 - val_acc: 0.8770\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9694\n",
      "Epoch 00062: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0969 - acc: 0.9694 - val_loss: 0.5940 - val_acc: 0.8756\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9695\n",
      "Epoch 00063: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0933 - acc: 0.9695 - val_loss: 0.5641 - val_acc: 0.8793\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9695\n",
      "Epoch 00064: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0918 - acc: 0.9695 - val_loss: 0.5824 - val_acc: 0.8735\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9699\n",
      "Epoch 00065: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0886 - acc: 0.9699 - val_loss: 0.5890 - val_acc: 0.8761\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9716\n",
      "Epoch 00066: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0876 - acc: 0.9716 - val_loss: 0.5836 - val_acc: 0.8803\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9704\n",
      "Epoch 00067: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0901 - acc: 0.9704 - val_loss: 0.5860 - val_acc: 0.8812\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9720\n",
      "Epoch 00068: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0887 - acc: 0.9720 - val_loss: 0.6366 - val_acc: 0.8710\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9726\n",
      "Epoch 00069: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0848 - acc: 0.9726 - val_loss: 0.5694 - val_acc: 0.8833\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9757\n",
      "Epoch 00070: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0777 - acc: 0.9757 - val_loss: 0.5905 - val_acc: 0.8798\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9743\n",
      "Epoch 00071: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0814 - acc: 0.9743 - val_loss: 0.6095 - val_acc: 0.8714\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9749\n",
      "Epoch 00072: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0805 - acc: 0.9749 - val_loss: 0.5655 - val_acc: 0.8840\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9739\n",
      "Epoch 00073: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0801 - acc: 0.9739 - val_loss: 0.5992 - val_acc: 0.8761\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9767\n",
      "Epoch 00074: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0737 - acc: 0.9767 - val_loss: 0.5741 - val_acc: 0.8842\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9740\n",
      "Epoch 00075: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0793 - acc: 0.9740 - val_loss: 0.5902 - val_acc: 0.8824\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9778\n",
      "Epoch 00076: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0700 - acc: 0.9778 - val_loss: 0.5657 - val_acc: 0.8840\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9767\n",
      "Epoch 00077: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0737 - acc: 0.9767 - val_loss: 0.6046 - val_acc: 0.8828\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9752\n",
      "Epoch 00078: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0747 - acc: 0.9752 - val_loss: 0.5968 - val_acc: 0.8821\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9746\n",
      "Epoch 00079: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0770 - acc: 0.9746 - val_loss: 0.5880 - val_acc: 0.8817\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9777\n",
      "Epoch 00080: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0708 - acc: 0.9777 - val_loss: 0.5875 - val_acc: 0.8805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9782\n",
      "Epoch 00081: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0672 - acc: 0.9782 - val_loss: 0.5956 - val_acc: 0.8805\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9783\n",
      "Epoch 00082: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0666 - acc: 0.9783 - val_loss: 0.5907 - val_acc: 0.8814\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9786\n",
      "Epoch 00083: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0673 - acc: 0.9786 - val_loss: 0.5819 - val_acc: 0.8826\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9792\n",
      "Epoch 00084: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0660 - acc: 0.9792 - val_loss: 0.5822 - val_acc: 0.8840\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9780\n",
      "Epoch 00085: val_loss did not improve from 0.50926\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0695 - acc: 0.9780 - val_loss: 0.6149 - val_acc: 0.8784\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 481us/sample - loss: 0.6220 - acc: 0.8351\n",
      "Loss: 0.6220000506190124 Accuracy: 0.8350986\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1931 - acc: 0.2864\n",
      "Epoch 00001: val_loss improved from inf to 1.55255, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/001-1.5525.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 2.1931 - acc: 0.2865 - val_loss: 1.5525 - val_acc: 0.5118\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5034 - acc: 0.5218\n",
      "Epoch 00002: val_loss improved from 1.55255 to 1.27464, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/002-1.2746.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 1.5034 - acc: 0.5218 - val_loss: 1.2746 - val_acc: 0.6080\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2749 - acc: 0.5998\n",
      "Epoch 00003: val_loss improved from 1.27464 to 1.06447, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/003-1.0645.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 1.2748 - acc: 0.5998 - val_loss: 1.0645 - val_acc: 0.6953\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.6598\n",
      "Epoch 00004: val_loss improved from 1.06447 to 1.01263, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/004-1.0126.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 1.1049 - acc: 0.6598 - val_loss: 1.0126 - val_acc: 0.6923\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9646 - acc: 0.7051\n",
      "Epoch 00005: val_loss improved from 1.01263 to 0.81072, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/005-0.8107.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.9646 - acc: 0.7051 - val_loss: 0.8107 - val_acc: 0.7603\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7372\n",
      "Epoch 00006: val_loss improved from 0.81072 to 0.70553, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/006-0.7055.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.8660 - acc: 0.7371 - val_loss: 0.7055 - val_acc: 0.8004\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7666\n",
      "Epoch 00007: val_loss improved from 0.70553 to 0.66230, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/007-0.6623.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.7737 - acc: 0.7666 - val_loss: 0.6623 - val_acc: 0.7983\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7069 - acc: 0.7865\n",
      "Epoch 00008: val_loss improved from 0.66230 to 0.59947, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/008-0.5995.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.7068 - acc: 0.7865 - val_loss: 0.5995 - val_acc: 0.8246\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6382 - acc: 0.8070\n",
      "Epoch 00009: val_loss improved from 0.59947 to 0.53937, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/009-0.5394.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.6381 - acc: 0.8070 - val_loss: 0.5394 - val_acc: 0.8432\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8197\n",
      "Epoch 00010: val_loss improved from 0.53937 to 0.51943, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/010-0.5194.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.5960 - acc: 0.8197 - val_loss: 0.5194 - val_acc: 0.8467\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.8302\n",
      "Epoch 00011: val_loss improved from 0.51943 to 0.48020, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/011-0.4802.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.5574 - acc: 0.8302 - val_loss: 0.4802 - val_acc: 0.8546\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8430\n",
      "Epoch 00012: val_loss improved from 0.48020 to 0.46946, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/012-0.4695.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.5136 - acc: 0.8430 - val_loss: 0.4695 - val_acc: 0.8598\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8512\n",
      "Epoch 00013: val_loss improved from 0.46946 to 0.42554, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/013-0.4255.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.4848 - acc: 0.8512 - val_loss: 0.4255 - val_acc: 0.8758\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8595\n",
      "Epoch 00014: val_loss did not improve from 0.42554\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.4553 - acc: 0.8595 - val_loss: 0.4394 - val_acc: 0.8707\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8697\n",
      "Epoch 00015: val_loss improved from 0.42554 to 0.40223, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/015-0.4022.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.4265 - acc: 0.8697 - val_loss: 0.4022 - val_acc: 0.8847\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8745\n",
      "Epoch 00016: val_loss improved from 0.40223 to 0.37579, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/016-0.3758.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.4064 - acc: 0.8745 - val_loss: 0.3758 - val_acc: 0.8887\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8813\n",
      "Epoch 00017: val_loss improved from 0.37579 to 0.35888, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/017-0.3589.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.3863 - acc: 0.8813 - val_loss: 0.3589 - val_acc: 0.8968\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8876\n",
      "Epoch 00018: val_loss improved from 0.35888 to 0.32988, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/018-0.3299.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.3642 - acc: 0.8876 - val_loss: 0.3299 - val_acc: 0.9068\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8913\n",
      "Epoch 00019: val_loss improved from 0.32988 to 0.32497, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/019-0.3250.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.3511 - acc: 0.8913 - val_loss: 0.3250 - val_acc: 0.9057\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8982\n",
      "Epoch 00020: val_loss improved from 0.32497 to 0.31933, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/020-0.3193.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3289 - acc: 0.8982 - val_loss: 0.3193 - val_acc: 0.9110\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9039\n",
      "Epoch 00021: val_loss improved from 0.31933 to 0.29854, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/021-0.2985.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3121 - acc: 0.9039 - val_loss: 0.2985 - val_acc: 0.9154\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9066\n",
      "Epoch 00022: val_loss did not improve from 0.29854\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.3029 - acc: 0.9066 - val_loss: 0.3156 - val_acc: 0.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9084\n",
      "Epoch 00023: val_loss improved from 0.29854 to 0.29783, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/023-0.2978.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2911 - acc: 0.9084 - val_loss: 0.2978 - val_acc: 0.9150\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9127\n",
      "Epoch 00024: val_loss improved from 0.29783 to 0.29425, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/024-0.2943.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2785 - acc: 0.9127 - val_loss: 0.2943 - val_acc: 0.9199\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9177\n",
      "Epoch 00025: val_loss did not improve from 0.29425\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2641 - acc: 0.9176 - val_loss: 0.3141 - val_acc: 0.9073\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9192\n",
      "Epoch 00026: val_loss improved from 0.29425 to 0.27563, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/026-0.2756.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2587 - acc: 0.9192 - val_loss: 0.2756 - val_acc: 0.9236\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9211\n",
      "Epoch 00027: val_loss did not improve from 0.27563\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2478 - acc: 0.9211 - val_loss: 0.2796 - val_acc: 0.9203\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9249\n",
      "Epoch 00028: val_loss improved from 0.27563 to 0.26887, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/028-0.2689.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2379 - acc: 0.9249 - val_loss: 0.2689 - val_acc: 0.9236\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9283\n",
      "Epoch 00029: val_loss did not improve from 0.26887\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2277 - acc: 0.9283 - val_loss: 0.2745 - val_acc: 0.9238\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9293\n",
      "Epoch 00030: val_loss improved from 0.26887 to 0.26010, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/030-0.2601.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2199 - acc: 0.9293 - val_loss: 0.2601 - val_acc: 0.9322\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9322\n",
      "Epoch 00031: val_loss did not improve from 0.26010\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.2160 - acc: 0.9322 - val_loss: 0.2675 - val_acc: 0.9259\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9344\n",
      "Epoch 00032: val_loss improved from 0.26010 to 0.24892, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/032-0.2489.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2086 - acc: 0.9344 - val_loss: 0.2489 - val_acc: 0.9338\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9352\n",
      "Epoch 00033: val_loss did not improve from 0.24892\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2019 - acc: 0.9352 - val_loss: 0.2656 - val_acc: 0.9255\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9371\n",
      "Epoch 00034: val_loss improved from 0.24892 to 0.24720, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/034-0.2472.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1971 - acc: 0.9371 - val_loss: 0.2472 - val_acc: 0.9327\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9400\n",
      "Epoch 00035: val_loss did not improve from 0.24720\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1865 - acc: 0.9400 - val_loss: 0.2894 - val_acc: 0.9143\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9410\n",
      "Epoch 00036: val_loss did not improve from 0.24720\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1852 - acc: 0.9410 - val_loss: 0.2489 - val_acc: 0.9273\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9438\n",
      "Epoch 00037: val_loss did not improve from 0.24720\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1747 - acc: 0.9437 - val_loss: 0.2548 - val_acc: 0.9278\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9455\n",
      "Epoch 00038: val_loss did not improve from 0.24720\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1697 - acc: 0.9456 - val_loss: 0.2522 - val_acc: 0.9285\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9460\n",
      "Epoch 00039: val_loss improved from 0.24720 to 0.24714, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/039-0.2471.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.1653 - acc: 0.9460 - val_loss: 0.2471 - val_acc: 0.9327\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9479\n",
      "Epoch 00040: val_loss did not improve from 0.24714\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1603 - acc: 0.9479 - val_loss: 0.2539 - val_acc: 0.9304\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9506\n",
      "Epoch 00041: val_loss did not improve from 0.24714\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1503 - acc: 0.9506 - val_loss: 0.2706 - val_acc: 0.9285\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9498\n",
      "Epoch 00042: val_loss did not improve from 0.24714\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1520 - acc: 0.9498 - val_loss: 0.2539 - val_acc: 0.9313\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9521\n",
      "Epoch 00043: val_loss improved from 0.24714 to 0.23839, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/043-0.2384.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1435 - acc: 0.9521 - val_loss: 0.2384 - val_acc: 0.9357\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9538\n",
      "Epoch 00044: val_loss did not improve from 0.23839\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1435 - acc: 0.9538 - val_loss: 0.2491 - val_acc: 0.9338\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9561\n",
      "Epoch 00045: val_loss improved from 0.23839 to 0.23468, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_6_conv_checkpoint/045-0.2347.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1337 - acc: 0.9561 - val_loss: 0.2347 - val_acc: 0.9378\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9570\n",
      "Epoch 00046: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1325 - acc: 0.9569 - val_loss: 0.2565 - val_acc: 0.9311\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9567\n",
      "Epoch 00047: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1313 - acc: 0.9567 - val_loss: 0.2416 - val_acc: 0.9390\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9573\n",
      "Epoch 00048: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1272 - acc: 0.9573 - val_loss: 0.2410 - val_acc: 0.9362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9595\n",
      "Epoch 00049: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1213 - acc: 0.9595 - val_loss: 0.2575 - val_acc: 0.9311\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9600\n",
      "Epoch 00050: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1184 - acc: 0.9600 - val_loss: 0.2480 - val_acc: 0.9352\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9614\n",
      "Epoch 00051: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1149 - acc: 0.9614 - val_loss: 0.2540 - val_acc: 0.9320\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9644\n",
      "Epoch 00052: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1101 - acc: 0.9644 - val_loss: 0.2620 - val_acc: 0.9320\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9634\n",
      "Epoch 00053: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1061 - acc: 0.9634 - val_loss: 0.2599 - val_acc: 0.9338\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9639\n",
      "Epoch 00054: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1070 - acc: 0.9639 - val_loss: 0.2564 - val_acc: 0.9369\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9660\n",
      "Epoch 00055: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1018 - acc: 0.9660 - val_loss: 0.2665 - val_acc: 0.9364\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9657\n",
      "Epoch 00056: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1037 - acc: 0.9657 - val_loss: 0.2567 - val_acc: 0.9357\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9680\n",
      "Epoch 00057: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0962 - acc: 0.9680 - val_loss: 0.2498 - val_acc: 0.9376\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9685\n",
      "Epoch 00058: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0929 - acc: 0.9685 - val_loss: 0.2621 - val_acc: 0.9373\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9678\n",
      "Epoch 00059: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0964 - acc: 0.9678 - val_loss: 0.2463 - val_acc: 0.9376\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9692\n",
      "Epoch 00060: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0891 - acc: 0.9692 - val_loss: 0.2569 - val_acc: 0.9380\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9693\n",
      "Epoch 00061: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0889 - acc: 0.9694 - val_loss: 0.2494 - val_acc: 0.9385\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9701\n",
      "Epoch 00062: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0870 - acc: 0.9701 - val_loss: 0.2476 - val_acc: 0.9413\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9720\n",
      "Epoch 00063: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0825 - acc: 0.9720 - val_loss: 0.2682 - val_acc: 0.9350\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9722\n",
      "Epoch 00064: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0813 - acc: 0.9722 - val_loss: 0.2519 - val_acc: 0.9408\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9723\n",
      "Epoch 00065: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0826 - acc: 0.9723 - val_loss: 0.2635 - val_acc: 0.9378\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9732\n",
      "Epoch 00066: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0812 - acc: 0.9731 - val_loss: 0.2547 - val_acc: 0.9408\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9747\n",
      "Epoch 00067: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0770 - acc: 0.9747 - val_loss: 0.2732 - val_acc: 0.9378\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9756\n",
      "Epoch 00068: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0710 - acc: 0.9756 - val_loss: 0.2683 - val_acc: 0.9380\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9751\n",
      "Epoch 00069: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0746 - acc: 0.9751 - val_loss: 0.2655 - val_acc: 0.9350\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9775\n",
      "Epoch 00070: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0680 - acc: 0.9775 - val_loss: 0.2693 - val_acc: 0.9385\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9758\n",
      "Epoch 00071: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0711 - acc: 0.9758 - val_loss: 0.2585 - val_acc: 0.9420\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9766\n",
      "Epoch 00072: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0677 - acc: 0.9766 - val_loss: 0.2663 - val_acc: 0.9362\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9770\n",
      "Epoch 00073: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0665 - acc: 0.9770 - val_loss: 0.2752 - val_acc: 0.9392\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9787\n",
      "Epoch 00074: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0639 - acc: 0.9787 - val_loss: 0.2859 - val_acc: 0.9345\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9783\n",
      "Epoch 00075: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0629 - acc: 0.9783 - val_loss: 0.2729 - val_acc: 0.9406\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9797\n",
      "Epoch 00076: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0621 - acc: 0.9797 - val_loss: 0.2623 - val_acc: 0.9399\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9798\n",
      "Epoch 00077: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0597 - acc: 0.9798 - val_loss: 0.2978 - val_acc: 0.9362\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9777\n",
      "Epoch 00078: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0646 - acc: 0.9777 - val_loss: 0.2812 - val_acc: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9796\n",
      "Epoch 00079: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0603 - acc: 0.9796 - val_loss: 0.2635 - val_acc: 0.9418\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9815\n",
      "Epoch 00080: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0555 - acc: 0.9815 - val_loss: 0.2873 - val_acc: 0.9387\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9810\n",
      "Epoch 00081: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0537 - acc: 0.9810 - val_loss: 0.2817 - val_acc: 0.9394\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9810\n",
      "Epoch 00082: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0582 - acc: 0.9810 - val_loss: 0.2737 - val_acc: 0.9413\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9798\n",
      "Epoch 00083: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0582 - acc: 0.9798 - val_loss: 0.2849 - val_acc: 0.9366\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9807\n",
      "Epoch 00084: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0550 - acc: 0.9807 - val_loss: 0.2851 - val_acc: 0.9385\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9821\n",
      "Epoch 00085: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0546 - acc: 0.9821 - val_loss: 0.3458 - val_acc: 0.9313\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9829\n",
      "Epoch 00086: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0516 - acc: 0.9829 - val_loss: 0.2796 - val_acc: 0.9415\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9837\n",
      "Epoch 00087: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0497 - acc: 0.9837 - val_loss: 0.2698 - val_acc: 0.9415\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9827\n",
      "Epoch 00088: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0510 - acc: 0.9827 - val_loss: 0.2802 - val_acc: 0.9413\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9833\n",
      "Epoch 00089: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0507 - acc: 0.9833 - val_loss: 0.3192 - val_acc: 0.9373\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9830\n",
      "Epoch 00090: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0493 - acc: 0.9830 - val_loss: 0.3172 - val_acc: 0.9334\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9833\n",
      "Epoch 00091: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0481 - acc: 0.9833 - val_loss: 0.2754 - val_acc: 0.9427\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9843\n",
      "Epoch 00092: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0469 - acc: 0.9843 - val_loss: 0.2859 - val_acc: 0.9422\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9845\n",
      "Epoch 00093: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0459 - acc: 0.9845 - val_loss: 0.2798 - val_acc: 0.9427\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9834\n",
      "Epoch 00094: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0490 - acc: 0.9834 - val_loss: 0.2779 - val_acc: 0.9450\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9836\n",
      "Epoch 00095: val_loss did not improve from 0.23468\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0474 - acc: 0.9836 - val_loss: 0.2919 - val_acc: 0.9434\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 471us/sample - loss: 0.2965 - acc: 0.9121\n",
      "Loss: 0.2964714678710371 Accuracy: 0.91214955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1833 - acc: 0.2830\n",
      "Epoch 00001: val_loss improved from inf to 1.44206, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/001-1.4421.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 2.1835 - acc: 0.2831 - val_loss: 1.4421 - val_acc: 0.5567\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3616 - acc: 0.5592\n",
      "Epoch 00002: val_loss improved from 1.44206 to 1.08113, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/002-1.0811.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 1.3614 - acc: 0.5592 - val_loss: 1.0811 - val_acc: 0.6620\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0736 - acc: 0.6557\n",
      "Epoch 00003: val_loss improved from 1.08113 to 0.84381, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/003-0.8438.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 1.0736 - acc: 0.6557 - val_loss: 0.8438 - val_acc: 0.7382\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8932 - acc: 0.7147\n",
      "Epoch 00004: val_loss improved from 0.84381 to 0.67484, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/004-0.6748.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.8931 - acc: 0.7147 - val_loss: 0.6748 - val_acc: 0.7952\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7589\n",
      "Epoch 00005: val_loss improved from 0.67484 to 0.58207, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/005-0.5821.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.7620 - acc: 0.7589 - val_loss: 0.5821 - val_acc: 0.8314\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.7883\n",
      "Epoch 00006: val_loss improved from 0.58207 to 0.49449, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/006-0.4945.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.6637 - acc: 0.7884 - val_loss: 0.4945 - val_acc: 0.8630\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8135\n",
      "Epoch 00007: val_loss improved from 0.49449 to 0.44760, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/007-0.4476.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.5901 - acc: 0.8135 - val_loss: 0.4476 - val_acc: 0.8742\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8325\n",
      "Epoch 00008: val_loss did not improve from 0.44760\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.5308 - acc: 0.8325 - val_loss: 0.4666 - val_acc: 0.8614\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8524\n",
      "Epoch 00009: val_loss improved from 0.44760 to 0.37130, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/009-0.3713.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.4748 - acc: 0.8524 - val_loss: 0.3713 - val_acc: 0.8891\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8653\n",
      "Epoch 00010: val_loss improved from 0.37130 to 0.33038, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/010-0.3304.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.4299 - acc: 0.8653 - val_loss: 0.3304 - val_acc: 0.9054\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8754\n",
      "Epoch 00011: val_loss improved from 0.33038 to 0.30115, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/011-0.3011.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.3936 - acc: 0.8754 - val_loss: 0.3011 - val_acc: 0.9122\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8875\n",
      "Epoch 00012: val_loss did not improve from 0.30115\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.3619 - acc: 0.8875 - val_loss: 0.3044 - val_acc: 0.9133\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8938\n",
      "Epoch 00013: val_loss improved from 0.30115 to 0.27803, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/013-0.2780.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.3356 - acc: 0.8938 - val_loss: 0.2780 - val_acc: 0.9154\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9010\n",
      "Epoch 00014: val_loss improved from 0.27803 to 0.25145, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/014-0.2515.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.3131 - acc: 0.9010 - val_loss: 0.2515 - val_acc: 0.9278\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9086\n",
      "Epoch 00015: val_loss improved from 0.25145 to 0.23766, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/015-0.2377.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.2932 - acc: 0.9086 - val_loss: 0.2377 - val_acc: 0.9315\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9136\n",
      "Epoch 00016: val_loss did not improve from 0.23766\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.2742 - acc: 0.9136 - val_loss: 0.2824 - val_acc: 0.9117\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9160\n",
      "Epoch 00017: val_loss did not improve from 0.23766\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.2659 - acc: 0.9159 - val_loss: 0.2385 - val_acc: 0.9290\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9187\n",
      "Epoch 00018: val_loss improved from 0.23766 to 0.21878, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/018-0.2188.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.2518 - acc: 0.9187 - val_loss: 0.2188 - val_acc: 0.9373\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9243\n",
      "Epoch 00019: val_loss improved from 0.21878 to 0.20380, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/019-0.2038.hdf5\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.2398 - acc: 0.9244 - val_loss: 0.2038 - val_acc: 0.9408\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9290\n",
      "Epoch 00020: val_loss improved from 0.20380 to 0.19917, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/020-0.1992.hdf5\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.2248 - acc: 0.9290 - val_loss: 0.1992 - val_acc: 0.9404\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9306\n",
      "Epoch 00021: val_loss improved from 0.19917 to 0.18740, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/021-0.1874.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.2170 - acc: 0.9306 - val_loss: 0.1874 - val_acc: 0.9443\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9348\n",
      "Epoch 00022: val_loss did not improve from 0.18740\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.2083 - acc: 0.9348 - val_loss: 0.1950 - val_acc: 0.9401\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9370\n",
      "Epoch 00023: val_loss did not improve from 0.18740\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1972 - acc: 0.9370 - val_loss: 0.2265 - val_acc: 0.9338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9373\n",
      "Epoch 00024: val_loss improved from 0.18740 to 0.17088, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/024-0.1709.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1921 - acc: 0.9373 - val_loss: 0.1709 - val_acc: 0.9499\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9399\n",
      "Epoch 00025: val_loss did not improve from 0.17088\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1835 - acc: 0.9399 - val_loss: 0.1942 - val_acc: 0.9399\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9450\n",
      "Epoch 00026: val_loss did not improve from 0.17088\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1741 - acc: 0.9450 - val_loss: 0.2058 - val_acc: 0.9380\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9443\n",
      "Epoch 00027: val_loss did not improve from 0.17088\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1714 - acc: 0.9443 - val_loss: 0.1771 - val_acc: 0.9490\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9464\n",
      "Epoch 00028: val_loss did not improve from 0.17088\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1649 - acc: 0.9464 - val_loss: 0.2049 - val_acc: 0.9385\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9486\n",
      "Epoch 00029: val_loss improved from 0.17088 to 0.16746, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/029-0.1675.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1583 - acc: 0.9486 - val_loss: 0.1675 - val_acc: 0.9513\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9490\n",
      "Epoch 00030: val_loss improved from 0.16746 to 0.16408, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/030-0.1641.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1522 - acc: 0.9490 - val_loss: 0.1641 - val_acc: 0.9525\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9519\n",
      "Epoch 00031: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1466 - acc: 0.9519 - val_loss: 0.1655 - val_acc: 0.9474\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9536\n",
      "Epoch 00032: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1407 - acc: 0.9536 - val_loss: 0.1692 - val_acc: 0.9476\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9552\n",
      "Epoch 00033: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1368 - acc: 0.9552 - val_loss: 0.1835 - val_acc: 0.9464\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9561\n",
      "Epoch 00034: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1333 - acc: 0.9561 - val_loss: 0.1893 - val_acc: 0.9401\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9586\n",
      "Epoch 00035: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1244 - acc: 0.9586 - val_loss: 0.1779 - val_acc: 0.9506\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9588\n",
      "Epoch 00036: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1229 - acc: 0.9588 - val_loss: 0.1736 - val_acc: 0.9506\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9616\n",
      "Epoch 00037: val_loss improved from 0.16408 to 0.16332, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/037-0.1633.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1161 - acc: 0.9616 - val_loss: 0.1633 - val_acc: 0.9525\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9608\n",
      "Epoch 00038: val_loss did not improve from 0.16332\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1167 - acc: 0.9608 - val_loss: 0.1738 - val_acc: 0.9522\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9600\n",
      "Epoch 00039: val_loss did not improve from 0.16332\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1160 - acc: 0.9600 - val_loss: 0.1764 - val_acc: 0.9488\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9657\n",
      "Epoch 00040: val_loss did not improve from 0.16332\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1052 - acc: 0.9657 - val_loss: 0.1796 - val_acc: 0.9504\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9653\n",
      "Epoch 00041: val_loss did not improve from 0.16332\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1037 - acc: 0.9653 - val_loss: 0.1636 - val_acc: 0.9553\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9633\n",
      "Epoch 00042: val_loss improved from 0.16332 to 0.15055, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_7_conv_checkpoint/042-0.1505.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1065 - acc: 0.9633 - val_loss: 0.1505 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9659\n",
      "Epoch 00043: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0969 - acc: 0.9659 - val_loss: 0.1553 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9679\n",
      "Epoch 00044: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0955 - acc: 0.9679 - val_loss: 0.1582 - val_acc: 0.9557\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9689\n",
      "Epoch 00045: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0925 - acc: 0.9689 - val_loss: 0.1616 - val_acc: 0.9569\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9684\n",
      "Epoch 00046: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0922 - acc: 0.9684 - val_loss: 0.1674 - val_acc: 0.9522\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9695\n",
      "Epoch 00047: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0904 - acc: 0.9695 - val_loss: 0.1574 - val_acc: 0.9541\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9705\n",
      "Epoch 00048: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0850 - acc: 0.9705 - val_loss: 0.1654 - val_acc: 0.9546\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9729\n",
      "Epoch 00049: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0806 - acc: 0.9729 - val_loss: 0.1631 - val_acc: 0.9569\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9726\n",
      "Epoch 00050: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0792 - acc: 0.9726 - val_loss: 0.1662 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9736\n",
      "Epoch 00051: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0792 - acc: 0.9736 - val_loss: 0.1585 - val_acc: 0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9735\n",
      "Epoch 00052: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0754 - acc: 0.9735 - val_loss: 0.1616 - val_acc: 0.9560\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9737\n",
      "Epoch 00053: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0739 - acc: 0.9736 - val_loss: 0.1666 - val_acc: 0.9567\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9747\n",
      "Epoch 00054: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0740 - acc: 0.9747 - val_loss: 0.1666 - val_acc: 0.9571\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9762\n",
      "Epoch 00055: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0691 - acc: 0.9762 - val_loss: 0.1559 - val_acc: 0.9569\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9767\n",
      "Epoch 00056: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0691 - acc: 0.9767 - val_loss: 0.1762 - val_acc: 0.9604\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9768\n",
      "Epoch 00057: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0662 - acc: 0.9769 - val_loss: 0.1678 - val_acc: 0.9562\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9769\n",
      "Epoch 00058: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0652 - acc: 0.9769 - val_loss: 0.1736 - val_acc: 0.9569\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9790\n",
      "Epoch 00059: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0615 - acc: 0.9790 - val_loss: 0.1640 - val_acc: 0.9553\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9791\n",
      "Epoch 00060: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0612 - acc: 0.9791 - val_loss: 0.1614 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9794\n",
      "Epoch 00061: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0591 - acc: 0.9794 - val_loss: 0.1753 - val_acc: 0.9562\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9787\n",
      "Epoch 00062: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0615 - acc: 0.9787 - val_loss: 0.1706 - val_acc: 0.9560\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9812\n",
      "Epoch 00063: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0553 - acc: 0.9812 - val_loss: 0.1656 - val_acc: 0.9585\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9796\n",
      "Epoch 00064: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0585 - acc: 0.9796 - val_loss: 0.1717 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9805\n",
      "Epoch 00065: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0578 - acc: 0.9805 - val_loss: 0.1712 - val_acc: 0.9569\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9818\n",
      "Epoch 00066: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0525 - acc: 0.9818 - val_loss: 0.1655 - val_acc: 0.9597\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9822\n",
      "Epoch 00067: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0524 - acc: 0.9822 - val_loss: 0.1667 - val_acc: 0.9578\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9820\n",
      "Epoch 00068: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0515 - acc: 0.9820 - val_loss: 0.1703 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9817\n",
      "Epoch 00069: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0519 - acc: 0.9817 - val_loss: 0.1737 - val_acc: 0.9606\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9826\n",
      "Epoch 00070: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0511 - acc: 0.9826 - val_loss: 0.1791 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9820\n",
      "Epoch 00071: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0501 - acc: 0.9820 - val_loss: 0.1655 - val_acc: 0.9599\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9828\n",
      "Epoch 00072: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0510 - acc: 0.9828 - val_loss: 0.1714 - val_acc: 0.9595\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9846\n",
      "Epoch 00073: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0439 - acc: 0.9846 - val_loss: 0.1679 - val_acc: 0.9588\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9836\n",
      "Epoch 00074: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0461 - acc: 0.9836 - val_loss: 0.1787 - val_acc: 0.9590\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9846\n",
      "Epoch 00075: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0440 - acc: 0.9846 - val_loss: 0.1762 - val_acc: 0.9595\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9854\n",
      "Epoch 00076: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0413 - acc: 0.9853 - val_loss: 0.1880 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9828\n",
      "Epoch 00077: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0531 - acc: 0.9828 - val_loss: 0.1649 - val_acc: 0.9604\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9856\n",
      "Epoch 00078: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0396 - acc: 0.9856 - val_loss: 0.1796 - val_acc: 0.9609\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9862\n",
      "Epoch 00079: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0397 - acc: 0.9862 - val_loss: 0.1769 - val_acc: 0.9588\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9853\n",
      "Epoch 00080: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0429 - acc: 0.9853 - val_loss: 0.1619 - val_acc: 0.9618\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9867\n",
      "Epoch 00081: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0393 - acc: 0.9867 - val_loss: 0.2042 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9852\n",
      "Epoch 00082: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0408 - acc: 0.9852 - val_loss: 0.1771 - val_acc: 0.9581\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9865\n",
      "Epoch 00083: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0389 - acc: 0.9865 - val_loss: 0.1787 - val_acc: 0.9604\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9871\n",
      "Epoch 00084: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0389 - acc: 0.9871 - val_loss: 0.1822 - val_acc: 0.9585\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9875\n",
      "Epoch 00085: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0373 - acc: 0.9875 - val_loss: 0.1629 - val_acc: 0.9623\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9882\n",
      "Epoch 00086: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0356 - acc: 0.9882 - val_loss: 0.1733 - val_acc: 0.9613\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9874\n",
      "Epoch 00087: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0370 - acc: 0.9874 - val_loss: 0.1801 - val_acc: 0.9599\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9878\n",
      "Epoch 00088: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0342 - acc: 0.9878 - val_loss: 0.1948 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9877\n",
      "Epoch 00089: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0350 - acc: 0.9877 - val_loss: 0.1868 - val_acc: 0.9595\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9881\n",
      "Epoch 00090: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0346 - acc: 0.9881 - val_loss: 0.1925 - val_acc: 0.9592\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9886\n",
      "Epoch 00091: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0321 - acc: 0.9886 - val_loss: 0.2000 - val_acc: 0.9592\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9873\n",
      "Epoch 00092: val_loss did not improve from 0.15055\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0362 - acc: 0.9873 - val_loss: 0.1808 - val_acc: 0.9609\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 508us/sample - loss: 0.1965 - acc: 0.9418\n",
      "Loss: 0.1964829167411087 Accuracy: 0.9418484\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0582 - acc: 0.3245\n",
      "Epoch 00001: val_loss improved from inf to 1.24283, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/001-1.2428.hdf5\n",
      "36805/36805 [==============================] - 39s 1ms/sample - loss: 2.0582 - acc: 0.3245 - val_loss: 1.2428 - val_acc: 0.6268\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2003 - acc: 0.6080\n",
      "Epoch 00002: val_loss improved from 1.24283 to 0.81819, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/002-0.8182.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 1.2002 - acc: 0.6080 - val_loss: 0.8182 - val_acc: 0.7468\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9186 - acc: 0.7002\n",
      "Epoch 00003: val_loss improved from 0.81819 to 0.65109, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/003-0.6511.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.9186 - acc: 0.7002 - val_loss: 0.6511 - val_acc: 0.8090\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7565\n",
      "Epoch 00004: val_loss improved from 0.65109 to 0.49936, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/004-0.4994.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.7505 - acc: 0.7565 - val_loss: 0.4994 - val_acc: 0.8579\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.7995\n",
      "Epoch 00005: val_loss improved from 0.49936 to 0.42166, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/005-0.4217.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.6205 - acc: 0.7995 - val_loss: 0.4217 - val_acc: 0.8724\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8245\n",
      "Epoch 00006: val_loss improved from 0.42166 to 0.38139, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/006-0.3814.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.5417 - acc: 0.8245 - val_loss: 0.3814 - val_acc: 0.8863\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4843 - acc: 0.8458\n",
      "Epoch 00007: val_loss improved from 0.38139 to 0.32772, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/007-0.3277.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.4843 - acc: 0.8459 - val_loss: 0.3277 - val_acc: 0.8994\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.8620\n",
      "Epoch 00008: val_loss improved from 0.32772 to 0.31900, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/008-0.3190.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.4291 - acc: 0.8620 - val_loss: 0.3190 - val_acc: 0.9015\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8748\n",
      "Epoch 00009: val_loss improved from 0.31900 to 0.28280, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/009-0.2828.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.3894 - acc: 0.8748 - val_loss: 0.2828 - val_acc: 0.9115\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8857\n",
      "Epoch 00010: val_loss improved from 0.28280 to 0.23811, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/010-0.2381.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.3570 - acc: 0.8857 - val_loss: 0.2381 - val_acc: 0.9264\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8963\n",
      "Epoch 00011: val_loss improved from 0.23811 to 0.23237, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/011-0.2324.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.3258 - acc: 0.8963 - val_loss: 0.2324 - val_acc: 0.9245\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8988\n",
      "Epoch 00012: val_loss improved from 0.23237 to 0.22581, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/012-0.2258.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.3119 - acc: 0.8988 - val_loss: 0.2258 - val_acc: 0.9276\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9086\n",
      "Epoch 00013: val_loss improved from 0.22581 to 0.21690, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/013-0.2169.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2854 - acc: 0.9086 - val_loss: 0.2169 - val_acc: 0.9331\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9158\n",
      "Epoch 00014: val_loss improved from 0.21690 to 0.18949, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/014-0.1895.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.2634 - acc: 0.9158 - val_loss: 0.1895 - val_acc: 0.9394\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9202\n",
      "Epoch 00015: val_loss improved from 0.18949 to 0.17282, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/015-0.1728.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.2508 - acc: 0.9202 - val_loss: 0.1728 - val_acc: 0.9436\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9234\n",
      "Epoch 00016: val_loss did not improve from 0.17282\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.2364 - acc: 0.9234 - val_loss: 0.1762 - val_acc: 0.9425\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9277\n",
      "Epoch 00017: val_loss did not improve from 0.17282\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2251 - acc: 0.9277 - val_loss: 0.1842 - val_acc: 0.9413\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9309\n",
      "Epoch 00018: val_loss improved from 0.17282 to 0.16911, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/018-0.1691.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2150 - acc: 0.9309 - val_loss: 0.1691 - val_acc: 0.9453\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9345\n",
      "Epoch 00019: val_loss did not improve from 0.16911\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2038 - acc: 0.9345 - val_loss: 0.1719 - val_acc: 0.9443\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9378\n",
      "Epoch 00020: val_loss improved from 0.16911 to 0.15884, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/020-0.1588.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1922 - acc: 0.9378 - val_loss: 0.1588 - val_acc: 0.9518\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9412\n",
      "Epoch 00021: val_loss improved from 0.15884 to 0.14859, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/021-0.1486.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1818 - acc: 0.9412 - val_loss: 0.1486 - val_acc: 0.9518\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9420\n",
      "Epoch 00022: val_loss did not improve from 0.14859\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1778 - acc: 0.9420 - val_loss: 0.1492 - val_acc: 0.9509\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9442\n",
      "Epoch 00023: val_loss did not improve from 0.14859\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1693 - acc: 0.9441 - val_loss: 0.1637 - val_acc: 0.9471\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9489\n",
      "Epoch 00024: val_loss improved from 0.14859 to 0.14739, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/024-0.1474.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1582 - acc: 0.9489 - val_loss: 0.1474 - val_acc: 0.9529\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9478\n",
      "Epoch 00025: val_loss improved from 0.14739 to 0.14627, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/025-0.1463.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1559 - acc: 0.9478 - val_loss: 0.1463 - val_acc: 0.9553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9524\n",
      "Epoch 00026: val_loss improved from 0.14627 to 0.14292, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/026-0.1429.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1429 - acc: 0.9524 - val_loss: 0.1429 - val_acc: 0.9553\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9527\n",
      "Epoch 00027: val_loss improved from 0.14292 to 0.13320, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/027-0.1332.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1421 - acc: 0.9528 - val_loss: 0.1332 - val_acc: 0.9548\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9554\n",
      "Epoch 00028: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1338 - acc: 0.9554 - val_loss: 0.1395 - val_acc: 0.9588\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9561\n",
      "Epoch 00029: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1327 - acc: 0.9561 - val_loss: 0.1471 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9588\n",
      "Epoch 00030: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1235 - acc: 0.9588 - val_loss: 0.1449 - val_acc: 0.9525\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9588\n",
      "Epoch 00031: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1221 - acc: 0.9588 - val_loss: 0.1345 - val_acc: 0.9574\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9605\n",
      "Epoch 00032: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1180 - acc: 0.9605 - val_loss: 0.1550 - val_acc: 0.9527\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9625\n",
      "Epoch 00033: val_loss did not improve from 0.13320\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1134 - acc: 0.9625 - val_loss: 0.1341 - val_acc: 0.9567\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9630\n",
      "Epoch 00034: val_loss improved from 0.13320 to 0.13261, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/034-0.1326.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.1094 - acc: 0.9630 - val_loss: 0.1326 - val_acc: 0.9578\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9650\n",
      "Epoch 00035: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1021 - acc: 0.9650 - val_loss: 0.1364 - val_acc: 0.9571\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9669\n",
      "Epoch 00036: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0994 - acc: 0.9669 - val_loss: 0.1373 - val_acc: 0.9567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9671\n",
      "Epoch 00037: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0973 - acc: 0.9672 - val_loss: 0.1503 - val_acc: 0.9581\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9691\n",
      "Epoch 00038: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0901 - acc: 0.9691 - val_loss: 0.1387 - val_acc: 0.9583\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9703\n",
      "Epoch 00039: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0859 - acc: 0.9703 - val_loss: 0.1627 - val_acc: 0.9509\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9700\n",
      "Epoch 00040: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0883 - acc: 0.9700 - val_loss: 0.1435 - val_acc: 0.9564\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9722\n",
      "Epoch 00041: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0826 - acc: 0.9722 - val_loss: 0.1376 - val_acc: 0.9595\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9722\n",
      "Epoch 00042: val_loss did not improve from 0.13261\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0817 - acc: 0.9722 - val_loss: 0.1333 - val_acc: 0.9597\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9731\n",
      "Epoch 00043: val_loss improved from 0.13261 to 0.12526, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_8_conv_checkpoint/043-0.1253.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0781 - acc: 0.9730 - val_loss: 0.1253 - val_acc: 0.9655\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9732\n",
      "Epoch 00044: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0771 - acc: 0.9732 - val_loss: 0.1579 - val_acc: 0.9555\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9755\n",
      "Epoch 00045: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0724 - acc: 0.9755 - val_loss: 0.1490 - val_acc: 0.9569\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9765\n",
      "Epoch 00046: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0693 - acc: 0.9765 - val_loss: 0.1377 - val_acc: 0.9623\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9760\n",
      "Epoch 00047: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0713 - acc: 0.9760 - val_loss: 0.1497 - val_acc: 0.9581\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9782\n",
      "Epoch 00048: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0627 - acc: 0.9782 - val_loss: 0.1624 - val_acc: 0.9571\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9778\n",
      "Epoch 00049: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0649 - acc: 0.9777 - val_loss: 0.1642 - val_acc: 0.9543\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9796\n",
      "Epoch 00050: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0596 - acc: 0.9796 - val_loss: 0.1590 - val_acc: 0.9597\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9783\n",
      "Epoch 00051: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0631 - acc: 0.9783 - val_loss: 0.1370 - val_acc: 0.9620\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9802\n",
      "Epoch 00052: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0599 - acc: 0.9802 - val_loss: 0.1537 - val_acc: 0.9590\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9817\n",
      "Epoch 00053: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0540 - acc: 0.9817 - val_loss: 0.1708 - val_acc: 0.9569\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9805\n",
      "Epoch 00054: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0581 - acc: 0.9805 - val_loss: 0.1564 - val_acc: 0.9599\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9818\n",
      "Epoch 00055: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0535 - acc: 0.9818 - val_loss: 0.2157 - val_acc: 0.9462\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9812\n",
      "Epoch 00056: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0556 - acc: 0.9811 - val_loss: 0.1582 - val_acc: 0.9602\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9790\n",
      "Epoch 00057: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0634 - acc: 0.9791 - val_loss: 0.1406 - val_acc: 0.9639\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9840\n",
      "Epoch 00058: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0476 - acc: 0.9840 - val_loss: 0.1594 - val_acc: 0.9611\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9852\n",
      "Epoch 00059: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0444 - acc: 0.9852 - val_loss: 0.1470 - val_acc: 0.9623\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9838\n",
      "Epoch 00060: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0472 - acc: 0.9838 - val_loss: 0.1670 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9833\n",
      "Epoch 00061: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0480 - acc: 0.9833 - val_loss: 0.1585 - val_acc: 0.9644\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9861\n",
      "Epoch 00062: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0413 - acc: 0.9861 - val_loss: 0.1594 - val_acc: 0.9651\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9849\n",
      "Epoch 00063: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0434 - acc: 0.9849 - val_loss: 0.1652 - val_acc: 0.9604\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9841\n",
      "Epoch 00064: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0461 - acc: 0.9841 - val_loss: 0.1652 - val_acc: 0.9562\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9854\n",
      "Epoch 00065: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0438 - acc: 0.9854 - val_loss: 0.1984 - val_acc: 0.9541\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9858\n",
      "Epoch 00066: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0417 - acc: 0.9858 - val_loss: 0.1988 - val_acc: 0.9560\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9859\n",
      "Epoch 00067: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0403 - acc: 0.9859 - val_loss: 0.1520 - val_acc: 0.9627\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9871\n",
      "Epoch 00068: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0380 - acc: 0.9871 - val_loss: 0.1543 - val_acc: 0.9604\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9867\n",
      "Epoch 00069: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0376 - acc: 0.9867 - val_loss: 0.1777 - val_acc: 0.9592\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9860\n",
      "Epoch 00070: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0413 - acc: 0.9860 - val_loss: 0.1599 - val_acc: 0.9630\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9881\n",
      "Epoch 00071: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0329 - acc: 0.9881 - val_loss: 0.1726 - val_acc: 0.9618\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 00072: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0397 - acc: 0.9870 - val_loss: 0.1594 - val_acc: 0.9632\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9887\n",
      "Epoch 00073: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0336 - acc: 0.9887 - val_loss: 0.1734 - val_acc: 0.9623\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9891\n",
      "Epoch 00074: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0315 - acc: 0.9891 - val_loss: 0.1820 - val_acc: 0.9616\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9885\n",
      "Epoch 00075: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0334 - acc: 0.9885 - val_loss: 0.2044 - val_acc: 0.9583\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9885\n",
      "Epoch 00076: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0345 - acc: 0.9885 - val_loss: 0.1758 - val_acc: 0.9616\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9890\n",
      "Epoch 00077: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0340 - acc: 0.9891 - val_loss: 0.1678 - val_acc: 0.9606\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9901\n",
      "Epoch 00078: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0292 - acc: 0.9901 - val_loss: 0.1998 - val_acc: 0.9599\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9897\n",
      "Epoch 00079: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0309 - acc: 0.9897 - val_loss: 0.1720 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9902\n",
      "Epoch 00080: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.1957 - val_acc: 0.9599\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9886\n",
      "Epoch 00081: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0323 - acc: 0.9886 - val_loss: 0.1718 - val_acc: 0.9627\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9901\n",
      "Epoch 00082: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0272 - acc: 0.9901 - val_loss: 0.1720 - val_acc: 0.9637\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9904\n",
      "Epoch 00083: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0289 - acc: 0.9904 - val_loss: 0.1698 - val_acc: 0.9644\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9907\n",
      "Epoch 00084: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0273 - acc: 0.9907 - val_loss: 0.1768 - val_acc: 0.9616\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9902\n",
      "Epoch 00085: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0271 - acc: 0.9902 - val_loss: 0.1789 - val_acc: 0.9618\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9909\n",
      "Epoch 00086: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0267 - acc: 0.9909 - val_loss: 0.1782 - val_acc: 0.9611\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9917\n",
      "Epoch 00087: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0262 - acc: 0.9917 - val_loss: 0.1812 - val_acc: 0.9620\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9910\n",
      "Epoch 00088: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0261 - acc: 0.9910 - val_loss: 0.1784 - val_acc: 0.9648\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00089: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.1965 - val_acc: 0.9611\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 00090: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.2169 - val_acc: 0.9606\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9916\n",
      "Epoch 00091: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0246 - acc: 0.9916 - val_loss: 0.1774 - val_acc: 0.9651\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9916\n",
      "Epoch 00092: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0240 - acc: 0.9916 - val_loss: 0.1921 - val_acc: 0.9627\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9931\n",
      "Epoch 00093: val_loss did not improve from 0.12526\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.1742 - val_acc: 0.9641\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 502us/sample - loss: 0.2091 - acc: 0.9421\n",
      "Loss: 0.20914874090924318 Accuracy: 0.94205606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 151616)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,467,344\n",
      "Trainable params: 2,467,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 476us/sample - loss: 1.3416 - acc: 0.5931\n",
      "Loss: 1.3415830714680324 Accuracy: 0.59314644\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50496)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 869,968\n",
      "Trainable params: 869,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 495us/sample - loss: 1.0058 - acc: 0.6922\n",
      "Loss: 1.005834775067564 Accuracy: 0.69221187\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20928)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 437,968\n",
      "Trainable params: 437,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 513us/sample - loss: 0.6220 - acc: 0.8351\n",
      "Loss: 0.6220000506190124 Accuracy: 0.8350986\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11008)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 361,296\n",
      "Trainable params: 361,296\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 527us/sample - loss: 0.2965 - acc: 0.9121\n",
      "Loss: 0.2964714678710371 Accuracy: 0.91214955\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 3584)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 324,560\n",
      "Trainable params: 324,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 560us/sample - loss: 0.1965 - acc: 0.9418\n",
      "Loss: 0.1964829167411087 Accuracy: 0.9418484\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1152)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 367,696\n",
      "Trainable params: 367,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 551us/sample - loss: 0.2091 - acc: 0.9421\n",
      "Loss: 0.20914874090924318 Accuracy: 0.94205606\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 151616)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,467,344\n",
      "Trainable params: 2,467,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 530us/sample - loss: 2.0789 - acc: 0.6503\n",
      "Loss: 2.0788792800804288 Accuracy: 0.6502596\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50496)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 869,968\n",
      "Trainable params: 869,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 558us/sample - loss: 1.3719 - acc: 0.7421\n",
      "Loss: 1.3719081022038762 Accuracy: 0.7420561\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20928)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 437,968\n",
      "Trainable params: 437,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 527us/sample - loss: 0.7413 - acc: 0.8482\n",
      "Loss: 0.7413235825541606 Accuracy: 0.84818274\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11008)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 361,296\n",
      "Trainable params: 361,296\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 587us/sample - loss: 0.3591 - acc: 0.9248\n",
      "Loss: 0.35914438934464815 Accuracy: 0.9248183\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 3584)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 324,560\n",
      "Trainable params: 324,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 578us/sample - loss: 0.2347 - acc: 0.9470\n",
      "Loss: 0.2346560198484861 Accuracy: 0.9470405\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1152)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 367,696\n",
      "Trainable params: 367,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 624us/sample - loss: 0.2782 - acc: 0.9495\n",
      "Loss: 0.27822224174172144 Accuracy: 0.9495327\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
