{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      kernel_initializer='he_uniform', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      kernel_initializer='he_uniform')) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same', kernel_initializer='he_uniform'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9689 - acc: 0.3819\n",
      "Epoch 00001: val_loss improved from inf to 1.52503, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv_checkpoint/001-1.5250.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.9689 - acc: 0.3819 - val_loss: 1.5250 - val_acc: 0.5262\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3567 - acc: 0.5841\n",
      "Epoch 00002: val_loss improved from 1.52503 to 1.37317, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv_checkpoint/002-1.3732.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.3568 - acc: 0.5841 - val_loss: 1.3732 - val_acc: 0.5651\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.6751\n",
      "Epoch 00003: val_loss improved from 1.37317 to 1.31726, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv_checkpoint/003-1.3173.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0671 - acc: 0.6751 - val_loss: 1.3173 - val_acc: 0.5968\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.7424\n",
      "Epoch 00004: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8376 - acc: 0.7424 - val_loss: 1.4289 - val_acc: 0.5707\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6488 - acc: 0.8021\n",
      "Epoch 00005: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6489 - acc: 0.8021 - val_loss: 1.4035 - val_acc: 0.5917\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8472\n",
      "Epoch 00006: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4984 - acc: 0.8472 - val_loss: 1.5634 - val_acc: 0.5719\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8796\n",
      "Epoch 00007: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3903 - acc: 0.8797 - val_loss: 1.6343 - val_acc: 0.5775\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9032\n",
      "Epoch 00008: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3148 - acc: 0.9032 - val_loss: 1.8123 - val_acc: 0.5849\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9223\n",
      "Epoch 00009: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2546 - acc: 0.9223 - val_loss: 1.8569 - val_acc: 0.5912\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9329\n",
      "Epoch 00010: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2165 - acc: 0.9329 - val_loss: 1.8981 - val_acc: 0.6019\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9427\n",
      "Epoch 00011: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1873 - acc: 0.9427 - val_loss: 1.9261 - val_acc: 0.5982\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9495\n",
      "Epoch 00012: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1675 - acc: 0.9495 - val_loss: 2.0646 - val_acc: 0.5949\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9550\n",
      "Epoch 00013: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1512 - acc: 0.9550 - val_loss: 2.0722 - val_acc: 0.5905\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9584\n",
      "Epoch 00014: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1364 - acc: 0.9583 - val_loss: 2.1037 - val_acc: 0.6049\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9627\n",
      "Epoch 00015: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1241 - acc: 0.9627 - val_loss: 2.2160 - val_acc: 0.5942\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9632\n",
      "Epoch 00016: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1234 - acc: 0.9632 - val_loss: 2.1928 - val_acc: 0.6049\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9648\n",
      "Epoch 00017: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1174 - acc: 0.9648 - val_loss: 2.2348 - val_acc: 0.5984\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9712\n",
      "Epoch 00018: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1029 - acc: 0.9713 - val_loss: 2.2974 - val_acc: 0.6026\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9723\n",
      "Epoch 00019: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0969 - acc: 0.9723 - val_loss: 2.3391 - val_acc: 0.5977\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9731\n",
      "Epoch 00020: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0959 - acc: 0.9731 - val_loss: 2.3273 - val_acc: 0.6040\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9733\n",
      "Epoch 00021: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0938 - acc: 0.9733 - val_loss: 2.3614 - val_acc: 0.6070\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9740\n",
      "Epoch 00022: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0875 - acc: 0.9741 - val_loss: 2.3912 - val_acc: 0.6017\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9764\n",
      "Epoch 00023: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0858 - acc: 0.9764 - val_loss: 2.3255 - val_acc: 0.6136\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9768\n",
      "Epoch 00024: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0834 - acc: 0.9768 - val_loss: 2.3821 - val_acc: 0.6024\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9795\n",
      "Epoch 00025: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0748 - acc: 0.9795 - val_loss: 2.5003 - val_acc: 0.6063\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9788\n",
      "Epoch 00026: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0737 - acc: 0.9788 - val_loss: 2.5037 - val_acc: 0.6047\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9767\n",
      "Epoch 00027: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0810 - acc: 0.9767 - val_loss: 2.3232 - val_acc: 0.6236\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9799\n",
      "Epoch 00028: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0730 - acc: 0.9799 - val_loss: 2.4537 - val_acc: 0.6150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9805\n",
      "Epoch 00029: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0693 - acc: 0.9805 - val_loss: 2.4832 - val_acc: 0.6094\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9818\n",
      "Epoch 00030: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0665 - acc: 0.9818 - val_loss: 2.3959 - val_acc: 0.6180\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9813\n",
      "Epoch 00031: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0687 - acc: 0.9813 - val_loss: 2.3711 - val_acc: 0.6264\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9828\n",
      "Epoch 00032: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0651 - acc: 0.9828 - val_loss: 2.4329 - val_acc: 0.6229\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9841\n",
      "Epoch 00033: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0613 - acc: 0.9841 - val_loss: 2.5990 - val_acc: 0.6203\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9827\n",
      "Epoch 00034: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0652 - acc: 0.9827 - val_loss: 2.3945 - val_acc: 0.6198\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9851\n",
      "Epoch 00035: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0577 - acc: 0.9851 - val_loss: 2.4484 - val_acc: 0.6285\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9858\n",
      "Epoch 00036: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0543 - acc: 0.9858 - val_loss: 2.4652 - val_acc: 0.6306\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9842\n",
      "Epoch 00037: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0595 - acc: 0.9842 - val_loss: 2.5104 - val_acc: 0.6152\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9840\n",
      "Epoch 00038: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0605 - acc: 0.9841 - val_loss: 2.5468 - val_acc: 0.6240\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9857\n",
      "Epoch 00039: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0550 - acc: 0.9857 - val_loss: 2.5935 - val_acc: 0.6152\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9860\n",
      "Epoch 00040: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0533 - acc: 0.9860 - val_loss: 2.5612 - val_acc: 0.6205\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9870\n",
      "Epoch 00041: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0531 - acc: 0.9870 - val_loss: 2.5554 - val_acc: 0.6115\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9858\n",
      "Epoch 00042: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0546 - acc: 0.9858 - val_loss: 2.6398 - val_acc: 0.6175\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9863\n",
      "Epoch 00043: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0515 - acc: 0.9863 - val_loss: 2.4676 - val_acc: 0.6229\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9871\n",
      "Epoch 00044: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0500 - acc: 0.9871 - val_loss: 2.3747 - val_acc: 0.6219\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9877\n",
      "Epoch 00045: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0479 - acc: 0.9877 - val_loss: 2.5045 - val_acc: 0.6240\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9851\n",
      "Epoch 00046: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0570 - acc: 0.9851 - val_loss: 2.5073 - val_acc: 0.6201\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9891\n",
      "Epoch 00047: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0456 - acc: 0.9891 - val_loss: 2.6359 - val_acc: 0.6143\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9877\n",
      "Epoch 00048: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0492 - acc: 0.9877 - val_loss: 2.6070 - val_acc: 0.6161\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9896\n",
      "Epoch 00049: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0427 - acc: 0.9896 - val_loss: 2.4860 - val_acc: 0.6313\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9893\n",
      "Epoch 00050: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0439 - acc: 0.9893 - val_loss: 2.5804 - val_acc: 0.6299\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9879\n",
      "Epoch 00051: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0480 - acc: 0.9879 - val_loss: 2.5559 - val_acc: 0.6348\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9879\n",
      "Epoch 00052: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0468 - acc: 0.9879 - val_loss: 2.5933 - val_acc: 0.6194\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9888\n",
      "Epoch 00053: val_loss did not improve from 1.31726\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0434 - acc: 0.9888 - val_loss: 2.5198 - val_acc: 0.6362\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 888us/sample - loss: 1.3940 - acc: 0.5680\n",
      "Loss: 1.3939541005642615 Accuracy: 0.5680166\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9355 - acc: 0.3811\n",
      "Epoch 00001: val_loss improved from inf to 1.41609, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv_checkpoint/001-1.4161.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.9356 - acc: 0.3811 - val_loss: 1.4161 - val_acc: 0.5453\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2938 - acc: 0.5940\n",
      "Epoch 00002: val_loss improved from 1.41609 to 1.12413, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv_checkpoint/002-1.1241.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.2938 - acc: 0.5940 - val_loss: 1.1241 - val_acc: 0.6520\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0774 - acc: 0.6636\n",
      "Epoch 00003: val_loss improved from 1.12413 to 1.04062, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv_checkpoint/003-1.0406.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0774 - acc: 0.6636 - val_loss: 1.0406 - val_acc: 0.6806\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9139 - acc: 0.7218\n",
      "Epoch 00004: val_loss improved from 1.04062 to 0.98508, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv_checkpoint/004-0.9851.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.9138 - acc: 0.7218 - val_loss: 0.9851 - val_acc: 0.6967\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7803 - acc: 0.7604\n",
      "Epoch 00005: val_loss did not improve from 0.98508\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7803 - acc: 0.7604 - val_loss: 1.0214 - val_acc: 0.6846\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6665 - acc: 0.7945\n",
      "Epoch 00006: val_loss improved from 0.98508 to 0.94737, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv_checkpoint/006-0.9474.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6665 - acc: 0.7945 - val_loss: 0.9474 - val_acc: 0.7116\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8236\n",
      "Epoch 00007: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5694 - acc: 0.8236 - val_loss: 0.9710 - val_acc: 0.7063\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8428\n",
      "Epoch 00008: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4945 - acc: 0.8428 - val_loss: 0.9671 - val_acc: 0.7221\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4215 - acc: 0.8675\n",
      "Epoch 00009: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4214 - acc: 0.8675 - val_loss: 1.0036 - val_acc: 0.7235\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8845\n",
      "Epoch 00010: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3612 - acc: 0.8845 - val_loss: 1.0209 - val_acc: 0.7205\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8973\n",
      "Epoch 00011: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3164 - acc: 0.8973 - val_loss: 1.0751 - val_acc: 0.7226\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9098\n",
      "Epoch 00012: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2818 - acc: 0.9098 - val_loss: 1.1253 - val_acc: 0.7209\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9165\n",
      "Epoch 00013: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2573 - acc: 0.9165 - val_loss: 1.1491 - val_acc: 0.7244\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9267\n",
      "Epoch 00014: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2291 - acc: 0.9267 - val_loss: 1.1213 - val_acc: 0.7293\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9331\n",
      "Epoch 00015: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2072 - acc: 0.9331 - val_loss: 1.1439 - val_acc: 0.7293\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9352\n",
      "Epoch 00016: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2018 - acc: 0.9353 - val_loss: 1.2125 - val_acc: 0.7319\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9389\n",
      "Epoch 00017: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1850 - acc: 0.9389 - val_loss: 1.1968 - val_acc: 0.7277\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9449\n",
      "Epoch 00018: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1696 - acc: 0.9449 - val_loss: 1.2798 - val_acc: 0.7254\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9478\n",
      "Epoch 00019: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1627 - acc: 0.9478 - val_loss: 1.2076 - val_acc: 0.7349\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9521\n",
      "Epoch 00020: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1529 - acc: 0.9521 - val_loss: 1.2312 - val_acc: 0.7312\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9492\n",
      "Epoch 00021: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1585 - acc: 0.9491 - val_loss: 1.2226 - val_acc: 0.7358\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9560\n",
      "Epoch 00022: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1352 - acc: 0.9560 - val_loss: 1.2715 - val_acc: 0.7352\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9569\n",
      "Epoch 00023: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1352 - acc: 0.9569 - val_loss: 1.2376 - val_acc: 0.7391\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9591\n",
      "Epoch 00024: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1273 - acc: 0.9591 - val_loss: 1.3254 - val_acc: 0.7442\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9579\n",
      "Epoch 00025: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1285 - acc: 0.9579 - val_loss: 1.2846 - val_acc: 0.7354\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9621\n",
      "Epoch 00026: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1187 - acc: 0.9621 - val_loss: 1.3144 - val_acc: 0.7354\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9640\n",
      "Epoch 00027: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1127 - acc: 0.9640 - val_loss: 1.3156 - val_acc: 0.7440\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9647\n",
      "Epoch 00028: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1121 - acc: 0.9647 - val_loss: 1.2709 - val_acc: 0.7482\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9649\n",
      "Epoch 00029: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1103 - acc: 0.9649 - val_loss: 1.2618 - val_acc: 0.7538\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9680\n",
      "Epoch 00030: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1047 - acc: 0.9680 - val_loss: 1.3433 - val_acc: 0.7442\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9639\n",
      "Epoch 00031: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1147 - acc: 0.9639 - val_loss: 1.2993 - val_acc: 0.7575\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9672\n",
      "Epoch 00032: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1030 - acc: 0.9672 - val_loss: 1.3053 - val_acc: 0.7591\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9688\n",
      "Epoch 00033: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1003 - acc: 0.9688 - val_loss: 1.2760 - val_acc: 0.7510\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9689\n",
      "Epoch 00034: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0997 - acc: 0.9689 - val_loss: 1.3041 - val_acc: 0.7536\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9713\n",
      "Epoch 00035: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0951 - acc: 0.9713 - val_loss: 1.3635 - val_acc: 0.7442\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9696\n",
      "Epoch 00036: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0940 - acc: 0.9696 - val_loss: 1.3079 - val_acc: 0.7577\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9705\n",
      "Epoch 00037: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0965 - acc: 0.9705 - val_loss: 1.2802 - val_acc: 0.7573\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9747\n",
      "Epoch 00038: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0839 - acc: 0.9747 - val_loss: 1.2984 - val_acc: 0.7603\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9734\n",
      "Epoch 00039: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0850 - acc: 0.9734 - val_loss: 1.3369 - val_acc: 0.7580\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9738\n",
      "Epoch 00040: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0879 - acc: 0.9738 - val_loss: 1.3072 - val_acc: 0.7468\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9738\n",
      "Epoch 00041: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0851 - acc: 0.9738 - val_loss: 1.4277 - val_acc: 0.7447\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9755\n",
      "Epoch 00042: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0783 - acc: 0.9755 - val_loss: 1.3245 - val_acc: 0.7559\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9755\n",
      "Epoch 00043: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0805 - acc: 0.9755 - val_loss: 1.3421 - val_acc: 0.7626\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9761\n",
      "Epoch 00044: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0787 - acc: 0.9761 - val_loss: 1.4080 - val_acc: 0.7519\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9777\n",
      "Epoch 00045: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0770 - acc: 0.9777 - val_loss: 1.3544 - val_acc: 0.7573\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9771\n",
      "Epoch 00046: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0752 - acc: 0.9771 - val_loss: 1.3749 - val_acc: 0.7505\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9753\n",
      "Epoch 00047: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0799 - acc: 0.9753 - val_loss: 1.3786 - val_acc: 0.7605\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9755\n",
      "Epoch 00048: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0787 - acc: 0.9755 - val_loss: 1.3358 - val_acc: 0.7577\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9764\n",
      "Epoch 00049: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0754 - acc: 0.9764 - val_loss: 1.3303 - val_acc: 0.7673\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9783\n",
      "Epoch 00050: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0734 - acc: 0.9782 - val_loss: 1.2926 - val_acc: 0.7594\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9783\n",
      "Epoch 00051: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0702 - acc: 0.9783 - val_loss: 1.2841 - val_acc: 0.7680\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9783\n",
      "Epoch 00052: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0735 - acc: 0.9783 - val_loss: 1.3626 - val_acc: 0.7629\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9780\n",
      "Epoch 00053: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0725 - acc: 0.9780 - val_loss: 1.3643 - val_acc: 0.7659\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9779\n",
      "Epoch 00054: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0741 - acc: 0.9779 - val_loss: 1.3237 - val_acc: 0.7640\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9810\n",
      "Epoch 00055: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0639 - acc: 0.9810 - val_loss: 1.3923 - val_acc: 0.7636\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9817\n",
      "Epoch 00056: val_loss did not improve from 0.94737\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0646 - acc: 0.9817 - val_loss: 1.3358 - val_acc: 0.7766\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmclk3xcghCWgIDthFYuCS+suVqmCu61iW/fa+pO6VGu1pS6t4lKLSl1qxX2rVKotEK2iBAw7yBYgIUAWErInM/P+/jhZIRthJhPC+3me+0zmru9Mkvvec8695xgRQSmllGqLI9ABKKWUOjpowlBKKdUumjCUUkq1iyYMpZRS7aIJQymlVLtowlBKKdUumjCUUkq1iyYMpZRS7aIJQymlVLsE+WvHxpi+wCtAT0CAeSLy5EHrGOBJ4FygHLhWRFbWLrsGuLd21YdE5OW2jpmYmCipqak++wxKKdXdrVixIl9Ektqzrt8SBuAGfikiK40xUcAKY8ynIrK+0TrnAINqpxOBvwAnGmPigfuB8dhks8IY86GI7G/tgKmpqWRkZPjjsyilVLdkjNnR3nX9ViUlIrl1pQURKQE2ACkHrXYh8IpYy4BYY0wycBbwqYgU1iaJT4Gz/RWrUkqptnVKG4YxJhUYA3x90KIUYFej99m181qa39y+bzDGZBhjMvLy8nwVslJKqYP4PWEYYyKBd4DbReSAr/cvIvNEZLyIjE9Kalc1nFJKqQ7wZxsGxhgXNlm8JiLvNrNKDtC30fs+tfNygFMPmr+kIzHU1NSQnZ1NZWVlRzY/5oWGhtKnTx9cLlegQ1FKBZg/75IywIvABhH5UwurfQjcbIxZgG30LhaRXGPMIuD3xpi42vXOBH7dkTiys7OJiooiNTUVG5JqLxGhoKCA7OxsBgwYEOhwlFIB5s8SxmTgKmCNMSazdt7dQD8AEXkOWIi9pXYL9rbaH9cuKzTG/A5YXrvdgyJS2JEgKisrNVl0kDGGhIQEtG1IKQV+TBgi8gXQ6lla7HB/N7WwbD4w3xexaLLoOP3ulFJ1jvknvUWEqqrduN3FgQ5FKaW6tGM+YRhjqK7e67eEUVRUxLPPPtuhbc8991yKioravf4DDzzAY4891qFjKaVUW475hAFgTBAibr/su7WE4Xa3fsyFCxcSGxvrj7CUUuqwacLAvwlj9uzZbN26lbS0NO68806WLFnCKaecwrRp0xg2bBgAP/zhDxk3bhzDhw9n3rx59dumpqaSn59PVlYWQ4cOZdasWQwfPpwzzzyTioqKVo+bmZnJpEmTGDVqFBdddBH799teVebOncuwYcMYNWoUM2fOBGDp0qWkpaWRlpbGmDFjKCkp8ct3oZQ6uvn1OYyuZvPm2yktzTxkvtdbAXhxOCIOe5+RkWkMGvREi8vnzJnD2rVrycy0x12yZAkrV65k7dq19beqzp8/n/j4eCoqKpgwYQLTp08nISHhoNg38/rrr/P8889z6aWX8s4773DllVe2eNyrr76ap556iqlTp/Kb3/yG3/72tzzxxBPMmTOH7du3ExISUl/d9dhjj/HMM88wefJkSktLCQ0NPezvQSnV/WkJAwCDvWGrc0ycOLHJcw1z585l9OjRTJo0iV27drF58+ZDthkwYABpaWkAjBs3jqysrBb3X1xcTFFREVOnTgXgmmuuIT09HYBRo0ZxxRVX8Pe//52gIHu9MHnyZO644w7mzp1LUVFR/XyllGrsmDoztFQSqKzMpqZmL5GRYzvlNtKIiIaSzJIlS/jss8/46quvCA8P59RTT232qfSQkJD6n51OZ5tVUi35+OOPSU9P56OPPuLhhx9mzZo1zJ49m/POO4+FCxcyefJkFi1axJAhQzq0f6VU96UlDGwbhu1F3ePzfUdFRbXaJlBcXExcXBzh4eFs3LiRZcuWHfExY2JiiIuL4/PPPwfg1VdfZerUqXi9Xnbt2sVpp53GH//4R4qLiyktLWXr1q2MHDmSu+66iwkTJrBx48YjjkEp1f0cUyWMljgc9mvwet04nb79ShISEpg8eTIjRozgnHPO4bzzzmuy/Oyzz+a5555j6NChnHDCCUyaNMknx3355Zf52c9+Rnl5OQMHDuRvf/sbHo+HK6+8kuLiYkSEW2+9ldjYWO677z4WL16Mw+Fg+PDhnHPOOT6JQSnVvZjOrLv3t/Hjx8vBAyht2LCBoUOHtrqd211MRcVmwsKGEBQU6c8Qj0rt+Q6VUkcnY8wKERnfnnW1Soq6Kin8dmutUkp1B5ow0IShlFLtoQmDxgmjJsCRKKVU16UJA7Bfg9EShlJKtUITBrYDQmNcmjCUUqoVmjBq+bM/KaWU6g40YdSyCaNrtGFERjZ/a29L85VSqjP4LWEYY+YbY/YZY9a2sPxOY0xm7bTWGOMxxsTXLssyxqypXZbR3Pa+j1dLGEop1Rp/ljBeAs5uaaGIPCoiaSKSBvwaWHrQuN2n1S5v1wMlR8pfbRizZ8/mmWeeqX9fN8hRaWkpZ5xxBmPHjmXkyJF88MEH7d6niHDnnXcyYsQIRo4cyRtvvAFAbm4uU6ZMIS0tjREjRvD555/j8Xi49tpr69f985//7PPPqJQ6NvhzTO90Y0xqO1e/DHjdX7HUu/12yDy0e3OAYG81QVKFOCMxrQ9F3lRaGjzRcvfmM2bM4Pbbb+emm+zQ5W+++SaLFi0iNDSU9957j+joaPLz85k0aRLTpk1rV+eH7777LpmZmaxatYr8/HwmTJjAlClT+Mc//sFZZ53FPffcg8fjoby8nMzMTHJycli71hb0DmcEP6WUaizgfUkZY8KxJZGbG80W4N/GGAH+KiLzmt3Yt4HYoyJwOAmjDWPGjGHfvn3s3r2bvLw84uLi6Nu3LzU1Ndx9992kp6fjcDjIyclh79699OrVq819fvHFF1x22WU4nU569uzJ1KlTWb58ORMmTOAnP/kJNTU1/PCHPyQtLY2BAweybds2brnlFs477zzOPPNMn302pdSxJeAJA7gA+N9B1VEni0iOMaYH8KkxZqOIpDe3sTHmBuAGgH79+rV+pFZKAp6aIiortxAePhSn8/AHUmrNJZdcwttvv82ePXuYMWMGAK+99hp5eXmsWLECl8tFampqs92aH44pU6aQnp7Oxx9/zLXXXssdd9zB1VdfzapVq1i0aBHPPfccb775JvPnz/fFx1JKHWO6wl1SMzmoOkpEcmpf9wHvARNb2lhE5onIeBEZn5SU1OEg/Nk9yIwZM1iwYAFvv/02l1xyCWC7Ne/Rowcul4vFixezY8eOdu/vlFNO4Y033sDj8ZCXl0d6ejoTJ05kx44d9OzZk1mzZnH99dezcuVK8vPz8Xq9TJ8+nYceeoiVK1f6/PMppY4NAS1hGGNigKnAlY3mRQAOESmp/flM4EH/x+K/hDF8+HBKSkpISUkhOTkZgCuuuIILLriAkSNHMn78+MMasOiiiy7iq6++YvTo0RhjeOSRR+jVqxcvv/wyjz76KC6Xi8jISF555RVycnL48Y9/jNfrBeAPf/iDzz+fUurY4LfuzY0xrwOnAonAXuB+wAUgIs/VrnMtcLaIzGy03UBsqQJsQvuHiDzcnmN2tHtzsGNhlJVlEhLSh+DgttsRjiXavblS3dfhdG/uz7ukLmvHOi9hb79tPG8bMNo/UbXMGCfan5RSSrWsK7RhdAm2P6kgvF5NGEop1RxNGI3o095KKdUyTRiNdKX+pJRSqqvRhNGIljCUUqplmjAa0TExlFKqZZowGrHPYngQ8fpsn0VFRTz77LMd2vbcc8/Vvp+UUl2GJoxG/PHwXmsJw+1u/TgLFy4kNjbWZ7EopdSR0ITRiD8SxuzZs9m6dStpaWnceeedLFmyhFNOOYVp06YxbNgwAH74wx8ybtw4hg8fzrx5Df0spqamkp+fT1ZWFkOHDmXWrFkMHz6cM888k4qKikOO9dFHH3HiiScyZswYvv/977N3714ASktL+fGPf8zIkSMZNWoU77zzDgCffPIJY8eOZfTo0Zxxxhk++8xKqe6pK3Q+2Gla6d0cAJFovN4TcDiCaUcv40CbvZszZ84c1q5dS2btgZcsWcLKlStZu3YtAwYMAGD+/PnEx8dTUVHBhAkTmD59OgkJCU32s3nzZl5//XWef/55Lr30Ut555x2uvPLKJuucfPLJLFu2DGMML7zwAo888giPP/44v/vd74iJiWHNmjUA7N+/n7y8PGbNmkV6ejoDBgygsLAQpZRqzTGVMNpWlyX8011KnYkTJ9YnC4C5c+fy3nu2N5Rdu3axefPmQxLGgAEDSEtLA2DcuHFkZWUdst/s7GxmzJhBbm4u1dXV9cf47LPPWLBgQf16cXFxfPTRR0yZMqV+nfj4eJ9+RqVU93NMJYzWSgIAXq+XsrJNhIT0JTi4p9/iiIho6D59yZIlfPbZZ3z11VeEh4dz6qmnNtvNeUhISP3PTqez2SqpW265hTvuuINp06axZMkSHnjgAb/Er5Q6NmkbRiP+aMOIioqipKSkxeXFxcXExcURHh7Oxo0bWbZsWYePVVxcTEpKCgAvv/xy/fwf/OAHTYaJ3b9/P5MmTSI9PZ3t27cDaJWUUqpNmjAascOj+vbhvYSEBCZPnsyIESO48847D1l+9tln43a7GTp0KLNnz2bSpEkdPtYDDzzAJZdcwrhx40hMTKyff++997J//35GjBjB6NGjWbx4MUlJScybN4+LL76Y0aNH1w/spJRSLfFb9+aBcCTdm9cpK1uLwxFGWNhxvg7vqKXdmyvVfR1O9+ZawjiIdg+ilFLN04RxEO2AUCmlmqcJ4yDan5RSSjVPE8ZB6qqkulPbjlJK+YLfEoYxZr4xZp8xZm0Ly081xhQbYzJrp980Wna2MWaTMWaLMWa2v2JsPi7f31qrlFLdgT9LGC8BZ7exzuciklY7PQhg7ODazwDnAMOAy4wxw/wYZxOaMJRSqnl+Sxgikg505GmwicAWEdkmItXAAuBCnwbXCmNcQGATRmRkZMCOrZRSLQl0G8ZJxphVxph/GWOG185LAXY1Wie7dl6zjDE3GGMyjDEZeXl5RxyQljCUUqp5gUwYK4H+IjIaeAp4vyM7EZF5IjJeRMYnJSUdcVC+ThizZ89u0i3HAw88wGOPPUZpaSlnnHEGY8eOZeTIkXzwwQdt7qulbtCb66a8pS7NlVKqowLW+aCIHGj080JjzLPGmEQgB+jbaNU+tfOO2O2f3E7mnlb6N6/l8ZRgTAgOR3Cb66b1SuOJs1vu1XDGjBncfvvt3HTTTQC8+eabLFq0iNDQUN577z2io6PJz89n0qRJTJs2rbZ7kuY11w261+tttpvy5ro0V0qpIxGwhGGM6QXsFRExxkzElnYKgCJgkDFmADZRzAQu7+To8FUX52PGjGHfvn3s3r2bvLw84uLi6Nu3LzU1Ndx9992kp6fjcDjIyclh79699OrVq8V9NdcNel5eXrPdlDfXpblSSh0JvyUMY8zrwKlAojEmG7gfcAGIyHPAj4CfG2PcQAUwU+zDD25jzM3AIsAJzBeRdb6IqbWSQGOlpWtwOiMICxvoi8NyySWX8Pbbb7Nnz576Tv5ee+018vLyWLFiBS6Xi9TU1Ga7Na/T3m7QlVLKX/yWMETksjaWPw083cKyhcBCf8TVHr7uT2rGjBnMmjWL/Px8li5dCtiuyHv06IHL5WLx4sXs2LGj1X201A36pEmTuPHGG9m+fXt9lVR8fHx9l+ZP1A4Csn//fi1lKKWOSKDvkuqSfN2f1PDhwykpKSElJYXk5GQArrjiCjIyMhg5ciSvvPIKQ4YMaXUfLXWD3lI35c11aa6UUkdCuzdvRkVFFh5PMZGRo30Z3lFLuzdXPjdvHqSnw6uvQis3eij/0+7ND4fXC1lZ0GjEOe1PSik/qqyEe++F116Djz8OdDTqMGjCcDiguNhOteyzGAJ4AhaWUt3WggWQlwdRUfCb30BHLszKy+Hqq+Gtt3wfn2rRMZEw2iwphIVBRUX9W4fD3gvg9erT3lrKUj4lAk8+CSNGwNy58O238P5hPrMrAjfcYKuzZs6EN97wT6xdUaPzVCB0+4QRGhpKQUFB6ye+uoRRu05X6E+qKxARCgoKCA0NDXQoqrtIT4fMTLj1VrjyShg8GO6/31YNt9fcubY66+67YfJkuOIKePdd/8XcVaSnQ3w83HNPwELo9o3eNTU1ZGdnt/7MQmkpFBRA797gcuH1VlFdvQeXKwmnM9zPUXdtoaGh9OnTB5fLFehQVHdw8cX2xLdrl71Q+8c/7An/zTfhkkva3n7pUjjjDDj/fJskysrgrLNg+XL7/oILDt1GBDIyYMsW21a5f799LSyE0FB48EHo0cP3n9WX9uyBMWOgqMi2AT3yCNx5p092fTiN3ohIt5nGjRsnHZKZKQIiCxaIiEh5+XZZvBjZvfvFju1Pqa6spkbE6+38427bJuJwiPz61w3z3G6RoUNFhg2zP7dm506RpCSRE04QKS5umF9UJDJhgkhwsMjChQ3z9+0Tefxxu3+bNhqmqCiR/v1FQkJE+vUT+fZbn37UFrndIqWlIvn5Irt2iWzeLLJlS+u/j5oakdNOEwkLs+eqGTPsZ3jhBZ+EBGRIO8+xAT/J+3LqcMKorBQJChK5+24REXG7S2XxYmTHjjkd259SXVVBgciIESInnyxSUtK5x77jDhGn054oG3vjDXsq+sc/Wt62osImhagokQ0bDl1eWCgyZoxNAE8+KXLJJSIul93vSSfZk+uGDSJ794pUVTVsl5Eh0qePPRm/+aZvPmdzvF6Ryy8/NHHVTVdcYRNJc+6+267z0kv2fVWVyNln2+T79ttHHJomjI4YMULk/PNFRMTr9crSpaGyefMvO74/pbqaigqRyZPtlbjTKXLGGXZeZzhwQCQmRmTmzEOXeTz2/2/wYHs1fTCvV+QnP7Gnq/fea/kY+fkiI0fa9eLjRW6/XWTNmrZjy80V+d737Hb33mvj8bX58+3+r71W5JFHRObOFXn+eZFXX7UlLmPsd7BpU9Pt/vlPu9311zedX1ZmYw4OFvn3v48oNE0YHXH55bZoWuvLL/vK+vVXd3x/SrXXiy+KDB9ur3b9xeMRmT7d/su/+abIK6/Yn6dNE6mu9s0x9uyx1SvNeeope7yvvmp++Tvv2OWvvNIwr6rKnjBnzmw4mbelsNBuU1l5eLFXVjYkpQsvtNVfbVWRtdeuXSLR0SJTp7acjBYtEklIsCWod96x87KyROLiRNLSRMrLD92msFBk1CiRiAiRZcs6HJ4mjI6YM8d+Hfv3i4jI8uVjZdWqczq+P3V08HgCU59fJyvL/sODrRZ56y3/HOf22+0xHn+8Yd4zz9h5l19+5FfV779vT24Oh8jPfmbbD+p4PCKDBolMnNjy9h6PPTEed5zIxx/bK/HYWBtfbKzIL37huxN4S7xeW53ldNrjOp0iKSk27osuEvm//xPJyzv8fZ59tkh4uMjWra2vu2OHrXYDkV/9yv4cHd1yEhaxpaPjjrNtOx2sYtSE0RELF9qvIz1dREQyM8+SjIzxHd+fOjrMnCly/PEiK1d2bHuPp+NX6F6vyFln2YSxfLmtaweRhx7ybRL705/sfm+77dD9/v73dtnPftaxY1ZUiNxyi93H2LEiN95oT7TR0SKPPmqv3D/+WNpsoxAR+fBDqa/Tj4kRueYau23jNofOsHKlyLPPitxzj01cP/iBbZQPChJJThb57LP27+vFF+3neeqp9q1fWSny8583fA/vvtv2Ntu2iXz0UftjOogmjI7IzrZfx9NPi4jI+vVXypdf9u/4/lTXV3ciCw210/z5La+blWWvxJOS7MksLKzhStQYewV/uFfAr75qt587176vqLCNnyBy5ZW+aV94800b3/TpLcc3e7Y95p13Hl7S2LTJlgrAfv66aqANG0TOO8/OP+44W23Su3fbidXrtSfqDz88/CqlzpCZae+4MkbkrrvaTmQ7d9rEeeqph1+Ce+stm2w6gSaMjvB6bUPZDTeIiMjmzb+QpUvDO74/1bVVVNiT2ZAh9mLh9NPtv8MNNzQ9WZWU2CvNuqRyzTUit95qqwzuvlvkgQdErr7abnv++e2vFti3z9ZZn3RS0xO512tLGHV398ybJ3Lfffa4p59uS0MDBtgTa3MNxHWqqmwiCgmxDd3N1YE3PuaNN9pjTpoksnhx67F7PPaOnYgI+xk+/LD59RYtsm0zIPLww63v82hRViby05/azzR+vMh33zW/XuPSY1tVUQGmCaOjTj3V/sOISFbW72XxYsTtLjuyfaqu6Xe/s3/+n35q39fUNFxpT5ggsn27LXH06iX19fw7drS8v2eesfX3aWmH3jbanMsvt7d9rl3b/PK33rKlGLD7TUmxCeTSSxvu6Bk61JaSGpcKPB6Rv//dJhWwSSY/v+14PB57105Kit3uzDObNsJ7vSLffGNvja1bZ8qUtj9rTY2twvFVw3pX8e679gIzIsK2bbz6qv2+6m6NfeEFaVxj0ZVpwuioW2+1fwAej+zd+6YsXowUF399ZPtULduzp/Nu62wsK8uejH/0o0OXvfeerUZwOOy/x4kntnxnz8EWLhSJjLTVL621idTdKvnAA63vLy/PJqmDSxJerz1hHX+83c/3v2+rSxYuFBk92s5LSxP55JPDb5coLxd57DFbcgD7Hd17b8OxXC57Z9WCBa2XcI4Fu3aJnHOObdto/ExFv362kbsjVVEB0CUSBjAf2AesbWH5FcBqYA3wJTC60bKs2vmZh/Nhjjhh1F0VbNkilZW5tQ/vPXpk+1TNy821d9VMmuT7q8/160VWrWp5+cUX23/olkoMmzbZB79ee+3w/+FXrRLp29deeLzyik2KjU/aBw7Y5cOGHXk9fVWVyBNP2O+x7mQ1cKBtXD7SE1VxschvfmM/h8Nhk9KLL9pbOVVT1dX2b+6dd2zJ9fLL7TMu27YFOrJ26SoJYwowtpWE8T0grvbnc4CvGy3LAhIP95hHnDC++UYa35mwbNlgWb36giPbp2rezJkNjca1T9j7xKJFNhk4HLb6pKzs0OX+rlPfvdvWb9edxBMT7dXmzTfbe/yNEfnyS98dr7BQ5MEHRf7yF9/fUVRU1PQWWdXtdImEYeMgtaWEcdB6cUBOo/eBSRhlZfafubaqYOPG6+Xzz2PF6+36xcqjSt0tzA88YJ9gNUbkP/858v2+9ZatMhk92jZe111x1+27qso+TTxokP/vwqmstMd98kmRWbNs+0NUlNTf3qpUF3E4CSOo3V0a+td1wL8avRfg38YYAf4qIvM6JYrwcDj+eFi9GoCYmCnk5r5AWdlaIiNHdUoI3V5ZGfz85zBkCMyeDW43fPGF7ep69WpITOzYfufPh1mz4KST4J//hNhYuPxyO++MM+D666FnT/juO/jXvyAkxLef62AhIXD66XaqIwK5uTYOpY5G7c0sHZloRwkDOA3YACQ0mpdS+9oDWAVMaWX7G4AMIKNfo649Omz6dHsFKiIVFVmyeDGya1c7H7pRbfvlL6XxA5IiYnsKDQ4WueCClhtpKypa7pzt8cftPs8669B1ysvtPfN11V8XXuibz6FUN8FhlDACOoCSMWYU8AJwoYgU1M0XkZza133Ae8DElvYhIvNEZLyIjE9KSjryoEaNsv3ml5URGtqfkJB+FBenH/l+FaxcCX/+s73qP+WUhvlpafDoo/DRR/DMM0232bULfvUre1UeGQkDBsB558H//R+89JJ9/eUv7VgKH34IERFNtw8Lgzlz4Ouv4Sc/gaee8vvHVKq7CliVlDGmH/AucJWIfNdofgTgEJGS2p/PBB7stMBGjbJVB+vWwcSJxMZOobDwU0QEY0ynhXHUOnDADkoTHNx0vttth9VMSoI//vHQ7W65Bf79b5scTjkFPB54/PGG4TcvuQSGDYMNG2D9evjPf6Cqyi677jr461/B6Ww5rnHj4MUXffMZlTpG+S1hGGNeB04FEo0x2cD9gAtARJ4DfgMkAM/WnojdYkd96gm8VzsvCPiHiHzirzgPMaq2rWL1apg4kZiYKezd+3cqKjYTHj6408I4Km3fDuPHQ02NHQXtggvg3HNtu8TTT8OKFbBgAcTFHbqtMfC3v8Ho0XDyyXYUxKgouO02O5xn//5N13e77fH274cJE+z2Sim/8lvCEJHL2lh+PXB9M/O3AaP9FVebUlNt1Udtw3ds7BQAiorSNWG0prLSlgK8Xrj0Uli4EN5+GxwO2xCdmQnnnGOXtSQpyQ7ZeeedMGOGrbqKiWl+3aAgGDTIP59FKdWsrnKXVNfhcMDIkfUJIyxsMC5XD4qL0+nd+5D8purccYctQXzwAUybZhPHypW2XeKjj+wdaM8+23ZJ4NRT7fjMSqkuRxNGc0aNsoPS17ZbxMZOoahIG75b9Npr8Je/2AboadPsPIfDVk+NHw+//W1g41NK+URA75LqskaNsnXjOTmAfR6jqmoHlZU7AhxYF7R+vW3MnjIFHn440NEopfxIE0ZzGjd807gd4/NARdQ1lZbCj35k23wWLLDtCkqpbksTRnNGjrSvtQkjImIEQUGx3f95jLo2CLe77XVFbMli0yabLJKT/R+fUiqgNGE0JybG3sZZmzCMcRITc3L3bMfweuHjj+G002x7ww9/aBPmu+/apNCcjAy47DJ4/XV48EG7rVKq29OE0ZJRo+oTBth2jIqKTVRX7w1gUD5UVWX7Xxo5Es4/H7ZutQ/KvfWWvZNp+nQ48UT473/t+jU19iG6733PPvfw8ce2kfvXvw7s51BKdRpNGC0ZNQo2brRPLtPN2jHWrLHPMFx3Hbhc8Pe/24Rxxx22TWLNGvsQ3Z49tuO+qVNtlxwzZ8K+ffDEE5CdbZ/YduifkFLHCv1vb8mFF9rqmvvvByAyciwOR/jR346xfLlNAF6v7Yrj22/hiits4qjjdMK119qeXf/0J8jKgqFD7fMUmzbZp69beqBOKdVtGWmpnvooNH78eMnIyPDdDm+80fZR9M03MG4cmZnfp6YmnwkTMn13jM70xRcNXXX85z+21KCUOqYZY1bUdsvUJi1htOaJlHAMAAAgAElEQVQPf4AePWwXFW43sbFTKCtbTU3N/kBHdvg++8z275ScDOnpmiyUUodNE0ZrYmJg7lxbbfPUU8TETAGE4uL/BTqyw/PRR7Zh+7jjbLLo0yfQESmljkKaMNryox/Z8Rfuu4/oot4Y46KoaEmgo2q/t9+Giy+2jfhLluhob0qpDtOE0RZj7KA+Ijhv/SWxMVMoKPiIo6Lt54MP7PMSJ55oq6Ti4wMdkVLqKNauhGGMuc0YE22sF40xK40xZ/o7uC6jf3/7gNo//0mfbwZSUfEd5eUbAh1V6xYtsl2Jjx1ruxqPjg50REqpo1x7Sxg/EZED2NHv4oCrgDl+i6oruu02SEsj/oEPcZZCXt67gY6oZYsX2ye2hw2DTz7RZKGU8on2Joy6QQzOBV4VkXWN5h0bgoJg3jzM3jyGP51I/r53Ah1R8/73v4YG7k8/bX50O6WU6oD2JowVxph/YxPGImNMFOBtayNjzHxjzD5jzNoWlhtjzFxjzBZjzGpjzNhGy64xxmyuna5pZ5z+NWECPPww8YvySf59JhXl2wIdUVPLl9tR7fr0sW0WiYmBjkgp1Y20N2FcB8wGJohIOXZs7h+3Y7uXgLNbWX4OMKh2ugH4C4AxJh47BviJwETgfmNM17hUvusuau64gZQPoObOWS2vt349nHIK3Hyzf+PxeGwV1I03wve/3/BQXq9e/j2uUuqY096EcRKwSUSKjDFXAvcCxW1tJCLpQGErq1wIvCLWMiDWGJMMnAV8KiKFIrIf+JTWE0/nMQbXY8+xb3oC0c/+1z7c15jXa/taGjvWPiH+zDO2+29fcrttUvjZz6B3bzj9dHjpJftg3uLF+pyFUsov2psw/gKUG2NGA78EtgKv+OD4KcCuRu+za+e1NL9rMIbyR25h7/eBu++2SQFg5074wQ/gF7+AM8+EbdvgpJPsiX3nTt8ce/VqGDfOliZefdWOgf3WW5CXZ4eV7d/fN8dRSqmDtDdhuMU+eHAh8LSIPANE+S+s9jPG3GCMyTDGZOTl5XXacZN6/oiNd0HFWaNttdPNN9uuwr/5Bp5/3j4DkZJie4L1eOCqq+xrR3k88Nhjth1l716737w82+X4j34EERG++3BKKdWM9o6pWWKM+TX2dtpTjDEObDvGkcoB+jZ636d2Xg5w6kHzlzS3AxGZB8wD2/mgD2Jql/DwYYRGDWbzgwmMcp9hSxmTJ8Mrr8DAgQ0rDhwITz9te3999FGYPfvwD7ZjB1xzDSxdChddBPPmaYO2Cii32/Zs74ve7T0eqKxsmJp7Jtbrtet5PE1/rqqyU+PtG8fmdDZMISEQFtYwhYbaeWCPWTd5vXZyu+0wMG53w88VFQ1Tebl9dbsbYm7peV5jGl7rJoej6fuD16uqguLiptOBAw0xeTwNscXFwYcfHvnvoi3tTRgzgMuxz2PsMcb0Ax71wfE/BG42xizANnAXi0iuMWYR8PtGDd1nAl1qpB5jDElJF7Nr12PUvLMd15drbDWU03noyldfbQccuu8+W2U1blzT5QUFNglkZUFCgn0iOyHBTrt3w1132b/El16y+zLH1h3N3YmIPckcfCKorra/9sREO8XGNpyMvV57oigqstOBA/bEWHeirHv1eA49cXk8UFZmh18vLW34WcSeMOtOnHUnz8YnobqTZGmpLczm5UF+vn0tKbH7d7kgONhOISH2z7/uhNt4au5E6nY3nOBVy4yxj1LFxNjX4GB7l39QkP2+Xa6GxOf3WNrbxYUxpicwofbtNyKyrx3bvI4tKSQCe7F3PrkAROQ5Y4wBnsY2aJcDPxaRjNptfwLcXburh0Xkb20dz+fdm7fhwIHlrFw5kSFDXqZXr6tbX7mw0PbnFBEBK1fa1y1b4M9/toMVVVTYnnELCw/9DzrlFHj5Ze1htp08HnuSq7sSa/xaVmavDMvKGiaPp+GkV3cCdDrtibzuJFl3oiwqanqVWTfV1DScbOuufutO4HVXrY2vYtvicNirRo/HxnGkPdEYA5GRdoqIsPuvqGi4Kq+7Ugb7HQQFNbxGREBSUkMyS0qysYnYZFVd3TC53fa7q7vCr7uKbq4k4nA0JKzGV/zNrWtM09JCXekhJMROjZOey9V8iaSqqunvrC7Z1u2/8VW/09lwUm78XTQuoYSH29e6oWQOLiXUafy7O/jvoHEyPfjV5bJJIjLSv+OUHU735u1KGMaYS7EliiXYB/ZOAe4UkbePIE6f6+yEISIsW9aPyMhxjBz5ftsbLF5sR7CbPt3+Bb//vv2ruPJKO9rd8OH2r6WkxJY6CgrsX/SkSc2XXLqRqip75Vx3JV13Rdt4Emm40qq72goNhV277P0F27fb1507fX/VGhNjT5YxMfZEUXeyqJtcroaTTN1r45Nl46qHiIiGz1A3BQfba4X8fPtrr0tSwcG2tNF4io62x2x8sgwJsceEQ6s/IiPt+m0VTL1eHUDxWHQ4CaO9VVL3YJ/B2Fd7gCTgM6BLJYzOZowhMfFicnPn4XaXEhQU2foGp50Gv/qVbcuIj7d3WN18c9NnJurKn9HRR3WJQsRe5RcWwv799oS/a1fDtHOnfS0osEmi7kqvOS6Xvao1xl5tl5Yeuk5Skv26JkyAGTPsyf3gK2WXy57oIyKaTk6nLSFUVzd9jY21+0lIsCfu7k6ThWpLexOG46AqqAK0p1sAkpIuJidnLoWFn9Cjx4/a3uD3v7fPTZxyylF1Z1NxsW17z8pqeN2zp6Fap66ap7zclhD272/+Kt8Y28N63762q6ukpIbSQuPXpKSGKSam6dWxx2MLYcXFtmqhTx97Fa2U8q/2JoxPahuiX699PwNY6J+Qji4xMSfjciWRn/9u+xJGUBCc3TWeQWyO12ubVlautONG1b0WFDRdLzTUPjNYVyceHm6vxMPD7ZV5fHzDFBdnl/Xta+80PtKrdaezoXpGKdV52pUwROROY8x0YHLtrHki8p7/wjp6GOMkMfFC9u17A4+nEqczNNAhtUnEJoXNm229//bttsSwfTt8911DlU9wsH205KKL4IQT7DOB/ftDampDFZFS6tjR3hIGIvIO0EW7aA2spKRLyc19gYKCD+nR49JAh3MItxsyM+Hzz+30xRe2TaFOSIhNAqmp9sH0sWPtNGzYsVF3r5Rqn1YThjGmBGjuNioDiIjoQAtAXNwZhIT0Jzf3hS6RMKqqICPDPueXnm57PK8rNQwYAOeeCyefbBPCgAG2TUEbPJVSbWk1YYhIl+j+o6szxkFy8k/IyrqfiorthIV17t1NIraLqQ8+sMN2f/WVvccc7J26V10FU6bYdvaUrtMjl1LqKNPuKinVul69fkxW1m/Zs2c+Awb8zu/HE7HdVr3zDrz7LmzdatsU0tLgpz+FqVNtgtAeRJRSvqIJw0dCQ/sSH382ubl/o3//+3E4/PPV7t0LTz5pO6rNzrY3XZ1xBvzf/9lRWXv08MthlVJKn6XwpeTk66muzqGw8BOf7zsrC266yTZMz5ljSxIvvwz79tlhu2+4QZOFUsq/tIThQwkJ5+Ny9SQ39wUSE8/3yT7XrYM//hH+8Q/bMH311bY0MXiwT3avlFLtpgnDhxwOF716XcuuXY9RVZVLSEhyh/azf78dE+mVV+wdTuHhcOuttrspHUxPKRUoWiXlY8nJ1wEe9ux56bC2q6mBf/4TLr0UkpNtw/X+/bb6aedO+NOfNFkopQJLSxg+Fh4+iNjYU8nNfYF+/e7CjjXVMhE7aN6vf23bKZKS7IiuV11lH57Tp6mVUl2FljD8IDl5FpWV2ygqWtLqeunpcOKJcNlltoO999+HnBx44gk7xpImC6VUV6IJww8SEy8mKCiO3Nznm13+3Xe2f6apU+2Aei+9BCtWwIUXNgzGopRSXY0mDD9wOkPp2fMq8vLepaamoZvXykq45x779PVnn8FDD9nkcc013X58JKVUN6AJw0+Sk69HpJo9e14FbHcdY8bY4TCuuML2FnvPPfYOKKWU6qgaTw3b9m/rlGP5NWEYY842xmwyxmwxxsxuZvmfjTGZtdN3xpiiRss8jZZ96M84/SEyciTR0ZPYunU+t93mZfJkO7jQJ5/YKqiePQMdoVKqvTxeD3tL9+IVb5vrllaXsm7fOnaX7Mbt9fFYwdgEsSx7GXO+mMM5r51D/CPxTPnbFNoz3PaR8ttdUsYYJ/AM8AMgG1hujPlQRNbXrSMiv2i0/i3AmEa7qBCRNH/F1xl27PgjN93Uh9xcBzfdBH/4A0Rpd46qncpryimsKKTSXdlkMhiOjz+eHhE9MD66M0JE2Fu2F4/Xg8M4cBgHTocTh3FQ7ammuLKY4qri+tfS6lJCg0KJcEUQERxBZHAkEa4Iekf1Ji4s7ohiqaipIKsoi+1F29m+fzvbi7ZT5a5iTPIYxvcez7CkYQS10PWOV7xUuasIc4W1eozCikK+zf2W7UXbCXeFEx0STXRINFHBUUSFRLG7ZDer965m1Z5VrNq7irX71lLhriA0KJRB8YM4IfEETkiwU7WnmvV561mfv571eevZWbyz/jgGQ4+IHiRHJZMcmUywM5jiqmIOVB1o8l26HC7CXGGEBYUR5goj3BVOsDMYp3E2+X3UeGpYmbuSspoyAIYnDeea0dcwtf9UvOLFafxbt238lZWMMScBD4jIWbXvfw0gIn9oYf0vgftF5NPa96UiclgDb44fP14yMjKOLHAfqKmB+++HOXOEPn2yuO++OVx//XM+++dWXZuIUFJdwt7SveSU5JBzIIfsA9nklOSQW5pLcmQyo3uOZlTPUYzoMaL+5FZWXcYXO79gSdYSFmctJmN3Bh7xtHic2NBYTkg4gSGJQzgh4QTCXeGUVpdSUl1S/+r2uhmSMISRPUcyqucoUmNTcdTe6r2zeCeLty/mv1n/5b/b/0v2gWyffP7j44/nxJQTOTHlRCamTCStVxohQSHNrnug6gAZuzP4Ovtrvtn9DctzlpNTktNknRBnCC6ni9Jq20d/WFAYY5LHMLbXWARhd8luckpy2F2ymz2le3B73SSFJzEgbgADYgeQGptKamwqe0v38u2eb/l2z7dNTuqtiQ+LZ3TP0YzuOZrU2FR2Fu9kU8EmNhVsYvv+7fW/n9CgUIYmDmVY0jCGJQ1jQOwAiiqLyC3NJbck176W5lLjqSEmNIaYkBhiQmOIDo4mKiSKGk8N5TXlVLgr7FRTQbWnGq948YoXj3jqSzdpPdOYmjqVKf2n0CPiyPsDMsasEJHx7VrXjwnjR8DZInJ97furgBNF5OZm1u0PLAP6iNjfgDHGDWQCbmCOiLzfwnFuAG4A6Nev37gdO3b44+O0244dcPnl8OWXcN11cNddL5CTM4u0tCXExk4NaGzHKq94KaosIr88v+Gft/Z1T+keHMbB8KThDO8xnOFJw+kX06/V5O72utlauNVeVeatZ2PBRnJLcskvzyevPI/88nyqPdWHbBcdEk2vyF7kHMipv0J0GAeD4gcRGxrLitwVuL1ughxBTEyZyGmpp5Eam0poUGiTqcZTw+bCzWzK38TGgo1szN/I7pLd9ccJdgYTGRxJZHAkBsOO4ob/iQhXBCN6jKCgooAthVsASAxP5LTU05jcdzJhrrD6k1Td5HK4mp7kQqKJDI6kyl1FWU0ZZdVllFaXUlpdyvai7Xyd8zVfZ39NbmkuAEGOIKJDoolwRRDuCq+f9lfuZ0PeBqR2yJ1B8YOYkDKBoYlDGRA7oP6E3zPS1t9uKdzC8pzlZOzOICM3g29zvyXYGUzvqN6kRKfQO6o3vSN7E+4KZ0fxjvoSys7indR4azAYTkg8gbReaYzpNYYxvcYwKGEQle5KSqpKOFB1gJJq+5oYnsjonqPpHdW7xb+Fak812/Zvw+VwkRqbitNxdN65cjQmjLuwyeKWRvNSRCTHGDMQ+C9whohsbe2YgS5hvPuuTRIeD/z1r/b5Co+ngmXL+hMVNYFRoz4OWGwd5fF6yC/Pp8Zbc8gyt9dtr2SrSppc2YYFhZEUkURieCJJ4UkkhCe0WIUAUOWuYkP+hvoqgO1F24kKiSIhLIH4sHjiw+JJCEug2lPN3rK97C3dy77yfewt3UtBRQEGg9PhJMgRhNPY1wp3BQXlBRRUFFBYUdhs3XOwM5jkyGSqPdX1JzeAqOAohiQOIdx16B0JhRWFbCrY1CQh9I3uS0p0CknhDZ85MTyRHhE96BPdh5ToFFKiUogKsfWRXvGybf+2JlUehRWFTO47mVNTT2Vyv8lEBh9W4ZoDVQdwe91EBkcS7Gw6TGJdnfrqvatZs28Na/atISo4itMHnM7pA05nRI8R9aUOX8o+kM3X2V+zMnclxVXFlNeUU1ZTRnlNOeU15YQFhTExZWL9FB8W7/MYwP4N55bmEhcaR0RwhF+OcTTrKgmj3VVSxphvgZtE5MsW9vUS8E8Rebu1YwYqYVRV2X6enn0Wxo+HBQvguOMalmdlPURW1n2MH7+GyMgR7d7vvrJ9fL7jcw5UHaDaU02Vp4oqdxVVniqcxkl0SHSTK7+o4CiqPdWH/GO6ve5DrlJDnCGUVpeyv3I/+yv2178WVBSwp3RP/ZRXnteuhr62xITE2HruRvXdEcERZB/IZmP+xvrGwdCgUAbEDqCspozCisL6aojGQpwh9IzsSc+IniSG2wE/3F43HvHg9rpxe92EOENIDE8kMTyRhLAE+xqeQK/IXiRHJpMclUxcaFz91eP+iv2sy1vHun3rWJe3jo35G1ssJdRVOwxLGsaQxCGHfXJXqivpKgkjCPgOOAPIAZYDl4vIuoPWGwJ8AgyQ2mCMMXFAuYhUGWMSga+ACxs3mDcnEAnD44GZM+Htt23S+MMfDh0Hu6amkK++6ktS0iUMPuFFMvdkEhMaQ++o3k2uYkWE9Xnr+XDTh3z03Ucsy15WX1zvDBGuiPqTaq/IXvSM6Fn/2lwdtNM4iQyOJCokiqjgqPpqkPKa8vqqmfzyfPLK8upP/mU1ZU1ee0T0qK8jHtVzFIMSBjUpjVR7qusTWZAjiJ4RPYkOidb2IKV85HASht/ukhIRtzHmZmAR4ATmi8g6Y8yDQIaI1N0qOxNYIE0z11Dgr8YYL/bW3zltJYtAEIHbbrPJ4vHHbcJojssVT89e1/PG6qdZ8J+v2FDwXf2ymBCbOHpH9Wbb/m1sL9oOwLjkcdw/9X7OGXQOPSJ6EOIMISQohGBnMCHOEDziobiy9m6L2rtXSqpLCHYGN6krjgiOwGmcVHmqDrnbJjI4krjQOOLC4ogNjT2kKqMrCHYG29JEpN6HrFSg+a2EEQidXcL4/e/hnkezOP3nH9L/xExG9xzN5H6TSeuVVn+VLCIs3LyQe/97F5l71zEwOp57Tn2UIEcQu0t21085JTkkhCVwweALOH/w+aRE6+DbSin/6xJVUoHQGQnDK15W5q7kt298wD+/+xB6rQbs7XeFFYUAhLvCOTHlRE7qcxL/zfovy7KXMTBuILMG9WRSxGomn5SNyxXr1ziVUqo9ukSVVHcjIry1/i3u+e899nZEr4O4sJO56/THuHjYNAYlDCL7QDb/2/k//rfLTnP+N4eUqBTmnT+Pa9OupapiPRkZaeze/Rz9+x/y4LtSSnVpWsJoh/Qd6dz56Z18k/MNAyNHsOuNXzIs6Hw+X5TY6pPb5TXlBDuDmzTirlp1FmVlqznxxO04naE+j1UppQ7H4ZQwtPPBVmzI28CFCy5k6ktTyTmQw5+m/I0Dj2TSr/Ba/v1+68kCbNXUwc8f9Ot3F9XVe8jN/asfI1dKKd/TKqkWvL/xfaa/OZ3I4Ej+cMYfuHHcrZx/VjgVZfD5UujRwSfy4+JOJy7u+2Rl/Y5eva4lKCjGt4ErpZSfaAmjGR6vh9mfzWZo4lC23LKF2SfPZs7vwvn8c/sE95AhR7b/gQPn4HYXsGvXY74JWCmlOoEmjGa8tf4tNhVs4oFTHyApIol//cs+kDdrlh3L4khFRY2jR4+Z7Nr1J6qqctveQCmlugBNGAfxipeH0h9iWNIwLh56Mbt2wVVXwahR8OSTvjvOgAEPIVLNjh0P+m6nSinlR5owDvLehvdYl7eOe0+5F4/bwcyZtq+ot96CsNa72D8sYWHH0bv3z9i9+3nKy79rewOllAowTRiNeMXLg+kPMjhhMJcOv5R777XdlM+bB4MH+/54/fvfh9MZxrZtd/t+50op5WOaMBr5aNNHrN67mntOuYfP05088gj89Ke2m3J/CA7uQd++vyI//x2Ki5f55yBKKeUjmjBqiQi/S/8dA+MGcvnIy3njDYiMhD//2b/H7dPnDlyuHmzbdlenjMmrlFIdpQmj1r+2/IsVuSu455R7CHIEsXQpnHyyb9stmhMUFEVq6m8oLk6nsPBf/j2YUkodAU0Y2NLFg0sfpH9Mf64adRX79sGGDTC1k0ZUTU6+gbCw49my5XY8nrLOOahSSh0mTRjAp9s+5eucr/n1yb/G5XSRnm7nd1bCcDhcDB48j4qKLWzdemfnHFQppQ7TMZ8w6koXfaL7cG3atQAsXQrh4Xa41c4SF3caffrcwe7df6Gg4Ogb+1sp1f0d8wnjQNUBAGZPnl0/DOnSpfC974HL1bmxDBz4MBERI9m48Tqqq/M69+BKKdUGvyYMY8zZxphNxpgtxphDBoAwxlxrjMkzxmTWTtc3WnaNMWZz7XSNv2KMCY3h8x9/zs8n/ByAggJYs6bzqqMaczhCGDr0Ndzu/WzaNEvvmlJKdSl+SxjGGCfwDHAOMAy4zBgzrJlV3xCRtNrphdpt44H7gROBicD9xpg4P8aKw9iv4vPP7bxAJAyAyMiRDBz4ewoKPmDPnvmBCUIppZrhzxLGRGCLiGwTkWpgAXBhO7c9C/hURApFZD/wKXC2n+JsYulSCA2FiRM742jN69PnF8TGnsbmzbdRUbE1cIEopVQj/kwYKcCuRu+za+cdbLoxZrUx5m1jTN/D3Nbnli6FSZMgJKQzjtY8YxwMGfIyDoeLDRuuwut1By4YpZSqFehG74+AVBEZhS1FvHy4OzDG3GCMyTDGZOTlHVlDcVERZGYGrjqqsdDQvgwa9CwHDnzFzp0PBzocpZTya8LIAfo2et+ndl49ESkQkaraty8A49q7baN9zBOR8SIyPikp6YgC/uILEOkaCQOgZ8/L6NnzSrKyHqS4+H+BDkcpdYzzZ8JYDgwyxgwwxgQDM4EPG69gjElu9HYasKH250XAmcaYuNrG7jNr5/nV0qUQHGyrpLqKQYOeITS0P+vXX4HbXRzocJRSxzC/JQwRcQM3Y0/0G4A3RWSdMeZBY8y02tVuNcasM8asAm4Frq3dthD4HTbpLAcerJ3nV0uX2sZuf/cfdTiCgqIZOvQfVFVl8913P9dbbZVSAWO60wlo/PjxkpGR0aFtS0ogLg5mz4aHHvJxYD6QlfUQWVn3MWTIK/TqdVWgw1FKdRPGmBUi0q5+LQLd6N1l/O9/4PF0nfaLg/Xv/2tiYk5h8+Yb9VZbpVRAaMKotXQpBAXZLkG6ImOcDB36d8DJ+vVX4PXWBDokpdQxRhNGraVLbWeDERGBjqRloaH9OOGEeZSUfE1W1m8CHY5S6hijCQMoK4Ply7tudVRjPXpcSnLy9ezcOYc9e14NdDhKqWNIUKAD6Aq++grc7qMjYYC91baiYiubNl1HSEgf4uJOC3RISqljgJYwsNVRDgdMnhzoSNrH4Qhm+PB3CQsbxNq1F1FWtj7QISmljgGaMLAJY+xYiI4OdCTt53LFMmrUQpzOMFavPpeqqj2BDkkp1c0d8wmjshK+/vroqY5qLDS0PyNGfERNTR5r1pyv44ErpfzqmE8YoaGwZQvcfnugI+mY6OjxDBu2gNLSb1m//jJEPIEOSSnVTR3zCQOgb1/o0yfQUXRcYuIFDBo0l4KCj1i7drqWNJRSfqEJo5tISbmJ44+3SePbb6dQVbU70CEppboZTRjdSJ8+tzBy5IdUVHzHihUTKSnJDHRISqluRBNGN5OQcB5jxnyBMYZvvz2ZgoKPAx2SUqqb0ITRDUVGjmbs2K8JDz+BNWumsWvX44h4Ax2WUuoopwmjmwoJ6c2YMekkJk5j69ZfsWrVGdrLrVLqiGjC6MaczgiGD3+XwYOfp6RkJcuXjyI7e66WNpRSHaIJo5szxtC79/VMmLCO2NipbNlyG5mZUykv3xzo0JRSRxlNGMeI0NA+jBz5MUOGvERZ2VoyMkaxdetdVFXlBjo0pdRRwq8JwxhztjFmkzFmizFmdjPL7zDGrDfGrDbG/McY07/RMo8xJrN2+tCfcR4rjDH06nUNEyasIzHxInbteoxly1LZtOkGLXEopdrktzG9jTFO4DvgB0A2sBy4TETWN1rnNOBrESk3xvwcOFVEZtQuKxWRyMM55pGM6X0sqqjYyq5dj5Gb+zdEqklKmk6/frOJihoX6NCUUp2kq4zpPRHYIiLbRKQaWABc2HgFEVksIuW1b5cBR3EHHUefsLDjGDz4L5x00g769ZtNYeGnrFgxnnXrLtUSh1LqEP5MGCnArkbvs2vnteQ64F+N3ocaYzKMMcuMMT9saSNjzA2162Xk5eUdWcTHqODgngwc+HtOOmkn/fvfT0HBQr75ZijfffdzbeNQStXrEo3expgrgfHAo41m968tJl0OPGGMOa65bUVknoiMF5HxSUlJnRBt9xUUFM2AAQ8wadJWevf+Gbm5L/D118ezbdu91NQUBjo8pVSA+TNh5AB9G73vUzuvCWPM94F7gGkiUlU3X0Ryal+3AUuAMX6MVTUSHNyTwYOfZsKEDSQkXMDOnQ/z1Vd92LjxOkpKVgY6PKVUgPgzYSwHBhljBhhjgoGZQJO7nYwxY4C/YpPFvkbz44wxIbU/JwKTAR2HtJOFhx/P8OELGD9+FT17Xsm+fQtYsWIcK1d+jz17/o7XW9X2Ts8NShwAAA1FSURBVJRS3Ybf7pICMMacCzwBOIH5IvKwMeZBIENEPjTGfAaMBOoqyneKyDRjzPewicSLTWpPiMiLbR1P75Lyr5qaIvbseYndu5+lomIzDkcYISF9CAlJITi4d/1rTMxJREVNwJguUeOplGrF4dwl5deE0dk0YXQOES/7939GYeG/qKraTVVVDtXVu6mq2k1drWJwcApJSReTmHgxMTEn43AEBThqpVRzDidh6H+xOmzGOIiPP5P4+DObzBcRamr2UVj4b/Lz3yU393lycp7C5UokIWEaCQnnExf3fYKCogIUuVLqSGgJQ/mNx1NGYeEn5OW9S0HBP/F4DmBMMLGxU0lIOI/4+PMIDz8+0GEqdUzTKinV5Xi9NRQX/4/Cwo8pKPiY8vINAAQFxRMWNpDQ0AGEhg4kLGwAoaHHERk5iuDgHgGOWqnuTxOG6vIqKrZRWPgvysrWUVGxjcrKbVRWZiFSU79OcHBvIiPT6ieHI5SqquzaNpMcqqqycbtL6NFjBsnJ1xEUFB3AT6TU0UkThjoqiXioqtpNRcV3lJauorQ0k9LSTMrLNyDibrSmk5CQZIKDUwAPJSUZOJ1RJCfPok+fWwkN7d/SIZRSB9FGb3VUMsZJaGhfQkP7Ehd3Rv18r7eKsrL1iNQQEtKH4OCe2L4trQMHMsjO/jPZ2U+Snf0kSUnTSUg4l4MfM7L7TyU8/ARcroTO+lhKdRtawlDdRmXlLnJynmb37r/i8RS3um5QUDzh4ScQFjaY4OAeeL3ViNQgUl37s4ewsOOJjBxNZGQaoaGpGGM66ZMo1Xm0Skod0zyecqqrD+000eutprJyG+Xlmygv/46Kik2Ul2/C7d6PMSE4HMEY48LhCAagsnIn9tlRcDpjiIwcRUTEcEJC+hES0pfQUPsaEpJSv42viHioqNiKy5WgpSHlV1olpY5pTmc4YWHN9lVJRMRQEhLOa9d+PJ5yysrW1rellJauYt++N3G7D+2I0ZggwIkxjScXDkcoDkdY/avTGYbLlURISEpt9VpKfcKx7TbfUlLyLWVlq/F6KwAICzuBmJiTiI7+HtHRJxERMUyfolcBoSUMpQ6Tx1NGZeUuqqp2UVW1k6qqbLzeKkQ8iHgAT+3PNXi9lXg8FXi9lXi9FXi95VRX76OqKgevt+yQfduSTBpRUWOIiBhFdfVeDhz4kgMHvqKmJh8AhyMUpzOGoKAonM6GyeEIqU1UQbVtPE4cjmBcriSCg3vgcvWsfe1BUFA0xrjqS1T2ZyceTzkeT2ntVILHU4rLlfD/7d1vjBx1Hcfx92dnd6+9vfaWq7UYyp8iTaBVLH+DAgYBFZUID1BRIMSY8KQmkGgUjMZIwgOfiD7ACBFiVRQQqTb6QLEQ/iQIXGm1UCDWAtpaewXb+9fr7c7u1wfz2+ve0ZRh27292fu+ks3uzM7O/b53c/Pd+f1mvkOp9AFPUl3KjzCca6MoKlEqnU6pdHrL6zAz4ng4nB68i3p9glLpgyxYsOKwYyVmxsTEdkZGnmF8fCtxPBJ26KPE8SiVyu5QDLI2lbiSxySVyl6g1nrAQD5fZvHiCymXP0p//8UsWnTOVDecWY16vRoS5ARxPEytNkIcD0+1M5dbSD6/mChaPPVcKCwhihYeVbvc7PKE4VwHSKJQKFMolCmVVqdavrd3Jb29K9/1zzKrE8f7qFSGqFT2UK3uoVYbCzv5StjRJwP9UVQiivrCUUsfUdTH5OROhoefZHj4KXbs+GNoT9IFl9xMs/VeimLx+HDRZuNxMiDq9XFqtXFqtQPU6+PU69VwZLVwWhff9OfG+wua5h+artVGQ/zJ76FS2UMc70fKNx1lFcnlikRRH/l8mXz+uKbnfqKod9oZevONJwznupyUmxo8L5XOaGkdxx9/PQCVyhDDw08zOjqIWZ1crjDVtdUYs8nn+8POdXF47gtdcyPhiGOEOB6lWt3DxMRrHDz4GiMjzzA09CBvPxLKEUUlpHzo1jvI0SSot4sO8zOPTCoSRb3kcr1EUW9TcimHrsIyUVSiVhsjjvcRx/unHvV65W2/Mykfzs47GB6T1OsHyecXh4tWz2bRorPp6zuLQmHgGMb+7vkYhnNuTqjXYyqV/5Akid6QKIrTuujMbGrnemhsqDE+NPP1RNNyE0RRH8XiMorFZWE8ZxlR1BfWG0+dUp3ssMepVhs7+31NO/wDYX0HwtHPgdAtOByWSZ5rtTGiaBH5fJlC4dBRilQMp283juqqmMXkcj3hkRwNST1Uq28yNraZycl/TcXf07OcXK43tPfQeFmhsITzztva0u/dxzCcc5mTy+VZsOCkIy4jKZwC3UM+33/MfrZUAApEUWlq3sI5MrxSrb7F6OhmxsZeYHx8K/V6ddrJDVJEPl+elbZ4wnDOuTmsUFjCwMDlDAxc3ummtPUWrc4557pIWxOGpCskvSppu6RbD/N+j6QHw/vPSjql6b3bwvxXJX2yne10zjn3ztqWMJSce3YX8ClgFfBFSatmLPYVYJ+ZnQbcCXw/fHYVcC2wGrgC+LHm87lszjk3B7TzCON8YLuZ7bDkZO0HgKtmLHMVsC68fhi4TMkpEVcBD5jZpJm9BmwP63POOdch7UwYJwD/bpreGeYddhlLbngwDCxJ+VnnnHOzKPOD3pJukjQoaXDv3r2dbo5zznWtdiaMXcCJTdPLw7zDLqOk1kA/8FbKzwJgZveY2blmdu7SpUuPUdOdc87N1M6E8TywUtIKSUWSQewNM5bZANwYXl8DPGbJpecbgGvDWVQrgJXAc21sq3POuXfQtgv3zCyW9FXgTyQFW+4zs5ck3Q4MmtkG4F7gF5K2A/8jSSqE5R4CtgExsNaS6+CPaNOmTW9KeqPFJr8HeLPFz851Hlt2dXN8HtvccHLaBbuqltTRkDSYtp5K1nhs2dXN8Xls2ZP5QW/nnHOzwxOGc865VDxhHHJPpxvQRh5bdnVzfB5bxvgYhnPOuVT8CMM551wq8z5hvFNF3ayRdJ+kIUkvNs0bkPSopH+E5+M62cZWSTpR0uOStkl6SdLNYX7m45O0QNJzkv4WYvtemL8iVHLeHio7Fzvd1lZJiiRtlvSHMN1Nsb0uaaukLZIGw7zMb5czzeuEkbKibtb8jKTCb7NbgY1mthLYGKazKAa+ZmargAuAteHv1Q3xTQKXmtmHgDXAFZIuIKngfGeo6LyPpMJzVt0MvNw03U2xAXzMzNY0nU7bDdvlNPM6YZCuom6mmNmTJBdBNmuuCrwOuHpWG3WMmNluM3shvB4l2fmcQBfEZ4mxMFkIDwMuJankDBmNDUDScuAzwE/DtOiS2I4g89vlTPM9YcyXqrjLzGx3eP1fYFknG3MshJttnQU8S5fEF7pstgBDwKPAP4H9oZIzZHv7/CHwDaAeppfQPbFBktz/LGmTpJvCvK7YLpv5Pb3nGTMzSZk+NU5SH/Bb4BYzG0m+rCayHF8of7NGUhlYD5ze4SYdE5KuBIbMbJOkSzrdnja5yMx2SXov8KikV5rfzPJ22Wy+H2GkroqbcXskvQ8gPA91uD0tk1QgSRb3m9kjYXbXxAdgZvuBx4EPA+VQyRmyu31eCHxW0usk3b6XAj+iO2IDwMx2hechkmR/Pl22XYInjDQVdbtBc1XgG4Hfd7AtLQv93vcCL5vZD5reynx8kpaGIwskLQQ+TjJG8zhJJWfIaGxmdpuZLTezU0j+xx4zs+vogtgAJJUkLWq8Bj4BvEgXbJczzfsL9yR9mqR/tVFR944ON+moSPo1cAlJtcw9wHeB3wEPAScBbwCfN7OZA+NznqSLgKeArRzqC/8WyThGpuOTdCbJwGhE8kXuITO7XdKpJN/KB4DNwPVmNtm5lh6d0CX1dTO7sltiC3GsD5N54FdmdoekJWR8u5xp3icM55xz6cz3LinnnHMpecJwzjmXiicM55xzqXjCcM45l4onDOecc6l4wnBuDpB0SaOKq3NzlScM55xzqXjCcO5dkHR9uG/FFkl3h4KBY5LuDPex2ChpaVh2jaS/Svq7pPWN+yFIOk3SX8K9L16Q9P6w+j5JD0t6RdL9ai6S5dwc4AnDuZQknQF8AbjQzNYANeA6oAQMmtlq4AmSq+sBfg5808zOJLk6vTH/fuCucO+LjwCNiqZnAbeQ3JvlVJIaTM7NGV6t1rn0LgPOAZ4PX/4XkhSUqwMPhmV+CTwiqR8om9kTYf464Deh5tAJZrYewMwOAoT1PWdmO8P0FuAU4On2h+VcOp4wnEtPwDozu23aTOk7M5Zrtd5Ocx2lGv7/6eYY75JyLr2NwDXhngeNezafTPJ/1Ki6+iXgaTMbBvZJujjMvwF4ItwpcKekq8M6eiT1zmoUzrXIv8E4l5KZbZP0bZI7q+WAKrAWGAfOD+8NkYxzQFLS+ichIewAvhzm3wDcLen2sI7PzWIYzrXMq9U6d5QkjZlZX6fb4Vy7eZeUc865VPwIwznnXCp+hOGccy4VTxjOOedS8YThnHMuFU8YzjnnUvGE4ZxzLhVPGM4551L5P2q2o2isgDNQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 966us/sample - loss: 1.0441 - acc: 0.6793\n",
      "Loss: 1.044071129400782 Accuracy: 0.6793354\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8848 - acc: 0.3873\n",
      "Epoch 00001: val_loss improved from inf to 1.31579, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/001-1.3158.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.8848 - acc: 0.3873 - val_loss: 1.3158 - val_acc: 0.5868\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2374 - acc: 0.6082\n",
      "Epoch 00002: val_loss improved from 1.31579 to 1.04575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/002-1.0458.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2373 - acc: 0.6083 - val_loss: 1.0458 - val_acc: 0.6858\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0169 - acc: 0.6835\n",
      "Epoch 00003: val_loss improved from 1.04575 to 0.91132, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/003-0.9113.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0169 - acc: 0.6834 - val_loss: 0.9113 - val_acc: 0.7147\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7323\n",
      "Epoch 00004: val_loss improved from 0.91132 to 0.85883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/004-0.8588.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8743 - acc: 0.7323 - val_loss: 0.8588 - val_acc: 0.7524\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.7654\n",
      "Epoch 00005: val_loss improved from 0.85883 to 0.79960, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/005-0.7996.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7691 - acc: 0.7654 - val_loss: 0.7996 - val_acc: 0.7638\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6877 - acc: 0.7918\n",
      "Epoch 00006: val_loss did not improve from 0.79960\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6877 - acc: 0.7918 - val_loss: 0.9252 - val_acc: 0.7282\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6138 - acc: 0.8135\n",
      "Epoch 00007: val_loss improved from 0.79960 to 0.71761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/007-0.7176.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6138 - acc: 0.8135 - val_loss: 0.7176 - val_acc: 0.7827\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.8346\n",
      "Epoch 00008: val_loss improved from 0.71761 to 0.69343, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/008-0.6934.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5486 - acc: 0.8346 - val_loss: 0.6934 - val_acc: 0.7969\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8493\n",
      "Epoch 00009: val_loss did not improve from 0.69343\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4920 - acc: 0.8493 - val_loss: 0.7059 - val_acc: 0.7885\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8665\n",
      "Epoch 00010: val_loss did not improve from 0.69343\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4381 - acc: 0.8666 - val_loss: 0.7292 - val_acc: 0.7915\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8816\n",
      "Epoch 00011: val_loss did not improve from 0.69343\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3884 - acc: 0.8816 - val_loss: 0.6938 - val_acc: 0.8032\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8914\n",
      "Epoch 00012: val_loss improved from 0.69343 to 0.69217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv_checkpoint/012-0.6922.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3538 - acc: 0.8914 - val_loss: 0.6922 - val_acc: 0.7962\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9027\n",
      "Epoch 00013: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3091 - acc: 0.9026 - val_loss: 0.7998 - val_acc: 0.7871\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9098\n",
      "Epoch 00014: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2826 - acc: 0.9098 - val_loss: 0.7785 - val_acc: 0.7862\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9187\n",
      "Epoch 00015: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2572 - acc: 0.9187 - val_loss: 0.7320 - val_acc: 0.8071\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9237\n",
      "Epoch 00016: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2370 - acc: 0.9237 - val_loss: 0.7674 - val_acc: 0.8018\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9290\n",
      "Epoch 00017: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2209 - acc: 0.9291 - val_loss: 0.7986 - val_acc: 0.7983\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9353\n",
      "Epoch 00018: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2014 - acc: 0.9353 - val_loss: 0.7636 - val_acc: 0.7987\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9410\n",
      "Epoch 00019: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1843 - acc: 0.9410 - val_loss: 0.8610 - val_acc: 0.8060\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9444\n",
      "Epoch 00020: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1754 - acc: 0.9444 - val_loss: 0.7794 - val_acc: 0.8032\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9454\n",
      "Epoch 00021: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1655 - acc: 0.9454 - val_loss: 0.8244 - val_acc: 0.8043\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9504\n",
      "Epoch 00022: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1535 - acc: 0.9504 - val_loss: 0.8345 - val_acc: 0.8057\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9540\n",
      "Epoch 00023: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1430 - acc: 0.9540 - val_loss: 0.8482 - val_acc: 0.8109\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9552\n",
      "Epoch 00024: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1391 - acc: 0.9553 - val_loss: 0.8325 - val_acc: 0.8174\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9580\n",
      "Epoch 00025: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1295 - acc: 0.9580 - val_loss: 0.9310 - val_acc: 0.8088\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9585\n",
      "Epoch 00026: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1270 - acc: 0.9585 - val_loss: 0.8673 - val_acc: 0.8176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9573\n",
      "Epoch 00027: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1342 - acc: 0.9573 - val_loss: 0.8554 - val_acc: 0.8123\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9638\n",
      "Epoch 00028: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1125 - acc: 0.9638 - val_loss: 0.9246 - val_acc: 0.8039\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9621\n",
      "Epoch 00029: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1163 - acc: 0.9621 - val_loss: 0.8733 - val_acc: 0.8143\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9627\n",
      "Epoch 00030: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1169 - acc: 0.9626 - val_loss: 0.8988 - val_acc: 0.8132\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9650\n",
      "Epoch 00031: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1131 - acc: 0.9650 - val_loss: 0.8154 - val_acc: 0.8225\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9661\n",
      "Epoch 00032: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1064 - acc: 0.9661 - val_loss: 0.8580 - val_acc: 0.8258\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9675\n",
      "Epoch 00033: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1041 - acc: 0.9675 - val_loss: 0.8621 - val_acc: 0.8230\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9678\n",
      "Epoch 00034: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1036 - acc: 0.9678 - val_loss: 0.8951 - val_acc: 0.8206\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9671\n",
      "Epoch 00035: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1023 - acc: 0.9671 - val_loss: 0.8666 - val_acc: 0.8199\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9712\n",
      "Epoch 00036: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0938 - acc: 0.9712 - val_loss: 0.8945 - val_acc: 0.8262\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9714\n",
      "Epoch 00037: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0927 - acc: 0.9714 - val_loss: 0.8963 - val_acc: 0.8274\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9730\n",
      "Epoch 00038: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0887 - acc: 0.9730 - val_loss: 0.9240 - val_acc: 0.8227\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9711\n",
      "Epoch 00039: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0959 - acc: 0.9711 - val_loss: 0.8631 - val_acc: 0.8290\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9726\n",
      "Epoch 00040: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0911 - acc: 0.9726 - val_loss: 0.8877 - val_acc: 0.8244\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9755\n",
      "Epoch 00041: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0825 - acc: 0.9755 - val_loss: 0.9202 - val_acc: 0.8251\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9743\n",
      "Epoch 00042: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0831 - acc: 0.9743 - val_loss: 0.8615 - val_acc: 0.8258\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9738\n",
      "Epoch 00043: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0836 - acc: 0.9738 - val_loss: 0.9733 - val_acc: 0.8209\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9740\n",
      "Epoch 00044: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0848 - acc: 0.9740 - val_loss: 0.8820 - val_acc: 0.8279\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9760\n",
      "Epoch 00045: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0784 - acc: 0.9760 - val_loss: 0.9541 - val_acc: 0.8188\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9752\n",
      "Epoch 00046: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0786 - acc: 0.9752 - val_loss: 0.9102 - val_acc: 0.8260\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9773\n",
      "Epoch 00047: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0767 - acc: 0.9773 - val_loss: 0.9372 - val_acc: 0.8307\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9756\n",
      "Epoch 00048: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0798 - acc: 0.9756 - val_loss: 0.9053 - val_acc: 0.8258\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9759\n",
      "Epoch 00049: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0777 - acc: 0.9759 - val_loss: 0.8863 - val_acc: 0.8332\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9779\n",
      "Epoch 00050: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0723 - acc: 0.9779 - val_loss: 0.8841 - val_acc: 0.8334\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9783\n",
      "Epoch 00051: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0727 - acc: 0.9783 - val_loss: 0.9606 - val_acc: 0.8216\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0730 - acc: 0.9780 - val_loss: 0.9390 - val_acc: 0.8300\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9780\n",
      "Epoch 00053: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0729 - acc: 0.9780 - val_loss: 0.8649 - val_acc: 0.8344\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9793\n",
      "Epoch 00054: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0675 - acc: 0.9793 - val_loss: 0.9381 - val_acc: 0.8300\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9798\n",
      "Epoch 00055: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0688 - acc: 0.9798 - val_loss: 0.9205 - val_acc: 0.8334\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9818\n",
      "Epoch 00056: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0653 - acc: 0.9818 - val_loss: 0.9086 - val_acc: 0.8376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9810\n",
      "Epoch 00057: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0646 - acc: 0.9810 - val_loss: 0.8637 - val_acc: 0.8416\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9821\n",
      "Epoch 00058: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0616 - acc: 0.9821 - val_loss: 0.9150 - val_acc: 0.8325\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9800\n",
      "Epoch 00059: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0669 - acc: 0.9800 - val_loss: 0.9149 - val_acc: 0.8360\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9810\n",
      "Epoch 00060: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0649 - acc: 0.9810 - val_loss: 0.9460 - val_acc: 0.8188\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9819\n",
      "Epoch 00061: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0599 - acc: 0.9819 - val_loss: 0.9302 - val_acc: 0.8321\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9818\n",
      "Epoch 00062: val_loss did not improve from 0.69217\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0618 - acc: 0.9818 - val_loss: 0.9712 - val_acc: 0.8358\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7888 - acc: 0.7759\n",
      "Loss: 0.7887800250469339 Accuracy: 0.7759086\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0580 - acc: 0.3242\n",
      "Epoch 00001: val_loss improved from inf to 1.51519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/001-1.5152.hdf5\n",
      "36805/36805 [==============================] - 96s 3ms/sample - loss: 2.0581 - acc: 0.3241 - val_loss: 1.5152 - val_acc: 0.5071\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3639 - acc: 0.5579\n",
      "Epoch 00002: val_loss improved from 1.51519 to 1.08926, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/002-1.0893.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3638 - acc: 0.5579 - val_loss: 1.0893 - val_acc: 0.6699\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1082 - acc: 0.6545\n",
      "Epoch 00003: val_loss improved from 1.08926 to 0.88476, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/003-0.8848.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1082 - acc: 0.6545 - val_loss: 0.8848 - val_acc: 0.7347\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9229 - acc: 0.7167\n",
      "Epoch 00004: val_loss improved from 0.88476 to 0.75883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/004-0.7588.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9228 - acc: 0.7168 - val_loss: 0.7588 - val_acc: 0.7820\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7965 - acc: 0.7575\n",
      "Epoch 00005: val_loss improved from 0.75883 to 0.68568, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/005-0.6857.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7964 - acc: 0.7575 - val_loss: 0.6857 - val_acc: 0.7950\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.7948\n",
      "Epoch 00006: val_loss improved from 0.68568 to 0.62400, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/006-0.6240.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6845 - acc: 0.7948 - val_loss: 0.6240 - val_acc: 0.8204\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.8181\n",
      "Epoch 00007: val_loss improved from 0.62400 to 0.61696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/007-0.6170.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6107 - acc: 0.8181 - val_loss: 0.6170 - val_acc: 0.8281\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.8355\n",
      "Epoch 00008: val_loss improved from 0.61696 to 0.52914, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/008-0.5291.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5548 - acc: 0.8355 - val_loss: 0.5291 - val_acc: 0.8521\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8497\n",
      "Epoch 00009: val_loss improved from 0.52914 to 0.46098, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/009-0.4610.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5036 - acc: 0.8497 - val_loss: 0.4610 - val_acc: 0.8724\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.8598\n",
      "Epoch 00010: val_loss did not improve from 0.46098\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4641 - acc: 0.8598 - val_loss: 0.4660 - val_acc: 0.8661\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8760\n",
      "Epoch 00011: val_loss improved from 0.46098 to 0.41821, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/011-0.4182.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4157 - acc: 0.8760 - val_loss: 0.4182 - val_acc: 0.8835\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8841\n",
      "Epoch 00012: val_loss improved from 0.41821 to 0.39411, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/012-0.3941.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3874 - acc: 0.8841 - val_loss: 0.3941 - val_acc: 0.8873\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8914\n",
      "Epoch 00013: val_loss improved from 0.39411 to 0.38777, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/013-0.3878.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3625 - acc: 0.8914 - val_loss: 0.3878 - val_acc: 0.8919\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.8983\n",
      "Epoch 00014: val_loss improved from 0.38777 to 0.37886, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/014-0.3789.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3360 - acc: 0.8982 - val_loss: 0.3789 - val_acc: 0.8910\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9041\n",
      "Epoch 00015: val_loss did not improve from 0.37886\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3141 - acc: 0.9041 - val_loss: 0.3862 - val_acc: 0.8975\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9108\n",
      "Epoch 00016: val_loss improved from 0.37886 to 0.35762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/016-0.3576.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2943 - acc: 0.9108 - val_loss: 0.3576 - val_acc: 0.9050\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9157\n",
      "Epoch 00017: val_loss did not improve from 0.35762\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2745 - acc: 0.9157 - val_loss: 0.3771 - val_acc: 0.8926\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9214\n",
      "Epoch 00018: val_loss improved from 0.35762 to 0.35621, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/018-0.3562.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2590 - acc: 0.9213 - val_loss: 0.3562 - val_acc: 0.9038\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9243\n",
      "Epoch 00019: val_loss did not improve from 0.35621\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2448 - acc: 0.9243 - val_loss: 0.3624 - val_acc: 0.8984\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9301\n",
      "Epoch 00020: val_loss did not improve from 0.35621\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2255 - acc: 0.9301 - val_loss: 0.3662 - val_acc: 0.9001\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9338\n",
      "Epoch 00021: val_loss did not improve from 0.35621\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2123 - acc: 0.9338 - val_loss: 0.3854 - val_acc: 0.9029\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9358\n",
      "Epoch 00022: val_loss improved from 0.35621 to 0.35179, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/022-0.3518.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2031 - acc: 0.9358 - val_loss: 0.3518 - val_acc: 0.9119\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9385\n",
      "Epoch 00023: val_loss did not improve from 0.35179\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1919 - acc: 0.9385 - val_loss: 0.3613 - val_acc: 0.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9425\n",
      "Epoch 00024: val_loss improved from 0.35179 to 0.34246, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv_checkpoint/024-0.3425.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1797 - acc: 0.9425 - val_loss: 0.3425 - val_acc: 0.9124\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9458\n",
      "Epoch 00025: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1696 - acc: 0.9458 - val_loss: 0.3575 - val_acc: 0.9115\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9467\n",
      "Epoch 00026: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1637 - acc: 0.9467 - val_loss: 0.3720 - val_acc: 0.9161\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9502\n",
      "Epoch 00027: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1554 - acc: 0.9502 - val_loss: 0.4239 - val_acc: 0.9064\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9505\n",
      "Epoch 00028: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1503 - acc: 0.9505 - val_loss: 0.4499 - val_acc: 0.8931\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9533\n",
      "Epoch 00029: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1409 - acc: 0.9533 - val_loss: 0.3559 - val_acc: 0.9171\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9540\n",
      "Epoch 00030: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1380 - acc: 0.9540 - val_loss: 0.3584 - val_acc: 0.9087\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9576\n",
      "Epoch 00031: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1317 - acc: 0.9576 - val_loss: 0.3601 - val_acc: 0.9145\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9581\n",
      "Epoch 00032: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1287 - acc: 0.9581 - val_loss: 0.4001 - val_acc: 0.8994\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9542\n",
      "Epoch 00033: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1414 - acc: 0.9542 - val_loss: 0.3813 - val_acc: 0.9136\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9628\n",
      "Epoch 00034: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1130 - acc: 0.9628 - val_loss: 0.3929 - val_acc: 0.9124\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9615\n",
      "Epoch 00035: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1153 - acc: 0.9615 - val_loss: 0.3844 - val_acc: 0.9152\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9652\n",
      "Epoch 00036: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1057 - acc: 0.9652 - val_loss: 0.3721 - val_acc: 0.9168\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9639\n",
      "Epoch 00037: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1084 - acc: 0.9639 - val_loss: 0.3615 - val_acc: 0.9201\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9677\n",
      "Epoch 00038: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0990 - acc: 0.9677 - val_loss: 0.3790 - val_acc: 0.9168\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9682\n",
      "Epoch 00039: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0987 - acc: 0.9682 - val_loss: 0.3503 - val_acc: 0.9182\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9694\n",
      "Epoch 00040: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0919 - acc: 0.9694 - val_loss: 0.3732 - val_acc: 0.9236\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9677\n",
      "Epoch 00041: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0969 - acc: 0.9677 - val_loss: 0.3904 - val_acc: 0.9157\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9710\n",
      "Epoch 00042: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0897 - acc: 0.9710 - val_loss: 0.4066 - val_acc: 0.9133\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9707\n",
      "Epoch 00043: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0899 - acc: 0.9707 - val_loss: 0.3715 - val_acc: 0.9133\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9740\n",
      "Epoch 00044: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0796 - acc: 0.9740 - val_loss: 0.4114 - val_acc: 0.9215\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9725\n",
      "Epoch 00045: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0846 - acc: 0.9725 - val_loss: 0.4310 - val_acc: 0.9115\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9735\n",
      "Epoch 00046: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0834 - acc: 0.9735 - val_loss: 0.3744 - val_acc: 0.9206\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9752\n",
      "Epoch 00047: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0786 - acc: 0.9752 - val_loss: 0.4194 - val_acc: 0.9182\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9752\n",
      "Epoch 00048: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0747 - acc: 0.9752 - val_loss: 0.3793 - val_acc: 0.9243\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9749\n",
      "Epoch 00049: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0785 - acc: 0.9749 - val_loss: 0.3965 - val_acc: 0.9208\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9756\n",
      "Epoch 00050: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0752 - acc: 0.9756 - val_loss: 0.4341 - val_acc: 0.9152\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9775\n",
      "Epoch 00051: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0709 - acc: 0.9775 - val_loss: 0.4056 - val_acc: 0.9159\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9769\n",
      "Epoch 00052: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0699 - acc: 0.9769 - val_loss: 0.4452 - val_acc: 0.9154\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9766\n",
      "Epoch 00053: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0713 - acc: 0.9766 - val_loss: 0.4008 - val_acc: 0.9269\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9768\n",
      "Epoch 00054: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0733 - acc: 0.9768 - val_loss: 0.3953 - val_acc: 0.9206\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9793\n",
      "Epoch 00055: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0641 - acc: 0.9792 - val_loss: 0.4081 - val_acc: 0.9285\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9793\n",
      "Epoch 00056: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0680 - acc: 0.9793 - val_loss: 0.4206 - val_acc: 0.9220\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9792\n",
      "Epoch 00057: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0622 - acc: 0.9792 - val_loss: 0.4161 - val_acc: 0.9224\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9793\n",
      "Epoch 00058: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0635 - acc: 0.9793 - val_loss: 0.4128 - val_acc: 0.9236\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9803\n",
      "Epoch 00059: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0632 - acc: 0.9803 - val_loss: 0.4024 - val_acc: 0.9173\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9811\n",
      "Epoch 00060: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0589 - acc: 0.9811 - val_loss: 0.3843 - val_acc: 0.9222\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9796\n",
      "Epoch 00061: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0613 - acc: 0.9796 - val_loss: 0.4020 - val_acc: 0.9217\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9784\n",
      "Epoch 00062: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0669 - acc: 0.9784 - val_loss: 0.4000 - val_acc: 0.9269\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9826\n",
      "Epoch 00063: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0541 - acc: 0.9826 - val_loss: 0.4268 - val_acc: 0.9182\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9804\n",
      "Epoch 00064: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0627 - acc: 0.9804 - val_loss: 0.3938 - val_acc: 0.9220\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9833\n",
      "Epoch 00065: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0526 - acc: 0.9833 - val_loss: 0.4593 - val_acc: 0.9173\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9835\n",
      "Epoch 00066: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0523 - acc: 0.9835 - val_loss: 0.4450 - val_acc: 0.9129\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9816\n",
      "Epoch 00067: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0580 - acc: 0.9816 - val_loss: 0.4596 - val_acc: 0.9229\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9828\n",
      "Epoch 00068: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0519 - acc: 0.9828 - val_loss: 0.4408 - val_acc: 0.9257\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9825\n",
      "Epoch 00069: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0557 - acc: 0.9825 - val_loss: 0.4732 - val_acc: 0.9138\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9815\n",
      "Epoch 00070: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0589 - acc: 0.9816 - val_loss: 0.4107 - val_acc: 0.9276\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9834\n",
      "Epoch 00071: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0530 - acc: 0.9834 - val_loss: 0.3889 - val_acc: 0.9208\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9843\n",
      "Epoch 00072: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0477 - acc: 0.9843 - val_loss: 0.4374 - val_acc: 0.9236\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9860\n",
      "Epoch 00073: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0453 - acc: 0.9860 - val_loss: 0.4334 - val_acc: 0.9283\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9845\n",
      "Epoch 00074: val_loss did not improve from 0.34246\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0490 - acc: 0.9845 - val_loss: 0.4355 - val_acc: 0.9208\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 997us/sample - loss: 0.4060 - acc: 0.8831\n",
      "Loss: 0.40602451181609805 Accuracy: 0.88307375\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1476 - acc: 0.2886\n",
      "Epoch 00001: val_loss improved from inf to 1.41290, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/001-1.4129.hdf5\n",
      "36805/36805 [==============================] - 99s 3ms/sample - loss: 2.1475 - acc: 0.2886 - val_loss: 1.4129 - val_acc: 0.5288\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3859 - acc: 0.5471\n",
      "Epoch 00002: val_loss improved from 1.41290 to 1.01466, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/002-1.0147.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.3858 - acc: 0.5471 - val_loss: 1.0147 - val_acc: 0.6904\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0239 - acc: 0.6790\n",
      "Epoch 00003: val_loss improved from 1.01466 to 0.73858, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/003-0.7386.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.0241 - acc: 0.6790 - val_loss: 0.7386 - val_acc: 0.7943\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8024 - acc: 0.7551\n",
      "Epoch 00004: val_loss improved from 0.73858 to 0.53813, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/004-0.5381.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8023 - acc: 0.7551 - val_loss: 0.5381 - val_acc: 0.8458\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6591 - acc: 0.7981\n",
      "Epoch 00005: val_loss improved from 0.53813 to 0.45008, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/005-0.4501.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6591 - acc: 0.7981 - val_loss: 0.4501 - val_acc: 0.8772\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.8302\n",
      "Epoch 00006: val_loss improved from 0.45008 to 0.38304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/006-0.3830.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5633 - acc: 0.8302 - val_loss: 0.3830 - val_acc: 0.8901\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8510\n",
      "Epoch 00007: val_loss did not improve from 0.38304\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4921 - acc: 0.8510 - val_loss: 0.4073 - val_acc: 0.8921\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8651\n",
      "Epoch 00008: val_loss improved from 0.38304 to 0.33704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/008-0.3370.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4409 - acc: 0.8652 - val_loss: 0.3370 - val_acc: 0.9054\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8782\n",
      "Epoch 00009: val_loss improved from 0.33704 to 0.32893, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/009-0.3289.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3988 - acc: 0.8782 - val_loss: 0.3289 - val_acc: 0.9064\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8886\n",
      "Epoch 00010: val_loss improved from 0.32893 to 0.28342, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/010-0.2834.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3601 - acc: 0.8886 - val_loss: 0.2834 - val_acc: 0.9220\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8986\n",
      "Epoch 00011: val_loss improved from 0.28342 to 0.26623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/011-0.2662.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3324 - acc: 0.8986 - val_loss: 0.2662 - val_acc: 0.9311\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9053\n",
      "Epoch 00012: val_loss did not improve from 0.26623\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3066 - acc: 0.9053 - val_loss: 0.2931 - val_acc: 0.9199\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9118\n",
      "Epoch 00013: val_loss did not improve from 0.26623\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2865 - acc: 0.9118 - val_loss: 0.2769 - val_acc: 0.9189\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9132\n",
      "Epoch 00014: val_loss improved from 0.26623 to 0.23966, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/014-0.2397.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2759 - acc: 0.9132 - val_loss: 0.2397 - val_acc: 0.9336\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9210\n",
      "Epoch 00015: val_loss improved from 0.23966 to 0.22103, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/015-0.2210.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2548 - acc: 0.9210 - val_loss: 0.2210 - val_acc: 0.9390\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9259\n",
      "Epoch 00016: val_loss improved from 0.22103 to 0.20650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/016-0.2065.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2363 - acc: 0.9259 - val_loss: 0.2065 - val_acc: 0.9415\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9277\n",
      "Epoch 00017: val_loss did not improve from 0.20650\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2261 - acc: 0.9277 - val_loss: 0.2627 - val_acc: 0.9287\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9308\n",
      "Epoch 00018: val_loss improved from 0.20650 to 0.20181, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/018-0.2018.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2169 - acc: 0.9308 - val_loss: 0.2018 - val_acc: 0.9441\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9338\n",
      "Epoch 00019: val_loss did not improve from 0.20181\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2111 - acc: 0.9338 - val_loss: 0.2213 - val_acc: 0.9343\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9387\n",
      "Epoch 00020: val_loss improved from 0.20181 to 0.18861, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/020-0.1886.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1950 - acc: 0.9387 - val_loss: 0.1886 - val_acc: 0.9460\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9404\n",
      "Epoch 00021: val_loss did not improve from 0.18861\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1880 - acc: 0.9404 - val_loss: 0.1932 - val_acc: 0.9457\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9430\n",
      "Epoch 00022: val_loss did not improve from 0.18861\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1775 - acc: 0.9430 - val_loss: 0.2125 - val_acc: 0.9436\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9445\n",
      "Epoch 00023: val_loss did not improve from 0.18861\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1724 - acc: 0.9445 - val_loss: 0.2414 - val_acc: 0.9343\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9450\n",
      "Epoch 00024: val_loss did not improve from 0.18861\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1672 - acc: 0.9450 - val_loss: 0.2031 - val_acc: 0.9460\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9484\n",
      "Epoch 00025: val_loss did not improve from 0.18861\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1597 - acc: 0.9484 - val_loss: 0.1944 - val_acc: 0.9457\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9506\n",
      "Epoch 00026: val_loss improved from 0.18861 to 0.18646, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/026-0.1865.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1517 - acc: 0.9506 - val_loss: 0.1865 - val_acc: 0.9471\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9527\n",
      "Epoch 00027: val_loss did not improve from 0.18646\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1465 - acc: 0.9527 - val_loss: 0.2054 - val_acc: 0.9497\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9547\n",
      "Epoch 00028: val_loss improved from 0.18646 to 0.17839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/028-0.1784.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1375 - acc: 0.9547 - val_loss: 0.1784 - val_acc: 0.9481\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9561\n",
      "Epoch 00029: val_loss did not improve from 0.17839\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1342 - acc: 0.9561 - val_loss: 0.1915 - val_acc: 0.9464\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9578\n",
      "Epoch 00030: val_loss did not improve from 0.17839\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1306 - acc: 0.9578 - val_loss: 0.1868 - val_acc: 0.9522\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9564\n",
      "Epoch 00031: val_loss did not improve from 0.17839\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1290 - acc: 0.9564 - val_loss: 0.1963 - val_acc: 0.9450\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9598\n",
      "Epoch 00032: val_loss improved from 0.17839 to 0.17009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/032-0.1701.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1223 - acc: 0.9598 - val_loss: 0.1701 - val_acc: 0.9539\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9615\n",
      "Epoch 00033: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1174 - acc: 0.9615 - val_loss: 0.2009 - val_acc: 0.9441\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9626\n",
      "Epoch 00034: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1125 - acc: 0.9626 - val_loss: 0.1918 - val_acc: 0.9497\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9638\n",
      "Epoch 00035: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1107 - acc: 0.9638 - val_loss: 0.1863 - val_acc: 0.9536\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9656\n",
      "Epoch 00036: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1059 - acc: 0.9656 - val_loss: 0.2301 - val_acc: 0.9483\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9649\n",
      "Epoch 00037: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1055 - acc: 0.9650 - val_loss: 0.2145 - val_acc: 0.9485\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9665\n",
      "Epoch 00038: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1015 - acc: 0.9665 - val_loss: 0.1866 - val_acc: 0.9562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9679\n",
      "Epoch 00039: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0961 - acc: 0.9679 - val_loss: 0.1836 - val_acc: 0.9546\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9691\n",
      "Epoch 00040: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0960 - acc: 0.9691 - val_loss: 0.1897 - val_acc: 0.9534\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9701\n",
      "Epoch 00041: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0889 - acc: 0.9701 - val_loss: 0.1788 - val_acc: 0.9546\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9714\n",
      "Epoch 00042: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0857 - acc: 0.9714 - val_loss: 0.2038 - val_acc: 0.9490\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9716\n",
      "Epoch 00043: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0833 - acc: 0.9716 - val_loss: 0.2153 - val_acc: 0.9464\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9705\n",
      "Epoch 00044: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0861 - acc: 0.9705 - val_loss: 0.1819 - val_acc: 0.9557\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9730\n",
      "Epoch 00045: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0808 - acc: 0.9730 - val_loss: 0.2348 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9734\n",
      "Epoch 00046: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0792 - acc: 0.9734 - val_loss: 0.1897 - val_acc: 0.9574\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9746\n",
      "Epoch 00047: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0775 - acc: 0.9745 - val_loss: 0.1910 - val_acc: 0.9534\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9745\n",
      "Epoch 00048: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0787 - acc: 0.9745 - val_loss: 0.1833 - val_acc: 0.9560\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9752\n",
      "Epoch 00049: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0725 - acc: 0.9752 - val_loss: 0.2045 - val_acc: 0.9546\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9758\n",
      "Epoch 00050: val_loss did not improve from 0.17009\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0711 - acc: 0.9758 - val_loss: 0.2053 - val_acc: 0.9518\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9775\n",
      "Epoch 00051: val_loss improved from 0.17009 to 0.16892, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv_checkpoint/051-0.1689.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0672 - acc: 0.9775 - val_loss: 0.1689 - val_acc: 0.9595\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0658 - acc: 0.9780 - val_loss: 0.2210 - val_acc: 0.9527\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9771\n",
      "Epoch 00053: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0662 - acc: 0.9771 - val_loss: 0.1891 - val_acc: 0.9581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9790\n",
      "Epoch 00054: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0618 - acc: 0.9791 - val_loss: 0.2053 - val_acc: 0.9550\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9777\n",
      "Epoch 00055: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0654 - acc: 0.9777 - val_loss: 0.1793 - val_acc: 0.9557\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9781\n",
      "Epoch 00056: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0650 - acc: 0.9781 - val_loss: 0.1953 - val_acc: 0.9527\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9804\n",
      "Epoch 00057: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0588 - acc: 0.9804 - val_loss: 0.1950 - val_acc: 0.9509\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9803\n",
      "Epoch 00058: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0572 - acc: 0.9803 - val_loss: 0.2167 - val_acc: 0.9557\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9820\n",
      "Epoch 00059: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0557 - acc: 0.9820 - val_loss: 0.1943 - val_acc: 0.9520\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9797\n",
      "Epoch 00060: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0598 - acc: 0.9797 - val_loss: 0.2180 - val_acc: 0.9546\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9811\n",
      "Epoch 00061: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0543 - acc: 0.9811 - val_loss: 0.2143 - val_acc: 0.9534\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9817\n",
      "Epoch 00062: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0551 - acc: 0.9817 - val_loss: 0.2281 - val_acc: 0.9488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9832\n",
      "Epoch 00063: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0490 - acc: 0.9832 - val_loss: 0.1993 - val_acc: 0.9560\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9830\n",
      "Epoch 00064: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0511 - acc: 0.9830 - val_loss: 0.2001 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9830\n",
      "Epoch 00065: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0501 - acc: 0.9830 - val_loss: 0.2140 - val_acc: 0.9543\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9830\n",
      "Epoch 00066: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0486 - acc: 0.9830 - val_loss: 0.2224 - val_acc: 0.9553\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9838\n",
      "Epoch 00067: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0492 - acc: 0.9838 - val_loss: 0.2241 - val_acc: 0.9569\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9845\n",
      "Epoch 00068: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0461 - acc: 0.9845 - val_loss: 0.1959 - val_acc: 0.9562\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9841\n",
      "Epoch 00069: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0485 - acc: 0.9841 - val_loss: 0.2250 - val_acc: 0.9548\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9848\n",
      "Epoch 00070: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0445 - acc: 0.9848 - val_loss: 0.2189 - val_acc: 0.9564\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9861\n",
      "Epoch 00071: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0430 - acc: 0.9861 - val_loss: 0.2602 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9842\n",
      "Epoch 00072: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0467 - acc: 0.9842 - val_loss: 0.2191 - val_acc: 0.9511\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9839\n",
      "Epoch 00073: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0476 - acc: 0.9839 - val_loss: 0.2185 - val_acc: 0.9548\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9858\n",
      "Epoch 00074: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0443 - acc: 0.9858 - val_loss: 0.2152 - val_acc: 0.9543\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9855\n",
      "Epoch 00075: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0428 - acc: 0.9855 - val_loss: 0.2129 - val_acc: 0.9541\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9862\n",
      "Epoch 00076: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0422 - acc: 0.9862 - val_loss: 0.2455 - val_acc: 0.9534\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9853\n",
      "Epoch 00077: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0428 - acc: 0.9853 - val_loss: 0.2110 - val_acc: 0.9550\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9858\n",
      "Epoch 00078: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 0.2156 - val_acc: 0.9509\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 00079: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0392 - acc: 0.9870 - val_loss: 0.1851 - val_acc: 0.9548\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9863\n",
      "Epoch 00080: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0405 - acc: 0.9863 - val_loss: 0.1985 - val_acc: 0.9592\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9875\n",
      "Epoch 00081: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0385 - acc: 0.9875 - val_loss: 0.2309 - val_acc: 0.9539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9881\n",
      "Epoch 00082: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0348 - acc: 0.9881 - val_loss: 0.2255 - val_acc: 0.9581\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9863\n",
      "Epoch 00083: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0422 - acc: 0.9863 - val_loss: 0.2046 - val_acc: 0.9583\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9885\n",
      "Epoch 00084: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0337 - acc: 0.9885 - val_loss: 0.2245 - val_acc: 0.9602\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9861\n",
      "Epoch 00085: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0402 - acc: 0.9861 - val_loss: 0.2403 - val_acc: 0.9506\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9883\n",
      "Epoch 00086: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0339 - acc: 0.9883 - val_loss: 0.2418 - val_acc: 0.9550\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9871\n",
      "Epoch 00087: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0381 - acc: 0.9871 - val_loss: 0.2358 - val_acc: 0.9534\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9879\n",
      "Epoch 00088: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0361 - acc: 0.9879 - val_loss: 0.2115 - val_acc: 0.9583\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9880\n",
      "Epoch 00089: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0333 - acc: 0.9880 - val_loss: 0.2485 - val_acc: 0.9590\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9892\n",
      "Epoch 00090: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0305 - acc: 0.9892 - val_loss: 0.2742 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9880\n",
      "Epoch 00091: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0357 - acc: 0.9880 - val_loss: 0.2292 - val_acc: 0.9536\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9885\n",
      "Epoch 00092: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0353 - acc: 0.9885 - val_loss: 0.2273 - val_acc: 0.9583\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9897\n",
      "Epoch 00093: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0309 - acc: 0.9897 - val_loss: 0.2530 - val_acc: 0.9509\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9893\n",
      "Epoch 00094: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0325 - acc: 0.9893 - val_loss: 0.2228 - val_acc: 0.9564\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9895\n",
      "Epoch 00095: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0319 - acc: 0.9895 - val_loss: 0.2607 - val_acc: 0.9536\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9897\n",
      "Epoch 00096: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0320 - acc: 0.9897 - val_loss: 0.2511 - val_acc: 0.9534\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9895\n",
      "Epoch 00097: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0301 - acc: 0.9895 - val_loss: 0.2352 - val_acc: 0.9569\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9901\n",
      "Epoch 00098: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0325 - acc: 0.9901 - val_loss: 0.2456 - val_acc: 0.9564\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9895\n",
      "Epoch 00099: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0313 - acc: 0.9895 - val_loss: 0.2258 - val_acc: 0.9539\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9904\n",
      "Epoch 00100: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0300 - acc: 0.9904 - val_loss: 0.2200 - val_acc: 0.9597\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9904\n",
      "Epoch 00101: val_loss did not improve from 0.16892\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0293 - acc: 0.9904 - val_loss: 0.2641 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2113 - acc: 0.9458\n",
      "Loss: 0.2112701848712155 Accuracy: 0.9457944\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2284 - acc: 0.2723\n",
      "Epoch 00001: val_loss improved from inf to 1.62077, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/001-1.6208.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 2.2284 - acc: 0.2724 - val_loss: 1.6208 - val_acc: 0.4838\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2999 - acc: 0.5728\n",
      "Epoch 00002: val_loss improved from 1.62077 to 0.80439, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/002-0.8044.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.2999 - acc: 0.5728 - val_loss: 0.8044 - val_acc: 0.7452\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9353 - acc: 0.6974\n",
      "Epoch 00003: val_loss improved from 0.80439 to 0.58228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/003-0.5823.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.9353 - acc: 0.6974 - val_loss: 0.5823 - val_acc: 0.8237\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7670\n",
      "Epoch 00004: val_loss improved from 0.58228 to 0.45099, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/004-0.4510.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7218 - acc: 0.7670 - val_loss: 0.4510 - val_acc: 0.8628\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5821 - acc: 0.8154\n",
      "Epoch 00005: val_loss improved from 0.45099 to 0.35398, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/005-0.3540.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.5821 - acc: 0.8154 - val_loss: 0.3540 - val_acc: 0.8924\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8461\n",
      "Epoch 00006: val_loss improved from 0.35398 to 0.28645, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/006-0.2864.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4870 - acc: 0.8461 - val_loss: 0.2864 - val_acc: 0.9196\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8690\n",
      "Epoch 00007: val_loss improved from 0.28645 to 0.28139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/007-0.2814.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4204 - acc: 0.8690 - val_loss: 0.2814 - val_acc: 0.9201\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8821\n",
      "Epoch 00008: val_loss improved from 0.28139 to 0.22570, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/008-0.2257.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3764 - acc: 0.8821 - val_loss: 0.2257 - val_acc: 0.9304\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8939\n",
      "Epoch 00009: val_loss improved from 0.22570 to 0.22044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/009-0.2204.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3372 - acc: 0.8938 - val_loss: 0.2204 - val_acc: 0.9383\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9020\n",
      "Epoch 00010: val_loss improved from 0.22044 to 0.18265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/010-0.1827.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3108 - acc: 0.9020 - val_loss: 0.1827 - val_acc: 0.9485\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9128\n",
      "Epoch 00011: val_loss did not improve from 0.18265\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2823 - acc: 0.9128 - val_loss: 0.1974 - val_acc: 0.9422\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9169\n",
      "Epoch 00012: val_loss improved from 0.18265 to 0.18167, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/012-0.1817.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2623 - acc: 0.9169 - val_loss: 0.1817 - val_acc: 0.9476\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9227\n",
      "Epoch 00013: val_loss did not improve from 0.18167\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2446 - acc: 0.9228 - val_loss: 0.1882 - val_acc: 0.9434\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9275\n",
      "Epoch 00014: val_loss did not improve from 0.18167\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2291 - acc: 0.9275 - val_loss: 0.1873 - val_acc: 0.9474\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9296\n",
      "Epoch 00015: val_loss improved from 0.18167 to 0.15890, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/015-0.1589.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2181 - acc: 0.9296 - val_loss: 0.1589 - val_acc: 0.9555\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9362\n",
      "Epoch 00016: val_loss improved from 0.15890 to 0.15127, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/016-0.1513.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2011 - acc: 0.9362 - val_loss: 0.1513 - val_acc: 0.9564\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9388\n",
      "Epoch 00017: val_loss did not improve from 0.15127\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1928 - acc: 0.9388 - val_loss: 0.1747 - val_acc: 0.9471\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9420\n",
      "Epoch 00018: val_loss improved from 0.15127 to 0.14251, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/018-0.1425.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1826 - acc: 0.9420 - val_loss: 0.1425 - val_acc: 0.9592\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9442\n",
      "Epoch 00019: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1730 - acc: 0.9442 - val_loss: 0.1467 - val_acc: 0.9553\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9464\n",
      "Epoch 00020: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1635 - acc: 0.9463 - val_loss: 0.1702 - val_acc: 0.9532\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9509\n",
      "Epoch 00021: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1555 - acc: 0.9509 - val_loss: 0.1518 - val_acc: 0.9555\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9508\n",
      "Epoch 00022: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1493 - acc: 0.9508 - val_loss: 0.1545 - val_acc: 0.9576\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9546\n",
      "Epoch 00023: val_loss improved from 0.14251 to 0.13708, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/023-0.1371.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1405 - acc: 0.9546 - val_loss: 0.1371 - val_acc: 0.9604\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9548\n",
      "Epoch 00024: val_loss did not improve from 0.13708\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1376 - acc: 0.9548 - val_loss: 0.1407 - val_acc: 0.9548\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9538\n",
      "Epoch 00025: val_loss improved from 0.13708 to 0.12503, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/025-0.1250.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1424 - acc: 0.9538 - val_loss: 0.1250 - val_acc: 0.9655\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9613\n",
      "Epoch 00026: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1189 - acc: 0.9613 - val_loss: 0.1395 - val_acc: 0.9623\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9602\n",
      "Epoch 00027: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1231 - acc: 0.9602 - val_loss: 0.1551 - val_acc: 0.9548\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9623\n",
      "Epoch 00028: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1161 - acc: 0.9623 - val_loss: 0.1327 - val_acc: 0.9616\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9623\n",
      "Epoch 00029: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1149 - acc: 0.9623 - val_loss: 0.1260 - val_acc: 0.9651\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9652\n",
      "Epoch 00030: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1055 - acc: 0.9652 - val_loss: 0.1278 - val_acc: 0.9639\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9667\n",
      "Epoch 00031: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1008 - acc: 0.9667 - val_loss: 0.1302 - val_acc: 0.9651\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9681\n",
      "Epoch 00032: val_loss did not improve from 0.12503\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0988 - acc: 0.9681 - val_loss: 0.1496 - val_acc: 0.9606\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9683\n",
      "Epoch 00033: val_loss improved from 0.12503 to 0.12178, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/033-0.1218.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0977 - acc: 0.9683 - val_loss: 0.1218 - val_acc: 0.9669\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9699\n",
      "Epoch 00034: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0910 - acc: 0.9699 - val_loss: 0.1336 - val_acc: 0.9662\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9694\n",
      "Epoch 00035: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0912 - acc: 0.9694 - val_loss: 0.1316 - val_acc: 0.9625\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9713\n",
      "Epoch 00036: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0869 - acc: 0.9713 - val_loss: 0.1237 - val_acc: 0.9634\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9719\n",
      "Epoch 00037: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0878 - acc: 0.9719 - val_loss: 0.1243 - val_acc: 0.9688\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9727\n",
      "Epoch 00038: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0819 - acc: 0.9727 - val_loss: 0.1426 - val_acc: 0.9620\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9732\n",
      "Epoch 00039: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0792 - acc: 0.9732 - val_loss: 0.1281 - val_acc: 0.9658\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9750\n",
      "Epoch 00040: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0737 - acc: 0.9750 - val_loss: 0.1419 - val_acc: 0.9660\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9754\n",
      "Epoch 00041: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0742 - acc: 0.9754 - val_loss: 0.1311 - val_acc: 0.9653\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9760\n",
      "Epoch 00042: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0707 - acc: 0.9760 - val_loss: 0.1477 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9771\n",
      "Epoch 00043: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0675 - acc: 0.9771 - val_loss: 0.1237 - val_acc: 0.9686\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9759\n",
      "Epoch 00044: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0697 - acc: 0.9759 - val_loss: 0.1545 - val_acc: 0.9639\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9779\n",
      "Epoch 00045: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0663 - acc: 0.9779 - val_loss: 0.1494 - val_acc: 0.9595\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9793\n",
      "Epoch 00046: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0609 - acc: 0.9793 - val_loss: 0.1325 - val_acc: 0.9683\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9791\n",
      "Epoch 00047: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0610 - acc: 0.9791 - val_loss: 0.1330 - val_acc: 0.9700\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9792\n",
      "Epoch 00048: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0626 - acc: 0.9792 - val_loss: 0.1418 - val_acc: 0.9658\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9817\n",
      "Epoch 00049: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0542 - acc: 0.9817 - val_loss: 0.1408 - val_acc: 0.9646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9815\n",
      "Epoch 00050: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0558 - acc: 0.9815 - val_loss: 0.1427 - val_acc: 0.9606\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9817\n",
      "Epoch 00051: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0566 - acc: 0.9817 - val_loss: 0.1335 - val_acc: 0.9683\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9806\n",
      "Epoch 00052: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0572 - acc: 0.9806 - val_loss: 0.1425 - val_acc: 0.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9826\n",
      "Epoch 00053: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0518 - acc: 0.9826 - val_loss: 0.1372 - val_acc: 0.9641\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9829\n",
      "Epoch 00054: val_loss did not improve from 0.12178\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0514 - acc: 0.9829 - val_loss: 0.1330 - val_acc: 0.9634\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 00055: val_loss improved from 0.12178 to 0.12045, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv_checkpoint/055-0.1204.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.1204 - val_acc: 0.9686\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9830\n",
      "Epoch 00056: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0498 - acc: 0.9830 - val_loss: 0.1419 - val_acc: 0.9630\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9830\n",
      "Epoch 00057: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0513 - acc: 0.9830 - val_loss: 0.1480 - val_acc: 0.9651\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9831\n",
      "Epoch 00058: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0492 - acc: 0.9831 - val_loss: 0.1375 - val_acc: 0.9683\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9843\n",
      "Epoch 00059: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0459 - acc: 0.9843 - val_loss: 0.1473 - val_acc: 0.9681\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9856\n",
      "Epoch 00060: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0441 - acc: 0.9856 - val_loss: 0.1453 - val_acc: 0.9667\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9853\n",
      "Epoch 00061: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0439 - acc: 0.9853 - val_loss: 0.1249 - val_acc: 0.9676\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9843\n",
      "Epoch 00062: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0458 - acc: 0.9843 - val_loss: 0.1592 - val_acc: 0.9660\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9853\n",
      "Epoch 00063: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0430 - acc: 0.9853 - val_loss: 0.1521 - val_acc: 0.9679\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9869\n",
      "Epoch 00064: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0396 - acc: 0.9869 - val_loss: 0.1396 - val_acc: 0.9667\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9880\n",
      "Epoch 00065: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0373 - acc: 0.9880 - val_loss: 0.1343 - val_acc: 0.9676\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9850\n",
      "Epoch 00066: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0447 - acc: 0.9850 - val_loss: 0.1532 - val_acc: 0.9672\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9872\n",
      "Epoch 00067: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0378 - acc: 0.9872 - val_loss: 0.1602 - val_acc: 0.9713\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9878\n",
      "Epoch 00068: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0360 - acc: 0.9878 - val_loss: 0.1604 - val_acc: 0.9697\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9876\n",
      "Epoch 00069: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0382 - acc: 0.9876 - val_loss: 0.1967 - val_acc: 0.9655\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9865\n",
      "Epoch 00070: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0390 - acc: 0.9866 - val_loss: 0.1611 - val_acc: 0.9683\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9874\n",
      "Epoch 00071: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0376 - acc: 0.9874 - val_loss: 0.1448 - val_acc: 0.9704\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9886\n",
      "Epoch 00072: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0344 - acc: 0.9886 - val_loss: 0.1592 - val_acc: 0.9662\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9881\n",
      "Epoch 00073: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0370 - acc: 0.9881 - val_loss: 0.1519 - val_acc: 0.9679\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9878\n",
      "Epoch 00074: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0369 - acc: 0.9878 - val_loss: 0.1712 - val_acc: 0.9665\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9895\n",
      "Epoch 00075: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0324 - acc: 0.9895 - val_loss: 0.1533 - val_acc: 0.9690\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9878\n",
      "Epoch 00076: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0374 - acc: 0.9878 - val_loss: 0.1493 - val_acc: 0.9700\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9895\n",
      "Epoch 00077: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0321 - acc: 0.9895 - val_loss: 0.1435 - val_acc: 0.9711\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 00078: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0311 - acc: 0.9895 - val_loss: 0.1849 - val_acc: 0.9660\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9882\n",
      "Epoch 00079: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0362 - acc: 0.9882 - val_loss: 0.1565 - val_acc: 0.9679\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9907\n",
      "Epoch 00080: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0273 - acc: 0.9907 - val_loss: 0.1811 - val_acc: 0.9667\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9906\n",
      "Epoch 00081: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0283 - acc: 0.9906 - val_loss: 0.1699 - val_acc: 0.9679\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9894\n",
      "Epoch 00082: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0319 - acc: 0.9894 - val_loss: 0.1640 - val_acc: 0.9662\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9905\n",
      "Epoch 00083: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0294 - acc: 0.9905 - val_loss: 0.1588 - val_acc: 0.9716\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9900\n",
      "Epoch 00084: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0309 - acc: 0.9900 - val_loss: 0.1684 - val_acc: 0.9688\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9914\n",
      "Epoch 00085: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0266 - acc: 0.9914 - val_loss: 0.1769 - val_acc: 0.9669\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9903\n",
      "Epoch 00086: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0317 - acc: 0.9903 - val_loss: 0.1506 - val_acc: 0.9665\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9920\n",
      "Epoch 00087: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0249 - acc: 0.9920 - val_loss: 0.1553 - val_acc: 0.9672\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9905\n",
      "Epoch 00088: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0293 - acc: 0.9905 - val_loss: 0.1397 - val_acc: 0.9711\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9899\n",
      "Epoch 00089: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0314 - acc: 0.9899 - val_loss: 0.1623 - val_acc: 0.9669\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9904\n",
      "Epoch 00090: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0289 - acc: 0.9904 - val_loss: 0.1878 - val_acc: 0.9662\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9899\n",
      "Epoch 00091: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0298 - acc: 0.9899 - val_loss: 0.1681 - val_acc: 0.9683\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9925\n",
      "Epoch 00092: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0220 - acc: 0.9925 - val_loss: 0.1751 - val_acc: 0.9679\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9911\n",
      "Epoch 00093: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0267 - acc: 0.9911 - val_loss: 0.1486 - val_acc: 0.9683\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9886\n",
      "Epoch 00094: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 0.1652 - val_acc: 0.9716\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9918\n",
      "Epoch 00095: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0257 - acc: 0.9918 - val_loss: 0.1543 - val_acc: 0.9711\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9926\n",
      "Epoch 00096: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0217 - acc: 0.9926 - val_loss: 0.1896 - val_acc: 0.9667\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9926\n",
      "Epoch 00097: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0226 - acc: 0.9926 - val_loss: 0.1622 - val_acc: 0.9695\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9928\n",
      "Epoch 00098: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0220 - acc: 0.9928 - val_loss: 0.2055 - val_acc: 0.9639\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9911\n",
      "Epoch 00099: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0283 - acc: 0.9911 - val_loss: 0.1748 - val_acc: 0.9674\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 00100: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.1636 - val_acc: 0.9706\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9925\n",
      "Epoch 00101: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0234 - acc: 0.9925 - val_loss: 0.1745 - val_acc: 0.9651\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9922\n",
      "Epoch 00102: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0239 - acc: 0.9922 - val_loss: 0.1926 - val_acc: 0.9690\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9938\n",
      "Epoch 00103: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0195 - acc: 0.9938 - val_loss: 0.1916 - val_acc: 0.9669\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9917\n",
      "Epoch 00104: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0261 - acc: 0.9917 - val_loss: 0.1828 - val_acc: 0.9641\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9926\n",
      "Epoch 00105: val_loss did not improve from 0.12045\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0234 - acc: 0.9926 - val_loss: 0.1848 - val_acc: 0.9658\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1693 - acc: 0.9593\n",
      "Loss: 0.16933327621576277 Accuracy: 0.9592939\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7783 - acc: 0.4233\n",
      "Epoch 00001: val_loss improved from inf to 0.82010, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/001-0.8201.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.7783 - acc: 0.4233 - val_loss: 0.8201 - val_acc: 0.7428\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8247 - acc: 0.7331\n",
      "Epoch 00002: val_loss improved from 0.82010 to 0.46015, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/002-0.4602.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.8247 - acc: 0.7331 - val_loss: 0.4602 - val_acc: 0.8530\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8152\n",
      "Epoch 00003: val_loss improved from 0.46015 to 0.43364, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/003-0.4336.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.5776 - acc: 0.8152 - val_loss: 0.4336 - val_acc: 0.8551\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8567\n",
      "Epoch 00004: val_loss improved from 0.43364 to 0.27152, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/004-0.2715.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.4467 - acc: 0.8567 - val_loss: 0.2715 - val_acc: 0.9131\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8815\n",
      "Epoch 00005: val_loss improved from 0.27152 to 0.24950, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/005-0.2495.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.3741 - acc: 0.8815 - val_loss: 0.2495 - val_acc: 0.9192\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8945\n",
      "Epoch 00006: val_loss improved from 0.24950 to 0.23660, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/006-0.2366.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.3306 - acc: 0.8945 - val_loss: 0.2366 - val_acc: 0.9276\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9088\n",
      "Epoch 00007: val_loss improved from 0.23660 to 0.20688, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/007-0.2069.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2896 - acc: 0.9088 - val_loss: 0.2069 - val_acc: 0.9334\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9170\n",
      "Epoch 00008: val_loss improved from 0.20688 to 0.17525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/008-0.1753.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2596 - acc: 0.9170 - val_loss: 0.1753 - val_acc: 0.9485\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9251\n",
      "Epoch 00009: val_loss improved from 0.17525 to 0.16780, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/009-0.1678.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2358 - acc: 0.9251 - val_loss: 0.1678 - val_acc: 0.9488\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9311\n",
      "Epoch 00010: val_loss did not improve from 0.16780\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2137 - acc: 0.9312 - val_loss: 0.1791 - val_acc: 0.9450\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9361\n",
      "Epoch 00011: val_loss did not improve from 0.16780\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1960 - acc: 0.9361 - val_loss: 0.1759 - val_acc: 0.9436\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9427\n",
      "Epoch 00012: val_loss improved from 0.16780 to 0.16685, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/012-0.1668.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1762 - acc: 0.9426 - val_loss: 0.1668 - val_acc: 0.9467\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9461\n",
      "Epoch 00013: val_loss improved from 0.16685 to 0.13454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/013-0.1345.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1647 - acc: 0.9461 - val_loss: 0.1345 - val_acc: 0.9583\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9518\n",
      "Epoch 00014: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1511 - acc: 0.9519 - val_loss: 0.1398 - val_acc: 0.9583\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9534\n",
      "Epoch 00015: val_loss improved from 0.13454 to 0.12760, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/015-0.1276.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1407 - acc: 0.9533 - val_loss: 0.1276 - val_acc: 0.9604\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9560\n",
      "Epoch 00016: val_loss did not improve from 0.12760\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1354 - acc: 0.9560 - val_loss: 0.1413 - val_acc: 0.9590\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9585\n",
      "Epoch 00017: val_loss did not improve from 0.12760\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1236 - acc: 0.9585 - val_loss: 0.1392 - val_acc: 0.9597\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9607\n",
      "Epoch 00018: val_loss did not improve from 0.12760\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1183 - acc: 0.9607 - val_loss: 0.1834 - val_acc: 0.9478\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9612\n",
      "Epoch 00019: val_loss did not improve from 0.12760\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1172 - acc: 0.9612 - val_loss: 0.1285 - val_acc: 0.9630\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9665\n",
      "Epoch 00020: val_loss improved from 0.12760 to 0.12524, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/020-0.1252.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1020 - acc: 0.9665 - val_loss: 0.1252 - val_acc: 0.9639\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9652\n",
      "Epoch 00021: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1034 - acc: 0.9652 - val_loss: 0.1328 - val_acc: 0.9651\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9698\n",
      "Epoch 00022: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0899 - acc: 0.9698 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9707\n",
      "Epoch 00023: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0873 - acc: 0.9707 - val_loss: 0.1315 - val_acc: 0.9644\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9719\n",
      "Epoch 00024: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0831 - acc: 0.9719 - val_loss: 0.1328 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9736\n",
      "Epoch 00025: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0784 - acc: 0.9736 - val_loss: 0.1351 - val_acc: 0.9627\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9757\n",
      "Epoch 00026: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0733 - acc: 0.9757 - val_loss: 0.1353 - val_acc: 0.9634\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9759\n",
      "Epoch 00027: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0718 - acc: 0.9759 - val_loss: 0.1363 - val_acc: 0.9630\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9783\n",
      "Epoch 00028: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0654 - acc: 0.9783 - val_loss: 0.1718 - val_acc: 0.9585\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9745\n",
      "Epoch 00029: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0818 - acc: 0.9745 - val_loss: 0.1419 - val_acc: 0.9653\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9803\n",
      "Epoch 00030: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0580 - acc: 0.9803 - val_loss: 0.1292 - val_acc: 0.9660\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9788\n",
      "Epoch 00031: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0623 - acc: 0.9788 - val_loss: 0.1306 - val_acc: 0.9681\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9815\n",
      "Epoch 00032: val_loss did not improve from 0.12524\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0562 - acc: 0.9815 - val_loss: 0.1385 - val_acc: 0.9632\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9799\n",
      "Epoch 00033: val_loss improved from 0.12524 to 0.11762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv_checkpoint/033-0.1176.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0597 - acc: 0.9799 - val_loss: 0.1176 - val_acc: 0.9662\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9827\n",
      "Epoch 00034: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0507 - acc: 0.9827 - val_loss: 0.1464 - val_acc: 0.9686\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9845\n",
      "Epoch 00035: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0465 - acc: 0.9845 - val_loss: 0.1465 - val_acc: 0.9646\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9818\n",
      "Epoch 00036: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0544 - acc: 0.9819 - val_loss: 0.1565 - val_acc: 0.9665\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9834\n",
      "Epoch 00037: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0514 - acc: 0.9834 - val_loss: 0.1327 - val_acc: 0.9665\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9855\n",
      "Epoch 00038: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0441 - acc: 0.9855 - val_loss: 0.1274 - val_acc: 0.9672\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9850\n",
      "Epoch 00039: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0453 - acc: 0.9850 - val_loss: 0.1486 - val_acc: 0.9623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9867\n",
      "Epoch 00040: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0404 - acc: 0.9867 - val_loss: 0.1610 - val_acc: 0.9618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9858\n",
      "Epoch 00041: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.1658 - val_acc: 0.9630\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9864\n",
      "Epoch 00042: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0403 - acc: 0.9864 - val_loss: 0.1669 - val_acc: 0.9616\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9869\n",
      "Epoch 00043: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0392 - acc: 0.9869 - val_loss: 0.1465 - val_acc: 0.9679\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0406 - acc: 0.9864 - val_loss: 0.1250 - val_acc: 0.9686\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9880\n",
      "Epoch 00045: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0368 - acc: 0.9880 - val_loss: 0.1641 - val_acc: 0.9639\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9868\n",
      "Epoch 00046: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0400 - acc: 0.9868 - val_loss: 0.1243 - val_acc: 0.9665\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9887\n",
      "Epoch 00047: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0355 - acc: 0.9887 - val_loss: 0.1925 - val_acc: 0.9588\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9880\n",
      "Epoch 00048: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0364 - acc: 0.9880 - val_loss: 0.1474 - val_acc: 0.9655\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9896\n",
      "Epoch 00049: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.1415 - val_acc: 0.9669\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9900\n",
      "Epoch 00050: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0321 - acc: 0.9900 - val_loss: 0.1469 - val_acc: 0.9655\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9883\n",
      "Epoch 00051: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0359 - acc: 0.9883 - val_loss: 0.1772 - val_acc: 0.9653\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9904\n",
      "Epoch 00052: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0298 - acc: 0.9904 - val_loss: 0.1743 - val_acc: 0.9667\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9890\n",
      "Epoch 00053: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0337 - acc: 0.9890 - val_loss: 0.1893 - val_acc: 0.9599\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9913\n",
      "Epoch 00054: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0275 - acc: 0.9913 - val_loss: 0.1398 - val_acc: 0.9672\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9897\n",
      "Epoch 00055: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0302 - acc: 0.9897 - val_loss: 0.1666 - val_acc: 0.9644\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9912\n",
      "Epoch 00056: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0273 - acc: 0.9912 - val_loss: 0.1659 - val_acc: 0.9648\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9907\n",
      "Epoch 00057: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0296 - acc: 0.9907 - val_loss: 0.1740 - val_acc: 0.9606\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 00058: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0284 - acc: 0.9909 - val_loss: 0.1366 - val_acc: 0.9690\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9911\n",
      "Epoch 00059: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0282 - acc: 0.9911 - val_loss: 0.1527 - val_acc: 0.9697\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9916\n",
      "Epoch 00060: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0281 - acc: 0.9916 - val_loss: 0.1503 - val_acc: 0.9613\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9925\n",
      "Epoch 00061: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0219 - acc: 0.9925 - val_loss: 0.1483 - val_acc: 0.9669\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9919\n",
      "Epoch 00062: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0264 - acc: 0.9919 - val_loss: 0.1537 - val_acc: 0.9641\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9918\n",
      "Epoch 00063: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0250 - acc: 0.9918 - val_loss: 0.1557 - val_acc: 0.9683\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9939\n",
      "Epoch 00064: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0191 - acc: 0.9939 - val_loss: 0.2179 - val_acc: 0.9597\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9924\n",
      "Epoch 00065: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0237 - acc: 0.9924 - val_loss: 0.1897 - val_acc: 0.9634\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9901\n",
      "Epoch 00066: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0328 - acc: 0.9901 - val_loss: 0.1769 - val_acc: 0.9639\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9933\n",
      "Epoch 00067: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0206 - acc: 0.9933 - val_loss: 0.1761 - val_acc: 0.9634\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9930\n",
      "Epoch 00068: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0221 - acc: 0.9930 - val_loss: 0.1463 - val_acc: 0.9702\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 00069: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0219 - acc: 0.9935 - val_loss: 0.1641 - val_acc: 0.9667\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9939\n",
      "Epoch 00070: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0203 - acc: 0.9939 - val_loss: 0.1482 - val_acc: 0.9679\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9918\n",
      "Epoch 00071: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0242 - acc: 0.9918 - val_loss: 0.1672 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9933\n",
      "Epoch 00072: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.1539 - val_acc: 0.9693\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9934\n",
      "Epoch 00073: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0207 - acc: 0.9934 - val_loss: 0.1709 - val_acc: 0.9669\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9938\n",
      "Epoch 00074: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0198 - acc: 0.9938 - val_loss: 0.1921 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9934\n",
      "Epoch 00075: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0198 - acc: 0.9934 - val_loss: 0.1939 - val_acc: 0.9658\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9931\n",
      "Epoch 00076: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0205 - acc: 0.9931 - val_loss: 0.1729 - val_acc: 0.9683\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9927\n",
      "Epoch 00077: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0239 - acc: 0.9927 - val_loss: 0.1829 - val_acc: 0.9632\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9949\n",
      "Epoch 00078: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0176 - acc: 0.9949 - val_loss: 0.1610 - val_acc: 0.9665\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9946\n",
      "Epoch 00079: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0183 - acc: 0.9946 - val_loss: 0.1866 - val_acc: 0.9655\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9943\n",
      "Epoch 00080: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 0.1703 - val_acc: 0.9644\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9941\n",
      "Epoch 00081: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0197 - acc: 0.9941 - val_loss: 0.2257 - val_acc: 0.9639\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00082: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1684 - val_acc: 0.9646\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9945\n",
      "Epoch 00083: val_loss did not improve from 0.11762\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0175 - acc: 0.9945 - val_loss: 0.1744 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8XVW5+P/POvPJ1IxN2qYzlM5N27QUGQqiyKCAIBQEFS6Dcp24XHtvVQQU/YqAP70oCBVRUKBgCwIKVoHWMgpp6Qil85B0yDyfkzM9vz/WOclJm7Rpm9O06fN+vfYrOXt89k7Oevbaa++1jYiglFJKHYyjrwNQSil1fNCEoZRSqkc0YSillOoRTRhKKaV6RBOGUkqpHtGEoZRSqkc0YSillOoRTRhKKaV6RBOGUkqpHnH1dQC9KT8/X0aMGNHXYSil1HFj+fLl1SJS0JN5+1XCGDFiBGVlZX0dhlJKHTeMMdt7Oq9eklJKKdUjmjCUUkr1iCYMpZRSPdKv2jC6Eg6HKS8vJxgM9nUoxyWfz0dxcTFut7uvQ1FK9bF+nzDKy8vJzMxkxIgRGGP6OpzjiohQU1NDeXk5I0eO7OtwlFJ9rN9fkgoGg+Tl5WmyOAzGGPLy8rR2ppQCToCEAWiyOAJ67JRSCSdEwjiYtrZdRCINfR2GUkod01KWMIwxjxljKo0xa7uZPtcYszI+rDXGRI0xufFp24wxa+LTUv4kXii0h0ikMSXrrq+v56GHHjqsZS+88ELq6+t7PP9dd93F/ffff1jbUkqpg0llDeMPwPndTRSR+0SkRERKgO8C/xKR2qRZzolPL01hjAAY4wBiKVn3gRJGJBI54LIvv/wy2dnZqQhLKaUOWcoShogsA2oPOqN1NfB0qmI5OAciqUkY8+bNY/PmzZSUlDB37lyWLl3KmWeeycUXX8z48eMBuPTSS5k+fToTJkxg/vz57cuOGDGC6upqtm3bxrhx47jpppuYMGEC5513HoFA4IDbXblyJbNmzWLy5Ml8/vOfp66uDoAHHniA8ePHM3nyZK666ioA/vWvf1FSUkJJSQlTp06lqakpJcdCKXV86/Pbao0xadiayDeSRgvwD2OMAI+IyPwuF7bL3wzcDDBs2LADbmvjxltpbl653/hotAVjHDgc/kOOPyOjhJNP/mW30++55x7Wrl3LypV2u0uXLmXFihWsXbu2/VbVxx57jNzcXAKBADNmzODyyy8nLy9vn9g38vTTT/Pb3/6WK6+8kkWLFnHttdd2u90vf/nL/OpXv2L27Nnccccd/PCHP+SXv/wl99xzD1u3bsXr9bZf7rr//vt58MEHOf3002lubsbn8x3ycVBK9X/HQqP354C39rkcdYaITAMuAL5ujDmru4VFZL6IlIpIaUFBjzpc3M/RvhFo5syZnZ5reOCBB5gyZQqzZs1i586dbNy4cb9lRo4cSUlJCQDTp09n27Zt3a6/oaGB+vp6Zs+eDcBXvvIVli1bBsDkyZO55ppr+NOf/oTLZc8XTj/9dG677TYeeOAB6uvr28crpVSyY6FkuIp9LkeJSEX8Z6Ux5nlgJrDsSDfUXU2gpeUjjHGSljbmSDfRI+np6e2/L126lFdffZV33nmHtLQ0zj777C6fe/B6ve2/O53Og16S6s7f/vY3li1bxksvvcRPfvIT1qxZw7x587jooot4+eWXOf3001m8eDFjx449rPUrpfqvPq1hGGMGALOBF5LGpRtjMhO/A+cBXd5p1XtxpK7ROzMz84BtAg0NDeTk5JCWlsb69et59913j3ibAwYMICcnhzfeeAOAP/7xj8yePZtYLMbOnTs555xz+NnPfkZDQwPNzc1s3ryZSZMm8b//+7/MmDGD9evXH3EMSqn+J2U1DGPM08DZQL4xphy4E3ADiMjD8dk+D/xDRFqSFi0Eno8/MOYCnhKRv6cqTsuBSDgla87Ly+P0009n4sSJXHDBBVx00UWdpp9//vk8/PDDjBs3jlNOOYVZs2b1ynYff/xxvva1r9Ha2sqoUaP4/e9/TzQa5dprr6WhoQER4Vvf+hbZ2dn84Ac/YMmSJTgcDiZMmMAFF1zQKzEopfoXIyJ9HUOvKS0tlX1foPTRRx8xbty4Ay4XCGwmFguSnj4hleEdt3pyDJVSxydjzPKePr5wLDR6HwNMym6rVUqp/kITBqltw1BKqf5CEwaQygf3lFKqv9CEAdjDoAlDKaUORBMGiUtSQn+6AUAppXqbJgwAEo96a8JQSqnuaMIgUcPgmGnHyMjIOKTxSil1NGjCADoOw7GRMJRS6likCYPU1jDmzZvHgw8+2P458ZKj5uZmzj33XKZNm8akSZN44YUXDrCWzkSEuXPnMnHiRCZNmsQzzzwDwO7duznrrLMoKSlh4sSJvPHGG0SjUa677rr2eX/xi1/0+j4qpU4Mx0Lng0fPrbfCyv27N3dKBH8sgMORDuYQc2hJCfyy++7N58yZw6233srXv/51AJ599lkWL16Mz+fj+eefJysri+rqambNmsXFF1/co3doP/fcc6xcuZJVq1ZRXV3NjBkzOOuss3jqqaf4zGc+w/e//32i0Sitra2sXLmSiooK1q613XEdyhv8lFIq2YmVMLrRUUT3fqP31KlTqaysZNeuXVRVVZGTk8PQoUMJh8N873vfY9myZTgcDioqKti7dy9FRUUHXeebb77J1VdfjdPppLCwkNmzZ/P+++8zY8YM/uM//oNwOMyll15KSUkJo0aNYsuWLXzzm9/koosu4rzzzuv1fVRKnRhOrITRTU0gGmkkENiA338KLldmr2/2iiuuYOHChezZs4c5c+YA8OSTT1JVVcXy5ctxu92MGDGiy27ND8VZZ53FsmXL+Nvf/sZ1113Hbbfdxpe//GVWrVrF4sWLefjhh3n22Wd57LHHemO3lFInGG3DAFLd6D1nzhwWLFjAwoULueKKKwDbrfnAgQNxu90sWbKE7du393h9Z555Js888wzRaJSqqiqWLVvGzJkz2b59O4WFhdx0003ceOONrFixgurqamKxGJdffjk//vGPWbFiRUr2USnV/51YNYxupPq22gkTJtDU1MSQIUMYNGgQANdccw2f+9znmDRpEqWlpYf0wqLPf/7zvPPOO0yZMgVjDPfeey9FRUU8/vjj3HfffbjdbjIyMnjiiSeoqKjg+uuvJxaz+/bTn/40JfuolOr/tHtzIBoN0tq6Fp9vJG533gHnPRFp9+ZK9V/avfkhStyZ1J+Sp1JK9TZNGIA+uKeUUgenCYNjr2sQpZQ6FmnCALSGoZRSB5eyhGGMecwYU2mMWdvN9LONMQ3GmJXx4Y6kaecbYz42xmwyxsxLVYxJ28M+vqcJQymlupPKGsYfgPMPMs8bIlISH34EYIxxAg8CFwDjgauNMeNTGGec0UZvpZQ6gJQlDBFZBtQexqIzgU0iskVEQsAC4JJeDa4LqXqvd319PQ899NBhLXvhhRdq309KqWNGX7dhnGaMWWWMecUYMyE+bgiwM2me8vi4LhljbjbGlBljyqqqqo4glNS81/tACSMSiRxw2Zdffpns7Oxej0kppQ5HXyaMFcBwEZkC/Ar4y+GsRETmi0ipiJQWFBQcdjCpqmHMmzePzZs3U1JSwty5c1m6dClnnnkmF198MePH2yttl156KdOnT2fChAnMnz+/fdkRI0ZQXV3Ntm3bGDduHDfddBMTJkzgvPPOIxAI7Letl156iVNPPZWpU6fyqU99ir179wLQ3NzM9ddfz6RJk5g8eTKLFi0C4O9//zvTpk1jypQpnHvuub2+70qp/qXPugYRkcak3182xjxkjMkHKoChSbMWx8cdsW56NwcgGh2JMQZH7/Zuzj333MPatWtZGd/w0qVLWbFiBWvXrmXkyJEAPPbYY+Tm5hIIBJgxYwaXX345eXmdnzjfuHEjTz/9NL/97W+58sorWbRoEddee22nec444wzeffddjDE8+uij3Hvvvfz85z/n7rvvZsCAAaxZswaAuro6qqqquOmmm1i2bBkjR46ktvZwrh4qpU4kfZYwjDFFwF4REWPMTGxtpwaoB042xozEJoqrgC/2VZypMHPmzPZkAfDAAw/w/PPPA7Bz5042bty4X8IYOXIkJSUlAEyfPp1t27btt97y8nLmzJnD7t27CYVC7dt49dVXWbBgQft8OTk5vPTSS5x11lnt8+Tm5vbqPiql+p+UJQxjzNPA2UC+MaYcuBNwA4jIw8AXgFuMMREgAFwl9jaliDHmG8BiwAk8JiLreiOmA9UEWlvLERHS03veCeDhSk9Pb/996dKlvPrqq7zzzjukpaVx9tlnd9nNudfrbf/d6XR2eUnqm9/8JrfddhsXX3wxS5cu5a677kpJ/EqpE1Mq75K6WkQGiYhbRIpF5Hci8nA8WSAivxaRCSIyRURmicjbScu+LCJjRGS0iPwkVTF2lpo2jMzMTJqamrqd3tDQQE5ODmlpaaxfv5533333sLfV0NDAkCH2/oDHH3+8ffynP/3pTq+JraurY9asWSxbtoytW7cC6CUppdRB9fVdUseMVDV65+XlcfrppzNx4kTmzp273/Tzzz+fSCTCuHHjmDdvHrNmzTrsbd11111cccUVTJ8+nfz8/Pbxt99+O3V1dUycOJEpU6awZMkSCgoKmD9/PpdddhlTpkxpf7GTUkp1R7s3jwsEthKNNpGRMTlV4R23tHtzpfov7d78MNgaRv9Jnkop1ds0YbQz2lutUkodgCaMdqlpw1BKqf5CE0Zc4pJUf2rTUUqp3qQJo52+E0MppQ5EE0Zcx3u9NWEopVRXNGG0SxyKvr8klZGR0dchKKXUfjRhxOl7vZVS6sA0YbRLTRvGvHnzOnXLcdddd3H//ffT3NzMueeey7Rp05g0aRIvvPDCQdfVXTfoXXVT3l2X5kopdbj6rLfavnDr329l5Z6u+zcXiRCLBXA40rBvie2ZkqISfnl+970azpkzh1tvvZWvf/3rADz77LMsXrwYn8/H888/T1ZWFtXV1cyaNYuLL764vS2lK111gx6LxbrspryrLs2VUupInFAJ48C6L6iPxNSpU6msrGTXrl1UVVWRk5PD0KFDCYfDfO9732PZsmU4HA4qKirYu3cvRUVF3a6rq27Qq6qquuymvKsuzZVS6kicUAnjQDWBSKSZQGA9fv/JuFwDenW7V1xxBQsXLmTPnj3tnfw9+eSTVFVVsXz5ctxuNyNGjOiyW/OEnnaDrpRSqaJtGHGpbPSeM2cOCxYsYOHChVxxxRWA7Yp84MCBuN1ulixZwvbt2w+4ju66Qe+um/KuujRXSqkjoQmjXeoe3JswYQJNTU0MGTKEQYMGAXDNNddQVlbGpEmTeOKJJxg79sAvbuquG/TuuinvqktzpZQ6Etq9eVwsFqKlZTVe73A8noJUhXhc0u7Nleq/tHvzw5Jo9NbnMJRSqiuaMOI62jD6T41LKaV6U8oShjHmMWNMpTFmbTfTrzHGrDbGrDHGvG2MmZI0bVt8/EpjTFlXyx+KniUB7XywK5pAlVIJqaxh/AE4/wDTtwKzRWQScDcwf5/p54hISU+vrXXH5/NRU1Nz0ILPPjBn0ITRQUSoqanB5/P1dShKqWNAyp7DEJFlxpgRB5j+dtLHd4HiVMRRXFxMeXk5VVVVB503GKzG6QzidjelIpTjks/no7g4JX8apdRx5lh5cO8G4JWkzwL8wxgjwCMism/to8fcbnf7U9AH89Zb55CffwmnnPLI4W5OKaX6rT5PGMaYc7AJ44yk0WeISIUxZiDwT2PMehFZ1s3yNwM3AwwbNuyIYnE6/cRigSNah1JK9Vd9epeUMWYy8ChwiYjUJMaLSEX8ZyXwPDCzu3WIyHwRKRWR0oKCI3t+wuHQhKGUUt3ps4RhjBkGPAd8SUQ2JI1PN8ZkJn4HzgO6vNOqtzkcfqJRTRhKKdWVlF2SMsY8DZwN5BtjyoE7ATeAiDwM3AHkAQ/Fu/SOxO+IKgSej49zAU+JyN9TFWcyh8OnNQyllOpGKu+Suvog028Ebuxi/BZgyv5LpJ5eklJKqe7pk95JtNFbKaW6pwkjidYwlFKqe5owkmijt1JKdU8TRhKtYSilVPc0YSTRNgyllOqeJowkelutUkp1TxNGEofDj0iEWCzS16EopdQxRxNGEofDD0AsFuzjSJRS6tijCSNJR8LQy1JKKbUvTRhJnE5NGEop1R1NGEm0hqGUUt3ThJEkkTD04T2llNqfJowkWsNQSqnuacJI4nD4AE0YSinVFU0YSbTRWymluqcJI4m2YSilVPc0YSTRB/eUUqp7mjCSaKO3Ukp1TxNGEm3DUEqp7qU0YRhjHjPGVBpj1nYz3RhjHjDGbDLGrDbGTEua9hVjzMb48JVUxpmgNQyllOpeqmsYfwDOP8D0C4CT48PNwG8AjDG5wJ3AqcBM4E5jTE5KI6Xjtlpt9FZKqf2lNGGIyDKg9gCzXAI8Ida7QLYxZhDwGeCfIlIrInXAPzlw4ukVxjgwxqM1DKWU6oKrj7c/BNiZ9Lk8Pq678Smnr2lV/YEIxGLgdHY9PRy2g88HjqTTxlAI6uqgsRH8fsjIsIPTCcEgNDfbIbFul6tjG4ltJn4mfo9GIRLp+BkKQVubHcLhjvW43fZ3YzriCQRsPPX1NiaPpyOmzEzIyYHcXDvEYrB7N+zZY4fEuhPrTGw/HO6IJzH4/ZCdbYesLBtjYl8DAbsfIjamWKzzsg5Hx3YSQ2Jc4mfiOLW22v1J7BPYefad3+GwMXc3QMf+RCKQlgbf/W7v/g91pa8TxhEzxtyMvZzFsGHDjnh9+prW41swaL9QHk/HF0sEWlqgqgpqaqChwRY+jY32y1lQAPn5dohEoKnJFhRNTXbe+no7BIMdX3yXy35uarLraWmx47xeO7hcHV/mRAGV/HtiHS6XjbOlxW6zpcVO9/lsIebz2cIrEXNzsy1M3G47QEeszc02pmi043h4PLYAzMy062pstPvS0tIxT1oapKfbgrG5uevjakxHgal6R1qa/VsmJ6BEoj0UDgcMGnRiJIwKYGjS5+L4uArg7H3GL+1qBSIyH5gPUFpaesT/0raGoc9h9IZYrKMwSxSs9fUdZ7CRSMcXpK0NamttgV5bawvSoUNh2DAYPBh27YI1a2DtWtiyxRbK6el2AJsMqqo6CkKXy56F+v12m4EUnQP4fLZATk+3+5M4c45EbIGeSArJv7tcdp8TSSQWs8snzpz9flvwJ5KU1wsDBthjkZ5u50/UEERsMkiccft8nc+qA4GO5BgI2PXk5NgzabfbnvG2tNjB77dn6jk5dp+SaxShkN1G4pi7XJ3PcKHzWXFi+4mz58R+O502iSUSq9vdeX+Sk52IjSkRb+LMP5FcGxvt/1JtrR3AFpyDBkFRkd1OojCOxTof/0QsiTP6xPFO1GQS/1+Jv0eiFpa8f4lBpHONJXmbyTWRSKTjGCeOf3eSa2tdDdB5H46Wvk4YLwLfMMYswDZwN4jIbmPMYuD/JTV0nwcchfxpE8aJ3ugdjcXYURFizUdt7KgIYjwBnN4AxhMg0OqkpjyXPVvy2LnVT2MjBCMBAtEmApFWgm1CMGi/gG0BF4QyoC0TYknfDhMDRwTEQMwF2KqA0wl5efYLFQrBc89BKBaEgnXgayAjK8zwUSHGT2sjKA00ReqoidZjcDLEfRKz0scwOvtkHMawp2UP1YG91If34sioxZlRC75a3L4QQ7KKGZkznNH5w3Djp6K6kV21DVQ1NGGcUbw+8HnB443h8YVx+cK4vWGy/H5GZ5/MyAFjGOgbjM9nDvilTyYibG/Yzge7P2BN5Rqa2poIRUOEY2EMhuHZwzk592ROyj2JLG8WNYEaqlurqQ3UUpxVTOngUnwuX6f1ba3fyqbaTTSHmmkJtdASbsFgSPekk+5OJ92TjsvhwmBwGAeC0BpubZ8305PJ9MHTGT5gOCZeHWsJtVC2q4y1lWvJ8GQwNn0gA9MHMsA3gGgsSiQWISpRsrxZFGcV43LYIiQSi7Bi9wqWbF3Cyr0rCUaChKNhQtEQ+f58zh5xNueMOIeTck8CYE/zHtZVreOjui0M8A5gYPpAitILcDlcbK/fztb6rWyr30a+O59PFnyS4qISHKZzydgWaSMcCxOJRYjEX6vscrja9/njmo8p21VG2a4y1levJ9efS3FWMUMyh5DtyyYQCRAIBwhGgoSiofZ9i8QitAXaaKtpoy3aRkxi5PhyyPXnkuvPJceXQ7Yvmxx/DgO8AwAIRoK0RdsIhAPtf7ua1hoEYUzeGMbmj+WUvFMwxrC7aTerdu6msqUSh3HgdXrxurz4XD58Lh9+lx+/24/X6cXpcOJyuHAaJ1GJth9TQRjgHUCOPwcPnkP+jh+ulCYMY8zT2JpCvjGmHHvnkxtARB4GXgYuBDYBrcD18Wm1xpi7gffjq/qRiByo8bzXHOttGCJC2a4y3tjxBjHpqLsGI0HqAnXUB+tpaGvA6XDiNemYSBoS9tPSGqWpNURLMIwjmMfQxjk4qydTX2fsmaRzJzsH/x97Bj1G1FN38EAKweR7EEfEJoCDcBkPDhxEJESMzvMbDOnudEbkjGBUzihG54xuL7jWVK4hHAsD0Aysiw8JiUJkc+JYJF7H7okPCRHICGXgirior64/+P4dRLo7nWxfdnsB5TAOgpFgeyEUlShp7jTS3Gmku9OpbKmkLljXvr9+tx+3w43H6SESi7RP647H6aF0cCklhSVsqttE2a4yagO985XIT8tnatFUqlqrWLN3DVGJHnwhwGmcDB0wlEEZg1hXtY7GtkYARmaPJMOTgdvpxu1ws7ZyLU+vfRqAQRmDaIu29Sh2t8Pd/rfP9edyxrAzCEVD7GzYyc7Gne3bO5gcXw4TBk5gU+0mlm1ftt+xNhi8Li9OEy+cHc72Qtzr9GKMoT5YT01rTXs8PeF2uDHGEIqGerzM4cjwZDAqZxSrvrYqpdsBMNKDC5PGmG8DvweagEeBqcA8EflHasM7NKWlpVJWVnZE6/jggzMxxk1Jyeu9FJUVkxivb32d3U27EYSYxHAYB0UZRQzNGkpxVjGZ3swul20ONbO1biuLPlrEU2ueYmPtxi7nc0TScYSyoc2eDYqrBTwt4ArYM/yo2/7014AzgrtuIvm7ryaSvZ7qoqfBCIU1X6DYN5ZBBT6GFHkpzPPiwo+J+CHix+mO4M+po81RS22gFpfDRaYnk0xvJmnutE5ngeFomOZQM02hJpramgDaCxGXw4Ug9qwuFqUp1MTW+q1sqdvClrot7QVk6aBSpg+eTkFaAW6nLWA9Tg/ZvmyyfdlkeDKIxCJsqdvChpoNbKzZiMM4KMwopCijiIHpA8lPyyfHl4PX5QWgqa2JHQ072N6wnbZIGwN8AxjgHUCmN7P9jBlsQZLYptvhpinUxMaajWyo2cCGmg00hZraz26jEu04O3T5cTqcBMIBWsL2bD7Hl8PUoqmUFJUwqXASae60Tn+7+mA9m2s3s6l2E02hJvLT8snz55Hjz2Fz7Wbe3PEmb+18i9V7V3NS7kmUDi5lxuAZjC8YT5Y3yyYmTzoiQku4pb0mEZUoMYmR+J4n5ktzp1HTWsPy3csp21XGit0ryEvL47Ti05hVPIuSohIC4QCVLZVUtVbREGxoL0ydxkldsI6tdVvZ1rCN8sZyxuWP45wR53D2iLMpzCjstG8iwoaaDSzZtoQ3d7xJujudiQMnMmHgBEbnjKY51ExVaxWVLZWEo2GGZw9nZPZIBmUOYk/zHl7f+jqvbX2Nt3e+TaYnk+KsYoZmDaUoo6hTQQ+01xAisQijckZROriUkdkj22tQAK3hVhrbGtv/Xh6np9P07iSObX2wvtMA4HP58DptLSEvLY+CtAIyPBnEJMa2+m18XPMx66vXYzAMyhzE4MzBDEwfCNiaUlu0zZ5whAPtJx1t0TaisWj7PjmNs/37Y4yhIdhAbaCWumAdTuPkvvPuO+g+dMUYs1xESns0bw8TxioRmWKM+QzwVeAHwB9FZNpBFj2qeiNhrFr1aaLRZqZNe6dXYgpGgvxp9Z/4+Ts/Z331+gPOm+ZOI8OT0X45oSnYyp7mPbTFWu0MYvBUfJLQ8i/Chs9C2BY6Q4fC8GIPWeme9uvMubkdjbl5eR2Nuvn5EPVW89z6P/OnNX/i7Z1vk+5O58ZpN/Jfs/6L4dnDe2W/j0Tif7InX2Kl1JE5lITR00tSiW/uhdhEsc7002+zw+EnFKo67OWbQ82srVzLmr1rWFO5hj9/+Gf2NO9hatFUnrrsKWYMmYHDOHAYB5FYhF2Nu1m1fScrNpazee9eqva2UNPUwq5AC23NfmgugpZCitIHMdZ7LqMHDmbI52ySmDDBDllZhxplPrfMuIVbZtxCeWM5GZ4Msn3Zh73Pva2f/mspddzracJYboz5BzAS+K4xJhM4xJu/jg+H2oZRH6xn2fZlLNm6hCXblrB672oEe4ac7k5n9ojZ3DbrNj458pMYY2hpgXfegWXL4K23YOXKk9rv8AAoLISxY2DMGJhwBkyfDlOn2jtgUqE4qzg1K1ZK9Ts9TRg3ACXAFhFpjXfdcX3qwuo7B0sYa/auYdFHi1i1dxWr9qxia/1WwF7D/MTQT3Dn7Dvbr1OPyB5BS7ODt96C786Hf/0Lysrs7XUOh00El18OU6bYYeJEe7udUkodi3qaME4DVopIizHmWmAa8H+pC6vv2Af3un4Oo7q1mrP+cBYNwQbG5I1hxpAZ3DjtRk4fejqnFp/afttjLAavvAK3PACvvWbvwXa5YOZMmDsXZs+GT3widbUGpZRKhZ4mjN8AU4wxU4D/xt4p9QQwO1WB9ZUD1TDuWnoXTW1NrL5lNRMHTtxvekMDPPEE/OpXsHGjfeBs7lw491w47bSOh8yUUup41NOEERERMcZcAvxaRH5njLkhlYH1le4e3FtXuY6Hyx7mltJbOiULEXj7bXj0UXj2Wfvk7KxZ8MMfwhe+cOCnOZVS6njS04TRZIz5LvAl4ExjjIP4A3j9jX0nRpRYLIzDYXdRRLjtH7eR6c3krrPvap93xQq47jrbZUVGBlx7Ldx0E5T26AaX+WGQAAAgAElEQVQ1pZQ6vvS0F5I5QBvwHyKyB9u30+E9JXKMS7wTI/my1CubXuEfm//BXbPvIi8tDxF48EF7mamuztYudu+GRx7RZKGU6r96lDDiSeJJYIAx5rNAUESeSGlkfWTf17SGo2FuW3wbY/LG8J8z/pOGBrjySvjGN+BTn4KVK+GGG2wNQyml+rMeXZIyxlyJrVEsxT7E9ytjzFwRWZjC2PpEc0S4fwPEyr9IUzjEnuY9bKrdxEtXv0RbwM2ZZ8KHH8K998J///fR7SlSKaX6Uk/bML4PzBCRSgBjTAHwKtDvEsZLW1byt90wqaCCgowhTC6czFenf5ULT7qIOXNg3Tr461/hggv6OlKllDq6epowHIlkEVdD6t8H3if+uX0VhV5489onycqa3j7+Zz+DhQvhvvs0WSilTkw9TRh/j7+j4un45znYrsn7DxFaIwGWla/mgkIQaWuftHixfZvVnDn2MpRSSp2IepQwRGSuMeZy4PT4qPki8nzqwjqKROwTdjfeyOtfOpVgNMRpeRCJ2D7zt2yBq6+GSZPgd7/r/L5hpZQ6kfT4BUoisghYlMJY+kYiA+zezUsfv0SGJ50p2S0EAraPqO9/3/b99Pzz+qS2UurEdsCEYYxpArp6YYYBREQOuWPtY1JREbJnN3/duJLPjD4fn+tlgsEtVFbCokXwn/8Jo0b1dZBKKdW3DpgwROTE6B6vsJAVwa3satrFZ8d8Fl9oPYHAFp57zr6Y/qtf7esAlVKq7/XLO50OWVERf02rwGC48OQL8ftH09q6hfnz4cwzYdy4vg5QKaX6XkoThjHmfGPMx8aYTcaYeV1M/4UxZmV82GCMqU+aFk2a9mIq46SwkJcGNzGreBYD0wfi94/irbeGsXmz1i6UUiqhx43eh8oY4wQeBD4NlAPvG2NeFJEPE/OIyH8lzf9NYGrSKgIiUpKq+JLtGuhneZrw/4aeC4DPN4oXX/wEeXkxLr9cK2FKKQWprWHMBDaJyBYRCQELgEsOMP/VdDzncVT9Na0cgM9lzQCgsXEcb755KVddtQefry8iUkqpY08qE8YQYGfS5/L4uP0YY4Zj3xf+etJonzGmzBjzrjHm0tSFCS9F1jG8Hia02h4E//znyUSjbq66qiyVm1VKqePKsXK95SpgoYhEk8YNF5FS4IvAL40xo7ta0BhzczyxlFVVVR3yhgPhAK/Wf8DnPgZTWUksBo8/nk9JyRKKi1ce1s4opVR/lMqEUQEMTfpcHB/XlavY53KUiFTEf27B9pI7df/FQETmi0ipiJQWFBQccpA+l493r1zMt/8N7NnDe+/B1q0OLrlkEYHAlkNen1JK9VepTBjvAycbY0YaYzzYpLDf3U7GmLFADvBO0rgcY4w3/ns+tkuSD/ddtjcYY5hy8pmc1OSGvXvZEs8RkyY1EgxqwlBKqYSU3SUlIhFjzDeAxYATeExE1hljfgSUiUgieVwFLBCR5CfKxwGPGGNi2KR2T/LdVb3O4YCBA2HPHipy7agRI9K1hqGUUklSljAARORl9unVVkTu2OfzXV0s9zYwKZWx7aeoCPbupTwLMjMhL28w27ZVEI0G2t/Cp5RSJ7JjpdG77xUW2hpGBQwZAn6/7TwqGNzWt3EppdQxQhNGQryGkUgYPp+9KUvbMZRSytKEkVBYaC9JlQvFxR01DG3HUEopSxNGQlER0aiwe7etYbjdBTgc6QQCm/s6MqWUOiZowkgoLKSSgUSjhiFD7O22fv8ovSSllFJxmjASiooopxiAYvsDv3+0XpJSSqk4TRgJhYVUxLu6GhLv8crnszWMzo+IKKXUiUkTRkJR0X4Jw+8fRSwWIBTa04eBKaXUsUETRsKAAVQ4huFyRBk40I7y+RLPYuhlKaWU0oSRYAzlvpMY7K/DET8qfr99FkPbMZRSShNGJxXOYQxxVbZ/9vmGA0ZrGEophSaMTipigxiS1AO7w+HF6y3WZzGUUgpNGO1EoLwtn+Jw59qEzzdKL0kppRSaMNo1NkJLxMeQwCaIdrz4z+8fTTCoNQyllNKEEVcRvxI1RMqhurp9fHr6REKhPbS1dfeyQKWUOjFowogrL7c/iymHvXvbx2dnnw1AXd2SPohKKaWOHZow4tprGFTAno4H9TIypuBy5VBfv7RvAlNKqWOEJoy4RMIYzK5ONQxjHAwYcBb19VrDUEqd2DRhxJWXQ35eDB9tnWoYADk55xAMbiEY3NFH0SmlVN9LacIwxpxvjPnYGLPJGDOvi+nXGWOqjDEr48ONSdO+YozZGB++kso4wdYwhhQb8Ps71TAAsrPPAdBahlLqhJayhGGMcQIPAhcA44GrjTHju5j1GREpiQ+PxpfNBe4ETgVmAncaY3JSFSvEE8YQY1/Vuk8NIz19Ii5XnrZjKKVOaKmsYcwENonIFhEJAQuAS3q47GeAf4pIrYjUAf8Ezk9RnADt7/JOvKo1mTEOsrNn651SSqkTWioTxhBgZ9Ln8vi4fV1ujFltjFlojBl6iMv2irY2qKyMvzipixoG2MtSbW3bCQS2pioMpZQ6pvV1o/dLwAgRmYytRTx+qCswxtxsjCkzxpRVVVUdVhC7d9uf3dUwwDZ8g7ZjKKVOXKlMGBXA0KTPxfFx7USkRkTa4h8fBab3dNmkdcwXkVIRKS0oKDi8QBPPYAzB1jCqqyES6TRPWtp43O4CbcdQSp2wUpkw3gdONsaMNMZ4gKuAF5NnMMYMSvp4MfBR/PfFwHnGmJx4Y/d58XEp0f6UdzG2hiEC+9RWjDFkZ59Nff0SfWWrUuqElLKEISIR4BvYgv4j4FkRWWeM+ZEx5uL4bN8yxqwzxqwCvgVcF1+2Frgbm3TeB34UH5cS+9UwoCOLJLHtGOXa3blS6oTkSuXKReRl4OV9xt2R9Pt3ge92s+xjwGOpjC+hosI+fpGdDZSUgNcLV14Jf/kLTJnSPl/H8xhLSUs76WiEppRSx4y+bvQ+JpSX29qFMcDIkbBsGYTDcNppsGBB+3xpaafg8RRRW/v3vgtWKaX6iCYMbA2juDhpxMyZUFYG06fD1VfDnXcCth1j4MBrqK7+C8Hg9r4JViml+ogmDJIe2ktWVASvvQZXXAF3323fsAQUF38bYwzl5b88+oEqpVQfOuETRiwGu3Z1kTAAPB64/np719QHHwDg8w1l4MCr2LXrt4TDdUc3WKWU6kMnfMIwxrZhzJ3bzQzT44+GvP9++6ihQ79DLNbCrl2PpD5ApZQ6RmjCMFBQAPn53cwwcCAMG2bbNOIyMqaQk/NpKir+j1isrZsFlVKqfznhE0aPlJZ2ShgAQ4fOJRTaw969T/ZRUEopdXRpwuiJGTNg82ao62izyMn5FOnpU9i5835EYn0YnFJKHR2aMHqitNT+XL68fZQxhqFDv0Nr60dUVS3qo8CUUuro0YTRE100fAMMHDiHjIypbNjwVQKBbUc/LqWUOoo0YfRETg6MHr1fO4bD4WbChD8jEuPDD6/UBnClVL+mCaOnZszYL2EA+P2jGTv29zQ1vc/mzd/pg8CUUuro0ITRU6WlsGOHfTXfPgoKPk9x8X9RUfFrKiuf7YPglFIq9TRh9FQXDd/JRo36GVlZp/HxxzfQ0vJRl/MopdTxTBNGT02bZp/y6+KyFNj2jPHjn8XhSGPt2ksIh+uPcoBKKZVamjB6KjMTxo7d706pZD5fMRMmLCQY3MpHH12DSPQoBqiUUqmlCeNQdPHE976ys8/kpJMeoLb2ZbZuveOA8yql1PFEE8ahKC2F3btt97YHMHjw1xg06CZ27Ph/2giulOo3NGEcikTD90FqGcYYTj75V2RlfYKPPrqWvXufPgrBKaVUaqU0YRhjzjfGfGyM2WSMmdfF9NuMMR8aY1YbY14zxgxPmhY1xqyMDy+mMs4eKykBp/OA7RgJDoeXyZNfjieNL1Je/n9HIUCllEqdlCUMY4wTeBC4ABgPXG2MGb/PbB8ApSIyGVgI3Js0LSAiJfHh4lTFeUjS0uDUU+G+++wQPXCjtss1gMmT/05+/mVs2nQrW7Z8FxE5SsEqpVTvSmUNYyawSUS2iEgIWABckjyDiCwRkdb4x3eBYo51ixbBBRfA//wPnH46fHTgZy6cTh8TJjzL4MFfY8eOe1i37gp9U59S6riUyoQxBNiZ9Lk8Pq47NwCvJH32GWPKjDHvGmMuTUWAh6WoCJ57Dp56CjZuhKlT4eab4b337Ktcu2CMk5NPfohRo+6jpuYFysqmUF//xlEOXCmljswx0ehtjLkWKAXuSxo9XERKgS8CvzTGjO5m2ZvjiaWsqqrqKESLfYDv6qvhww/h2mvhySftparJk+Ghh+yLwvePk2HDvsPUqW/jcHhZufJstm69g1gsfHRiVkqpI5TKhFEBDE36XBwf14kx5lPA94GLRaS9u1cRqYj/3AIsBaZ2tRERmS8ipSJSWlBQ0HvR90RhITz6qL3N9uGHweeDr3/dJpG2rnuuzcqawfTpKygq+jLbt9/NihWzaG5ee3TjVkodX2pqur2CcTSlMmG8D5xsjBlpjPEAVwGd7nYyxkwFHsEmi8qk8TnGGG/893zgdODDFMZ6ZAYMgK9+1V6W+tnP4Omn4aKLoLGxy9ldrkzGjv09EyYsoq1tJ8uXT2P79p8Qi0WOcuBKHUfKyiAQ6Osojr6NG2HYMLjttr6OJHUJQ0QiwDeAxcBHwLMiss4Y8yNjTOKup/uADODP+9w+Ow4oM8asApYA94jIsZswEoyxjeFPPAH/+hfMng179nQ7e0HBZcyYsY78/M+zdevtrFgxk9raV49iwEodJ157zb5i4Kyz7MOzJwoR+Na3oLUVfvUrWLeuT8Mx/ek2z9LSUik7yEN1R83ixXD55fay1WuvwYgRB5y9snIhmzf/N21tO8jJ+RSjRt1DZub0oxOrUscyEZg5E3buhKYmyMuDv/7Vthn2dy+8AJdeCrffDg8+aB8eXrzYnpz2EmPM8nh78cHn1YSRQu+9B+efD+npNmmMGXPA2WOxNioqfsP27T8mEqlh4MAvMnr0fXi9g49SwOqYFg7bO/Q+9zn7TFBvamkBvx8cx8R9MJ0tWgRf+AL84Q8waZLd/6YmWLAALryw7+KKxeDPf7aXjBob7WAM3HornHLKka8/EIDx4yEjAz74AH7zG1vbeOEFuLj3Hk07lISBiPSbYfr06XLMWbVKpKBApLBQZM2aHi0SDjfI5s3fl6VLvbJsWYbs2PFziUZDqYnvX/8S2bAhNetWvevBB0VA5Mtf7r11fvihyI03ini9Ijfd1Hvr7S3hsMgpp4iMHy8Sidhx5eUiU6faY3HaaSK//a1IY+PRjWvdOrttW/8R8flEBg4USUsT8XhEfvQjkba2jvmbmkT++leRjz/ef12xmN2HT35SZOFCkWjUjr/jDrvupUvt51BIZNw4kdGjRYLBXtsVoEx6WMb2eSHfm8MxmTBERD76SGTwYJHcXJGHHhJZvbrzP/8vfiFy6qkiWVkiJ58scuaZIldeKaE7b5MNT35ClryK/PvfE2TXrkclFKrtvbjeekvE6RQZOVKktbX31nukAgE7qA5tbSLDhomkp9uv7eOPH9ryr78u8pnPiFx2mcgtt4jcdZfIZz/bUdjNmmV//9vfer7OWMwWgE89JVJff2jx9NSjj9q4/vKXzuObm0Xuv98WoGAL6nPPtft0+eUi115rT4Z6WzBoj53bbb/Pjz/eOTHs3i0yZ46Nafx4kXvvtcfd47Hj3G6Ru++2hb+ISEuLyHXX2WlZWfbnpEm2nPB6Ra6+uvP2//53O8/PfmaT6dq19vj/+teHvUuaMI5FmzeLjB0r7WckWVkiU6aIGGM/l5SIfP3r9p9t9mybOOLzRrMzpPrcTHn3T8jSpW5ZvfpzsmfPUxKJHEGhWl0tMnSoPSsCkTvv7K09PTJ799ozyhEjRNav7+toei4Ws4XFsmW2kPvhD0XmzRP59rdFbr5Z5JFHOk4SDqS2VmTr1v3H/+539u/00ksiZ51lE0dXZ6td+fOfbYE1ZIgtxPLy7Lry823hV1lpC8KJE+2JTe0+JyXvvGP347bb7D794Af2/7SoqOP/+cwzez/Jt7bamGfNsse3K7GYje+mm+x8U6d27KPbLfL00/svU1NjT+K6smGDyNy5drnks/hIROSJJ+zZPYh88Yv2f7U7f/2r/X6B/S7fdpst7K+6yo6bOlXkxRdtGQC2NtHWJvKnP9n/fxDJyBCpqNh/3Z/7nN03r7fj+Ofmdn+MDkITxrEqFhPZtMn+491yi8g559iqa3df/MpK+497/fUSy86WaFG+bHvlOnn77WJZsgR588182bTpf6S1dfOhx/HZz9pCpKzM/hN7vTa2vlRfbxOn328v4+Xlibz77tHZ9o4d9kytvLzny8RiIm++aS/p5OR0fHkTg9ttTwxyc+3n0lKR5cu7X19trciYMbagWL26Y3w4LHLSSSLTp9tt7txp1zl16sEvTTzyiD0p+cQnOieCUMiuN1lZma1xfuUrHePmz7f74fPZuBJnykOG2ELz4Yc7LpV94Qsdl1P2PU49EQjYAvu990T++U+Rb31LOl2SORR1dTaxgq3Bi9hayd13i2Rm2vFnn223E4vZ/73vfMfua+Lvl59vx/3+9x0neyUlIq+80rMYWltFtm3bf/yiRR0najk5+9fqIhGb5F9/vev1bt0qcsUVNrH98Y/2fyW5lnOINGH0R+vW2XaQgQMltnqV1NT8U9asuUyWLHHKkiVGPvjgk1Je/msJBHYefF3332//9A88YD+Xl9vC4MILD/ss5Yi1tIiccYb9wr7yisjGjSKjRtlLDS+/3DvbaG62Z37PPy/y3HP2evEPfmALgUQhkZNjv9AH0tZmj+FJJ9ll0tNFvvQlezxfecUm3lBSm1MsZpNRYaGIw2ELwoaGzusMhew1bI/HFiYjRtgTBhGRJ5+023n++Y75X3xR2tsz3nvPHr+EQEBk5UqR733PznPRRZ2nH8jtt9tlnnvOntSAyPnn2wI4eX/29fOf23lvvdV+jkREFiwQmTzZJqH8fHvm/IlP2P+zL37Rrv/WW+0Z8+jR9tjsm3QvuaRncXclELCX4KBzjejSS0XuucfWphKJvKDAJtbrr7ffh8WL7aUtl0vaLy8lty8cqepqkZ/+tOva5FGmCaO/Wr/e/pPn5Yn8+98ia9ZI6MmHpfbWs2XbrQPlrYXIkiVIWdkM2bDh27Jrw/9Jyy/mSvTC8+wX54YbRL75TfsluOyyzl/8RBJ54YWDx7F7t7008uMfi/zhDyJbthw80YTDtsBuarINlPX1tkq/bZvdrwsusF/YZ57pvJ2pU2283/72wW8a2L7dLt/V5YYdO+y14X0LJIfDXk752c9EXnvNFh5gLyM1N++/nvfe61jPWWfZ/W9qOvgxE7GF7i232P0cOrQjEcZitpaSaJt47z17Rn/GGbbQGz/eXi7at7D6znc69sMYWzs5+eTOBe+XvtQ5eR1MW1vn4/Q//9OzS2mxmP0bgd2XxBn52LEi//u/Il/7mj0r/uQnRaZNswkiP9+eEEycaKfdcYfd/xdftJf2Vq3avxZ0qCKRjsR3xhm23S4hGLQ1pHHjbG2jrGz/5XfvFlmypGfH4Dh1KAlDb6s93mzaBJ/8pL0nPcEYEEEcDoJnncTec2K4V2+j8O8RXC3QOgSMLw1PiwdHUxgzZgy8/jpkZ3esIxy27/toabG3Kw4ebDtaFIEVK+Ctt+Dtt+2twhX79fACxcX2XnkR+5BRa6u99bG2Furq7O8H88gjtiPHZI2NtruVZ56xMc6cCVddBR6PjbWlBbZsgTfegO3b7TIul32A8vbb7a2iK1faJ++bmuB3v4OTTrLHzBgYOhRyczu2FwrBD34A994Lo0bBuefCuHF2+Oc/4Ze/tMflN785/Fsb33kHbrjB9nT8pS/B6NFw113w/e/Dj39s51mwwPZXNm2aPf5PP233e1+bN8Pq1R2DwwETJtjbMRM/D/We/ZUrbVzf+56NoadiMZgzBxYutM9I3H47XHaZfYdMXxKBrVth5MhefX6hv9DnMPq7HTtsgTJ0qC3IxoyxCeSJJ+CPf4SdOxGPh9hlF9L85TOoPaWRyqoFBAIbMMZDdvbZpKdPIj19HGlp4/B4inA4/DjfKMP5mUsxyZ0nulwQiXdZMmoUzJpln7gtLYUpU2wh/cYbsGyZvVfc47HPCKSl2fvHc3MhJ8cOPp/9wjoc9qfP1zGMHAmnndb9PldXw5/+ZAv8tfv0vVVYCGeeaYcZM2ziefxxmxi+9jVbGGdnw8sv2/v4e+LVV23hvXat7ccn4ZZb4Kc/td3BHIm2NvjJT+y6IhH7nMEzz3R+DuKOO+Duu+Hkk21y6euCtyfCYZtwpk8/Np/pUPvRhHEii8VsnzsjRsDAge2jRYSmpuVUVj5JXd0SAoGPicWC+y3uq4Ci+lMZGDkLf306pq3NFsKnnWbPrPuaiO0awuWyD0R297DZa6/ZZLFpk01sf/sbDDlQ7/oHUFVlC+zs7N5/unjVKvjLX2Du3P0fxovFbEI5+2z77hWlUkAThjookSjB4HZaWj4kEqkhFgsSjQYIhSrYvfv3RCI1ZGXNoqjoBoxxEYu1Eo224vMNJz//YhwOb1/vwsEFAvDii/Zp4MzMvo5GqWOSJgx1RKLRVvbs+T07d/5/BINb9pvudhcwaNANDBp0M37/yD6IUCnVWzRhqF4hEqW1dSMOhxenMx2Hw09j49vs2vUw1dUvAjHc7kLc7nzc7nw8ngK83uH4fCPw+0fi843E5xuF0+k7ojhisQgOh6t3dkop1cmhJAz9FqpuGeMkPX1sp3G5uZ8hN/czBIPl7N37R4LBrYTD1YTD1TQ3r6a6+iWS3oMFGLzeofj9J5OWdgppaWNJSzsFv38MbncuTmc6xnTdmBsKVbJz531UVDxEbu4FnHLKfNzu3C7nVUqlntYwVK8SiREKVRIMbiMY3Exr60YCgU0EAhtpbf2YaLRhv2UcDh8uVy4ZGVPIyJhKRsZUGhvfZdeuh4jF2sjNPY+6utfweAoZN+5JsrPP6oM9U6p/0hqG6jPGOPB6i/B6ixgwYFanaSJCKLSXQOBjWls3Eo02EI22EI02Ewrtpbl5JbW1/wCigIPCwmsYPvx20tLG0NS0nA8/vJqVK89hyJCv43CkEQxuJRjcgkiE9PTJ8YQzBXDQ1lZBKLSLcLiajIxp5OSci8dzlF/hq1Q/ozUMdUyJRoO0tKzF7c7H7x/RaVok0symTd9kz54/YIwbn284Pt9IwEFLy2pCof3fxGaMC/vyR8jImEZ29mz8/pPw+Ubg840gHK6moeEN6uvfoKnp3xjjxuMpwuMpxOMZQmZmKVlZs8jImILD4T4KR0Cpo0sbvVW/Fg7X4XJl7df2EQpV0dKyGttuMgSPZwhOp5+mpuXU1f2T2tp/0Nj4733aWKy0tAkMGPAJwEEotIdQaA/B4DbC4b2AvWzm9Q4jFgsSiwWIxYK43Xn4/aeQljYGn28ksVhre3tONBrA7c5rvyHA5RqAw+GPDz5EQvHaVQsiEbzeQXi9Q/F6h+Jw+AmFdrfH4XJl4fONwucbjsPhOQpHWJ1INGEo1Q3bxrI33sayFaczgwEDTsftzutiXqGtbSeNje/S2PgubW0V9ol4px9jvITDlbS2fkwgsIFotBkApzMDtzsfh8NPOFxDOFyDvcTWGxx4PIPisbURi7XFxxXGhyJcrhyczjQcjjQcDi/hcBXB4A7a2nYSidSTljaWjIwppKdPwe3OiU/bQTC4E4+nIN6GVEJa2inEYkFCoSrC4ap4gizA4ynE5bJdykSjLUQidUQi9Rjjaj82DocPcAAGYwzQMRjj6NWkJyIEAhvilzINAwdegcdT2GvrPxEcMwnDGHM+8H+AE3hURO7ZZ7oXeAKYDtQAc0RkW3zad4EbsN+2b4nI4oNtTxOG6gsiQiRSi8ORvt8txCIxIpF6otEmotFAe+3E4fDE57d3ibW17aKtbSdtbTuJxYJ4PIPaL41FIg0Eg1sIBDbT1rYTcOBweHE4vIhECYUq47WR3USjjUSjrUSjLUAUp3MAPt8wvN6hOJ2ZtLZ+SGvrR+2X6SwHHk8R4XA1IqH2cRCjK8a44/sWPqzj5XYXkpFREk9MYwiF9hAIbCYQ2EwkUoPDkRZPeun4fEPj7VOTSUsbRzhcG7+JYhMtLauorf0HbW07ktbuJC/vAgoLv4TXO4xotJlYrIVotDUeeyKRuXE6M+JDOtFoE8HgDoLB7YRCu3A40uI1xDxcrpykZGgTojGe+N/AEz9WYJOiIBJBJEQsFiYabaStraK9Tc3lyiEjYyqZmVO7PUmJRpuJRhuJxUIY48QYV3xwt//du7uz8HAcEwnD2D3aAHwaKAfeB64WkQ+T5vlPYLKIfM0YcxXweRGZY4wZDzwNzAQGA68CY0TkgKdqmjCU6tDd8yuxWBstLR8RjTbh8w3D4xmCw+EiFgvT2rqe5uaVtLaux+XKwu0uwO0uiNdWqgmF9hIK7cUYg8uVg8uVi8s1AJFoPBkG4l3OCCIxQOK/J35GCQY309y8kpaWde1Jx+MpwucbjcdTEE+s9nJdMLiVSKS+y/1zOgeQk3MuubnnkZNzHrFYgD17nmDv3j8SCu067OPmdGYRiwWTkmdvcZJc2/R4BuNweInFQoiEicXaiEYbscfs4OtyOtPiz0el4fUOYerUZYcV1bFyl9RMYJOIbIkHtQC4BPgwaZ5LgLvivy8Efm1sHfYSYIHYi81bjTGb4ut7J4XxKtWvdPewo8PhJTOzpIvxbjIyJpGR0cMOGo9QLBairW0nHk8RTmd6l/PYy4LltLSspqXlo/jNECfh95+Ex1MYv+TVYfToexg16ic0NLxFNNrSXotwOPwYY+JJLEYsFmq/Qy8abcLlylSwFZYAAAa3SURBVMLrtTUxlysjfqbfQiRSQzhcl5QMA/F2rFC8FtFGR0K0HA43xnjaazFe72C83iG43QVEInU0NX1Ac/MHtLSsA2IY426vPbhcWTidA+JtdB4gikg0nlDC7dtMtKVFoy3EYq04HP6U/Z2SpTJhDAGS+uCmHDi1u3lEJGKMaQDy4uPf3WfZw+w5Til1LHI4PPj9ow84jzEGn28oPt9Q8vIu6tF6jXEe8bM6tgaVgcuVgc83/IjWlcztziM391Pk/v/t3UvMXVUZxvH/o1Wk1FCqlUCLtECDopGijQERQoABIEEGIOUWQkKc1AhGo2A0RBIHJEZwQLgENFUbw63ExoGohTQwsKVQQKAQjAiUFPsZLoqJXB8Hax04rU26KJxv73Y/v8n37cvZWWflPXnPWWvv9c458X275nTa5dcflvR1SeslrZ+amuq6ORERu61JJozngAPGtufXfds9R9IMYG/K5HfLawGwfYPtJbaXzJ2bB7MiIiZlkgnjPmCRpIUqg3FLgVXbnLMKuKD+fwZwVy0ZuApYKmkPSQuBRcC6CbY1IiJ2YGJzGHVO4hvAnZTbA35u+1FJV1BqyK4CbgJ+VSe1X6AkFep5t1AmyN8Alu3oDqmIiJisPLgXETFg7+a22l1+0jsiIqZHEkZERDRJwoiIiCa71RyGpCng6Z18+ceBf76PzdldpZ/apJ/apJ/aTaqvDrTd9EzCbpUw3gtJ61snfoYs/dQm/dQm/dSuD32VIamIiGiShBEREU2SMN5xQ9cN2EWkn9qkn9qkn9p13leZw4iIiCb5hREREU0GnzAknSTpCUl/lXRp1+3pC0kHSLpb0mOSHpV0cd0/R9IfJT1Z/+7TdVv7QNIHJW2Q9Lu6vVDS2hpXN9cFOAdP0mxJt0l6XNJGSUclpv6fpG/Vz90jkn4j6SN9iKlBJ4xaRvYa4GTgMODsWh42yqKP37Z9GHAksKz2zaXAatuLgNV1O+BiYOPY9pXAVbYPAV6k1KcP+Bnwe9ufAg6n9FliaoykecA3gSW2P0tZvHUpPYipQScMxsrIuhTwHZWRHTzbm20/UP//N+WDPY/SP8vracuB07tpYX9Img98Bbixbgs4nlJ2GNJPAEjaGziWsko1tl+z/RKJqe2ZAexZ6wTNBDbTg5gaesLYXhnZlILdhqQFwBHAWmBf25vroeeBfTtqVp9cDXwXeKtufwx4yfYbdTtxVSwEpoBf1OG7GyXtRWJqK7afA34CPENJFC8D99ODmBp6wogdkDQLuB24xPa/xo/VYleDvs1O0qnAFtv3d92WXcAM4PPAtbaPAP7DNsNPiSmoczhfpSTY/YG9gJM6bVQ19ITRXAp2iCR9iJIsVtheWXf/Q9J+9fh+wJau2tcTRwOnSfo7ZUjzeMo4/ew6nACJq5FNwCbba+v2bZQEkpja2onAU7anbL8OrKTEWecxNfSE0VJGdpDqOPxNwEbbPx07NF5W9wLgt9Pdtj6xfZnt+bYXUOLnLtvnAndTyg5D+gkA288Dz0o6tO46gVJVMzG1tWeAIyXNrJ/DUT91HlODf3BP0imUMehRGdkfd9ykXpD0ZeAe4C+8Mzb/fco8xi3AJykrA3/N9gudNLJnJB0HfMf2qZIOovzimANsAM6z/WqX7esDSYspNwd8GPgbcCHli2tiaoykHwFnUe5W3ABcRJmz6DSmBp8wIiKizdCHpCIiolESRkRENEnCiIiIJkkYERHRJAkjIiKaJGFE9ICk40Yr3Ub0VRJGREQ0ScKIeBcknSdpnaQHJV1f62C8IumqWr9gtaS59dzFkv4s6WFJd4zqPEg6RNKfJD0k6QFJB9fLzxqrFbGiPuUb0RtJGBGNJH2a8vTt0bYXA28C51IWh1tv+zPAGuDy+pJfAt+z/TnKE/Oj/SuAa2wfDnyJsiIplBWBL6HUZjmIsn5QRG/M2PEpEVGdAHwBuK9++d+TslDeW8DN9ZxfAytr7YfZttfU/cuBWyV9FJhn+w4A2/8FqNdbZ3tT3X4QWADcO/m3FdEmCSOinYDlti/baqf0w23O29n1dsbXBXqTfD6jZzIkFdFuNXCGpE/A2/XND6R8jkariJ4D3Gv7ZeBFScfU/ecDa2r1wk2STq/X2EPSzGl9FxE7Kd9gIhrZfkzSD4A/SPoA8DqwjFII6Iv12BbKPAeUJaivqwlhtDIrlORxvaQr6jXOnMa3EbHTslptxHsk6RXbs7puR8SkZUgqIiKa5BdGREQ0yS+MiIhokoQRERFNkjAiIqJJEkZERDRJwoiIiCZJGBER0eR/eCGW1G+tTHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1830 - acc: 0.9533\n",
      "Loss: 0.18304576338853915 Accuracy: 0.95327103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_conv_3_VGG_he-uniform_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.3940 - acc: 0.5680\n",
      "Loss: 1.3939541005642615 Accuracy: 0.5680166\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 875us/sample - loss: 1.0441 - acc: 0.6793\n",
      "Loss: 1.044071129400782 Accuracy: 0.6793354\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 924us/sample - loss: 0.7888 - acc: 0.7759\n",
      "Loss: 0.7887800250469339 Accuracy: 0.7759086\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 935us/sample - loss: 0.4060 - acc: 0.8831\n",
      "Loss: 0.40602451181609805 Accuracy: 0.88307375\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2113 - acc: 0.9458\n",
      "Loss: 0.2112701848712155 Accuracy: 0.9457944\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 982us/sample - loss: 0.1693 - acc: 0.9593\n",
      "Loss: 0.16933327621576277 Accuracy: 0.9592939\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1830 - acc: 0.9533\n",
      "Loss: 0.18304576338853915 Accuracy: 0.95327103\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_he-uniform_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.7883 - acc: 0.5948\n",
      "Loss: 2.7882802063059584 Accuracy: 0.59480786\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6423 - acc: 0.7350\n",
      "Loss: 1.642292417296492 Accuracy: 0.7349948\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.1940 - acc: 0.7996\n",
      "Loss: 1.1940153389085986 Accuracy: 0.7995846\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4735 - acc: 0.9034\n",
      "Loss: 0.4734820108392652 Accuracy: 0.90342677\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2778 - acc: 0.9468\n",
      "Loss: 0.2778356967471588 Accuracy: 0.94683284\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2212 - acc: 0.9583\n",
      "Loss: 0.22119335982880722 Accuracy: 0.95825547\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_he-uniform_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2779 - acc: 0.9462\n",
      "Loss: 0.27794505369521655 Accuracy: 0.9462098\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
