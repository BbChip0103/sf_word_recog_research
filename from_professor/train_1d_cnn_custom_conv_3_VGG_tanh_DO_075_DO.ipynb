{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0494 - acc: 0.3454\n",
      "Epoch 00001: val_loss improved from inf to 1.68682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/001-1.6868.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.0494 - acc: 0.3454 - val_loss: 1.6868 - val_acc: 0.4447\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5249 - acc: 0.5171\n",
      "Epoch 00002: val_loss improved from 1.68682 to 1.49431, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/002-1.4943.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.5248 - acc: 0.5171 - val_loss: 1.4943 - val_acc: 0.5311\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3214 - acc: 0.5831\n",
      "Epoch 00003: val_loss improved from 1.49431 to 1.42791, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/003-1.4279.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.3215 - acc: 0.5831 - val_loss: 1.4279 - val_acc: 0.5518\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1832 - acc: 0.6272\n",
      "Epoch 00004: val_loss improved from 1.42791 to 1.38964, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/004-1.3896.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1832 - acc: 0.6272 - val_loss: 1.3896 - val_acc: 0.5721\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0877 - acc: 0.6569\n",
      "Epoch 00005: val_loss improved from 1.38964 to 1.36698, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/005-1.3670.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0876 - acc: 0.6569 - val_loss: 1.3670 - val_acc: 0.5835\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9891 - acc: 0.6857\n",
      "Epoch 00006: val_loss did not improve from 1.36698\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9890 - acc: 0.6858 - val_loss: 1.3749 - val_acc: 0.5826\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9064 - acc: 0.7127\n",
      "Epoch 00007: val_loss did not improve from 1.36698\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9064 - acc: 0.7127 - val_loss: 1.3689 - val_acc: 0.5870\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8453 - acc: 0.7324\n",
      "Epoch 00008: val_loss did not improve from 1.36698\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8452 - acc: 0.7324 - val_loss: 1.4044 - val_acc: 0.5961\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7712 - acc: 0.7547\n",
      "Epoch 00009: val_loss improved from 1.36698 to 1.35266, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/009-1.3527.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7715 - acc: 0.7547 - val_loss: 1.3527 - val_acc: 0.5989\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.7695\n",
      "Epoch 00010: val_loss improved from 1.35266 to 1.34829, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv_checkpoint/010-1.3483.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7256 - acc: 0.7695 - val_loss: 1.3483 - val_acc: 0.6108\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6634 - acc: 0.7878\n",
      "Epoch 00011: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6634 - acc: 0.7878 - val_loss: 1.3843 - val_acc: 0.5993\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6226 - acc: 0.7996\n",
      "Epoch 00012: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6226 - acc: 0.7997 - val_loss: 1.3928 - val_acc: 0.6098\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.8169\n",
      "Epoch 00013: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5697 - acc: 0.8169 - val_loss: 1.3832 - val_acc: 0.6196\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.8285\n",
      "Epoch 00014: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5361 - acc: 0.8285 - val_loss: 1.3844 - val_acc: 0.6177\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.8365\n",
      "Epoch 00015: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5046 - acc: 0.8365 - val_loss: 1.3763 - val_acc: 0.6257\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8468\n",
      "Epoch 00016: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4759 - acc: 0.8468 - val_loss: 1.3981 - val_acc: 0.6296\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8554\n",
      "Epoch 00017: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4464 - acc: 0.8554 - val_loss: 1.3992 - val_acc: 0.6280\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8613\n",
      "Epoch 00018: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4198 - acc: 0.8613 - val_loss: 1.4225 - val_acc: 0.6308\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8721\n",
      "Epoch 00019: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3960 - acc: 0.8721 - val_loss: 1.4497 - val_acc: 0.6296\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8789\n",
      "Epoch 00020: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3784 - acc: 0.8789 - val_loss: 1.4355 - val_acc: 0.6292\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8871\n",
      "Epoch 00021: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3479 - acc: 0.8871 - val_loss: 1.4404 - val_acc: 0.6350\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8923\n",
      "Epoch 00022: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3340 - acc: 0.8923 - val_loss: 1.4230 - val_acc: 0.6434\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8944\n",
      "Epoch 00023: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3195 - acc: 0.8944 - val_loss: 1.4665 - val_acc: 0.6422\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9016\n",
      "Epoch 00024: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3034 - acc: 0.9016 - val_loss: 1.4505 - val_acc: 0.6506\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9059\n",
      "Epoch 00025: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2922 - acc: 0.9059 - val_loss: 1.4599 - val_acc: 0.6534\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9093\n",
      "Epoch 00026: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2813 - acc: 0.9093 - val_loss: 1.4490 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9157\n",
      "Epoch 00027: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2598 - acc: 0.9157 - val_loss: 1.4683 - val_acc: 0.6580\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9162\n",
      "Epoch 00028: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2585 - acc: 0.9162 - val_loss: 1.4793 - val_acc: 0.6564\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9207\n",
      "Epoch 00029: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2451 - acc: 0.9207 - val_loss: 1.4939 - val_acc: 0.6494\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9234\n",
      "Epoch 00030: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2367 - acc: 0.9234 - val_loss: 1.5098 - val_acc: 0.6553\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9265\n",
      "Epoch 00031: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2269 - acc: 0.9265 - val_loss: 1.5299 - val_acc: 0.6587\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9281\n",
      "Epoch 00032: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2238 - acc: 0.9281 - val_loss: 1.5072 - val_acc: 0.6636\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9318\n",
      "Epoch 00033: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2157 - acc: 0.9318 - val_loss: 1.5211 - val_acc: 0.6622\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9349\n",
      "Epoch 00034: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2024 - acc: 0.9348 - val_loss: 1.5304 - val_acc: 0.6599\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9351\n",
      "Epoch 00035: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2028 - acc: 0.9351 - val_loss: 1.5502 - val_acc: 0.6627\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9366\n",
      "Epoch 00036: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1969 - acc: 0.9366 - val_loss: 1.5748 - val_acc: 0.6711\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9403\n",
      "Epoch 00037: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1891 - acc: 0.9403 - val_loss: 1.5669 - val_acc: 0.6629\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9413\n",
      "Epoch 00038: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1853 - acc: 0.9413 - val_loss: 1.5398 - val_acc: 0.6695\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9435\n",
      "Epoch 00039: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1739 - acc: 0.9435 - val_loss: 1.5713 - val_acc: 0.6671\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9457\n",
      "Epoch 00040: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1698 - acc: 0.9457 - val_loss: 1.6121 - val_acc: 0.6650\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9464\n",
      "Epoch 00041: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1661 - acc: 0.9464 - val_loss: 1.5695 - val_acc: 0.6709\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9477\n",
      "Epoch 00042: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1620 - acc: 0.9477 - val_loss: 1.5988 - val_acc: 0.6737\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9468\n",
      "Epoch 00043: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1679 - acc: 0.9468 - val_loss: 1.5904 - val_acc: 0.6753\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9509\n",
      "Epoch 00044: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1552 - acc: 0.9509 - val_loss: 1.6032 - val_acc: 0.6753\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9511\n",
      "Epoch 00045: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1543 - acc: 0.9511 - val_loss: 1.6367 - val_acc: 0.6744\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9513\n",
      "Epoch 00046: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1498 - acc: 0.9512 - val_loss: 1.6476 - val_acc: 0.6690\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9525\n",
      "Epoch 00047: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1520 - acc: 0.9525 - val_loss: 1.6522 - val_acc: 0.6711\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9519\n",
      "Epoch 00048: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1535 - acc: 0.9519 - val_loss: 1.6519 - val_acc: 0.6702\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9563\n",
      "Epoch 00049: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1391 - acc: 0.9563 - val_loss: 1.6624 - val_acc: 0.6646\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9556\n",
      "Epoch 00050: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1383 - acc: 0.9555 - val_loss: 1.6354 - val_acc: 0.6762\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9543\n",
      "Epoch 00051: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1436 - acc: 0.9543 - val_loss: 1.6585 - val_acc: 0.6781\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9564\n",
      "Epoch 00052: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1384 - acc: 0.9564 - val_loss: 1.6058 - val_acc: 0.6825\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9572\n",
      "Epoch 00053: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1363 - acc: 0.9572 - val_loss: 1.6352 - val_acc: 0.6795\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9570\n",
      "Epoch 00054: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1348 - acc: 0.9570 - val_loss: 1.6543 - val_acc: 0.6830\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9584\n",
      "Epoch 00055: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1306 - acc: 0.9584 - val_loss: 1.6917 - val_acc: 0.6744\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9592\n",
      "Epoch 00056: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1335 - acc: 0.9592 - val_loss: 1.6466 - val_acc: 0.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9585\n",
      "Epoch 00057: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1299 - acc: 0.9585 - val_loss: 1.6890 - val_acc: 0.6778\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9620\n",
      "Epoch 00058: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1209 - acc: 0.9620 - val_loss: 1.7114 - val_acc: 0.6813\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9594\n",
      "Epoch 00059: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1270 - acc: 0.9594 - val_loss: 1.6602 - val_acc: 0.6834\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9611\n",
      "Epoch 00060: val_loss did not improve from 1.34829\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1208 - acc: 0.9611 - val_loss: 1.7122 - val_acc: 0.6802\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 904us/sample - loss: 1.4452 - acc: 0.5736\n",
      "Loss: 1.4451579620781223 Accuracy: 0.5736241\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9469 - acc: 0.3704\n",
      "Epoch 00001: val_loss improved from inf to 1.48889, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/001-1.4889.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.9468 - acc: 0.3704 - val_loss: 1.4889 - val_acc: 0.5532\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4218 - acc: 0.5519\n",
      "Epoch 00002: val_loss improved from 1.48889 to 1.24301, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/002-1.2430.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.4218 - acc: 0.5519 - val_loss: 1.2430 - val_acc: 0.6303\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2461 - acc: 0.6144\n",
      "Epoch 00003: val_loss improved from 1.24301 to 1.16080, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/003-1.1608.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2462 - acc: 0.6143 - val_loss: 1.1608 - val_acc: 0.6534\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1340 - acc: 0.6540\n",
      "Epoch 00004: val_loss improved from 1.16080 to 1.11664, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/004-1.1166.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1340 - acc: 0.6540 - val_loss: 1.1166 - val_acc: 0.6664\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0500 - acc: 0.6810\n",
      "Epoch 00005: val_loss improved from 1.11664 to 1.05049, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/005-1.0505.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0501 - acc: 0.6809 - val_loss: 1.0505 - val_acc: 0.6911\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9848 - acc: 0.6986\n",
      "Epoch 00006: val_loss improved from 1.05049 to 1.03672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/006-1.0367.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9848 - acc: 0.6986 - val_loss: 1.0367 - val_acc: 0.6907\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7188\n",
      "Epoch 00007: val_loss improved from 1.03672 to 1.01605, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/007-1.0160.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9251 - acc: 0.7187 - val_loss: 1.0160 - val_acc: 0.6867\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7363\n",
      "Epoch 00008: val_loss improved from 1.01605 to 1.00786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/008-1.0079.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8742 - acc: 0.7363 - val_loss: 1.0079 - val_acc: 0.6993\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8268 - acc: 0.7480\n",
      "Epoch 00009: val_loss improved from 1.00786 to 1.00237, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/009-1.0024.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8268 - acc: 0.7480 - val_loss: 1.0024 - val_acc: 0.7021\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.7637\n",
      "Epoch 00010: val_loss improved from 1.00237 to 0.95375, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/010-0.9537.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7788 - acc: 0.7637 - val_loss: 0.9537 - val_acc: 0.7193\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.7728\n",
      "Epoch 00011: val_loss did not improve from 0.95375\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7425 - acc: 0.7729 - val_loss: 0.9551 - val_acc: 0.7251\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7090 - acc: 0.7839\n",
      "Epoch 00012: val_loss improved from 0.95375 to 0.94756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/012-0.9476.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7091 - acc: 0.7839 - val_loss: 0.9476 - val_acc: 0.7300\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6758 - acc: 0.7951\n",
      "Epoch 00013: val_loss improved from 0.94756 to 0.93816, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/013-0.9382.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6758 - acc: 0.7951 - val_loss: 0.9382 - val_acc: 0.7375\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6457 - acc: 0.8018\n",
      "Epoch 00014: val_loss improved from 0.93816 to 0.90976, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/014-0.9098.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6457 - acc: 0.8018 - val_loss: 0.9098 - val_acc: 0.7396\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8129\n",
      "Epoch 00015: val_loss did not improve from 0.90976\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6148 - acc: 0.8129 - val_loss: 0.9232 - val_acc: 0.7393\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5879 - acc: 0.8185\n",
      "Epoch 00016: val_loss did not improve from 0.90976\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5879 - acc: 0.8184 - val_loss: 0.9107 - val_acc: 0.7461\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.8260\n",
      "Epoch 00017: val_loss improved from 0.90976 to 0.89244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/017-0.8924.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5669 - acc: 0.8261 - val_loss: 0.8924 - val_acc: 0.7491\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.8334\n",
      "Epoch 00018: val_loss did not improve from 0.89244\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5411 - acc: 0.8334 - val_loss: 0.9048 - val_acc: 0.7419\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.8395\n",
      "Epoch 00019: val_loss improved from 0.89244 to 0.88747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/019-0.8875.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5248 - acc: 0.8395 - val_loss: 0.8875 - val_acc: 0.7501\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8441\n",
      "Epoch 00020: val_loss did not improve from 0.88747\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5056 - acc: 0.8441 - val_loss: 0.8880 - val_acc: 0.7489\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8504\n",
      "Epoch 00021: val_loss did not improve from 0.88747\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4833 - acc: 0.8505 - val_loss: 0.9093 - val_acc: 0.7433\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.8530\n",
      "Epoch 00022: val_loss improved from 0.88747 to 0.88727, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/022-0.8873.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4700 - acc: 0.8530 - val_loss: 0.8873 - val_acc: 0.7552\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8551\n",
      "Epoch 00023: val_loss did not improve from 0.88727\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4584 - acc: 0.8550 - val_loss: 0.8922 - val_acc: 0.7496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.8599\n",
      "Epoch 00024: val_loss did not improve from 0.88727\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4407 - acc: 0.8599 - val_loss: 0.9017 - val_acc: 0.7510\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8663\n",
      "Epoch 00025: val_loss did not improve from 0.88727\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4277 - acc: 0.8663 - val_loss: 0.8910 - val_acc: 0.7580\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8719\n",
      "Epoch 00026: val_loss improved from 0.88727 to 0.88101, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/026-0.8810.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4099 - acc: 0.8719 - val_loss: 0.8810 - val_acc: 0.7594\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8743\n",
      "Epoch 00027: val_loss did not improve from 0.88101\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3984 - acc: 0.8742 - val_loss: 0.8818 - val_acc: 0.7582\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8752\n",
      "Epoch 00028: val_loss improved from 0.88101 to 0.87705, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv_checkpoint/028-0.8770.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3912 - acc: 0.8752 - val_loss: 0.8770 - val_acc: 0.7671\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8801\n",
      "Epoch 00029: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3752 - acc: 0.8801 - val_loss: 0.9030 - val_acc: 0.7540\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8826\n",
      "Epoch 00030: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3658 - acc: 0.8826 - val_loss: 0.8864 - val_acc: 0.7678\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8833\n",
      "Epoch 00031: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3608 - acc: 0.8832 - val_loss: 0.8980 - val_acc: 0.7654\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8860\n",
      "Epoch 00032: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3535 - acc: 0.8859 - val_loss: 0.9051 - val_acc: 0.7654\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8902\n",
      "Epoch 00033: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3422 - acc: 0.8902 - val_loss: 0.9012 - val_acc: 0.7706\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8902\n",
      "Epoch 00034: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3394 - acc: 0.8902 - val_loss: 0.8991 - val_acc: 0.7598\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8948\n",
      "Epoch 00035: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3269 - acc: 0.8948 - val_loss: 0.8887 - val_acc: 0.7692\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8969\n",
      "Epoch 00036: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3164 - acc: 0.8969 - val_loss: 0.9045 - val_acc: 0.7694\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8994\n",
      "Epoch 00037: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3131 - acc: 0.8994 - val_loss: 0.8838 - val_acc: 0.7701\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8980\n",
      "Epoch 00038: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3101 - acc: 0.8980 - val_loss: 0.9062 - val_acc: 0.7675\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9021\n",
      "Epoch 00039: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3005 - acc: 0.9021 - val_loss: 0.9357 - val_acc: 0.7652\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9053\n",
      "Epoch 00040: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2945 - acc: 0.9053 - val_loss: 0.9207 - val_acc: 0.7680\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9064\n",
      "Epoch 00041: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2900 - acc: 0.9064 - val_loss: 0.8971 - val_acc: 0.7803\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9081\n",
      "Epoch 00042: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2810 - acc: 0.9081 - val_loss: 0.9044 - val_acc: 0.7754\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9110\n",
      "Epoch 00043: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2751 - acc: 0.9110 - val_loss: 0.9317 - val_acc: 0.7720\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9074\n",
      "Epoch 00044: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2787 - acc: 0.9074 - val_loss: 0.9179 - val_acc: 0.7727\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9110\n",
      "Epoch 00045: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2712 - acc: 0.9110 - val_loss: 0.9412 - val_acc: 0.7696\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9152\n",
      "Epoch 00046: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2599 - acc: 0.9153 - val_loss: 0.9281 - val_acc: 0.7689\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9140\n",
      "Epoch 00047: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2587 - acc: 0.9140 - val_loss: 0.9009 - val_acc: 0.7773\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9192\n",
      "Epoch 00048: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2503 - acc: 0.9192 - val_loss: 0.9448 - val_acc: 0.7706\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9162\n",
      "Epoch 00049: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2553 - acc: 0.9162 - val_loss: 0.9224 - val_acc: 0.7731\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9188\n",
      "Epoch 00050: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2494 - acc: 0.9188 - val_loss: 0.9424 - val_acc: 0.7768\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9174\n",
      "Epoch 00051: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2512 - acc: 0.9174 - val_loss: 0.9753 - val_acc: 0.7652\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9207\n",
      "Epoch 00052: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2413 - acc: 0.9207 - val_loss: 0.9306 - val_acc: 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9216\n",
      "Epoch 00053: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2388 - acc: 0.9216 - val_loss: 0.9697 - val_acc: 0.7738\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9207\n",
      "Epoch 00054: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2355 - acc: 0.9207 - val_loss: 0.9395 - val_acc: 0.7780\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9251\n",
      "Epoch 00055: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2293 - acc: 0.9251 - val_loss: 0.9095 - val_acc: 0.7864\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9241\n",
      "Epoch 00056: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2301 - acc: 0.9241 - val_loss: 0.9515 - val_acc: 0.7773\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9254\n",
      "Epoch 00057: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2307 - acc: 0.9254 - val_loss: 0.9151 - val_acc: 0.7836\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9252\n",
      "Epoch 00058: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2253 - acc: 0.9251 - val_loss: 0.9646 - val_acc: 0.7722\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9249\n",
      "Epoch 00059: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2272 - acc: 0.9249 - val_loss: 0.9305 - val_acc: 0.7852\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9294\n",
      "Epoch 00060: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2181 - acc: 0.9294 - val_loss: 0.9379 - val_acc: 0.7845\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9295\n",
      "Epoch 00061: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2132 - acc: 0.9295 - val_loss: 0.9389 - val_acc: 0.7827\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9295\n",
      "Epoch 00062: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2106 - acc: 0.9295 - val_loss: 0.9348 - val_acc: 0.7836\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9291\n",
      "Epoch 00063: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2146 - acc: 0.9291 - val_loss: 0.9374 - val_acc: 0.7883\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9315\n",
      "Epoch 00064: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2090 - acc: 0.9315 - val_loss: 0.9732 - val_acc: 0.7803\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9303\n",
      "Epoch 00065: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2075 - acc: 0.9303 - val_loss: 0.9349 - val_acc: 0.7850\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9326\n",
      "Epoch 00066: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2073 - acc: 0.9326 - val_loss: 0.9262 - val_acc: 0.7859\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9330\n",
      "Epoch 00067: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2032 - acc: 0.9330 - val_loss: 0.9337 - val_acc: 0.7920\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9331\n",
      "Epoch 00068: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2023 - acc: 0.9331 - val_loss: 0.9278 - val_acc: 0.7897\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9343\n",
      "Epoch 00069: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1999 - acc: 0.9342 - val_loss: 0.9384 - val_acc: 0.7880\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9335\n",
      "Epoch 00070: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1988 - acc: 0.9335 - val_loss: 0.9424 - val_acc: 0.7885\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9329\n",
      "Epoch 00071: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2045 - acc: 0.9329 - val_loss: 0.9159 - val_acc: 0.7894\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9327\n",
      "Epoch 00072: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1985 - acc: 0.9327 - val_loss: 0.9333 - val_acc: 0.7894\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9362\n",
      "Epoch 00073: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1932 - acc: 0.9362 - val_loss: 0.9545 - val_acc: 0.7932\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9375\n",
      "Epoch 00074: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1899 - acc: 0.9375 - val_loss: 0.9890 - val_acc: 0.7850\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9377\n",
      "Epoch 00075: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1896 - acc: 0.9377 - val_loss: 0.9442 - val_acc: 0.7945\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9375\n",
      "Epoch 00076: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1894 - acc: 0.9375 - val_loss: 0.9636 - val_acc: 0.7862\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9390\n",
      "Epoch 00077: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1873 - acc: 0.9390 - val_loss: 0.9811 - val_acc: 0.7824\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9389\n",
      "Epoch 00078: val_loss did not improve from 0.87705\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1858 - acc: 0.9389 - val_loss: 0.9565 - val_acc: 0.7934\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvmclksu8hCSGQsEMSCBAQRcEVQS1qEZGK+1qtrdVaqV3Uan+urZZWa2mrVauoVXFFUasIyCIQ9jUsAbLvIetklvP74yQhQBICZJgA7+d55glz13cm5L73LPccpbVGCCGEOBKLrwMQQghxcpCEIYQQolMkYQghhOgUSRhCCCE6RRKGEEKITpGEIYQQolMkYQghhOgUSRhCCCE6RRKGEEKITvHzdQBdKSYmRicnJ/s6DCGEOGmsXr26VGsd25ltvZYwlFJJwGtAHKCBOVrrPx+yjQL+DFwC1AE3aq2zmtbdAPymadPHtdavHumcycnJrFq1qus+hBBCnOKUUns6u603Sxgu4H6tdZZSKhRYrZT6Umu9udU2k4EBTa8zgL8BZyilooCHgUxMslmtlPpIa13hxXiFEEJ0wGttGFrrgubSgta6GtgCJB6y2eXAa9pYDkQopRKAi4EvtdblTUniS2CSt2IVQghxZCek0VsplQyMAFYcsioR2NfqfW7TsvaWt3Xs25VSq5RSq0pKSroqZCGEEIfweqO3UioEeA+4V2u9v6uPr7WeA8wByMzMPGysdqfTSW5uLg0NDV196tNCQEAAvXr1wmaz+ToUIYSPeTVhKKVsmGTxhtb6/TY2yQOSWr3v1bQsDzj3kOULjyWG3NxcQkNDSU5OxrSxi87SWlNWVkZubi4pKSm+DkcI4WNeq5Jq6gH1L2CL1vpP7Wz2EXC9MsYCVVrrAmABMFEpFamUigQmNi07ag0NDURHR0uyOAZKKaKjo6V0JoQAvFvCGAdcB2xQSq1tWvYQ0BtAa/0SMB/TpXYHplvtTU3rypVSjwErm/b7vda6/FgDkWRx7OS7E0I081rC0FovATq82mgzP+zd7ax7GXjZC6Edeh4aGwuwWoPx8wv39umEEOKkddoPDaKUorGxEJeryivHr6ys5MUXXzymfS+55BIqKys7vf0jjzzCs88+e0znEkKIIzntEwaAUn5o7fbKsTtKGC6Xq8N958+fT0REhDfCEkKIoyYJg+aE0fHF+1jNmjWLnTt3kpGRwQMPPMDChQs555xzmDJlCkOHDgXgiiuuYNSoUaSmpjJnzpyWfZOTkyktLSUnJ4chQ4Zw2223kZqaysSJE6mvr+/wvGvXrmXs2LEMGzaMK6+8kooK85D87NmzGTp0KMOGDeOaa64B4NtvvyUjI4OMjAxGjBhBdXW1V74LIcTJ7ZQafPBIsrPvpaZm7WHLPZ46ACyWoKM+ZkhIBgMGPN/u+ieffJKNGzeydq0578KFC8nKymLjxo0tXVVffvlloqKiqK+vZ/To0UydOpXo6OhDYs9m7ty5/OMf/+Dqq6/mvffeY+bMme2e9/rrr+cvf/kLEyZM4He/+x2PPvoozz//PE8++SS7d+/Gbre3VHc9++yzvPDCC4wbN46amhoCAgKO+nsQQpz6pIQBmLb5w57585oxY8Yc9FzD7NmzGT58OGPHjmXfvn1kZ2cftk9KSgoZGRkAjBo1ipycnHaPX1VVRWVlJRMmTADghhtuYNGiRQAMGzaMa6+9lv/85z/4+Zn7hXHjxnHfffcxe/ZsKisrW5YLIURrp9WVob2SQH19Dm53FSEhw09IHMHBwS3/XrhwIV999RXLli0jKCiIc889t83nHux2e8u/rVbrEauk2vPpp5+yaNEiPv74Y/7whz+wYcMGZs2axaWXXsr8+fMZN24cCxYsYPDgwcd0fCHEqUtKGIBSVq+1YYSGhnbYJlBVVUVkZCRBQUFs3bqV5cuXH/c5w8PDiYyMZPHixQC8/vrrTJgwAY/Hw759+zjvvPN46qmnqKqqoqamhp07d5Kens6DDz7I6NGj2bp163HHIIQ49ZxWJYz2KOUHaLT2oFTX5tDo6GjGjRtHWloakydP5tJLLz1o/aRJk3jppZcYMmQIgwYNYuzYsV1y3ldffZU777yTuro6+vbtyyuvvILb7WbmzJlUVVWhteanP/0pERER/Pa3v+Wbb77BYrGQmprK5MmTuyQGIcSpRZln504NmZmZ+tAJlLZs2cKQIUM63K+xsRiHYy/BwcOwWPy9GeJJqTPfoRDi5KSUWq21zuzMtlIlRXMJA689iyGEEKcCSRiYNgzAa+0YQghxKpCEgZQwhBCiMyRhAGBt+iklDCGEaI8kDKSEIYQQnSEJg9ZtGJIwhBCiPZIwaJ4kyHsP7x2tkJCQo1ouhBAngiSMJt582lsIIU4FkjCaeGtOjFmzZvHCCy+0vG+e5KimpoYLLriAkSNHkp6ezocfftjpY2qteeCBB0hLSyM9PZ23334bgIKCAsaPH09GRgZpaWksXrwYt9vNjTfe2LLtc8891+WfUQhxeji9hga5915Ye/jw5gABnjozYK31KIc4z8iA59sf3nz69Once++93H23mYn2nXfeYcGCBQQEBDBv3jzCwsIoLS1l7NixTJkypVNzaL///vusXbuWdevWUVpayujRoxk/fjxvvvkmF198Mb/+9a9xu93U1dWxdu1a8vLy2LhxI8BRzeAnhBCteS1hKKVeBi4DirXWaW2sfwC4tlUcQ4BYrXW5UioHqAbcgKuzj60fZ8SAp8uPOmLECIqLi8nPz6ekpITIyEiSkpJwOp089NBDLFq0CIvFQl5eHkVFRcTHxx/xmEuWLGHGjBlYrVbi4uKYMGECK1euZPTo0dx88804nU6uuOIKMjIy6Nu3L7t27eKee+7h0ksvZeLEiV3+GYUQpwdvljD+DfwVeK2tlVrrZ4BnAJRSPwB+rrUub7XJeVrr0i6NqIOSgLMhB5fLO0OcT5s2jXfffZfCwkKmT58OwBtvvEFJSQmrV6/GZrORnJzc5rDmR2P8+PEsWrSITz/9lBtvvJH77ruP66+/nnXr1rFgwQJeeukl3nnnHV5++eWu+FhCiNOM19owtNaLgPIjbmjMAOZ6K5bOMdO0emMwxunTp/PWW2/x7rvvMm3aNMAMa96jRw9sNhvffPMNe/bs6fTxzjnnHN5++23cbjclJSUsWrSIMWPGsGfPHuLi4rjtttu49dZbycrKorS0FI/Hw9SpU3n88cfJysrq8s8nhDg9+LwNQykVBEwCftJqsQa+UEpp4O9a6zlt7mz2vx24HaB3797HEYe16bQeDjz53TVSU1Oprq4mMTGRhIQEAK699lp+8IMfkJ6eTmZm5lFNWHTllVeybNkyhg8fjlKKp59+mvj4eF599VWeeeYZbDYbISEhvPbaa+Tl5XHTTTfh8ZjqtieeeKJLP5sQ4vTh1eHNlVLJwCdttWG02mY6MFNr/YNWyxK11nlKqR7Al8A9TSWWDh3r8OYAjY0lOBx7ZIjzNsjw5kKcuk624c2v4ZDqKK11XtPPYmAeMMbbQcjT3kII0TGfJgylVDgwAfiw1bJgpVRo87+BicBG78fSPJ6UPLwnhBBt8Wa32rnAuUCMUioXeBiwAWitX2ra7ErgC611batd44B5Tc8j+AFvaq0/91acB+KVEoYQQnTEawlDaz2jE9v8G9P9tvWyXUDX9209AilhCCFEx7pDG0Y30dwzSkoYQgjRFkkYTWSaViGE6JgkjCYHhjjv2hJGZWUlL7744jHte8kll8jYT0KIbkMSRitmxNquLWF0lDBcro7PNX/+fCIiIro0HiGEOFaSMFoxc2J0bQlj1qxZ7Ny5k4yMDB544AEWLlzIOeecw5QpUxg6dCgAV1xxBaNGjSI1NZU5cw481J6cnExpaSk5OTkMGTKE2267jdTUVCZOnEh9ff1h5/r4448544wzGDFiBBdeeCFFRUUA1NTUcNNNN5Gens6wYcN47733APj8888ZOXIkw4cP54ILLujSzy2EOPX4fGiQE6mD0c0B8HiS0VpjPYqRQY4wujlPPvkkGzduZG3TiRcuXEhWVhYbN24kJSUFgJdffpmoqCjq6+sZPXo0U6dOJTo6+qDjZGdnM3fuXP7xj39w9dVX89577zFz5syDtjn77LNZvnw5Sin++c9/8vTTT/PHP/6Rxx57jPDwcDZs2ABARUUFJSUl3HbbbSxatIiUlBTKyzs77JcQ4nR1WiWMI1OY8aS8a8yYMS3JAmD27NnMmzcPgH379pGdnX1YwkhJSSEjIwOAUaNGkZOTc9hxc3NzmT59OgUFBTQ2Nrac46uvvuKtt95q2S4yMpKPP/6Y8ePHt2wTFRXVpZ9RCHHqOa0SRkclAYCGhmJcrgpCQjK8GkdwcHDLvxcuXMhXX33FsmXLCAoK4txzz21zmHO73d7yb6vV2maV1D333MN9993HlClTWLhwIY888ohX4hdCnJ6kDeMgpg2jKwdkDA0Npbq6ut31VVVVREZGEhQUxNatW1m+fPkxn6uqqorExEQAXn311ZblF1100UHTxFZUVDB27FgWLVrE7t27AaRKSghxRJIwWjl4iPOuER0dzbhx40hLS+OBBx44bP2kSZNwuVwMGTKEWbNmMXbs2GM+1yOPPMK0adMYNWoUMTExLct/85vfUFFRQVpaGsOHD+ebb74hNjaWOXPm8MMf/pDhw4e3TOwkhBDt8erw5ifa8QxvDjLEeXtkeHMhTl0n2/Dm3YY87S2EEO2ThNHKgQEIZTwpIYQ4lCSMVqSEIYQQ7ZOE0YqUMIQQon2SMA7S/Ii3lDCEEOJQkjBakVn3hBCifZIwWjFDnHf9iLVHKyQkxKfnF0KItkjCOIQ3RqwVQohTgdcShlLqZaVUsVJqYzvrz1VKVSml1ja9ftdq3SSl1Dal1A6l1Cxvxdh2XNYuLWHMmjXroGE5HnnkEZ599llqamq44IILGDlyJOnp6Xz44YdHPFZ7w6C3NUx5e0OaCyHEsfLm4IP/Bv4KvNbBNou11pe1XqBMQ8ILwEVALrBSKfWR1nrz8QZ07+f3srawg/HNAY+nvmmI86BOHTMjPoPnJ7U/quH06dO59957ufvuuwF45513WLBgAQEBAcybN4+wsDBKS0sZO3YsU6ZMaaoWa1tbw6B7PJ42hylva0hzIYQ4Hl5LGFrrRUqp5GPYdQywQ2u9C0Ap9RZwOXDcCaPzum64lBEjRlBcXEx+fj4lJSVERkaSlJSE0+nkoYceYtGiRVgsFvLy8igqKiI+Pr7dY7U1DHpJSUmbw5S3NaS5EEIcD18Pb36mUmodkA/8Qmu9CUgE9rXaJhc4o70DKKVuB24H6N27d4cna7MkoDU0T5Vqs9HQsKfLhzifNm0a7777LoWFhS2D/L3xxhuUlJSwevVqbDYbycnJbQ5r3qyzw6ALIYS3+LLROwvoo7UeDvwF+OBYDqK1nqO1ztRaZ8bGxh5bJOvXQ9N0ps1tGF05KOP06dN56623ePfdd5k2bRpghiLv0aMHNpuNb775hj179nR4jPaGQW9vmPK2hjQXQojj4bOEobXer7Wuafr3fMCmlIoB8oCkVpv2alrmHUqB3Q4OR9OC5kJX1w1xnpqaSnV1NYmJiSQkJABw7bXXsmrVKtLT03nttdcYPHhwh8dobxj09oYpb2tIcyGEOB4+q5JSSsUDRVprrZQag0leZUAlMEAplYJJFNcAP/JqMHY7NFXvtB5PqvnfXaG58blZTEwMy5Yta3PbmpqaNkK089lnn7W5/eTJk5k8efJBy0JCQg6aREkIIY6X1xKGUmoucC4Qo5TKBR4GbABa65eAq4AfK6VcQD1wjTb1QC6l1E+ABZixOl5uatvwnoAAqK4GrWU8KSGEaIc3e0nNOML6v2K63ba1bj4w3xtxtcluB48HnE6URUasFUKItpwWT3ofsQE7IMD8dDikhHGIU2lGRiHE8TnlE0ZAQABlZWUdX/jsdvOzoUHmxGhFa01ZWRkBzQlVCHFa8/VzGF7Xq1cvcnNzKSkpaX8jraG0FBob0SXhOByl+Pm58fMrPXGBdlMBAQH06tXL12EIIbqBUz5h2Gy2lqegOzR1Kgwdin73XZYsGUtc3A0MHNhmE4sQQpyWTvkqqU7r3x+ys1FKERIykpqa1b6OSAghuhVJGM0GDIAdO0BrQkNHUVOzFo9H2jGEEKKZJIxmAwZAfT3k5xMaOgqPp4G6uhM43qEQQnRzkjCa9e9vfu7YQWhoJgDV1VItJYQQzSRhNBswwPzMziYwcABWa6gkDCGEaEUSRrOkJPD3b2r4thASMoLq6lW+jkoIIboNSRjNrFbo29c0fAOhoZnU1q6Thm8hhGgiCaO1AQMgOxtAGr6FEOIQkjBaa+5a6/EQGjoKkIZvIYRoJgmjtf79TdfagoJWDd/SjiGEECAJ42CtekqZhu+RUsIQQogmkjBaa34Wo1U7hmn4dvowKCGE6B4kYbTW3LW2VU8pafgWQghDEkZrViv063dQCQOk4VsIIUASxuH6928pYQQG9pcnvoUQoonXEoZS6mWlVLFSamM7669VSq1XSm1QSi1VSg1vtS6naflapdSJ7abUqmutUhZCQ0dJTykhhMC7JYx/A5M6WL8bmKC1TgceA+Ycsv48rXWG1jrTS/G1rdWotQAhIaOoqZGGbyGE8FrC0FovAso7WL9Ua13R9HY50D3mAW01ai2YdgytHdLwLYQ47XWXNoxbgM9avdfAF0qp1Uqp2zvaUSl1u1JqlVJqVYfzdndWq2cxgJahzvfvX3n8xxZCiJOYzxOGUuo8TMJ4sNXis7XWI4HJwN1KqfHt7a+1nqO1ztRaZ8bGxh5/QElJEBUFCxYApuHb3z+R8vLPjrCjEEKc2nyaMJRSw4B/Apdrrcual2ut85p+FgPzgDEnLCiLBW69FT74APbuRSlFTMzllJd/jttdf8LCEEKI7sZnCUMp1Rt4H7hOa7291fJgpVRo87+BiUCbPa285q67QGv4298AiIm5Ao+njoqKL09oGEII0Z14s1vtXGAZMEgplauUukUpdadS6s6mTX4HRAMvHtJ9Ng5YopRaB3wPfKq1/txbcbapTx+4/HL4xz+gvp6IiHOxWsMpLf3ghIYhhBDdiZ+3Dqy1nnGE9bcCt7axfBcw/PA9TrCf/hTmzYO5c7HcfDPR0ZdRWvoRHo8Li8VrX5sQQnRbPm/07rYmTIC0NJg9G7QmJuYKXK4y9u//zteRCSGET0jCaI9SppSxbh0sWUJU1CSUsku1lBDitCUJoyPXXguRkTB7Nn5+IURFXURp6QdorX0dmRBCnHCSMDoSFAS33WbaMvbuJSbmChoacqipWefryIQQ4oSThHEkd91lft57L9GRlwEWqZYSQpyWJGEcSZ8+8MwzMG8e/n/6F+Hh4ygtnefrqIQQ4oSThNEZ995r2jN+8xt6bRhMbe166ut3+ToqIYQ4oSRhdIZSMGcODB9OzM/eJjAPiovf8nVUQghxQnUqYSilfqaUClPGv5RSWUqpid4OrlsJCoJ581AWP4Y9HEzhjr/h8bh8HZUQQpwwnS1h3Ky13o8Z1ykSuA540mtRdVfJyfD22wTsqiPhH7mUlX3i64iEEOKE6WzCUE0/LwFe11pvarXs9HLhhTDzWnrNU5Ss+aOvoxFCiBOmswljtVLqC0zCWNA0mqzHe2F1b+r3j6GwEDF7CbW1W30djhBCnBCdTRi3ALOA0VrrOsAG3OS1qLq75GQ8t99MwmdQsuQPvo5GCCFOiM4mjDOBbVrrSqXUTOA3QJX3wur+rL99HE+AH8FPvoXLVe3rcIQQwus6mzD+BtQppYYD9wM7gde8FtXJoEcPnPfcQOxCF+ULHvd1NEII4XWdTRgubUbcuxz4q9b6BSDUe2GdHOy/+iPOCD/sj74gAxIKIU55nU0Y1UqpX2G6036qlLJg2jFOayo8nLr7phG+spbGayfDihVmalchhDgFdTZhTAccmOcxCoFewDNei+okEnL/3yi8Ihi/eV/C2LEwbBj8+c9QV+fr0IQQokt1KmE0JYk3gHCl1GVAg9b69G7DaGINCke/+GeWvuuh+tk7ITDQjD01eDD8979S4hBCnDI6OzTI1cD3wDTgamCFUuqqTuz3slKqWCm1sZ31Sik1Wym1Qym1Xik1stW6G5RS2U2vGzr3cXwjLu4G7LFD2Hz2N3iWL4Vvv4WoKLj6arjgAtjY5scXQoiTSmerpH6NeQbjBq319cAY4Led2O/fwKQO1k8GBjS9bsf0xkIpFQU8DJzRdK6HlVKRnYz1hLNY/Ojb9wnq67dRWPgyjB8Pq1fDiy+aKV6HDTOvH/8Y/vMf2LULPKftc49CiJNUZxOGRWtd3Op9WWf21VovAso72ORy4DVtLAcilFIJwMXAl1rrcq11BfAlHScen4uOnkJY2Fnk5DyC210LVqtJENu3w+OPQ8+e8OabcN110K8fhIbCyJEwYwb86U/gdPr6IwghRIc6mzA+V0otUErdqJS6EfgUmN8F508E9rV6n9u0rL3l3ZZSir59n6KxsYDc3D8fWBEdDQ89BJ9/DuXlsHYt/P3vcPvtEB8Py5fD/ffDI4/4LHYhhOgMv85spLV+QCk1FRjXtGiO1rpbTDunlLodU51F7969fRpLRMTZREdPYe/ep0hIuB1//5iDN7BaYfhw82rt1lvhiSdMe8f555+4gIUQvvf00/DSSzB/vukscyTNHWnUiR//tdMTKGmt39Na39f06qpkkQcktXrfq2lZe8vbimuO1jpTa50ZGxvbRWEdu759n8DtriUnpzNNPE3+/GcYONBUV5WWei84ITqrqgrmzgXXKTLnS2EhOBy+juJwv/89PPgg7NkDP/whVHcwzJDbbaq1hw6FiAh49dUT3guzw4ShlKpWSu1v41WtlNrfBef/CLi+qbfUWKBKa10ALAAmKqUimxq7JzYt6/aCg4eSmHgX+fl/p7p6TWd3Mn+cpaVw883SFVf4VlERnHsu/OhH5mLWlk8+gYsvhm3buuac1dWQn981xzrUN99A375w3nlQW+udcxwtreG3v4WHH4YbboAFC8x3edNNh//9u92ms0xqqpkq2s8P0tPhxhth2rQTe5OptfbaC5gLFABOTDvELcCdwJ1N6xXwAmZsqg1AZqt9bwZ2NL1u6sz5Ro0apbuDxsZyvWRJjM7KOlt7PJ7O7/jcc1qD1s8/r3VRkda7d2u9ebPW27Zp7XJ5LV4hWuzZo/WAAVoHBmp96aXm/+PcuQdvs3ix1na7WRcZqfW33x7fORsatM7I0Do4WOuFC4/vWIf69lutg4K0Tk7W2mLRetIkrR2Orj3H0fJ4tH7wQfP93XKL1m63Wf7ss2bZ008f2O6TT7ROTTXL09O1fvdds73LpfVTT2lts2kdH6/1Z58dczjAKt3Za3pnNzwZXt0lYWitdX7+P/U336ALC//T+Z08Hq0nTza/lkNfoaFan3ee+Y/2wQdaV1R4L3hxajrSzcuWLVr36qV1eLjWS5aYC+vZZ5sL7oYNZpsNG7SOiNB64ECtV6zQevBgc9F6/fWOjz13rtY336x1be3h637xC/N/PDHRJKovvzy2z3eo774zSWjwYK0LC7WeM8ecZ8aMAxfpE237dpO0QOs77zw4Do9H66uvNonthRfM3zto3b+/1u+803bMa9eahNKjh9Y1NccU0tEkDKVPoeqPzMxMvWrVKl+HAYDWHrKyxuJw5DJmzDb8/Do5VmNVlSl+gplHPCgI6uth1SozVtXataZe2WKBMWPMDIBXXAGjRnnvw4jup6wMZs+GpCTzgGhYWMfbL1oEU6aYaqYnnoDw8APrtIb33zfdwJUy1SMZGWZdQYHp/h0SAvPmwaRJ5hmipUvNlMUVFTB1qqn2efhh8zq0Mfabb2DiRPP/duJE+PBDCAgw6776Ci66CO68Ex591Px/3r7dxHPJJWYbtxu2bIG8PNMO0fwKCYHevc0rOtqct6HBfDcbNpjvJT7ePEibkGCO9cQTptfiT35ivr9jaTiurobFi83zVHv3mtf+/ab6bsKEtvepq4P/+z945hk89kAqZz1J7cw7qG9Q1NWZj+PxgKemDn37HZCzGxUejuXmG1FXXoHHaqO2FmpqTK1ac3OMUqBcToIq85n+yz5H/1kApdRqrXVmp7aVhOE9+/evICtrLElJv6Rfv6e65qANDSZx/O9/8OWX8P335g/+8cfhV7/ySc8J0Yb8fLjqKlO/fNddpm669UX6SNxu+Pe/TfvWFVccuMACfPCBucAWFZn3gYFw5ZWmLvzCC83NRGv79pkbCqVMPPHx5qHSyy+HrCz4+c9NQklLg/feMx0wWvvuO9OmobWJZ/Fi8yBqs8ZGuOMOE++118I//3kg3u3bzRhrCQkm5p/+FC67zJynutocJzTUxBEUZC72EyeaC/7NN+PavJ3cVYXk1PeginA8WNBtzQ7tb8eCB2tjHVbcWHFTF9eX8l/8H+U6koqKpq8qQBP4v08IWLSA+qRBVPRKpyKqH5WB8TRWN+Iuq8BdXoW7qgaPzQ4hIejgUHRAII6yahqKq6mvduLQ/lhx46+c2PwV/h4Hfs46bP1649cvGT+bwmIBhQdL3j7cW7ZTWB9OQVA/ChujcLm69u80Ls606x8LSRjdyNatN1NU9B8yM9cRHDyk609QUQF3320aza+6Cl55xdx5gfnjmzsXSkrMuvT0rj+/OFxWlrmbr6w0F+EVK8yF9vrrzTM3/fp1vH9xsXmg8+uvzfuoKLPvtGnw17+a32lGhvldOxymt8xbb5n/C5dcAq+/bvYBUzodPx7H1t3kvreCUL2f6AduxrphrUkiWVnoqGhqf/skVVfcgD3Yj6Agc71XyhR4CwuhcM5HlM55H8fPfomj31AaG02Bwd+/6WXTOOd9wt73vienxxnkDJpIWYUF/+zN+LvrsY8ehiU4kPrdBdTvzKc+tAdOjxVLbQ2W5N6owABzgVVg0W5Uzi6qam3soxfuzvX+75Bf0yEO7fTlh5NIKginCjsOk2yUB2ugHYu7ERwOFBqFxo6DwCALAT0j8e/VA3dIOE5MWWluAAAgAElEQVSLHadT0VjvwrVpO66Sclzh0Th790NX7cdTWIxudKICA4hLi6VnahQJCeYCHxxscmRgINjtpte9Ugfu+Zrroz0ecw8QHGz+tIODzfatt7FYTEHrWEjC6EYaG4v5/vuhBAQkM3LkMiwWL4wKrzX88Y+mSJyaCr/+tbmD+/BDc/enlNkmPd1USVx3HSR20XOQbjctf+nCfO/XXQcxMfDxx+aZm6ws+MtfzIVeKfjDH+BnPzNXiEPo75ZSPe1mSsoslPziKYKTY+g5/19Effo6ytkINhuNv3qYgut+SV6xjb17TY/MPbvc5C7dg9q0icBACLzgTGxx0ez7bAPb84LJsfTF4zG/I6U0MUF1RDYUUB0QS5kzjMbGw39/fn7H1qs2gQKS7QXEBtfhrKjGMeJMHIEReDzm4hhYuJvAzauw4UQPH4Fn0BBTHeM5+CIZHOQhpa+FlBRISTE5sPmCeuh/t+Z93O4Dr6Ags09UlLnIKmU+T329KagHBkKwpxq1bCksW2ZKXmPGmL8TW9PfaW0tbN0KO3eaJH1o6evQIObMMb/bxkbzftQo+N3v4Ac/6LZ/I5IwupmSkvfZtGkqffr8lpSU33vvRAsWwDXXmDvb6GiYOdNUhSQkwDvvmD7cy5aZW8I77jB1ufHxB/YvLjb11CEhpv7X1kFy09q0tTzwgLljfumlI5dgtDb1vgkJ5q/5eDU2miq5UaPMX3976upM28+qVebqGh8PvXqZ+v+EBIiMNG0Afh3cyXo85mn9qipzp3/otg4HPPYY/OEP6DPGUvHKB+xrjCM/v+kOvRAKd9aw/4vlNOwroSEqkYbUUdSpYGr2u6kudVBT6aKsxk4j9sNOb7drEkJrqPUEUlJ+eJyRkebjqPpa6nYXUe/yx2EPJcmxg4Fp/gz8YTrJyearKC42r/JyUxsUHW1e4eHmK62rMy+n0+S9+HhzRxwTc+BuuPmO2Ok0+zQ2mvuGXr0gYO1yU91VXGyqqW5oY+zQv/0N1q83JaY2EudJbc0aM9zPNdeYEl83TRTNJGF0Q1u23EBR0RuMHPkdYWFneO9EubmwaZOpc7YffuFh50546il4+WWTOO65xzwINHeuaYB0u812yckmodxwg9mutY0bTTXYokWmQXTPHnMh/cUvTN/y5mSgtWmo/PZb+OIL0+ZSUGCuOhddZKptLrvMXI2O1s6d5g9y1Sqz/733mkbb8HBz3tWrzfDyn31mvo/mwR4DAsztZVtCQtCJvSidMJXd6VPYHTmS/DwPVYs3UPXdRiorPFQSQXlwEmWxQyh3BNPYCEFWB0GVeQQ1VlIfHs8+VwK1tYdfJMLCICJCE9C4H3txLnbdQHCgh9C6IkKoJoQaogZEE3vdZGJ7BxITYy7ceXmmSSQ/3+Tynj0PvHr3hj59zIW/RWmpKUl++aW5s/3gg8PbNbyt+f/hxRef2POKoyYJoxtyuapYuXIYFoudzMw1WK3Bvg1oxw4zftWbb5oLbHKyqTefMcP0+vj9783de1KSubg3l/NrakxVS3g4PPkk3HKLuVX95S9NnXrfvqYhc+dO82qeSCo62jTIjh9verx89JE5j1KQmQmTJ5seOGPGHPmO8+234bbbzHaPPGKGVPjiCzxhEdRMnoZz6Uqc+wpwWQNoOGMC5YPPojRxOKXRgyhqjGRPdiN7sh3s2QMl5Va0R6M9pi6k1uFHrfvw0kqopYaISEV4kIvo4s1EOQqI6huBPdBK/aad1AXFUJd2Bv5JcSQlmQt5UpKp+Wuusz6oEJSfb6oQy8th9OgDrx49uub363abZHneeaY+Roh2SMLopioqFrJu3fn07PljBg58wdfhGNu3m9JBZubBRWetTangiScgO9tUwfj5mYv0+eebXlkxh4yVtXChKWXU15tqqv79zc+xY2HEiIPvcrU2VRIffWQubCtWmFJAeDgEB6MdjdQ47VS5Q6jv0YfGXn1xJCRTW+5g91c72JF4LjtGTSenJJjSUigvdlFRZcHTidFuwsPNXXmfPuZC3pyflDIFkOQEByn715Gy7TN6ufcSdvd1WM+fcOD7qasz4/889ZSpi/npT011VHNnAyFOIpIwurEdO+4nN/dPpKV9TEzMZb4Ox6fq6kytxebNsCWrnq1Ly9i+00pJfQiVjUG4PO2XNCwWTZ8+ipQUc1Pe3LgZHm5q4przm91uCjcxMeYVG3vkRxY6bd8+0zV06NAuOqAQJ97RJIzj768mjkpKyh+orPyarVtvIDNzHQEBvXwdkldpbWpdmmuoduwwXezXrTMFl+b7FZstkAEDejH4XJgQZxpxIyLMq6Wh1V8T6OckeaA/ycnqsKaVEy4p6cjbCHEKkYRxglmtAQwd+jarV49iy5YZDB/+DRbLyfdrqK01pYP16011fFmZSQzl5aaTVmWlqemqrDx8vLfmZo4ZM8zP1FSzrKNOWYYCfJ0lhDh9nXxXqlNAUNBABg78O1u2XEtOzsP07fsHX4fUJrfblAqys01v2N27zc/Nm01JoXVtZni4qRKKjDSv+HizLDzc3Ij362defft2TY9aIcSJJwnDR+LifkRFxdfs3fsEERETiIqa6NN4amtN9/GVK80jCxs3msTQugdqYKB5gCo93Tzi0TxVee/eHT/CIIQ4NcifuQ8NGDCb/fuXs2XLdWRmrsVuTzgh53W7TTJYscLMEPv99wc/qpCQYJLCXXeZn4MGmUQRF9ftn0ESQniRJAwfslqDSE19h9WrM9m69XqGDVuAUl37gFVJiUkMW7ea15Ytpt2hpsasj4oyjz5ceeWBRwGO5Tk6IcSpTxKGjwUHD6V//9ls334be/c+TZ8+s477mHv2mBE+5s2DJUsOlBx69IAhQ8zD22PHwhlnmEclpNQghOgMSRjdQELCLVRUfMXu3b8hImIC4eFndnrfqiqTFNasOfDavdusS0sz4xBefLFJFM0DmAohfEtrjWrnTs3hclBcW0yvsF7tbnPosYpqi4gPiT/itsdLEkY3oJRi0KC/U139PZs3zyAzcy02W0S72xcUmIFo580zc9M4nWZ5//7mge277zZjv/Xvf4I+gBCHyK/Op6imiPS4dPyOodu4w+WgqLYIm8WGzWrDZrERZAvCZm2/73W9s56cyhx2VewipzKH6KBozko6i6SwpIMuvFprKhsq2e/Yj8PtoNHdSL2znh3lO9hYvJGNJRvZWroVP4sfkQGRRAZGEhkQSah/KCH+IYT4hxBkC0KjcXvcuLUbl8eFw2WO1XzM1hrdjRTUFJC3P4+86jxqGmsYmTCScUnjGJc0jt7hvVm0ZxFf7vqSb/d8S52zjrjgOM7pcw7n9D6HMYlj6B3em7jgOKwWKy6PiyV7l/DB1g/4cNuHaK3Z/bPdnUowx0Oe9O5G9u9fwZo1ZxMdfTmpqf896JefnW3GkJs3zzRUa226qV55JVx6qRkDsMueYBZet6tiF//d9F/qXfX0i+xH/6j+9IvqR2xQbJf80TtcDjaXbKZXWC9ig2M73NajPZTWlVLVUEX/qP5tnn9D0QYW5iykT0QfhsQMISUyBT+LH26Pm8KaQvZU7WFLyRYW713M4r2L2VWxC4BQ/1AmJE/g/OTzSYlMYVPxJjYUb2B90Xrc2s2UgVOYOnQqYxLHoFAs3beUV9e9yjub3qHKUXVYHHarnVB7KGH2MKzKisPtwOFy4HA7qGyobPPzJYYmMrbXWDSa3RW72VWxq81jA1iVlUExgxgSMwSNpqK+goqGCiobKqlprKGmsYYGV9uDV1qVFX+rP3Y/O/5Wf1SriZ6sFisJIQkkhiWSGJpIoF8g3+d/z8q8lTjcjpbtBkUP4qK+FzEweiAr8laweO9i9lbtPegc8SHx1DnrqGiowG61c1G/i7h80OXcmHHjMSXnbjM0iFJqEvBnwAr8U2v95CHrnwPOa3obBPTQWkc0rXMDG5rW7dVaTznS+U72hAGwd+8z7Nr1SyIjHyQv7wm+/lrx+eemFxOYIZmuuMIkirS006/9ocHVQIBfQJvrtNbsrtxNkC2IyIBI7H6Hj9artaa0rpTs8my2l22nzlnHxH4T6R/V/7DttpRuYWvpVjzag0d70Frj8rhwepw43U6cHid+Fj+iAqOIDowmOiiaqMAoIgMiCbIFtVx4PdpDRX0FxbXFfLXrK97c+CbLc5cDoFBoDvwNBvgFkBCSQEJoAgkhCfhb/XF6nDS6G3F5XITZw+gR1IMewT2IDY41d91Nd+EAWQVZLN67mBW5K1ouRImhiYxIGEFabBpOj5Oy+jLK6soorSslvzqf/Op8nB5TTE0KS+KqoVdx1dCryIjP4N3N7/L31X9n6b6lB30//lZ/egT3oLCmEJfnwKQZMUExnN37bM7pfQ7xIfEs2rOIr3d/TXZ5dss2KREpDIsbhsPt4H+7/ofT4yQxNJEAvwB2VuwkyBbE1CFTOaf3Obi1G6fbfP56Vz3VjmqqG6vZ79iPR3vMxdliLtIxQTH0i+xHSmQKyRHJFFQXsHTfUpblLmN57nL8rf70jexL38i+pESkEBkYaS7wVjt2PzspESkMjB7Y5v+b1lweF3XOOhQKq8WKn8UPq7JitRz9MO0Ol4PVBavJqcxhXNI4+kQcPs3q3qq9rC1c21I6yavOw4KFSwdeysR+EwnxP74xzLpFwlBKWYHtwEVALrASmKG13tzO9vcAI7TWNze9r9FaH9U3cbInjIYGeP11zUsv7WbduiTcbhv+/ppx4xSXX24SRZ9jm7a326pz1lHVUEWds446Zx31rnpsFhsBfgEE+AWg0SzPXc7CnIUszFlIdnk2ZyWdxfXDrmd62nQiAiIoqC7g1XWv8q81/2JH+Y6WYwf6BRJmD8PSqudZnbOuzbvLITFDmDJoCgOiBvDtnm/5atdXFNQUHPPnsllsRAZG4tEeyuvL8WhPy7rhccP5UfqPmJ46nfiQeHZX7mZn+U52lO8gd38u+TX5FFQXUFBTgNvjbqmS8bP4sd+xn6LaIvY79rd5XquyMiJhBOf0PofRPUeTX53PmsI1rClcw9bSrditdqKDookONMmt+Y43MTQRf6s/n2Z/yoKdC2h0N2JVVtzazcDogdwx6g6mDplKYU0hW0q3sKVkC4W1hSSGJtI7vDd9wvvQL6ofA6IGtFlC2Ve1j/zqfIbGDiXUfmAs9sqGSj7Z/gnvbXmPOmcdM9JmMHXI1IO2Ed7VXRLGmcAjWuuLm97/CkBr/UQ72y8FHtZaf9n0/rRJGEVFZj6ZF1803WDT0jRnnrmAQYOe5ZJLzmXIkN947dwNrgasytpu3XBhTSGVDZUtF/BAv0BC7aEHXYQBqh3VfLTtIz7Y9gE2i43BMYMZEjOEQTGD8Lf6tySEakc1m0o2sbpgNVkFWWwv296pOMPt4YzvM56hsUP5ePvHbC7ZjN1qZ1TPUazIXYFbu5nQZwLTU6cDUNFQQUV9Bfsd+w+6g7db7S0XtgHRA7AoC/Oz5/PRto/4ds+3uDwuYoNiuaDvBVyYciEjE0Zis9qwKEvLHWXrenWnx0l5fTlldWWU1ZdRXl9OZUNlS1WGQhEbHEtMUAyxQbEMjx/O0NjjH6ywwdVASW0JDa6GlhJP88W9vTtOj/Yc9ntrS1VDFR9v/5jV+au5fPDlTOgzwet148J3ukvCuAqYpLW+ten9dcAZWuuftLFtH2A50Etr7W5a5gLWAi7gSa31B0c658mUMKqrzbQS//2vmc6hsdHMdfPzn5u5j8DDtm23Ulj4Cikp/0efPr865nNVNlSytnAtawrMnebOip0U1xZTXFvMfsd+/K3+DI8bzuieo8nsmUmju5Hv9n3Hd/u+a6mLbi3IFtSSEAZGD2Rt4VrmZ8/H4XbQM7QndqudnMqcgy7Uh+od3ptRCaMYET+CHsE9CLIFEWQLIsAvAJfHRb2rngZXAy6Pi5EJIxkeN7ylyK+1ZnXBal5d+yqL9i5iUr9J3DLyFgZGdzB9Zie/p4LqAgbFDOrUhVWIU8HJmDAexCSLe1otS9Ra5yml+gJfAxdorXe2se/twO0AvXv3HrVnzx6vfJ6usmgRPP+8SRIOh5k17aqrzFPVgwYdvK3WbrZsuZ7i4jdJTn6UPn1+i1KKRncj64vW833e96zIW8G20m1UNlRS5aiiqqGKelc9CmXuipU6qI45ISSBwTGDiQuJIy44jtigWKocVazMX8nq/NVUN1YD0CO4B+OSxnF277PpGdqTBlcD9c566l317KvaZ6olSrewt2ovCSEJTBs6jelp0xnbaywWZaHOWcf2su1sK92GR3taEkKQLYiB0QOP2BArhDgxusvw5nlA6/GfezUta8s1wN2tF2it85p+7lJKLQRGAIclDK31HGAOmBLGcUftJYsWwaOPwtdfmwfo7rjDTA191lltz56ptaakroyCgJtYVL2TLf97mApep8BhJ7s8u6XbXo/gHqT3SKd3eG/C7eGEB4SbLn9ao9F4tIcwexgZ8RmMiB9BXEj7j3F7tIftZdvxs/jRL7Jfp6oh6p312P3sh92RB9mCyIjPICM+4+i+KCFEt+XNhLESGKCUSsEkimuAHx26kVJqMBAJLGu1LBKo01o7lFIxwDjgaS/G6jWrVpmZOL/+2gy58dxzcPvtbY/Y2uhu5PV1r/P6+tfZWLyRsvqylnUBVj8SAnbQNyKZS874GWMSxzAmccxhfcyPh0VZGBwz+Kj2CbQdPp2pEOLU5LWEobV2KaV+AizAdKt9WWu9SSn1e2CV1vqjpk2vAd7SB9eNDQH+rpTyABZMG0abvau6q/x8eOghePVVM8tbR4nC4XLwytpXeHLJk+yp2kNajzSmDpnK0NihpPZIZUjMEBJCEti9+yH27XuKuLgCBg264qScR0MIcfLy6hVHaz0fmH/Ist8d8v6RNvZbCqR7MzZvqa01bRRPPGGewH7wQZM4mh+qW1+0nvnZ8ymsKaS0rpTSulLWF62noKaAsb3G8rdL/8ak/pPaLDX06/ckfn5h7N79a9zuWoYOnYvF0nGfcSGE6Cpyi3qcKuormL1iNmU1+1m/zsr3y/2or/FnxI/iuWtmL0YPTKKgMYC/LfmA/2z4DxuLNwLmCdiYoBhig2M5M+lMfpz5Yy5IueCI1Ut9+jyE1RrKjh0/ZcOGH5CWNg+rNfhEfFQhxGlOhgY5Dv/b9T9u/OBG8qrz0M4gwI2yutAWV5vbn9nrTGYOm8m0odOOu5dQQcG/2bbtFsLCziQ9/ZMOx54SQoj2dJdeUqesBlcDD/3vIZ5b/hyBtYPQb3zPxLRMHnkEzjyTlvF1cvfnsm//Psrry7kg5QL6RfXrshgSEm7Ezy+UzZtnsG7deaSnf4rd3rPLji+EEIeSEsZRKK8v562Nb/HX7//KltItBG68C754hjkvBDFzptdO23FM5QvYtOkq/PwiSE//hJCQ4b4JRAhxUpISRhfSWrNg5wL+kfUPPt72MU6Pk3iVjnrzU/pYLuHdpZCa6rv4oqIuZsSIJWzYcBlr1pzN0KFvEx19ie8CEkKcsmT8gw7kV+dz5dtXMvmNySzes5i7R9/N3dY1FD68jukjL2HlSt8mi2YhIcMZOXIFgYED2bDhB+Tm/tXXIQkhTkGSMNqgteaVNa8w9IWhLNi5gGcueoa8+/IYXvgcL/w2g5kzFW++CSHHN6pwl7LbezJixCKioy9jx4572LHjF+hWI6QKIcTxkiqpQ3i0hyvfvpKPtn3EOb3P4V9T/sWA6AEsWAC33QYXXgj/+lf3nIfCag0mLe19duy4l9zcP+Jw7GHw4NexWtueP0IIIY6GJIxDfLL9Ez7a9hGPnvsovxn/GyzKQlYWTJ1qJix67z3w9/d1lO1Tykr//rMJCEhm585f4HAUkJ7+ITZbtK9DE0Kc5KRK6hDPLH2G3uG9+dXZv8KiLFRUmClQY2Lg009PjmlQlVIkJd3P0KHvUF29itWrz6CmZp2vwxJCnOQkYbSyPHc5S/Yu4edjf94yodDjj5sJjubNM0ORn0x69JhGRsbXeDx1ZGWNpaDg374OSQhxEpOE0cozS58hIiCCW0feCkB2NvzlL3DLLWYu7ZNRePhZZGauISzsLLZtu4mtW2/F7a73dVhCiJOQJIwmO8p3MG/LPH6c+eOWKS4ffNC0Vzz2mI+DO07+/nEMH/4Fffr8hsLCf7F6dSYVFQt9HZYQ4iQjCaPJn5b9CZvVxj1jzKR/335rqqF+9SuIj/dxcF1AKSspKY8xbNjneDx1rFt3Hps3z6ChIdfXoQkhThKSMICS2hJeWfsK1w27joTQBDweuO8+SEoyP08lUVEXM3r0ZpKTH6G09AO+/34w+/b9SZ7ZEEIckSQM4IWVL9DgauD+M+8H4PXXISsLnnwSAk/BCeWs1kCSkx9m9OjNREaez86d97N+/SU0Nhb5OjQhRDd22ieMOmcdf/3+r/xg4A8YEjsEjwd+/WsYMwauucbX0XlXYGAKaWkfMnDgS1RVfcvKlcMpL//K12EJIbqp0z5h2Cw2np/0PL+bYCYCzM6GvDy44w6wnAbfjlKKnj3vYOTI77HZoli/fiLbt/+Ehoa9vg5NCNHNnAaXxI7ZrDZmDptJZk8zum9Wllk+apQPg/KBkJB0Ro1aSc+ed5Kf/xIrVvRjy5brqanZ6OvQhBDdhFcThlJqklJqm1Jqh1JqVhvrb1RKlSil1ja9bm217galVHbT6wZvxtlaVpbpSjt06Ik6Y/dhtQYzcOCLjB27k54976ak5D1WrUpn48Yrqa3d4uvwhBA+5rWEoZSyAi8Ak4GhwAylVFuX4be11hlNr3827RsFPAycAYwBHlZKRXor1tbWrIH0dLDZTsTZuqeAgD4MGPA8Z565l+TkR6io+B8rV6axbdsdOBwFvg5PCOEj3ixhjAF2aK13aa0bgbeAyzu578XAl1rrcq11BfAlMMlLcbbQ2pQwRo709plODjZbNMnJD3PGGTtJTPwJhYWvsGJFf3JyHsPjafR1eEKIE8ybCSMR2NfqfW7TskNNVUqtV0q9q5RKOsp9u9SePVBRIQnjUP7+sQwY8GfGjNlCdPQl5OT8jlWrRlBVtdTXoQkhTiBfN3p/DCRrrYdhShGvHu0BlFK3K6VWKaVWlZSUHFcwa9aYnyfruFHeFhjYj9TU/5Ke/gludw1r1oxj+/a7cDrLfR2aEOIE8GbCyAOSWr3v1bSshda6TGvtaHr7T2BUZ/dtdYw5WutMrXVmbGzscQWclQVWKwwbdlyHOeVFR1/K6NGb6NXr5+Tn/52lS3uyadM1lJV9jtZuX4cnhPASbyaMlcAApVSKUsofuAb4qPUGSqmEVm+nAM1dcRYAE5VSkU2N3ROblnlVVhYMGXJqPt3d1fz8Qujf/09kZq6jZ8/bqaj4kg0bJrNsWR927PgFVVXLZbgRIU4xXptxT2vtUkr9BHOhtwIva603KaV+D6zSWn8E/FQpNQVwAeXAjU37liulHsMkHYDfa629Xu+xZo2ZglV0XkhIGgMGzKZfv2coK/uEwsJ/k5c3m9zcP2K39yIm5of07HkHwcGnYT9lIU4xSmvt6xi6TGZmpl61atUx7VtQYCZIeu45uPfeLg7sNON0VlJW9gklJe9SXv45WjfSo8d0+vT5HcHBQ3wdnhCiFaXUaq11Zme29XWjd7fR3OAtPaSOn80WQXz8TNLTP+DMM3Pp3ftBSks/ZuXKVDZvvpb9+1dyKt2oCHG6kITRpDlhZGT4No5Tjb9/DH37PsHYsbtJSnqA0tIPyMoaw8qVqezd+xQOR5t9GYQQ3ZAkjCZZWdC/P4SF+TqSU5O/fyz9+j3FWWflM3DgHPz8oti1axbLliWxefMMGXpEiJOAJIwm8oT3ieHnF07PnrcxcuQSxozJbip1HKiuqqvb5usQhRDtkISBebo7J0cSxokWFNSffv2eOqi66vvvh7J27fnk5b0g41YJ0c14rVvtyUSe8Pat5uqqpKT7yct7gZKSd8jO/gnZ2fcQFnYWERHnEBqaSWhoJnZ7b5RSvg5ZiNOSJAwOzIEhCcO3/P17kJLyKCkpj1Jbu5mSkncpLf2IffueRWsXADZbDxISbiEx8R7s9oQjHFEI0ZXkOQzg2mth8WLYK5PMdUtudwO1tRuorl5FRcWXlJZ+iFJ+xMXNJCnpfnkoUIjjcDTPYUgJA1PCkNJF92W1BhAWNpqwsNEkJv6Y+vqd7Nv3JwoLX6Gw8GUCAwcSGXkRkZEXEhFxLjZbhK9DFuKUdNonjMZGqK2VBu+TSWBgPwYOfIHk5EcpKvoPFRVfUlj4b/LzXwDA378ngYH9CQwcQFDQQEJDRxEamomfX7iPIxfi5CZVUk1cLvA77dPnycvjaWT//uVUVS2hvj6b+vod1NVl43QWtWwTGDiQsLAziYmZQlTUxVitwT6MWIjuQaqkjoEki5ObxeJPRMR4IiLGH7Tc6SynunoV1dUr2b9/JWVlH1NU9CoWSyBRUZOIipqEv39P/P1jsdli8PfvidUqwxUL0Ra5TIpTms0WRVTURKKiJgLg8bioqlpMaen7lJS8T2npvIO2V8pOdPRlxMVdS1TUZKzWAF+ELUS3JFVS4rSltYeGhj04nSU4naU4nSVUV6+muPhtnM5irNZwYmOvJDp6CpGRF+HnF+LrkIXockdTJSUJQ4hDeDwuKiu/pqjoDUpLP8TtrkIpfyIiziMiYgJ2e09stjj8/ePx94/DZovBYrH5Omwhjom0YQhxHCwWv5ZqLI/HSVXVd5SVfUxZ2cfs3t32xI9+fpHYbD0IChpIZOREoqIuIjBwoDyVLk4pUsIQ4ii4XDU4nUU0NhbR2FhIY2MhTmcJjY0lOJ1FVFevoaFhJwB2e2/Cw88hODi16ZVGQEAySskQbqL7kBKGEF7i5xeCn18IgYH92t2mvn4n5eVfUlHxBVVViygufqNlnVI2AgL6EBCQQuV6RNUAAAzdSURBVEBACiEhGURGni+lEXFSkBKGEF7mclVRW7uZ2tpN1Ndn09Cwm4aGHOrrd+FylQHg759ARMT5hIWNITg4laCgVPz941BK4XbX0dhYgMNRgN3ei4CAPpJcRJfpNiUMpdQk4M+AFfin1vrJQ9bfB9wKuIAS4Gat9Z6mdW5gQ9Ome7XWU7wZqxDe4ucXTnj4mf/f3r3HxnWWeRz/PmfuYycTO3ZIiJMmIb2kVCVhIU0phTZtaUAsFwlEC63QqlIlVLR0tdIuEZcF/uEqLkIIWlgou1u1VUtbqqy4tElUBFqapCWQW1PSpjSOnDhx49iZ8Yxnznn2j/PambhuOHHjzNv4+UhHnnOb+c2cM37mvOdGqXTlKcNVlZGR5xkc3MTg4GaOHXvilK2RdLoTiGg0Bk+ZL5PpZtas1cyevZrOzhuZNevt1sxlzolp28IQkRTwHHAD0AtsBW5W1d1N01wLPKWqFRH5FHCNqn7MjTuhqmd0HKNtYZjXM1VldPQQ5fIuKpVdlMu7EcmQyy0cPzKrWt3P8PAWhoa2UKnsAZRcroeurg/T1fVh8vklBEGBIMiTShUJgmyr35bxnC9bGKuBfar6ggt1P/BBYLxgqOrmpun/CNwyjXmM8ZqIkMstIJdbQGfn9aeZ8lNAfBb7wMD/cuTIL+jr+zEHD37/FVMWiysola6mVLqa2bPfTr3+MiMj+xgZ2UetdpB8/oLxnfL5/JtQbRCGw4ThCSAin19mzV9m3HQWjIXAgab+XuCK00x/G/Crpv68iGwjbq76mqo+evYjGvP6lcl0Mn/+rcyffyuNxgkGBzdTrw8QRSNEUZVG4zjDw1vo77+fvr67J8wtZDLd1Ov9p32NbHY+nZ3r6Oi4kY6OtWQy3VZAZjAvjpISkVuAtwHvbhp8gaoeFJFlwCYR2aGqz08y7+3A7QCLFy8+J3mN8U063U5X1z9OOk41pFzeyfDwM2Qy3e5KvksJghxhWKZc3kOlsouRkRdcU1Y7qdQsVEc5dmwTR4/+kkOH7nHPliKT6SCd7iSTmetOXoy7XK5nfIe9nRV/fprOfRhXAl9S1Rtd/3oAVf3qhOmuB74PvFtVJ/25IyL3ABtU9aHTvabtwzDm7FMNGRraytDQ/1GvD9BovEy9/rK7nMpharW+8aO9xuTzy8jnlwIhUTSK6ihRVEe1gWr8NwgKFIsXUSxeTKFwEZlMF2E4TKMxRBgOkU530tFxrTWLTTNf9mFsBS4UkaXAQeAm4OPNE4jIKuAuYF1zsRCRDqCiqjUR6QKuAr4xjVmNMa9CJEWptIZSac2rThNFo1SrL1Eu76Rc3kG5vINarReRDKlUEZESIhnXpQmCDI3GEJXKHgYGNqBaf9XnzuUW09FxHbNnX+G2jpaTy/UQH1czuTCsEgSZ005jzty0FQxVbYjIp4HfEB9W+1NV3SUiXwG2qepjwDeBduBB9wti7PDZFcBdIhIBAfE+jN2TvpAxpuWCIEuxuJxicTnd3R86o3mjqEG1+iKNxiDp9GxSqVmkUrOo1XoZHNzU1Cz2s/F5RLJkswvIZOaOd2FYoVY7QK12gHr9KEGQp1C4iGLxEorFFWSz8wiCIqlUmytiGUDcIckB2ew8crlFpNOzz+6Hcx6xE/eMMd5TjajVehkZeX78KK/R0T7q9QHXTDZAEBTI5Ra5rocwPO72zzxLtbofiBK9VipVIp9fRCrVjkiOIIi7dLrk9t10kk7PIQiKBEGeIMgD4m7atYdKZTfV6gEKhaW0tV1GW9tlFItvpli8hFxuoXfNa740SRljzFkhEpDPLyafX0xHx7VnPH8U1Wg0jhOGZcKwTBRV3P6UCIhQDRkdPUyt9hK12gGq1QNEUWV8viiqEobHqdePEYbHX/V1stmFtLWtYO7cy6lW93P06KP09f1kfHwQFCkWL6JQWO5u3HXyisciaSAY3+Jp/iuScVtfJfe3vSUna1rBMMac94IgRzY776w8VxQ1CMOh8cOXo6iKaoN8fsmk940fHe2nXN5JpfIcIyN7qVSe48SJHYyOPn7a4vP3xM1r7aRSbeRyPaxa9bvX8rYSsYJhjDFnIAjSBEFn4umz2Xlks2vp6Fj7inFhOMLo6GHq9aPEWzont3hAx/ujqEYYDtFoDLktpaGmraWyaxabflYwjDGmRVKpAoXCEgqFJa2OkohdscwYY0wiVjCMMcYkYgXDGGNMIlYwjDHGJGIFwxhjTCJWMIwxxiRiBcMYY0wiVjCMMcYkcl5dfFBEjgB/m+LsXcDRsxjnbPI5G/idz+ds4Hc+n7OB3/l8zgan5rtAVbuTzHReFYzXQkS2Jb1i47nmczbwO5/P2cDvfD5nA7/z+ZwNpp7PmqSMMcYkYgXDGGNMIlYwTrq71QFOw+ds4Hc+n7OB3/l8zgZ+5/M5G0wxn+3DMMYYk4htYRhjjElkxhcMEVknIntFZJ+IfNaDPD8VkX4R2dk0rFNEHheRv7q/HS3KtkhENovIbhHZJSKf8SxfXkS2iMifXb4vu+FLReQpt4wfEJFsK/K5LCkR+ZOIbPAw24siskNEtovINjfMl2U7R0QeEpFnRWSPiFzpUbaL3Wc21g2JyJ0e5fsX933YKSL3ue/JlNa7GV0wRCQF/AB4L3ApcLOIXNraVNwDrJsw7LPARlW9ENjo+luhAfyrql4KrAHucJ+XL/lqwFpVfQuwElgnImuArwPfUdXlwDHgthblA/gMsKep36dsANeq6sqmQy59WbbfA36tqpcAbyH+DL3Ipqp73We2EvgHoAI84kM+EVkI/DPwNlW9DEgBNzHV9U5VZ2wHXAn8pql/PbDeg1xLgJ1N/XuBBe7xAmBvqzO6LL8EbvAxH1AEngGuID5BKT3ZMj/HmXqI/3GsBTYA4ks29/ovAl0ThrV82QIlYD9un6tP2SbJ+h7gD77kAxYCB4BO4jusbgBunOp6N6O3MDj5YY7pdcN88wZV7XOPDwFvaGUYABFZAqwCnsKjfK7JZzvQDzwOPA8MqmrDTdLKZfxd4N+AyPXPxZ9sAAr8VkSeFpHb3TAflu1S4AjwM9ec9xMRafMk20Q3Afe5xy3Pp6oHgW8BLwF9wHHgaaa43s30gvG6o/FPgpYe2iYi7cAvgDtVdah5XKvzqWqocdNAD7AauKRVWZqJyPuBflV9utVZTuOdqvpW4ibaO0TkXc0jW7hs08BbgR+q6iqgzITmnVavdwBuP8AHgAcnjmtVPrff5IPERfeNQBuvbPJObKYXjIPAoqb+HjfMN4dFZAGA+9vfqiAikiEuFveq6sO+5RujqoPAZuLN7TkiknajWrWMrwI+ICIvAvcTN0t9z5NswPivUVS1n7gNfjV+LNteoFdVn3L9DxEXEB+yNXsv8IyqHnb9PuS7HtivqkdUtQ48TLwuTmm9m+kFYytwoTtiIEu8OflYizNN5jHgk+7xJ4n3HZxzIiLAfwJ7VPXbTaN8ydctInPc4wLx/pU9xIXjI63Mp6rrVbVHVZcQr2ebVPUTPmQDEJE2EZk19pi4LX4nHixbVT0EHBCRi92g64DdPmSb4GZONkeBH/leAtaISNF9f8c+u6mtd63eSdTqDngf8BxxW/fnPMhzH3FbY534l9VtxG3dG4G/Ak8AnS3K9k7izeq/ANtd9z6P8l0O/Mnl2wl80Q1fBmwB9hE3F+RavIyvATb4lM3l+LPrdo19FzxatiuBbW7ZPgp0+JLN5WsDBoBS0zAv8gFfBp5134n/BnJTXe/sTG9jjDGJzPQmKWOMMQlZwTDGGJOIFQxjjDGJWMEwxhiTiBUMY4wxiVjBMMYDInLN2BVsjfGVFQxjjDGJWMEw5gyIyC3unhvbReQud7HDEyLyHXfPgY0i0u2mXSkifxSRv4jII2P3QxCR5SLyhLtvxzMi8ib39O1N93y4152Za4w3rGAYk5CIrAA+Blyl8QUOQ+ATxGf5blPVNwNPAv/hZvkv4N9V9XJgR9Pwe4EfaHzfjncQn9kP8dV/7yS+N8sy4mv+GOON9N+fxBjjXEd8g5yt7sd/gfiCchHwgJvmf4CHRaQEzFHVJ93wnwMPuus1LVTVRwBUtQrgnm+Lqva6/u3E90X5/fS/LWOSsYJhTHIC/FxV158yUOQLE6ab6vV2ak2PQ+z7aTxjTVLGJLcR+IiIzIPx+11fQPw9Grvy58eB36vqceCYiFztht8KPKmqw0CviHzIPUdORIrn9F0YM0X2C8aYhFR1t4h8nviudAHxFYXvIL6hz2o3rp94PwfEl43+kSsILwD/5IbfCtwlIl9xz/HRc/g2jJkyu1qtMa+RiJxQ1fZW5zBmulmTlDHGmERsC8MYY0witoVhjDEmESsYxhhjErGCYYwxJhErGMYYYxKxgmGMMSYRKxjGGGMS+X+eNyGqizkaJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 934us/sample - loss: 0.9308 - acc: 0.7410\n",
      "Loss: 0.9308356591721809 Accuracy: 0.74101764\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8388 - acc: 0.4055\n",
      "Epoch 00001: val_loss improved from inf to 1.45153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/001-1.4515.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.8389 - acc: 0.4055 - val_loss: 1.4515 - val_acc: 0.5467\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3967 - acc: 0.5630\n",
      "Epoch 00002: val_loss improved from 1.45153 to 1.20839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/002-1.2084.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3966 - acc: 0.5630 - val_loss: 1.2084 - val_acc: 0.6380\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2412 - acc: 0.6194\n",
      "Epoch 00003: val_loss improved from 1.20839 to 1.11783, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/003-1.1178.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2412 - acc: 0.6195 - val_loss: 1.1178 - val_acc: 0.6646\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1462 - acc: 0.6484\n",
      "Epoch 00004: val_loss improved from 1.11783 to 1.05545, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/004-1.0554.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1461 - acc: 0.6484 - val_loss: 1.0554 - val_acc: 0.6804\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6750\n",
      "Epoch 00005: val_loss improved from 1.05545 to 0.98826, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/005-0.9883.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0708 - acc: 0.6750 - val_loss: 0.9883 - val_acc: 0.7095\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0107 - acc: 0.6961\n",
      "Epoch 00006: val_loss improved from 0.98826 to 0.95300, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/006-0.9530.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0107 - acc: 0.6961 - val_loss: 0.9530 - val_acc: 0.7174\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.7138\n",
      "Epoch 00007: val_loss improved from 0.95300 to 0.91875, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/007-0.9188.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9478 - acc: 0.7138 - val_loss: 0.9188 - val_acc: 0.7230\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9063 - acc: 0.7289\n",
      "Epoch 00008: val_loss improved from 0.91875 to 0.88082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/008-0.8808.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9063 - acc: 0.7289 - val_loss: 0.8808 - val_acc: 0.7470\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8573 - acc: 0.7430\n",
      "Epoch 00009: val_loss improved from 0.88082 to 0.83485, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/009-0.8349.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8573 - acc: 0.7430 - val_loss: 0.8349 - val_acc: 0.7636\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8161 - acc: 0.7562\n",
      "Epoch 00010: val_loss did not improve from 0.83485\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8160 - acc: 0.7563 - val_loss: 0.8491 - val_acc: 0.7498\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7795 - acc: 0.7683\n",
      "Epoch 00011: val_loss improved from 0.83485 to 0.80915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/011-0.8091.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7795 - acc: 0.7683 - val_loss: 0.8091 - val_acc: 0.7650\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7450 - acc: 0.7782\n",
      "Epoch 00012: val_loss improved from 0.80915 to 0.78990, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/012-0.7899.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7452 - acc: 0.7781 - val_loss: 0.7899 - val_acc: 0.7720\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7145 - acc: 0.7883\n",
      "Epoch 00013: val_loss improved from 0.78990 to 0.77567, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/013-0.7757.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7145 - acc: 0.7883 - val_loss: 0.7757 - val_acc: 0.7720\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7963\n",
      "Epoch 00014: val_loss improved from 0.77567 to 0.77182, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/014-0.7718.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6828 - acc: 0.7963 - val_loss: 0.7718 - val_acc: 0.7736\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.8043\n",
      "Epoch 00015: val_loss improved from 0.77182 to 0.74255, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/015-0.7425.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6657 - acc: 0.8043 - val_loss: 0.7425 - val_acc: 0.7897\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6399 - acc: 0.8096\n",
      "Epoch 00016: val_loss improved from 0.74255 to 0.73297, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/016-0.7330.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6400 - acc: 0.8096 - val_loss: 0.7330 - val_acc: 0.7904\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.8124\n",
      "Epoch 00017: val_loss improved from 0.73297 to 0.72923, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/017-0.7292.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6240 - acc: 0.8124 - val_loss: 0.7292 - val_acc: 0.7945\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8217\n",
      "Epoch 00018: val_loss improved from 0.72923 to 0.72605, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/018-0.7260.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5964 - acc: 0.8217 - val_loss: 0.7260 - val_acc: 0.7943\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.8315\n",
      "Epoch 00019: val_loss improved from 0.72605 to 0.71526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/019-0.7153.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5697 - acc: 0.8314 - val_loss: 0.7153 - val_acc: 0.7945\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.8330\n",
      "Epoch 00020: val_loss did not improve from 0.71526\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5541 - acc: 0.8331 - val_loss: 0.7174 - val_acc: 0.7862\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.8374\n",
      "Epoch 00021: val_loss improved from 0.71526 to 0.70283, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/021-0.7028.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5427 - acc: 0.8374 - val_loss: 0.7028 - val_acc: 0.8018\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.8452\n",
      "Epoch 00022: val_loss did not improve from 0.70283\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5207 - acc: 0.8453 - val_loss: 0.7078 - val_acc: 0.7952\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8492\n",
      "Epoch 00023: val_loss improved from 0.70283 to 0.68262, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/023-0.6826.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5076 - acc: 0.8492 - val_loss: 0.6826 - val_acc: 0.8006\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8535\n",
      "Epoch 00024: val_loss did not improve from 0.68262\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4898 - acc: 0.8535 - val_loss: 0.6985 - val_acc: 0.8004\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8570\n",
      "Epoch 00025: val_loss improved from 0.68262 to 0.66409, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/025-0.6641.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4792 - acc: 0.8570 - val_loss: 0.6641 - val_acc: 0.8109\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8597\n",
      "Epoch 00026: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4599 - acc: 0.8597 - val_loss: 0.6781 - val_acc: 0.8050\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8652\n",
      "Epoch 00027: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4495 - acc: 0.8652 - val_loss: 0.6921 - val_acc: 0.8041\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8671\n",
      "Epoch 00028: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4372 - acc: 0.8671 - val_loss: 0.6683 - val_acc: 0.8139\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8710\n",
      "Epoch 00029: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4239 - acc: 0.8711 - val_loss: 0.6740 - val_acc: 0.8125\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8709\n",
      "Epoch 00030: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4169 - acc: 0.8709 - val_loss: 0.6874 - val_acc: 0.8146\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8731\n",
      "Epoch 00031: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4096 - acc: 0.8731 - val_loss: 0.6852 - val_acc: 0.8081\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8778\n",
      "Epoch 00032: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3907 - acc: 0.8778 - val_loss: 0.6732 - val_acc: 0.8178\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8840\n",
      "Epoch 00033: val_loss did not improve from 0.66409\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3772 - acc: 0.8840 - val_loss: 0.6734 - val_acc: 0.8109\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8833\n",
      "Epoch 00034: val_loss improved from 0.66409 to 0.65550, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv_checkpoint/034-0.6555.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3761 - acc: 0.8832 - val_loss: 0.6555 - val_acc: 0.8132\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8871\n",
      "Epoch 00035: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3656 - acc: 0.8871 - val_loss: 0.6959 - val_acc: 0.8085\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8889\n",
      "Epoch 00036: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3581 - acc: 0.8890 - val_loss: 0.6653 - val_acc: 0.8169\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8908\n",
      "Epoch 00037: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3486 - acc: 0.8908 - val_loss: 0.6935 - val_acc: 0.8078\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8946\n",
      "Epoch 00038: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3412 - acc: 0.8946 - val_loss: 0.6965 - val_acc: 0.8106\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8956\n",
      "Epoch 00039: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3378 - acc: 0.8956 - val_loss: 0.6664 - val_acc: 0.8183\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9001\n",
      "Epoch 00040: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3227 - acc: 0.9000 - val_loss: 0.6769 - val_acc: 0.8157\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8996\n",
      "Epoch 00041: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3177 - acc: 0.8996 - val_loss: 0.7026 - val_acc: 0.8111\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9001\n",
      "Epoch 00042: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3182 - acc: 0.9001 - val_loss: 0.7020 - val_acc: 0.8078\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9023\n",
      "Epoch 00043: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3075 - acc: 0.9023 - val_loss: 0.6878 - val_acc: 0.8143\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9047\n",
      "Epoch 00044: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2995 - acc: 0.9047 - val_loss: 0.6740 - val_acc: 0.8211\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9061\n",
      "Epoch 00045: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2931 - acc: 0.9060 - val_loss: 0.7040 - val_acc: 0.8169\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9083\n",
      "Epoch 00046: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2907 - acc: 0.9084 - val_loss: 0.6908 - val_acc: 0.8206\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9104\n",
      "Epoch 00047: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2817 - acc: 0.9103 - val_loss: 0.6957 - val_acc: 0.8171\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9097\n",
      "Epoch 00048: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2811 - acc: 0.9097 - val_loss: 0.7104 - val_acc: 0.8104\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9121\n",
      "Epoch 00049: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2717 - acc: 0.9121 - val_loss: 0.7109 - val_acc: 0.8176\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9127\n",
      "Epoch 00050: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2693 - acc: 0.9127 - val_loss: 0.6945 - val_acc: 0.8199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9143\n",
      "Epoch 00051: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2642 - acc: 0.9144 - val_loss: 0.7009 - val_acc: 0.8162\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9168\n",
      "Epoch 00052: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2563 - acc: 0.9168 - val_loss: 0.7354 - val_acc: 0.8104\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9149\n",
      "Epoch 00053: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2608 - acc: 0.9149 - val_loss: 0.7062 - val_acc: 0.8181\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9193\n",
      "Epoch 00054: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2520 - acc: 0.9194 - val_loss: 0.7055 - val_acc: 0.8181\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9199\n",
      "Epoch 00055: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2490 - acc: 0.9199 - val_loss: 0.6946 - val_acc: 0.8258\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9195\n",
      "Epoch 00056: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2475 - acc: 0.9195 - val_loss: 0.6870 - val_acc: 0.8190\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9183\n",
      "Epoch 00057: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2510 - acc: 0.9183 - val_loss: 0.6947 - val_acc: 0.8185\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9227\n",
      "Epoch 00058: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2391 - acc: 0.9228 - val_loss: 0.6952 - val_acc: 0.8213\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9246\n",
      "Epoch 00059: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2287 - acc: 0.9247 - val_loss: 0.7317 - val_acc: 0.8150\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9244\n",
      "Epoch 00060: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2308 - acc: 0.9244 - val_loss: 0.6880 - val_acc: 0.8230\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9261\n",
      "Epoch 00061: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2294 - acc: 0.9262 - val_loss: 0.6946 - val_acc: 0.8260\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9258\n",
      "Epoch 00062: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2264 - acc: 0.9257 - val_loss: 0.7136 - val_acc: 0.8218\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9283\n",
      "Epoch 00063: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2218 - acc: 0.9283 - val_loss: 0.7264 - val_acc: 0.8209\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9298\n",
      "Epoch 00064: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2188 - acc: 0.9298 - val_loss: 0.7045 - val_acc: 0.8211\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9297\n",
      "Epoch 00065: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2189 - acc: 0.9297 - val_loss: 0.7380 - val_acc: 0.8130\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9287\n",
      "Epoch 00066: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2230 - acc: 0.9287 - val_loss: 0.7340 - val_acc: 0.8225\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9324\n",
      "Epoch 00067: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2070 - acc: 0.9323 - val_loss: 0.7259 - val_acc: 0.8220\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9281\n",
      "Epoch 00068: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2185 - acc: 0.9281 - val_loss: 0.7240 - val_acc: 0.8225\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9343\n",
      "Epoch 00069: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2065 - acc: 0.9343 - val_loss: 0.7132 - val_acc: 0.8276\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9305\n",
      "Epoch 00070: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2112 - acc: 0.9305 - val_loss: 0.7008 - val_acc: 0.8213\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9323\n",
      "Epoch 00071: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2068 - acc: 0.9323 - val_loss: 0.7068 - val_acc: 0.8265\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9332\n",
      "Epoch 00072: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2030 - acc: 0.9332 - val_loss: 0.7154 - val_acc: 0.8260\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9338\n",
      "Epoch 00073: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1981 - acc: 0.9338 - val_loss: 0.6901 - val_acc: 0.8323\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9351\n",
      "Epoch 00074: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1981 - acc: 0.9351 - val_loss: 0.7035 - val_acc: 0.8272\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9373\n",
      "Epoch 00075: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1931 - acc: 0.9373 - val_loss: 0.7213 - val_acc: 0.8248\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9364\n",
      "Epoch 00076: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1929 - acc: 0.9364 - val_loss: 0.7423 - val_acc: 0.8262\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9349\n",
      "Epoch 00077: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1953 - acc: 0.9349 - val_loss: 0.7124 - val_acc: 0.8248\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9353\n",
      "Epoch 00078: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1936 - acc: 0.9353 - val_loss: 0.7218 - val_acc: 0.8258\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9386\n",
      "Epoch 00079: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1895 - acc: 0.9386 - val_loss: 0.7179 - val_acc: 0.8265\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9397\n",
      "Epoch 00080: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1857 - acc: 0.9397 - val_loss: 0.7028 - val_acc: 0.8307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9382\n",
      "Epoch 00081: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1872 - acc: 0.9382 - val_loss: 0.7699 - val_acc: 0.8123\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9388\n",
      "Epoch 00082: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1858 - acc: 0.9388 - val_loss: 0.7282 - val_acc: 0.8253\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9385\n",
      "Epoch 00083: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1874 - acc: 0.9385 - val_loss: 0.7052 - val_acc: 0.8318\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9389\n",
      "Epoch 00084: val_loss did not improve from 0.65550\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1834 - acc: 0.9388 - val_loss: 0.7274 - val_acc: 0.8239\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 973us/sample - loss: 0.7363 - acc: 0.7848\n",
      "Loss: 0.7362892188758493 Accuracy: 0.78483903\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0107 - acc: 0.3380\n",
      "Epoch 00001: val_loss improved from inf to 1.51269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/001-1.5127.hdf5\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 2.0106 - acc: 0.3380 - val_loss: 1.5127 - val_acc: 0.5302\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5392 - acc: 0.5048\n",
      "Epoch 00002: val_loss improved from 1.51269 to 1.24850, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/002-1.2485.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.5391 - acc: 0.5048 - val_loss: 1.2485 - val_acc: 0.6385\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3119 - acc: 0.5894\n",
      "Epoch 00003: val_loss improved from 1.24850 to 1.10558, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/003-1.1056.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3120 - acc: 0.5894 - val_loss: 1.1056 - val_acc: 0.6783\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.6340\n",
      "Epoch 00004: val_loss improved from 1.10558 to 1.01279, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/004-1.0128.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1843 - acc: 0.6340 - val_loss: 1.0128 - val_acc: 0.7093\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6667\n",
      "Epoch 00005: val_loss improved from 1.01279 to 0.94886, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/005-0.9489.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0948 - acc: 0.6667 - val_loss: 0.9489 - val_acc: 0.7191\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0274 - acc: 0.6862\n",
      "Epoch 00006: val_loss improved from 0.94886 to 0.86642, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/006-0.8664.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0273 - acc: 0.6862 - val_loss: 0.8664 - val_acc: 0.7414\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9649 - acc: 0.7087\n",
      "Epoch 00007: val_loss improved from 0.86642 to 0.85195, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/007-0.8520.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9648 - acc: 0.7087 - val_loss: 0.8520 - val_acc: 0.7503\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7293\n",
      "Epoch 00008: val_loss improved from 0.85195 to 0.81628, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/008-0.8163.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9121 - acc: 0.7293 - val_loss: 0.8163 - val_acc: 0.7580\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8704 - acc: 0.7415\n",
      "Epoch 00009: val_loss improved from 0.81628 to 0.75205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/009-0.7520.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8704 - acc: 0.7415 - val_loss: 0.7520 - val_acc: 0.7785\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8278 - acc: 0.7542\n",
      "Epoch 00010: val_loss did not improve from 0.75205\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8277 - acc: 0.7542 - val_loss: 0.7532 - val_acc: 0.7741\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7852 - acc: 0.7674\n",
      "Epoch 00011: val_loss improved from 0.75205 to 0.67598, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/011-0.6760.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7852 - acc: 0.7674 - val_loss: 0.6760 - val_acc: 0.8097\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7506 - acc: 0.7801\n",
      "Epoch 00012: val_loss improved from 0.67598 to 0.65600, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/012-0.6560.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7505 - acc: 0.7802 - val_loss: 0.6560 - val_acc: 0.8143\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7873\n",
      "Epoch 00013: val_loss improved from 0.65600 to 0.64600, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/013-0.6460.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7237 - acc: 0.7873 - val_loss: 0.6460 - val_acc: 0.8125\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.7956\n",
      "Epoch 00014: val_loss improved from 0.64600 to 0.61286, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/014-0.6129.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6948 - acc: 0.7956 - val_loss: 0.6129 - val_acc: 0.8253\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.8034\n",
      "Epoch 00015: val_loss did not improve from 0.61286\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6671 - acc: 0.8034 - val_loss: 0.6203 - val_acc: 0.8239\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6465 - acc: 0.8094\n",
      "Epoch 00016: val_loss improved from 0.61286 to 0.58275, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/016-0.5827.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6465 - acc: 0.8094 - val_loss: 0.5827 - val_acc: 0.8325\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.8154\n",
      "Epoch 00017: val_loss improved from 0.58275 to 0.57570, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/017-0.5757.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6277 - acc: 0.8154 - val_loss: 0.5757 - val_acc: 0.8330\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8202\n",
      "Epoch 00018: val_loss did not improve from 0.57570\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6106 - acc: 0.8203 - val_loss: 0.5784 - val_acc: 0.8334\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.8267\n",
      "Epoch 00019: val_loss improved from 0.57570 to 0.56375, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/019-0.5638.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5916 - acc: 0.8267 - val_loss: 0.5638 - val_acc: 0.8374\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8298\n",
      "Epoch 00020: val_loss improved from 0.56375 to 0.53799, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/020-0.5380.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5790 - acc: 0.8298 - val_loss: 0.5380 - val_acc: 0.8456\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8379\n",
      "Epoch 00021: val_loss did not improve from 0.53799\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5601 - acc: 0.8379 - val_loss: 0.5706 - val_acc: 0.8379\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.8378\n",
      "Epoch 00022: val_loss improved from 0.53799 to 0.51249, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/022-0.5125.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5461 - acc: 0.8378 - val_loss: 0.5125 - val_acc: 0.8567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8436\n",
      "Epoch 00023: val_loss improved from 0.51249 to 0.50594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/023-0.5059.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5340 - acc: 0.8437 - val_loss: 0.5059 - val_acc: 0.8528\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8475\n",
      "Epoch 00024: val_loss improved from 0.50594 to 0.50569, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/024-0.5057.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5216 - acc: 0.8475 - val_loss: 0.5057 - val_acc: 0.8609\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8505\n",
      "Epoch 00025: val_loss did not improve from 0.50569\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5067 - acc: 0.8505 - val_loss: 0.5122 - val_acc: 0.8535\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8486\n",
      "Epoch 00026: val_loss did not improve from 0.50569\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5032 - acc: 0.8486 - val_loss: 0.5120 - val_acc: 0.8516\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8572\n",
      "Epoch 00027: val_loss did not improve from 0.50569\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4852 - acc: 0.8572 - val_loss: 0.5590 - val_acc: 0.8432\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8591\n",
      "Epoch 00028: val_loss improved from 0.50569 to 0.49842, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/028-0.4984.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4799 - acc: 0.8590 - val_loss: 0.4984 - val_acc: 0.8546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.8616\n",
      "Epoch 00029: val_loss improved from 0.49842 to 0.49756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/029-0.4976.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4671 - acc: 0.8616 - val_loss: 0.4976 - val_acc: 0.8602\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.8648\n",
      "Epoch 00030: val_loss improved from 0.49756 to 0.47394, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/030-0.4739.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4533 - acc: 0.8648 - val_loss: 0.4739 - val_acc: 0.8630\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8679\n",
      "Epoch 00031: val_loss improved from 0.47394 to 0.46593, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/031-0.4659.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4467 - acc: 0.8679 - val_loss: 0.4659 - val_acc: 0.8686\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.8690\n",
      "Epoch 00032: val_loss did not improve from 0.46593\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4395 - acc: 0.8690 - val_loss: 0.4690 - val_acc: 0.8686\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8715\n",
      "Epoch 00033: val_loss improved from 0.46593 to 0.45678, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/033-0.4568.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4315 - acc: 0.8715 - val_loss: 0.4568 - val_acc: 0.8691\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8755\n",
      "Epoch 00034: val_loss did not improve from 0.45678\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4194 - acc: 0.8755 - val_loss: 0.4674 - val_acc: 0.8721\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8760\n",
      "Epoch 00035: val_loss did not improve from 0.45678\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4148 - acc: 0.8759 - val_loss: 0.4718 - val_acc: 0.8728\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8788\n",
      "Epoch 00036: val_loss did not improve from 0.45678\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4096 - acc: 0.8788 - val_loss: 0.4634 - val_acc: 0.8721\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8811\n",
      "Epoch 00037: val_loss improved from 0.45678 to 0.43401, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/037-0.4340.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3966 - acc: 0.8810 - val_loss: 0.4340 - val_acc: 0.8770\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8843\n",
      "Epoch 00038: val_loss did not improve from 0.43401\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3922 - acc: 0.8843 - val_loss: 0.4554 - val_acc: 0.8737\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8867\n",
      "Epoch 00039: val_loss did not improve from 0.43401\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3790 - acc: 0.8867 - val_loss: 0.4659 - val_acc: 0.8733\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8862\n",
      "Epoch 00040: val_loss improved from 0.43401 to 0.43321, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/040-0.4332.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3814 - acc: 0.8862 - val_loss: 0.4332 - val_acc: 0.8747\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8911\n",
      "Epoch 00041: val_loss did not improve from 0.43321\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3627 - acc: 0.8911 - val_loss: 0.4383 - val_acc: 0.8810\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8885\n",
      "Epoch 00042: val_loss did not improve from 0.43321\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3679 - acc: 0.8885 - val_loss: 0.4420 - val_acc: 0.8782\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8941\n",
      "Epoch 00043: val_loss improved from 0.43321 to 0.42928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/043-0.4293.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3502 - acc: 0.8941 - val_loss: 0.4293 - val_acc: 0.8803\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8943\n",
      "Epoch 00044: val_loss did not improve from 0.42928\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3495 - acc: 0.8943 - val_loss: 0.4368 - val_acc: 0.8824\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8976\n",
      "Epoch 00045: val_loss improved from 0.42928 to 0.41205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/045-0.4121.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3408 - acc: 0.8976 - val_loss: 0.4121 - val_acc: 0.8852\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.9001\n",
      "Epoch 00046: val_loss did not improve from 0.41205\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3336 - acc: 0.9000 - val_loss: 0.4163 - val_acc: 0.8903\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.9018\n",
      "Epoch 00047: val_loss did not improve from 0.41205\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3261 - acc: 0.9018 - val_loss: 0.4234 - val_acc: 0.8896\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9036\n",
      "Epoch 00048: val_loss did not improve from 0.41205\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3207 - acc: 0.9036 - val_loss: 0.4249 - val_acc: 0.8838\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.9013\n",
      "Epoch 00049: val_loss did not improve from 0.41205\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3215 - acc: 0.9013 - val_loss: 0.4183 - val_acc: 0.8838\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9053\n",
      "Epoch 00050: val_loss improved from 0.41205 to 0.40533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/050-0.4053.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3135 - acc: 0.9053 - val_loss: 0.4053 - val_acc: 0.8915\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9051\n",
      "Epoch 00051: val_loss did not improve from 0.40533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3087 - acc: 0.9050 - val_loss: 0.4414 - val_acc: 0.8870\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9065\n",
      "Epoch 00052: val_loss did not improve from 0.40533\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3046 - acc: 0.9065 - val_loss: 0.4275 - val_acc: 0.8866\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9089\n",
      "Epoch 00053: val_loss improved from 0.40533 to 0.40081, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/053-0.4008.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2969 - acc: 0.9089 - val_loss: 0.4008 - val_acc: 0.8915\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9134\n",
      "Epoch 00054: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2858 - acc: 0.9134 - val_loss: 0.4222 - val_acc: 0.8884\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9128\n",
      "Epoch 00055: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2877 - acc: 0.9128 - val_loss: 0.4328 - val_acc: 0.8882\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9135\n",
      "Epoch 00056: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2775 - acc: 0.9135 - val_loss: 0.4159 - val_acc: 0.8863\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9146\n",
      "Epoch 00057: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2765 - acc: 0.9146 - val_loss: 0.4074 - val_acc: 0.8928\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9156\n",
      "Epoch 00058: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2718 - acc: 0.9156 - val_loss: 0.4082 - val_acc: 0.8870\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9187\n",
      "Epoch 00059: val_loss did not improve from 0.40081\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2686 - acc: 0.9187 - val_loss: 0.4183 - val_acc: 0.8861\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9200\n",
      "Epoch 00060: val_loss improved from 0.40081 to 0.38112, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/060-0.3811.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2608 - acc: 0.9200 - val_loss: 0.3811 - val_acc: 0.8956\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9210\n",
      "Epoch 00061: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2563 - acc: 0.9210 - val_loss: 0.4020 - val_acc: 0.8924\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9205\n",
      "Epoch 00062: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2554 - acc: 0.9205 - val_loss: 0.4241 - val_acc: 0.8924\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9219\n",
      "Epoch 00063: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2496 - acc: 0.9218 - val_loss: 0.3890 - val_acc: 0.8919\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9217\n",
      "Epoch 00064: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2564 - acc: 0.9217 - val_loss: 0.3857 - val_acc: 0.9022\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9232\n",
      "Epoch 00065: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2408 - acc: 0.9232 - val_loss: 0.4549 - val_acc: 0.8921\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9220\n",
      "Epoch 00066: val_loss did not improve from 0.38112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2542 - acc: 0.9220 - val_loss: 0.3917 - val_acc: 0.8975\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9279\n",
      "Epoch 00067: val_loss improved from 0.38112 to 0.36306, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv_checkpoint/067-0.3631.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2355 - acc: 0.9279 - val_loss: 0.3631 - val_acc: 0.9024\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9278\n",
      "Epoch 00068: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2323 - acc: 0.9278 - val_loss: 0.3993 - val_acc: 0.8952\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9305\n",
      "Epoch 00069: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2247 - acc: 0.9305 - val_loss: 0.3854 - val_acc: 0.9010\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9307\n",
      "Epoch 00070: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2211 - acc: 0.9306 - val_loss: 0.4039 - val_acc: 0.8973\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9299\n",
      "Epoch 00071: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2237 - acc: 0.9300 - val_loss: 0.3862 - val_acc: 0.8989\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9328\n",
      "Epoch 00072: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2180 - acc: 0.9328 - val_loss: 0.3819 - val_acc: 0.8994\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9320\n",
      "Epoch 00073: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2157 - acc: 0.9320 - val_loss: 0.3929 - val_acc: 0.8991\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9343\n",
      "Epoch 00074: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2099 - acc: 0.9343 - val_loss: 0.4121 - val_acc: 0.8928\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9330\n",
      "Epoch 00075: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2106 - acc: 0.9330 - val_loss: 0.3937 - val_acc: 0.8956\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9367\n",
      "Epoch 00076: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2047 - acc: 0.9367 - val_loss: 0.3886 - val_acc: 0.9001\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9357\n",
      "Epoch 00077: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2062 - acc: 0.9357 - val_loss: 0.4245 - val_acc: 0.8970\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9395\n",
      "Epoch 00078: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1940 - acc: 0.9395 - val_loss: 0.4186 - val_acc: 0.8966\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9344\n",
      "Epoch 00079: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2053 - acc: 0.9344 - val_loss: 0.3791 - val_acc: 0.9015\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9378\n",
      "Epoch 00080: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1992 - acc: 0.9378 - val_loss: 0.3729 - val_acc: 0.8994\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9383\n",
      "Epoch 00081: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1952 - acc: 0.9383 - val_loss: 0.4170 - val_acc: 0.8945\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9383\n",
      "Epoch 00082: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1940 - acc: 0.9383 - val_loss: 0.4169 - val_acc: 0.8926\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9390\n",
      "Epoch 00083: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1960 - acc: 0.9391 - val_loss: 0.3917 - val_acc: 0.8977\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9414\n",
      "Epoch 00084: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1828 - acc: 0.9414 - val_loss: 0.3906 - val_acc: 0.9047\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9406\n",
      "Epoch 00085: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1839 - acc: 0.9406 - val_loss: 0.3976 - val_acc: 0.8954\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9432\n",
      "Epoch 00086: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1813 - acc: 0.9432 - val_loss: 0.4005 - val_acc: 0.8987\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9429\n",
      "Epoch 00087: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1789 - acc: 0.9429 - val_loss: 0.4329 - val_acc: 0.8928\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9437\n",
      "Epoch 00088: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1766 - acc: 0.9437 - val_loss: 0.3897 - val_acc: 0.8989\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9446\n",
      "Epoch 00089: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1737 - acc: 0.9446 - val_loss: 0.3995 - val_acc: 0.9022\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9437\n",
      "Epoch 00090: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1754 - acc: 0.9437 - val_loss: 0.3643 - val_acc: 0.9082\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9449\n",
      "Epoch 00091: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1761 - acc: 0.9449 - val_loss: 0.3915 - val_acc: 0.9005\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9452\n",
      "Epoch 00092: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1714 - acc: 0.9452 - val_loss: 0.3934 - val_acc: 0.8987\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9485\n",
      "Epoch 00093: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1624 - acc: 0.9484 - val_loss: 0.3950 - val_acc: 0.9015\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9446\n",
      "Epoch 00094: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1700 - acc: 0.9447 - val_loss: 0.3682 - val_acc: 0.9064\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9496\n",
      "Epoch 00095: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1610 - acc: 0.9497 - val_loss: 0.4218 - val_acc: 0.9012\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9477\n",
      "Epoch 00096: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1636 - acc: 0.9477 - val_loss: 0.4020 - val_acc: 0.8963\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9508\n",
      "Epoch 00097: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1545 - acc: 0.9508 - val_loss: 0.4018 - val_acc: 0.8952\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9515\n",
      "Epoch 00098: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1526 - acc: 0.9515 - val_loss: 0.4105 - val_acc: 0.8984\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9500\n",
      "Epoch 00099: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1555 - acc: 0.9500 - val_loss: 0.3893 - val_acc: 0.9003\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9514\n",
      "Epoch 00100: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1512 - acc: 0.9514 - val_loss: 0.4198 - val_acc: 0.8970\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9507\n",
      "Epoch 00101: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1542 - acc: 0.9507 - val_loss: 0.3719 - val_acc: 0.9096\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9517\n",
      "Epoch 00102: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1480 - acc: 0.9517 - val_loss: 0.4195 - val_acc: 0.9040\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9509\n",
      "Epoch 00103: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1531 - acc: 0.9509 - val_loss: 0.3901 - val_acc: 0.9036\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9532\n",
      "Epoch 00104: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1454 - acc: 0.9532 - val_loss: 0.3728 - val_acc: 0.9094\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9523\n",
      "Epoch 00105: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1472 - acc: 0.9523 - val_loss: 0.4142 - val_acc: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9534\n",
      "Epoch 00106: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1424 - acc: 0.9534 - val_loss: 0.3644 - val_acc: 0.9071\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9545\n",
      "Epoch 00107: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1424 - acc: 0.9545 - val_loss: 0.3921 - val_acc: 0.9029\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9535\n",
      "Epoch 00108: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1393 - acc: 0.9535 - val_loss: 0.3968 - val_acc: 0.9061\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9551\n",
      "Epoch 00109: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1396 - acc: 0.9551 - val_loss: 0.3929 - val_acc: 0.9024\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9555\n",
      "Epoch 00110: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1379 - acc: 0.9555 - val_loss: 0.3940 - val_acc: 0.9045\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9567\n",
      "Epoch 00111: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1350 - acc: 0.9567 - val_loss: 0.4080 - val_acc: 0.9047\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9563\n",
      "Epoch 00112: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1352 - acc: 0.9562 - val_loss: 0.3898 - val_acc: 0.9019\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9559\n",
      "Epoch 00113: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1397 - acc: 0.9559 - val_loss: 0.4085 - val_acc: 0.9029\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9589\n",
      "Epoch 00114: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1266 - acc: 0.9589 - val_loss: 0.3978 - val_acc: 0.9038\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9563\n",
      "Epoch 00115: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1333 - acc: 0.9563 - val_loss: 0.3984 - val_acc: 0.9026\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9598\n",
      "Epoch 00116: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1286 - acc: 0.9598 - val_loss: 0.3922 - val_acc: 0.8998\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9580\n",
      "Epoch 00117: val_loss did not improve from 0.36306\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1290 - acc: 0.9580 - val_loss: 0.3696 - val_acc: 0.9082\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 989us/sample - loss: 0.4636 - acc: 0.8696\n",
      "Loss: 0.4636152361412286 Accuracy: 0.86957425\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0965 - acc: 0.3146\n",
      "Epoch 00001: val_loss improved from inf to 1.47757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/001-1.4776.hdf5\n",
      "36805/36805 [==============================] - 98s 3ms/sample - loss: 2.0964 - acc: 0.3146 - val_loss: 1.4776 - val_acc: 0.5528\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5658 - acc: 0.4996\n",
      "Epoch 00002: val_loss improved from 1.47757 to 1.22458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/002-1.2246.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.5657 - acc: 0.4996 - val_loss: 1.2246 - val_acc: 0.6364\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3246 - acc: 0.5869\n",
      "Epoch 00003: val_loss improved from 1.22458 to 1.02603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/003-1.0260.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.3246 - acc: 0.5869 - val_loss: 1.0260 - val_acc: 0.6937\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1641 - acc: 0.6390\n",
      "Epoch 00004: val_loss improved from 1.02603 to 0.91233, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/004-0.9123.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1641 - acc: 0.6390 - val_loss: 0.9123 - val_acc: 0.7296\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0544 - acc: 0.6753\n",
      "Epoch 00005: val_loss improved from 0.91233 to 0.87271, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/005-0.8727.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0545 - acc: 0.6753 - val_loss: 0.8727 - val_acc: 0.7498\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9587 - acc: 0.7090\n",
      "Epoch 00006: val_loss improved from 0.87271 to 0.76009, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/006-0.7601.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9587 - acc: 0.7090 - val_loss: 0.7601 - val_acc: 0.7794\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8708 - acc: 0.7369\n",
      "Epoch 00007: val_loss improved from 0.76009 to 0.66508, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/007-0.6651.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8708 - acc: 0.7369 - val_loss: 0.6651 - val_acc: 0.8174\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7970 - acc: 0.7645\n",
      "Epoch 00008: val_loss improved from 0.66508 to 0.65668, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/008-0.6567.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7970 - acc: 0.7645 - val_loss: 0.6567 - val_acc: 0.8113\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.7789\n",
      "Epoch 00009: val_loss improved from 0.65668 to 0.56786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/009-0.5679.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7419 - acc: 0.7788 - val_loss: 0.5679 - val_acc: 0.8381\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7955\n",
      "Epoch 00010: val_loss improved from 0.56786 to 0.53686, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/010-0.5369.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6918 - acc: 0.7955 - val_loss: 0.5369 - val_acc: 0.8505\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.8096\n",
      "Epoch 00011: val_loss improved from 0.53686 to 0.52308, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/011-0.5231.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6459 - acc: 0.8096 - val_loss: 0.5231 - val_acc: 0.8532\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.8214\n",
      "Epoch 00012: val_loss improved from 0.52308 to 0.47579, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/012-0.4758.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6132 - acc: 0.8214 - val_loss: 0.4758 - val_acc: 0.8730\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8326\n",
      "Epoch 00013: val_loss did not improve from 0.47579\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5770 - acc: 0.8326 - val_loss: 0.4881 - val_acc: 0.8658\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8418\n",
      "Epoch 00014: val_loss improved from 0.47579 to 0.44007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/014-0.4401.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5466 - acc: 0.8418 - val_loss: 0.4401 - val_acc: 0.8779\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8482\n",
      "Epoch 00015: val_loss improved from 0.44007 to 0.41185, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/015-0.4118.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5211 - acc: 0.8482 - val_loss: 0.4118 - val_acc: 0.8908\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8555\n",
      "Epoch 00016: val_loss improved from 0.41185 to 0.38956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/016-0.3896.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4969 - acc: 0.8555 - val_loss: 0.3896 - val_acc: 0.8940\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8629\n",
      "Epoch 00017: val_loss improved from 0.38956 to 0.36808, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/017-0.3681.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4785 - acc: 0.8629 - val_loss: 0.3681 - val_acc: 0.9026\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8677\n",
      "Epoch 00018: val_loss improved from 0.36808 to 0.36486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/018-0.3649.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4592 - acc: 0.8677 - val_loss: 0.3649 - val_acc: 0.9038\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8716\n",
      "Epoch 00019: val_loss improved from 0.36486 to 0.35443, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/019-0.3544.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4406 - acc: 0.8716 - val_loss: 0.3544 - val_acc: 0.9054\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8751\n",
      "Epoch 00020: val_loss did not improve from 0.35443\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4311 - acc: 0.8751 - val_loss: 0.3640 - val_acc: 0.9012\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8797\n",
      "Epoch 00021: val_loss improved from 0.35443 to 0.34447, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/021-0.3445.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4124 - acc: 0.8797 - val_loss: 0.3445 - val_acc: 0.9071\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8848\n",
      "Epoch 00022: val_loss did not improve from 0.34447\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3993 - acc: 0.8848 - val_loss: 0.3489 - val_acc: 0.9061\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8873\n",
      "Epoch 00023: val_loss improved from 0.34447 to 0.32240, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/023-0.3224.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3872 - acc: 0.8873 - val_loss: 0.3224 - val_acc: 0.9085\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8910\n",
      "Epoch 00024: val_loss did not improve from 0.32240\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3724 - acc: 0.8910 - val_loss: 0.3403 - val_acc: 0.9052\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8926\n",
      "Epoch 00025: val_loss did not improve from 0.32240\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3648 - acc: 0.8925 - val_loss: 0.3378 - val_acc: 0.9115\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8978\n",
      "Epoch 00026: val_loss improved from 0.32240 to 0.30722, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/026-0.3072.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3544 - acc: 0.8978 - val_loss: 0.3072 - val_acc: 0.9161\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8980\n",
      "Epoch 00027: val_loss improved from 0.30722 to 0.28948, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/027-0.2895.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3469 - acc: 0.8980 - val_loss: 0.2895 - val_acc: 0.9285\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.9032\n",
      "Epoch 00028: val_loss did not improve from 0.28948\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3315 - acc: 0.9032 - val_loss: 0.3328 - val_acc: 0.9106\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.9058\n",
      "Epoch 00029: val_loss did not improve from 0.28948\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3237 - acc: 0.9059 - val_loss: 0.2895 - val_acc: 0.9257\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9081\n",
      "Epoch 00030: val_loss improved from 0.28948 to 0.28757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/030-0.2876.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3167 - acc: 0.9081 - val_loss: 0.2876 - val_acc: 0.9208\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9099\n",
      "Epoch 00031: val_loss did not improve from 0.28757\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3087 - acc: 0.9099 - val_loss: 0.3163 - val_acc: 0.9161\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9113\n",
      "Epoch 00032: val_loss did not improve from 0.28757\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3032 - acc: 0.9113 - val_loss: 0.3064 - val_acc: 0.9203\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9135\n",
      "Epoch 00033: val_loss did not improve from 0.28757\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2962 - acc: 0.9135 - val_loss: 0.2959 - val_acc: 0.9220\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9147\n",
      "Epoch 00034: val_loss improved from 0.28757 to 0.26515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/034-0.2652.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2920 - acc: 0.9147 - val_loss: 0.2652 - val_acc: 0.9294\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9173\n",
      "Epoch 00035: val_loss did not improve from 0.26515\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2804 - acc: 0.9172 - val_loss: 0.2843 - val_acc: 0.9257\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9177\n",
      "Epoch 00036: val_loss did not improve from 0.26515\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2845 - acc: 0.9177 - val_loss: 0.2802 - val_acc: 0.9266\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9183\n",
      "Epoch 00037: val_loss improved from 0.26515 to 0.25559, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/037-0.2556.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2714 - acc: 0.9184 - val_loss: 0.2556 - val_acc: 0.9345\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9215\n",
      "Epoch 00038: val_loss did not improve from 0.25559\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2662 - acc: 0.9216 - val_loss: 0.2583 - val_acc: 0.9292\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9242\n",
      "Epoch 00039: val_loss did not improve from 0.25559\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2614 - acc: 0.9242 - val_loss: 0.2936 - val_acc: 0.9227\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9251\n",
      "Epoch 00040: val_loss did not improve from 0.25559\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2545 - acc: 0.9251 - val_loss: 0.2752 - val_acc: 0.9290\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9261\n",
      "Epoch 00041: val_loss improved from 0.25559 to 0.24733, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/041-0.2473.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2480 - acc: 0.9261 - val_loss: 0.2473 - val_acc: 0.9338\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9292\n",
      "Epoch 00042: val_loss did not improve from 0.24733\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2401 - acc: 0.9292 - val_loss: 0.3006 - val_acc: 0.9271\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9306\n",
      "Epoch 00043: val_loss improved from 0.24733 to 0.23071, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/043-0.2307.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2344 - acc: 0.9306 - val_loss: 0.2307 - val_acc: 0.9399\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9318\n",
      "Epoch 00044: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2288 - acc: 0.9318 - val_loss: 0.2405 - val_acc: 0.9350\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9335\n",
      "Epoch 00045: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2268 - acc: 0.9335 - val_loss: 0.2464 - val_acc: 0.9366\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9341\n",
      "Epoch 00046: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2231 - acc: 0.9341 - val_loss: 0.2561 - val_acc: 0.9338\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9345\n",
      "Epoch 00047: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2205 - acc: 0.9345 - val_loss: 0.2581 - val_acc: 0.9311\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9342\n",
      "Epoch 00048: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2180 - acc: 0.9342 - val_loss: 0.2455 - val_acc: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9348\n",
      "Epoch 00049: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2180 - acc: 0.9347 - val_loss: 0.2435 - val_acc: 0.9362\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9386\n",
      "Epoch 00050: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2088 - acc: 0.9385 - val_loss: 0.2505 - val_acc: 0.9341\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9373\n",
      "Epoch 00051: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2124 - acc: 0.9373 - val_loss: 0.2393 - val_acc: 0.9422\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9401\n",
      "Epoch 00052: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2000 - acc: 0.9401 - val_loss: 0.2536 - val_acc: 0.9338\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9413\n",
      "Epoch 00053: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1946 - acc: 0.9413 - val_loss: 0.2360 - val_acc: 0.9392\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9412\n",
      "Epoch 00054: val_loss did not improve from 0.23071\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1932 - acc: 0.9412 - val_loss: 0.2381 - val_acc: 0.9383\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9438\n",
      "Epoch 00055: val_loss improved from 0.23071 to 0.23027, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/055-0.2303.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1863 - acc: 0.9438 - val_loss: 0.2303 - val_acc: 0.9394\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9450\n",
      "Epoch 00056: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1826 - acc: 0.9450 - val_loss: 0.2533 - val_acc: 0.9392\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9451\n",
      "Epoch 00057: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1829 - acc: 0.9451 - val_loss: 0.2364 - val_acc: 0.9408\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9471\n",
      "Epoch 00058: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1755 - acc: 0.9471 - val_loss: 0.2570 - val_acc: 0.9387\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9467\n",
      "Epoch 00059: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1730 - acc: 0.9467 - val_loss: 0.2429 - val_acc: 0.9348\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9463\n",
      "Epoch 00060: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1732 - acc: 0.9463 - val_loss: 0.2431 - val_acc: 0.9429\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9469\n",
      "Epoch 00061: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1766 - acc: 0.9469 - val_loss: 0.2367 - val_acc: 0.9380\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9493\n",
      "Epoch 00062: val_loss did not improve from 0.23027\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1675 - acc: 0.9493 - val_loss: 0.2434 - val_acc: 0.9369\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9498\n",
      "Epoch 00063: val_loss improved from 0.23027 to 0.22840, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/063-0.2284.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1638 - acc: 0.9498 - val_loss: 0.2284 - val_acc: 0.9411\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9521\n",
      "Epoch 00064: val_loss did not improve from 0.22840\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1569 - acc: 0.9521 - val_loss: 0.2294 - val_acc: 0.9436\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9541\n",
      "Epoch 00065: val_loss did not improve from 0.22840\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1551 - acc: 0.9541 - val_loss: 0.2294 - val_acc: 0.9422\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9519\n",
      "Epoch 00066: val_loss did not improve from 0.22840\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1600 - acc: 0.9519 - val_loss: 0.2469 - val_acc: 0.9401\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9514\n",
      "Epoch 00067: val_loss improved from 0.22840 to 0.22735, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/067-0.2274.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1612 - acc: 0.9514 - val_loss: 0.2274 - val_acc: 0.9420\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9520\n",
      "Epoch 00068: val_loss improved from 0.22735 to 0.22524, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/068-0.2252.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1544 - acc: 0.9520 - val_loss: 0.2252 - val_acc: 0.9429\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9570\n",
      "Epoch 00069: val_loss did not improve from 0.22524\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1427 - acc: 0.9570 - val_loss: 0.2310 - val_acc: 0.9387\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9579\n",
      "Epoch 00070: val_loss improved from 0.22524 to 0.21714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/070-0.2171.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1376 - acc: 0.9579 - val_loss: 0.2171 - val_acc: 0.9422\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9552\n",
      "Epoch 00071: val_loss did not improve from 0.21714\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1433 - acc: 0.9553 - val_loss: 0.2463 - val_acc: 0.9385\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9563\n",
      "Epoch 00072: val_loss did not improve from 0.21714\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1384 - acc: 0.9563 - val_loss: 0.2291 - val_acc: 0.9462\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9596\n",
      "Epoch 00073: val_loss did not improve from 0.21714\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1311 - acc: 0.9596 - val_loss: 0.2425 - val_acc: 0.9387\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9552\n",
      "Epoch 00074: val_loss improved from 0.21714 to 0.20628, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv_checkpoint/074-0.2063.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1413 - acc: 0.9552 - val_loss: 0.2063 - val_acc: 0.9476\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9614\n",
      "Epoch 00075: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1268 - acc: 0.9614 - val_loss: 0.2372 - val_acc: 0.9415\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9611\n",
      "Epoch 00076: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1282 - acc: 0.9611 - val_loss: 0.2372 - val_acc: 0.9415\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9610\n",
      "Epoch 00077: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1256 - acc: 0.9610 - val_loss: 0.2237 - val_acc: 0.9441\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9620\n",
      "Epoch 00078: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1217 - acc: 0.9620 - val_loss: 0.2512 - val_acc: 0.9425\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9615\n",
      "Epoch 00079: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1200 - acc: 0.9616 - val_loss: 0.2332 - val_acc: 0.9415\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9632\n",
      "Epoch 00080: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1179 - acc: 0.9632 - val_loss: 0.2532 - val_acc: 0.9404\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9657\n",
      "Epoch 00081: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1134 - acc: 0.9657 - val_loss: 0.2248 - val_acc: 0.9427\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9651\n",
      "Epoch 00082: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1126 - acc: 0.9650 - val_loss: 0.2206 - val_acc: 0.9464\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9640\n",
      "Epoch 00083: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1175 - acc: 0.9640 - val_loss: 0.2177 - val_acc: 0.9436\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9677\n",
      "Epoch 00084: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1052 - acc: 0.9677 - val_loss: 0.2555 - val_acc: 0.9399\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9657\n",
      "Epoch 00085: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1081 - acc: 0.9657 - val_loss: 0.2187 - val_acc: 0.9492\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9657\n",
      "Epoch 00086: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1097 - acc: 0.9657 - val_loss: 0.2548 - val_acc: 0.9399\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9697\n",
      "Epoch 00087: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0970 - acc: 0.9697 - val_loss: 0.2369 - val_acc: 0.9455\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9654\n",
      "Epoch 00088: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1086 - acc: 0.9653 - val_loss: 0.2377 - val_acc: 0.9427\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9680\n",
      "Epoch 00089: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1045 - acc: 0.9680 - val_loss: 0.2522 - val_acc: 0.9434\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9706\n",
      "Epoch 00090: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0939 - acc: 0.9706 - val_loss: 0.2154 - val_acc: 0.9474\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9689\n",
      "Epoch 00091: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0972 - acc: 0.9689 - val_loss: 0.2383 - val_acc: 0.9439\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9693\n",
      "Epoch 00092: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0952 - acc: 0.9693 - val_loss: 0.2611 - val_acc: 0.9404\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9677\n",
      "Epoch 00093: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1001 - acc: 0.9677 - val_loss: 0.2294 - val_acc: 0.9476\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9704\n",
      "Epoch 00094: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0895 - acc: 0.9704 - val_loss: 0.2430 - val_acc: 0.9399\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9707\n",
      "Epoch 00095: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0912 - acc: 0.9707 - val_loss: 0.2436 - val_acc: 0.9422\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9726\n",
      "Epoch 00096: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0882 - acc: 0.9726 - val_loss: 0.2358 - val_acc: 0.9450\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9719\n",
      "Epoch 00097: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0886 - acc: 0.9719 - val_loss: 0.2384 - val_acc: 0.9460\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9730\n",
      "Epoch 00098: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0858 - acc: 0.9730 - val_loss: 0.2283 - val_acc: 0.9453\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9741\n",
      "Epoch 00099: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0800 - acc: 0.9741 - val_loss: 0.2305 - val_acc: 0.9476\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9719\n",
      "Epoch 00100: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0889 - acc: 0.9719 - val_loss: 0.2431 - val_acc: 0.9478\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9777\n",
      "Epoch 00101: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0728 - acc: 0.9777 - val_loss: 0.2373 - val_acc: 0.9474\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9744\n",
      "Epoch 00102: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0822 - acc: 0.9744 - val_loss: 0.2309 - val_acc: 0.9462\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9743\n",
      "Epoch 00103: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0832 - acc: 0.9743 - val_loss: 0.2319 - val_acc: 0.9415\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9760\n",
      "Epoch 00104: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0754 - acc: 0.9760 - val_loss: 0.2370 - val_acc: 0.9460\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9757\n",
      "Epoch 00105: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0770 - acc: 0.9757 - val_loss: 0.2472 - val_acc: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9777\n",
      "Epoch 00106: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0719 - acc: 0.9777 - val_loss: 0.2418 - val_acc: 0.9462\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9782\n",
      "Epoch 00107: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0692 - acc: 0.9782 - val_loss: 0.2372 - val_acc: 0.9476\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9756\n",
      "Epoch 00108: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0768 - acc: 0.9756 - val_loss: 0.2489 - val_acc: 0.9448\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9766\n",
      "Epoch 00109: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0740 - acc: 0.9766 - val_loss: 0.2638 - val_acc: 0.9418\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9765\n",
      "Epoch 00110: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0719 - acc: 0.9765 - val_loss: 0.2549 - val_acc: 0.9425\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9742\n",
      "Epoch 00111: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0812 - acc: 0.9742 - val_loss: 0.2565 - val_acc: 0.9415\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9799\n",
      "Epoch 00112: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0641 - acc: 0.9799 - val_loss: 0.2480 - val_acc: 0.9467\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9809\n",
      "Epoch 00113: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0612 - acc: 0.9809 - val_loss: 0.2591 - val_acc: 0.9422\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9795\n",
      "Epoch 00114: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0644 - acc: 0.9795 - val_loss: 0.2534 - val_acc: 0.9413\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9799\n",
      "Epoch 00115: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0621 - acc: 0.9799 - val_loss: 0.2815 - val_acc: 0.9385\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9746\n",
      "Epoch 00116: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0801 - acc: 0.9747 - val_loss: 0.2387 - val_acc: 0.9455\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9813\n",
      "Epoch 00117: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0587 - acc: 0.9813 - val_loss: 0.2523 - val_acc: 0.9492\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9816\n",
      "Epoch 00118: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0586 - acc: 0.9816 - val_loss: 0.2350 - val_acc: 0.9464\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9830\n",
      "Epoch 00119: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0540 - acc: 0.9830 - val_loss: 0.2846 - val_acc: 0.9392\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9804\n",
      "Epoch 00120: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0631 - acc: 0.9804 - val_loss: 0.2825 - val_acc: 0.9387\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9820\n",
      "Epoch 00121: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0577 - acc: 0.9820 - val_loss: 0.2412 - val_acc: 0.9474\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9827\n",
      "Epoch 00122: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0554 - acc: 0.9827 - val_loss: 0.2707 - val_acc: 0.9418\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9820\n",
      "Epoch 00123: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0566 - acc: 0.9820 - val_loss: 0.2761 - val_acc: 0.9394\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9816\n",
      "Epoch 00124: val_loss did not improve from 0.20628\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0576 - acc: 0.9816 - val_loss: 0.2546 - val_acc: 0.9448\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2361 - acc: 0.9304\n",
      "Loss: 0.23610414765148519 Accuracy: 0.93042576\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1512 - acc: 0.2995\n",
      "Epoch 00001: val_loss improved from inf to 1.54326, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/001-1.5433.hdf5\n",
      "36805/36805 [==============================] - 101s 3ms/sample - loss: 2.1512 - acc: 0.2995 - val_loss: 1.5433 - val_acc: 0.5260\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5826 - acc: 0.4929\n",
      "Epoch 00002: val_loss improved from 1.54326 to 1.15795, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/002-1.1580.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.5825 - acc: 0.4929 - val_loss: 1.1580 - val_acc: 0.6695\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2781 - acc: 0.6001\n",
      "Epoch 00003: val_loss improved from 1.15795 to 0.88416, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/003-0.8842.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.2781 - acc: 0.6001 - val_loss: 0.8842 - val_acc: 0.7498\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0467 - acc: 0.6826\n",
      "Epoch 00004: val_loss improved from 0.88416 to 0.73403, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/004-0.7340.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0467 - acc: 0.6827 - val_loss: 0.7340 - val_acc: 0.7913\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7343\n",
      "Epoch 00005: val_loss improved from 0.73403 to 0.61651, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/005-0.6165.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.8864 - acc: 0.7344 - val_loss: 0.6165 - val_acc: 0.8272\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7747\n",
      "Epoch 00006: val_loss improved from 0.61651 to 0.52145, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/006-0.5214.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7624 - acc: 0.7747 - val_loss: 0.5214 - val_acc: 0.8642\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.8075\n",
      "Epoch 00007: val_loss improved from 0.52145 to 0.44208, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/007-0.4421.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.6621 - acc: 0.8075 - val_loss: 0.4421 - val_acc: 0.8791\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.8325\n",
      "Epoch 00008: val_loss improved from 0.44208 to 0.38306, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/008-0.3831.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.5779 - acc: 0.8325 - val_loss: 0.3831 - val_acc: 0.8956\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8503\n",
      "Epoch 00009: val_loss improved from 0.38306 to 0.34704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/009-0.3470.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5211 - acc: 0.8503 - val_loss: 0.3470 - val_acc: 0.9094\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.8623\n",
      "Epoch 00010: val_loss improved from 0.34704 to 0.31771, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/010-0.3177.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4780 - acc: 0.8623 - val_loss: 0.3177 - val_acc: 0.9180\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8746\n",
      "Epoch 00011: val_loss improved from 0.31771 to 0.30934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/011-0.3093.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4388 - acc: 0.8746 - val_loss: 0.3093 - val_acc: 0.9224\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8833\n",
      "Epoch 00012: val_loss improved from 0.30934 to 0.28726, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/012-0.2873.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.4045 - acc: 0.8834 - val_loss: 0.2873 - val_acc: 0.9206\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8900\n",
      "Epoch 00013: val_loss improved from 0.28726 to 0.26330, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/013-0.2633.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3807 - acc: 0.8900 - val_loss: 0.2633 - val_acc: 0.9306\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8966\n",
      "Epoch 00014: val_loss improved from 0.26330 to 0.25196, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/014-0.2520.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3558 - acc: 0.8966 - val_loss: 0.2520 - val_acc: 0.9359\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.9041\n",
      "Epoch 00015: val_loss improved from 0.25196 to 0.23213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/015-0.2321.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3325 - acc: 0.9041 - val_loss: 0.2321 - val_acc: 0.9373\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9073\n",
      "Epoch 00016: val_loss did not improve from 0.23213\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3214 - acc: 0.9073 - val_loss: 0.2402 - val_acc: 0.9355\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9126\n",
      "Epoch 00017: val_loss improved from 0.23213 to 0.21525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/017-0.2153.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3029 - acc: 0.9126 - val_loss: 0.2153 - val_acc: 0.9436\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9154\n",
      "Epoch 00018: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2953 - acc: 0.9153 - val_loss: 0.2545 - val_acc: 0.9350\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9167\n",
      "Epoch 00019: val_loss improved from 0.21525 to 0.19956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/019-0.1996.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2826 - acc: 0.9167 - val_loss: 0.1996 - val_acc: 0.9495\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9215\n",
      "Epoch 00020: val_loss did not improve from 0.19956\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2694 - acc: 0.9215 - val_loss: 0.2066 - val_acc: 0.9404\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9233\n",
      "Epoch 00021: val_loss improved from 0.19956 to 0.19677, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/021-0.1968.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2596 - acc: 0.9234 - val_loss: 0.1968 - val_acc: 0.9471\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9274\n",
      "Epoch 00022: val_loss improved from 0.19677 to 0.18619, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/022-0.1862.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2483 - acc: 0.9274 - val_loss: 0.1862 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9298\n",
      "Epoch 00023: val_loss did not improve from 0.18619\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2403 - acc: 0.9298 - val_loss: 0.1894 - val_acc: 0.9476\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9309\n",
      "Epoch 00024: val_loss did not improve from 0.18619\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2375 - acc: 0.9309 - val_loss: 0.1941 - val_acc: 0.9460\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9336\n",
      "Epoch 00025: val_loss did not improve from 0.18619\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2269 - acc: 0.9337 - val_loss: 0.1937 - val_acc: 0.9485\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9361\n",
      "Epoch 00026: val_loss improved from 0.18619 to 0.18553, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/026-0.1855.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2179 - acc: 0.9361 - val_loss: 0.1855 - val_acc: 0.9511\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9367\n",
      "Epoch 00027: val_loss improved from 0.18553 to 0.18526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/027-0.1853.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2096 - acc: 0.9367 - val_loss: 0.1853 - val_acc: 0.9474\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9383\n",
      "Epoch 00028: val_loss improved from 0.18526 to 0.18198, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/028-0.1820.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2066 - acc: 0.9384 - val_loss: 0.1820 - val_acc: 0.9488\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9411\n",
      "Epoch 00029: val_loss did not improve from 0.18198\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1957 - acc: 0.9411 - val_loss: 0.1876 - val_acc: 0.9474\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9416\n",
      "Epoch 00030: val_loss improved from 0.18198 to 0.18083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/030-0.1808.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1963 - acc: 0.9416 - val_loss: 0.1808 - val_acc: 0.9497\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9444\n",
      "Epoch 00031: val_loss did not improve from 0.18083\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1859 - acc: 0.9444 - val_loss: 0.1865 - val_acc: 0.9525\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9454\n",
      "Epoch 00032: val_loss improved from 0.18083 to 0.18026, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/032-0.1803.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1852 - acc: 0.9454 - val_loss: 0.1803 - val_acc: 0.9495\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9482\n",
      "Epoch 00033: val_loss improved from 0.18026 to 0.17118, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/033-0.1712.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1763 - acc: 0.9482 - val_loss: 0.1712 - val_acc: 0.9509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9496\n",
      "Epoch 00034: val_loss did not improve from 0.17118\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1693 - acc: 0.9496 - val_loss: 0.1806 - val_acc: 0.9471\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9482\n",
      "Epoch 00035: val_loss improved from 0.17118 to 0.16756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/035-0.1676.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1744 - acc: 0.9481 - val_loss: 0.1676 - val_acc: 0.9546\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9502\n",
      "Epoch 00036: val_loss improved from 0.16756 to 0.15973, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/036-0.1597.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1620 - acc: 0.9502 - val_loss: 0.1597 - val_acc: 0.9564\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9535\n",
      "Epoch 00037: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1562 - acc: 0.9534 - val_loss: 0.1689 - val_acc: 0.9555\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9537\n",
      "Epoch 00038: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1505 - acc: 0.9537 - val_loss: 0.1653 - val_acc: 0.9529\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9539\n",
      "Epoch 00039: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1493 - acc: 0.9539 - val_loss: 0.1765 - val_acc: 0.9525\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9552\n",
      "Epoch 00040: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1484 - acc: 0.9551 - val_loss: 0.1867 - val_acc: 0.9488\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9549\n",
      "Epoch 00041: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1464 - acc: 0.9549 - val_loss: 0.1672 - val_acc: 0.9525\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9587\n",
      "Epoch 00042: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1369 - acc: 0.9587 - val_loss: 0.1650 - val_acc: 0.9529\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9599\n",
      "Epoch 00043: val_loss did not improve from 0.15973\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1295 - acc: 0.9599 - val_loss: 0.1709 - val_acc: 0.9550\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9601\n",
      "Epoch 00044: val_loss improved from 0.15973 to 0.15900, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/044-0.1590.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1279 - acc: 0.9601 - val_loss: 0.1590 - val_acc: 0.9562\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9614\n",
      "Epoch 00045: val_loss did not improve from 0.15900\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1240 - acc: 0.9614 - val_loss: 0.1676 - val_acc: 0.9550\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9628\n",
      "Epoch 00046: val_loss did not improve from 0.15900\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1214 - acc: 0.9628 - val_loss: 0.1829 - val_acc: 0.9509\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9653\n",
      "Epoch 00047: val_loss did not improve from 0.15900\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1146 - acc: 0.9653 - val_loss: 0.1614 - val_acc: 0.9564\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9642\n",
      "Epoch 00048: val_loss improved from 0.15900 to 0.15312, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv_checkpoint/048-0.1531.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1156 - acc: 0.9642 - val_loss: 0.1531 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9664\n",
      "Epoch 00049: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1088 - acc: 0.9664 - val_loss: 0.1714 - val_acc: 0.9511\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9680\n",
      "Epoch 00050: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1061 - acc: 0.9680 - val_loss: 0.1841 - val_acc: 0.9502\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9682\n",
      "Epoch 00051: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1021 - acc: 0.9682 - val_loss: 0.1766 - val_acc: 0.9532\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9680\n",
      "Epoch 00052: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1011 - acc: 0.9680 - val_loss: 0.1676 - val_acc: 0.9557\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9712\n",
      "Epoch 00053: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0921 - acc: 0.9713 - val_loss: 0.1601 - val_acc: 0.9569\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9718\n",
      "Epoch 00054: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0924 - acc: 0.9718 - val_loss: 0.1682 - val_acc: 0.9553\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9703\n",
      "Epoch 00055: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0938 - acc: 0.9703 - val_loss: 0.1651 - val_acc: 0.9550\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9760\n",
      "Epoch 00056: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0810 - acc: 0.9759 - val_loss: 0.1584 - val_acc: 0.9574\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9739\n",
      "Epoch 00057: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0853 - acc: 0.9739 - val_loss: 0.1627 - val_acc: 0.9567\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9740\n",
      "Epoch 00058: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0827 - acc: 0.9740 - val_loss: 0.1703 - val_acc: 0.9562\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9770\n",
      "Epoch 00059: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0751 - acc: 0.9770 - val_loss: 0.1677 - val_acc: 0.9555\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9766\n",
      "Epoch 00060: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0726 - acc: 0.9766 - val_loss: 0.1773 - val_acc: 0.9513\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9767\n",
      "Epoch 00061: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0725 - acc: 0.9767 - val_loss: 0.1748 - val_acc: 0.9539\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9799\n",
      "Epoch 00062: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0645 - acc: 0.9799 - val_loss: 0.1810 - val_acc: 0.9557\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9790\n",
      "Epoch 00063: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0688 - acc: 0.9790 - val_loss: 0.1692 - val_acc: 0.9564\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9796\n",
      "Epoch 00064: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0648 - acc: 0.9796 - val_loss: 0.1733 - val_acc: 0.9583\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9801\n",
      "Epoch 00065: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0631 - acc: 0.9801 - val_loss: 0.1746 - val_acc: 0.9557\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9800\n",
      "Epoch 00066: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0634 - acc: 0.9800 - val_loss: 0.1817 - val_acc: 0.9534\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9810\n",
      "Epoch 00067: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0605 - acc: 0.9810 - val_loss: 0.1792 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9818\n",
      "Epoch 00068: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0585 - acc: 0.9819 - val_loss: 0.1665 - val_acc: 0.9574\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9864\n",
      "Epoch 00069: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0462 - acc: 0.9864 - val_loss: 0.1918 - val_acc: 0.9541\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9803\n",
      "Epoch 00070: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0612 - acc: 0.9803 - val_loss: 0.1946 - val_acc: 0.9511\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9845\n",
      "Epoch 00071: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0508 - acc: 0.9845 - val_loss: 0.1760 - val_acc: 0.9574\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9848\n",
      "Epoch 00072: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0501 - acc: 0.9848 - val_loss: 0.2004 - val_acc: 0.9497\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9850\n",
      "Epoch 00073: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0482 - acc: 0.9850 - val_loss: 0.1729 - val_acc: 0.9567\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9848\n",
      "Epoch 00074: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0479 - acc: 0.9848 - val_loss: 0.1799 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9870\n",
      "Epoch 00075: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0421 - acc: 0.9870 - val_loss: 0.1808 - val_acc: 0.9571\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9859\n",
      "Epoch 00076: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0454 - acc: 0.9859 - val_loss: 0.1873 - val_acc: 0.9562\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9864\n",
      "Epoch 00077: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0436 - acc: 0.9864 - val_loss: 0.1956 - val_acc: 0.9548\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9855\n",
      "Epoch 00078: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0482 - acc: 0.9855 - val_loss: 0.1749 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9879\n",
      "Epoch 00079: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0375 - acc: 0.9879 - val_loss: 0.2052 - val_acc: 0.9539\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9877\n",
      "Epoch 00080: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0384 - acc: 0.9877 - val_loss: 0.1839 - val_acc: 0.9562\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9889\n",
      "Epoch 00081: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0358 - acc: 0.9889 - val_loss: 0.1986 - val_acc: 0.9527\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9896\n",
      "Epoch 00082: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0348 - acc: 0.9896 - val_loss: 0.1989 - val_acc: 0.9525\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9893\n",
      "Epoch 00083: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0354 - acc: 0.9893 - val_loss: 0.2221 - val_acc: 0.9527\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9901\n",
      "Epoch 00084: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0321 - acc: 0.9901 - val_loss: 0.2108 - val_acc: 0.9525\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9875\n",
      "Epoch 00085: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0380 - acc: 0.9875 - val_loss: 0.2137 - val_acc: 0.9495\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9908\n",
      "Epoch 00086: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0309 - acc: 0.9908 - val_loss: 0.2452 - val_acc: 0.9460\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9902\n",
      "Epoch 00087: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0325 - acc: 0.9902 - val_loss: 0.1885 - val_acc: 0.9557\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9901\n",
      "Epoch 00088: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0338 - acc: 0.9901 - val_loss: 0.1889 - val_acc: 0.9567\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9901\n",
      "Epoch 00089: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0338 - acc: 0.9901 - val_loss: 0.2051 - val_acc: 0.9546\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9908\n",
      "Epoch 00090: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.2076 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9917\n",
      "Epoch 00091: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0259 - acc: 0.9917 - val_loss: 0.2017 - val_acc: 0.9548\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9904\n",
      "Epoch 00092: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0315 - acc: 0.9904 - val_loss: 0.2398 - val_acc: 0.9495\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9910\n",
      "Epoch 00093: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0311 - acc: 0.9910 - val_loss: 0.2268 - val_acc: 0.9478\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9860\n",
      "Epoch 00094: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0499 - acc: 0.9860 - val_loss: 0.2005 - val_acc: 0.9595\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9955\n",
      "Epoch 00095: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0178 - acc: 0.9955 - val_loss: 0.2082 - val_acc: 0.9543\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9942\n",
      "Epoch 00096: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0210 - acc: 0.9942 - val_loss: 0.2090 - val_acc: 0.9585\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9920\n",
      "Epoch 00097: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0262 - acc: 0.9920 - val_loss: 0.2289 - val_acc: 0.9506\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9918\n",
      "Epoch 00098: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0259 - acc: 0.9918 - val_loss: 0.1912 - val_acc: 0.9576\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2042 - acc: 0.9404\n",
      "Loss: 0.2041721895723947 Accuracy: 0.9403946\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7517 - acc: 0.4373\n",
      "Epoch 00001: val_loss improved from inf to 0.99562, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/001-0.9956.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.7517 - acc: 0.4373 - val_loss: 0.9956 - val_acc: 0.7226\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0208 - acc: 0.6857\n",
      "Epoch 00002: val_loss improved from 0.99562 to 0.64536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/002-0.6454.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 1.0208 - acc: 0.6857 - val_loss: 0.6454 - val_acc: 0.8181\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7826\n",
      "Epoch 00003: val_loss improved from 0.64536 to 0.45139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/003-0.4514.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.7165 - acc: 0.7826 - val_loss: 0.4514 - val_acc: 0.8772\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.8391\n",
      "Epoch 00004: val_loss improved from 0.45139 to 0.30307, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/004-0.3031.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.5425 - acc: 0.8391 - val_loss: 0.3031 - val_acc: 0.9166\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8700\n",
      "Epoch 00005: val_loss did not improve from 0.30307\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.4402 - acc: 0.8700 - val_loss: 0.3072 - val_acc: 0.9140\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8903\n",
      "Epoch 00006: val_loss improved from 0.30307 to 0.21796, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/006-0.2180.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.3672 - acc: 0.8903 - val_loss: 0.2180 - val_acc: 0.9376\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.9003\n",
      "Epoch 00007: val_loss did not improve from 0.21796\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.3294 - acc: 0.9003 - val_loss: 0.2507 - val_acc: 0.9236\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9137\n",
      "Epoch 00008: val_loss improved from 0.21796 to 0.19569, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/008-0.1957.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2873 - acc: 0.9137 - val_loss: 0.1957 - val_acc: 0.9436\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9221\n",
      "Epoch 00009: val_loss improved from 0.19569 to 0.16728, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/009-0.1673.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2637 - acc: 0.9222 - val_loss: 0.1673 - val_acc: 0.9534\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9282\n",
      "Epoch 00010: val_loss did not improve from 0.16728\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2458 - acc: 0.9282 - val_loss: 0.1736 - val_acc: 0.9478\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9329\n",
      "Epoch 00011: val_loss improved from 0.16728 to 0.16125, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/011-0.1613.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2260 - acc: 0.9329 - val_loss: 0.1613 - val_acc: 0.9553\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9379\n",
      "Epoch 00012: val_loss did not improve from 0.16125\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2096 - acc: 0.9379 - val_loss: 0.1719 - val_acc: 0.9488\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9425\n",
      "Epoch 00013: val_loss improved from 0.16125 to 0.14696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/013-0.1470.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1949 - acc: 0.9425 - val_loss: 0.1470 - val_acc: 0.9546\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9449\n",
      "Epoch 00014: val_loss improved from 0.14696 to 0.14326, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/014-0.1433.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1847 - acc: 0.9449 - val_loss: 0.1433 - val_acc: 0.9562\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9480\n",
      "Epoch 00015: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1729 - acc: 0.9480 - val_loss: 0.1505 - val_acc: 0.9534\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9485\n",
      "Epoch 00016: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1704 - acc: 0.9485 - val_loss: 0.1438 - val_acc: 0.9583\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9532\n",
      "Epoch 00017: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1537 - acc: 0.9531 - val_loss: 0.1510 - val_acc: 0.9567\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9518\n",
      "Epoch 00018: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1574 - acc: 0.9517 - val_loss: 0.1456 - val_acc: 0.9555\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9561\n",
      "Epoch 00019: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1463 - acc: 0.9561 - val_loss: 0.1514 - val_acc: 0.9546\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9593\n",
      "Epoch 00020: val_loss did not improve from 0.14326\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1341 - acc: 0.9594 - val_loss: 0.1529 - val_acc: 0.9553\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9618\n",
      "Epoch 00021: val_loss improved from 0.14326 to 0.13555, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/021-0.1355.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1256 - acc: 0.9618 - val_loss: 0.1355 - val_acc: 0.9585\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9642\n",
      "Epoch 00022: val_loss improved from 0.13555 to 0.13554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/022-0.1355.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1191 - acc: 0.9642 - val_loss: 0.1355 - val_acc: 0.9583\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9653\n",
      "Epoch 00023: val_loss did not improve from 0.13554\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1134 - acc: 0.9653 - val_loss: 0.1388 - val_acc: 0.9606\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9655\n",
      "Epoch 00024: val_loss did not improve from 0.13554\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1141 - acc: 0.9655 - val_loss: 0.1394 - val_acc: 0.9599\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9675\n",
      "Epoch 00025: val_loss did not improve from 0.13554\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1074 - acc: 0.9675 - val_loss: 0.1459 - val_acc: 0.9578\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9683\n",
      "Epoch 00026: val_loss improved from 0.13554 to 0.12192, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv_checkpoint/026-0.1219.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0990 - acc: 0.9683 - val_loss: 0.1219 - val_acc: 0.9606\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9710\n",
      "Epoch 00027: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0937 - acc: 0.9710 - val_loss: 0.1624 - val_acc: 0.9502\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9714\n",
      "Epoch 00028: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0916 - acc: 0.9714 - val_loss: 0.1264 - val_acc: 0.9616\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9733\n",
      "Epoch 00029: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0854 - acc: 0.9733 - val_loss: 0.1363 - val_acc: 0.9597\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9734\n",
      "Epoch 00030: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0835 - acc: 0.9734 - val_loss: 0.1333 - val_acc: 0.9590\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9760\n",
      "Epoch 00031: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0783 - acc: 0.9760 - val_loss: 0.1374 - val_acc: 0.9613\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9771\n",
      "Epoch 00032: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0742 - acc: 0.9771 - val_loss: 0.1390 - val_acc: 0.9632\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9777\n",
      "Epoch 00033: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0704 - acc: 0.9777 - val_loss: 0.1316 - val_acc: 0.9620\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9795\n",
      "Epoch 00034: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0665 - acc: 0.9795 - val_loss: 0.1426 - val_acc: 0.9613\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9795\n",
      "Epoch 00035: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0667 - acc: 0.9795 - val_loss: 0.1284 - val_acc: 0.9660\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9821\n",
      "Epoch 00036: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0569 - acc: 0.9821 - val_loss: 0.1280 - val_acc: 0.9651\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9831\n",
      "Epoch 00037: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0557 - acc: 0.9831 - val_loss: 0.1449 - val_acc: 0.9611\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9838\n",
      "Epoch 00038: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0532 - acc: 0.9838 - val_loss: 0.1520 - val_acc: 0.9609\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9826\n",
      "Epoch 00039: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0546 - acc: 0.9826 - val_loss: 0.1585 - val_acc: 0.9562\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9847\n",
      "Epoch 00040: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0491 - acc: 0.9847 - val_loss: 0.1405 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9858\n",
      "Epoch 00041: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0448 - acc: 0.9858 - val_loss: 0.1355 - val_acc: 0.9648\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9856\n",
      "Epoch 00042: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0443 - acc: 0.9856 - val_loss: 0.1374 - val_acc: 0.9632\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9883\n",
      "Epoch 00043: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0374 - acc: 0.9883 - val_loss: 0.1419 - val_acc: 0.9634\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9866\n",
      "Epoch 00044: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0422 - acc: 0.9866 - val_loss: 0.1639 - val_acc: 0.9604\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9874\n",
      "Epoch 00045: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0391 - acc: 0.9874 - val_loss: 0.1477 - val_acc: 0.9611\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9883\n",
      "Epoch 00046: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0370 - acc: 0.9883 - val_loss: 0.1689 - val_acc: 0.9574\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9891\n",
      "Epoch 00047: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0350 - acc: 0.9891 - val_loss: 0.1572 - val_acc: 0.9632\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9901\n",
      "Epoch 00048: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0325 - acc: 0.9901 - val_loss: 0.1463 - val_acc: 0.9637\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9903\n",
      "Epoch 00049: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0310 - acc: 0.9903 - val_loss: 0.1471 - val_acc: 0.9623\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9907\n",
      "Epoch 00050: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0293 - acc: 0.9907 - val_loss: 0.1726 - val_acc: 0.9588\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 00051: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0293 - acc: 0.9906 - val_loss: 0.1542 - val_acc: 0.9632\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9912\n",
      "Epoch 00052: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0275 - acc: 0.9912 - val_loss: 0.1671 - val_acc: 0.9611\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9930\n",
      "Epoch 00053: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0232 - acc: 0.9930 - val_loss: 0.1575 - val_acc: 0.9616\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9920\n",
      "Epoch 00054: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0256 - acc: 0.9920 - val_loss: 0.1587 - val_acc: 0.9641\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9910\n",
      "Epoch 00055: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0277 - acc: 0.9910 - val_loss: 0.1917 - val_acc: 0.9578\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9930\n",
      "Epoch 00056: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0224 - acc: 0.9930 - val_loss: 0.1519 - val_acc: 0.9618\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9903\n",
      "Epoch 00057: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0308 - acc: 0.9903 - val_loss: 0.1602 - val_acc: 0.9606\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00058: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0195 - acc: 0.9939 - val_loss: 0.1499 - val_acc: 0.9632\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9924\n",
      "Epoch 00059: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0248 - acc: 0.9924 - val_loss: 0.1735 - val_acc: 0.9560\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9930\n",
      "Epoch 00060: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0207 - acc: 0.9930 - val_loss: 0.1592 - val_acc: 0.9641\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9931\n",
      "Epoch 00061: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0234 - acc: 0.9931 - val_loss: 0.1590 - val_acc: 0.9641\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9934\n",
      "Epoch 00062: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0206 - acc: 0.9934 - val_loss: 0.1710 - val_acc: 0.9623\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9937\n",
      "Epoch 00063: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0193 - acc: 0.9937 - val_loss: 0.1888 - val_acc: 0.9574\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9935\n",
      "Epoch 00064: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0191 - acc: 0.9935 - val_loss: 0.1598 - val_acc: 0.9613\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9968\n",
      "Epoch 00065: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0121 - acc: 0.9968 - val_loss: 0.1708 - val_acc: 0.9611\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9923\n",
      "Epoch 00066: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0234 - acc: 0.9923 - val_loss: 0.1873 - val_acc: 0.9595\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9937\n",
      "Epoch 00067: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0203 - acc: 0.9937 - val_loss: 0.1859 - val_acc: 0.9609\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9944\n",
      "Epoch 00068: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0173 - acc: 0.9944 - val_loss: 0.1777 - val_acc: 0.9606\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9939\n",
      "Epoch 00069: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0182 - acc: 0.9939 - val_loss: 0.1574 - val_acc: 0.9639\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0130 - acc: 0.9959 - val_loss: 0.1800 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9942\n",
      "Epoch 00071: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0189 - acc: 0.9942 - val_loss: 0.1612 - val_acc: 0.9630\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 00072: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0161 - acc: 0.9951 - val_loss: 0.1930 - val_acc: 0.9604\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9953\n",
      "Epoch 00073: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0153 - acc: 0.9953 - val_loss: 0.1651 - val_acc: 0.9625\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 00074: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.1943 - val_acc: 0.9620\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9929\n",
      "Epoch 00075: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0228 - acc: 0.9929 - val_loss: 0.1902 - val_acc: 0.9567\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 00076: val_loss did not improve from 0.12192\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0117 - acc: 0.9964 - val_loss: 0.1662 - val_acc: 0.9630\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1734 - acc: 0.9522\n",
      "Loss: 0.17343066223778086 Accuracy: 0.9522326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 999us/sample - loss: 1.4452 - acc: 0.5736\n",
      "Loss: 1.4451579620781223 Accuracy: 0.5736241\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9308 - acc: 0.7410\n",
      "Loss: 0.9308356591721809 Accuracy: 0.74101764\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7363 - acc: 0.7848\n",
      "Loss: 0.7362892188758493 Accuracy: 0.78483903\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4636 - acc: 0.8696\n",
      "Loss: 0.4636152361412286 Accuracy: 0.86957425\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2361 - acc: 0.9304\n",
      "Loss: 0.23610414765148519 Accuracy: 0.93042576\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2042 - acc: 0.9404\n",
      "Loss: 0.2041721895723947 Accuracy: 0.9403946\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1734 - acc: 0.9522\n",
      "Loss: 0.17343066223778086 Accuracy: 0.9522326\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.9370 - acc: 0.6388\n",
      "Loss: 1.9369777159032298 Accuracy: 0.638837\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.0396 - acc: 0.7560\n",
      "Loss: 1.0396372800551719 Accuracy: 0.7559709\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8005 - acc: 0.8064\n",
      "Loss: 0.8005038899548452 Accuracy: 0.8064382\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4693 - acc: 0.8766\n",
      "Loss: 0.4693457949570035 Accuracy: 0.8766355\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2756 - acc: 0.9302\n",
      "Loss: 0.2756348603796736 Accuracy: 0.93021804\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_140 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2191 - acc: 0.9448\n",
      "Loss: 0.21914691060224425 Accuracy: 0.944756\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2217 - acc: 0.9541\n",
      "Loss: 0.22173708323742286 Accuracy: 0.95410174\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
