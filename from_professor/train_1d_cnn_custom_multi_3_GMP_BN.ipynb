{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 192)          768         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3088        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 192)          768         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           3088        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 256)          1024        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4112        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320)          0           global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 320)          1280        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           5136        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 384)          1536        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           6160        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 384)          1536        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           6160        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0607 - acc: 0.3368\n",
      "Epoch 00001: val_loss improved from inf to 2.07677, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/001-2.0768.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.0607 - acc: 0.3368 - val_loss: 2.0768 - val_acc: 0.3545\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5371 - acc: 0.5077\n",
      "Epoch 00002: val_loss improved from 2.07677 to 1.42827, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/002-1.4283.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.5372 - acc: 0.5077 - val_loss: 1.4283 - val_acc: 0.5493\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2890 - acc: 0.5971\n",
      "Epoch 00003: val_loss improved from 1.42827 to 1.19695, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/003-1.1969.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2890 - acc: 0.5970 - val_loss: 1.1969 - val_acc: 0.6341\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1181 - acc: 0.6553\n",
      "Epoch 00004: val_loss improved from 1.19695 to 1.07696, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/004-1.0770.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1182 - acc: 0.6553 - val_loss: 1.0770 - val_acc: 0.6580\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9951 - acc: 0.6971\n",
      "Epoch 00005: val_loss improved from 1.07696 to 0.90999, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/005-0.9100.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9951 - acc: 0.6972 - val_loss: 0.9100 - val_acc: 0.7254\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7276\n",
      "Epoch 00006: val_loss improved from 0.90999 to 0.87020, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/006-0.8702.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9043 - acc: 0.7276 - val_loss: 0.8702 - val_acc: 0.7319\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7478\n",
      "Epoch 00007: val_loss improved from 0.87020 to 0.82174, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/007-0.8217.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8341 - acc: 0.7478 - val_loss: 0.8217 - val_acc: 0.7386\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7747 - acc: 0.7650\n",
      "Epoch 00008: val_loss improved from 0.82174 to 0.76609, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/008-0.7661.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7747 - acc: 0.7650 - val_loss: 0.7661 - val_acc: 0.7624\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7810\n",
      "Epoch 00009: val_loss improved from 0.76609 to 0.73674, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/009-0.7367.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7297 - acc: 0.7810 - val_loss: 0.7367 - val_acc: 0.7876\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6954 - acc: 0.7892\n",
      "Epoch 00010: val_loss improved from 0.73674 to 0.66641, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/010-0.6664.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6954 - acc: 0.7892 - val_loss: 0.6664 - val_acc: 0.7934\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.8016\n",
      "Epoch 00011: val_loss did not improve from 0.66641\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6612 - acc: 0.8015 - val_loss: 0.6882 - val_acc: 0.7750\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6300 - acc: 0.8121\n",
      "Epoch 00012: val_loss improved from 0.66641 to 0.62947, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/012-0.6295.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6299 - acc: 0.8121 - val_loss: 0.6295 - val_acc: 0.8088\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.8154\n",
      "Epoch 00013: val_loss improved from 0.62947 to 0.62907, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/013-0.6291.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6093 - acc: 0.8154 - val_loss: 0.6291 - val_acc: 0.8076\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8226\n",
      "Epoch 00014: val_loss improved from 0.62907 to 0.60514, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/014-0.6051.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5889 - acc: 0.8226 - val_loss: 0.6051 - val_acc: 0.8141\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.8278\n",
      "Epoch 00015: val_loss improved from 0.60514 to 0.58337, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/015-0.5834.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5698 - acc: 0.8278 - val_loss: 0.5834 - val_acc: 0.8239\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8329\n",
      "Epoch 00016: val_loss did not improve from 0.58337\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5505 - acc: 0.8329 - val_loss: 0.6088 - val_acc: 0.8127\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8398\n",
      "Epoch 00017: val_loss improved from 0.58337 to 0.56734, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/017-0.5673.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5330 - acc: 0.8398 - val_loss: 0.5673 - val_acc: 0.8258\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8440\n",
      "Epoch 00018: val_loss did not improve from 0.56734\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5194 - acc: 0.8439 - val_loss: 0.6161 - val_acc: 0.8092\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8473\n",
      "Epoch 00019: val_loss did not improve from 0.56734\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5061 - acc: 0.8473 - val_loss: 0.5903 - val_acc: 0.8181\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8509\n",
      "Epoch 00020: val_loss improved from 0.56734 to 0.54304, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/020-0.5430.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4938 - acc: 0.8509 - val_loss: 0.5430 - val_acc: 0.8328\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.8528\n",
      "Epoch 00021: val_loss improved from 0.54304 to 0.53953, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/021-0.5395.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4840 - acc: 0.8528 - val_loss: 0.5395 - val_acc: 0.8362\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.8573\n",
      "Epoch 00022: val_loss did not improve from 0.53953\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4681 - acc: 0.8573 - val_loss: 0.5409 - val_acc: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.8615\n",
      "Epoch 00023: val_loss did not improve from 0.53953\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4572 - acc: 0.8615 - val_loss: 0.5919 - val_acc: 0.8192\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8637\n",
      "Epoch 00024: val_loss did not improve from 0.53953\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4496 - acc: 0.8637 - val_loss: 0.5438 - val_acc: 0.8323\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8655\n",
      "Epoch 00025: val_loss did not improve from 0.53953\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4412 - acc: 0.8654 - val_loss: 0.5407 - val_acc: 0.8344\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8677\n",
      "Epoch 00026: val_loss improved from 0.53953 to 0.50580, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/026-0.5058.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4338 - acc: 0.8677 - val_loss: 0.5058 - val_acc: 0.8509\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8698\n",
      "Epoch 00027: val_loss did not improve from 0.50580\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4233 - acc: 0.8697 - val_loss: 0.5456 - val_acc: 0.8376\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8722\n",
      "Epoch 00028: val_loss did not improve from 0.50580\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4181 - acc: 0.8721 - val_loss: 0.5373 - val_acc: 0.8346\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8763\n",
      "Epoch 00029: val_loss did not improve from 0.50580\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4059 - acc: 0.8762 - val_loss: 0.5402 - val_acc: 0.8337\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8760\n",
      "Epoch 00030: val_loss did not improve from 0.50580\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3998 - acc: 0.8759 - val_loss: 0.5355 - val_acc: 0.8330\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8817\n",
      "Epoch 00031: val_loss did not improve from 0.50580\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3919 - acc: 0.8817 - val_loss: 0.5616 - val_acc: 0.8283\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8812\n",
      "Epoch 00032: val_loss improved from 0.50580 to 0.49528, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/032-0.4953.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3861 - acc: 0.8812 - val_loss: 0.4953 - val_acc: 0.8470\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8848\n",
      "Epoch 00033: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3794 - acc: 0.8848 - val_loss: 0.5821 - val_acc: 0.8211\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8859\n",
      "Epoch 00034: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3719 - acc: 0.8859 - val_loss: 0.5138 - val_acc: 0.8442\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8875\n",
      "Epoch 00035: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3679 - acc: 0.8875 - val_loss: 0.5561 - val_acc: 0.8314\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8900\n",
      "Epoch 00036: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3610 - acc: 0.8900 - val_loss: 0.5097 - val_acc: 0.8446\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8913\n",
      "Epoch 00037: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3558 - acc: 0.8913 - val_loss: 0.5114 - val_acc: 0.8479\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8921\n",
      "Epoch 00038: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3518 - acc: 0.8921 - val_loss: 0.5702 - val_acc: 0.8295\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8949\n",
      "Epoch 00039: val_loss did not improve from 0.49528\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3428 - acc: 0.8949 - val_loss: 0.5386 - val_acc: 0.8381\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8951\n",
      "Epoch 00040: val_loss improved from 0.49528 to 0.47535, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_3_conv_checkpoint/040-0.4753.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3390 - acc: 0.8950 - val_loss: 0.4753 - val_acc: 0.8551\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8978\n",
      "Epoch 00041: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3327 - acc: 0.8978 - val_loss: 0.5236 - val_acc: 0.8374\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.9009\n",
      "Epoch 00042: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3266 - acc: 0.9009 - val_loss: 0.6215 - val_acc: 0.8071\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9002\n",
      "Epoch 00043: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3250 - acc: 0.9002 - val_loss: 0.5091 - val_acc: 0.8507\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9021\n",
      "Epoch 00044: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3196 - acc: 0.9020 - val_loss: 0.5073 - val_acc: 0.8532\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9022\n",
      "Epoch 00045: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3148 - acc: 0.9022 - val_loss: 0.5223 - val_acc: 0.8484\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9053\n",
      "Epoch 00046: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3107 - acc: 0.9053 - val_loss: 0.5027 - val_acc: 0.8546\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9080\n",
      "Epoch 00047: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3026 - acc: 0.9079 - val_loss: 0.5931 - val_acc: 0.8239\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9069\n",
      "Epoch 00048: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3030 - acc: 0.9068 - val_loss: 0.5627 - val_acc: 0.8297\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9090\n",
      "Epoch 00049: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2969 - acc: 0.9090 - val_loss: 0.5409 - val_acc: 0.8390\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9082\n",
      "Epoch 00050: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2976 - acc: 0.9082 - val_loss: 0.5494 - val_acc: 0.8386\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9095\n",
      "Epoch 00051: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2892 - acc: 0.9095 - val_loss: 0.5303 - val_acc: 0.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9149\n",
      "Epoch 00052: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2820 - acc: 0.9148 - val_loss: 0.4931 - val_acc: 0.8563\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9134\n",
      "Epoch 00053: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2810 - acc: 0.9134 - val_loss: 0.5173 - val_acc: 0.8463\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9140\n",
      "Epoch 00054: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2784 - acc: 0.9140 - val_loss: 0.6514 - val_acc: 0.8081\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9181\n",
      "Epoch 00055: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2710 - acc: 0.9181 - val_loss: 0.5608 - val_acc: 0.8344\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9167\n",
      "Epoch 00056: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2675 - acc: 0.9167 - val_loss: 0.4929 - val_acc: 0.8577\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9182\n",
      "Epoch 00057: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2676 - acc: 0.9182 - val_loss: 0.5316 - val_acc: 0.8428\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9172\n",
      "Epoch 00058: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2684 - acc: 0.9171 - val_loss: 0.6522 - val_acc: 0.8039\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9206\n",
      "Epoch 00059: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2585 - acc: 0.9206 - val_loss: 0.6572 - val_acc: 0.8130\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9202\n",
      "Epoch 00060: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2621 - acc: 0.9201 - val_loss: 0.5278 - val_acc: 0.8453\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9230\n",
      "Epoch 00061: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2510 - acc: 0.9229 - val_loss: 0.5507 - val_acc: 0.8474\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9243\n",
      "Epoch 00062: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2491 - acc: 0.9243 - val_loss: 0.5335 - val_acc: 0.8488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9253\n",
      "Epoch 00063: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2443 - acc: 0.9252 - val_loss: 0.6071 - val_acc: 0.8293\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9264\n",
      "Epoch 00064: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2428 - acc: 0.9264 - val_loss: 0.5297 - val_acc: 0.8481\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9259\n",
      "Epoch 00065: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2411 - acc: 0.9259 - val_loss: 0.6030 - val_acc: 0.8206\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9274\n",
      "Epoch 00066: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2387 - acc: 0.9274 - val_loss: 0.5472 - val_acc: 0.8430\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9292\n",
      "Epoch 00067: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2310 - acc: 0.9292 - val_loss: 0.5183 - val_acc: 0.8512\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9300\n",
      "Epoch 00068: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2293 - acc: 0.9300 - val_loss: 0.5370 - val_acc: 0.8449\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9324\n",
      "Epoch 00069: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2261 - acc: 0.9324 - val_loss: 0.5115 - val_acc: 0.8530\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9296\n",
      "Epoch 00070: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2250 - acc: 0.9296 - val_loss: 0.5239 - val_acc: 0.8514\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9308\n",
      "Epoch 00071: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2246 - acc: 0.9308 - val_loss: 0.5818 - val_acc: 0.8288\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9323\n",
      "Epoch 00072: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2197 - acc: 0.9323 - val_loss: 0.5552 - val_acc: 0.8451\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9327\n",
      "Epoch 00073: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2188 - acc: 0.9327 - val_loss: 0.5427 - val_acc: 0.8432\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9357\n",
      "Epoch 00074: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2138 - acc: 0.9357 - val_loss: 0.6962 - val_acc: 0.8081\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9341\n",
      "Epoch 00075: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2144 - acc: 0.9341 - val_loss: 0.5607 - val_acc: 0.8386\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9343\n",
      "Epoch 00076: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2146 - acc: 0.9343 - val_loss: 0.6206 - val_acc: 0.8309\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9366\n",
      "Epoch 00077: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2056 - acc: 0.9365 - val_loss: 0.6919 - val_acc: 0.8092\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9354\n",
      "Epoch 00078: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2099 - acc: 0.9354 - val_loss: 0.5600 - val_acc: 0.8437\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9350\n",
      "Epoch 00079: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2068 - acc: 0.9350 - val_loss: 0.6116 - val_acc: 0.8255\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9405\n",
      "Epoch 00080: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1996 - acc: 0.9404 - val_loss: 0.5595 - val_acc: 0.8421\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9377\n",
      "Epoch 00081: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2009 - acc: 0.9377 - val_loss: 0.5635 - val_acc: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9390\n",
      "Epoch 00082: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1978 - acc: 0.9390 - val_loss: 0.6524 - val_acc: 0.8197\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9414\n",
      "Epoch 00083: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1945 - acc: 0.9413 - val_loss: 0.6773 - val_acc: 0.8118\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9413\n",
      "Epoch 00084: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1945 - acc: 0.9413 - val_loss: 0.5584 - val_acc: 0.8451\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9429\n",
      "Epoch 00085: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1890 - acc: 0.9428 - val_loss: 0.5725 - val_acc: 0.8418\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9430\n",
      "Epoch 00086: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1885 - acc: 0.9430 - val_loss: 0.6104 - val_acc: 0.8318\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9426\n",
      "Epoch 00087: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1896 - acc: 0.9426 - val_loss: 0.5773 - val_acc: 0.8414\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9442\n",
      "Epoch 00088: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1835 - acc: 0.9441 - val_loss: 0.5848 - val_acc: 0.8390\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9407\n",
      "Epoch 00089: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1899 - acc: 0.9406 - val_loss: 0.5495 - val_acc: 0.8519\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9436\n",
      "Epoch 00090: val_loss did not improve from 0.47535\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1845 - acc: 0.9436 - val_loss: 0.5542 - val_acc: 0.8521\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 577us/sample - loss: 0.5715 - acc: 0.8216\n",
      "Loss: 0.5714522497676243 Accuracy: 0.8215992\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9158 - acc: 0.3882\n",
      "Epoch 00001: val_loss improved from inf to 1.81063, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/001-1.8106.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.9158 - acc: 0.3882 - val_loss: 1.8106 - val_acc: 0.4570\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2704 - acc: 0.6013\n",
      "Epoch 00002: val_loss improved from 1.81063 to 1.10176, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/002-1.1018.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2703 - acc: 0.6013 - val_loss: 1.1018 - val_acc: 0.6629\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0014 - acc: 0.6945\n",
      "Epoch 00003: val_loss improved from 1.10176 to 0.89525, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/003-0.8953.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0014 - acc: 0.6945 - val_loss: 0.8953 - val_acc: 0.7244\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7495\n",
      "Epoch 00004: val_loss improved from 0.89525 to 0.77092, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/004-0.7709.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8352 - acc: 0.7495 - val_loss: 0.7709 - val_acc: 0.7678\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.7881\n",
      "Epoch 00005: val_loss improved from 0.77092 to 0.65161, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/005-0.6516.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7245 - acc: 0.7881 - val_loss: 0.6516 - val_acc: 0.8106\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.8135\n",
      "Epoch 00006: val_loss improved from 0.65161 to 0.60036, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/006-0.6004.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6410 - acc: 0.8135 - val_loss: 0.6004 - val_acc: 0.8199\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8320\n",
      "Epoch 00007: val_loss improved from 0.60036 to 0.55355, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/007-0.5536.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5782 - acc: 0.8320 - val_loss: 0.5536 - val_acc: 0.8432\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.8449\n",
      "Epoch 00008: val_loss improved from 0.55355 to 0.51920, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/008-0.5192.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5308 - acc: 0.8449 - val_loss: 0.5192 - val_acc: 0.8456\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.8582\n",
      "Epoch 00009: val_loss improved from 0.51920 to 0.48253, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/009-0.4825.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4906 - acc: 0.8582 - val_loss: 0.4825 - val_acc: 0.8577\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8654\n",
      "Epoch 00010: val_loss improved from 0.48253 to 0.47647, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/010-0.4765.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4573 - acc: 0.8654 - val_loss: 0.4765 - val_acc: 0.8614\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8764\n",
      "Epoch 00011: val_loss improved from 0.47647 to 0.45103, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/011-0.4510.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4263 - acc: 0.8764 - val_loss: 0.4510 - val_acc: 0.8623\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8838\n",
      "Epoch 00012: val_loss did not improve from 0.45103\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4000 - acc: 0.8838 - val_loss: 0.4677 - val_acc: 0.8558\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8897\n",
      "Epoch 00013: val_loss did not improve from 0.45103\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3805 - acc: 0.8897 - val_loss: 0.4738 - val_acc: 0.8521\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8960\n",
      "Epoch 00014: val_loss improved from 0.45103 to 0.42738, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/014-0.4274.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3637 - acc: 0.8959 - val_loss: 0.4274 - val_acc: 0.8717\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8993\n",
      "Epoch 00015: val_loss did not improve from 0.42738\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3449 - acc: 0.8992 - val_loss: 0.4344 - val_acc: 0.8656\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9026\n",
      "Epoch 00016: val_loss improved from 0.42738 to 0.40659, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/016-0.4066.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3298 - acc: 0.9026 - val_loss: 0.4066 - val_acc: 0.8758\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9101\n",
      "Epoch 00017: val_loss did not improve from 0.40659\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3127 - acc: 0.9101 - val_loss: 0.4213 - val_acc: 0.8749\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9136\n",
      "Epoch 00018: val_loss improved from 0.40659 to 0.39786, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/018-0.3979.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2993 - acc: 0.9136 - val_loss: 0.3979 - val_acc: 0.8826\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9180\n",
      "Epoch 00019: val_loss improved from 0.39786 to 0.38258, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/019-0.3826.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2856 - acc: 0.9180 - val_loss: 0.3826 - val_acc: 0.8824\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9201\n",
      "Epoch 00020: val_loss did not improve from 0.38258\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2764 - acc: 0.9201 - val_loss: 0.3835 - val_acc: 0.8835\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9252\n",
      "Epoch 00021: val_loss did not improve from 0.38258\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2634 - acc: 0.9251 - val_loss: 0.3955 - val_acc: 0.8782\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9265\n",
      "Epoch 00022: val_loss improved from 0.38258 to 0.36984, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/022-0.3698.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2582 - acc: 0.9265 - val_loss: 0.3698 - val_acc: 0.8891\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9318\n",
      "Epoch 00023: val_loss did not improve from 0.36984\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2424 - acc: 0.9317 - val_loss: 0.3875 - val_acc: 0.8800\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9342\n",
      "Epoch 00024: val_loss did not improve from 0.36984\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2347 - acc: 0.9342 - val_loss: 0.3853 - val_acc: 0.8810\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9363\n",
      "Epoch 00025: val_loss did not improve from 0.36984\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2265 - acc: 0.9363 - val_loss: 0.3871 - val_acc: 0.8772\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9387\n",
      "Epoch 00026: val_loss improved from 0.36984 to 0.36230, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/026-0.3623.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2185 - acc: 0.9386 - val_loss: 0.3623 - val_acc: 0.8903\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9407\n",
      "Epoch 00027: val_loss did not improve from 0.36230\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2096 - acc: 0.9407 - val_loss: 0.3760 - val_acc: 0.8845\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9411\n",
      "Epoch 00028: val_loss did not improve from 0.36230\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2065 - acc: 0.9410 - val_loss: 0.4086 - val_acc: 0.8698\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9457\n",
      "Epoch 00029: val_loss did not improve from 0.36230\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1961 - acc: 0.9457 - val_loss: 0.3747 - val_acc: 0.8868\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9483\n",
      "Epoch 00030: val_loss did not improve from 0.36230\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1871 - acc: 0.9482 - val_loss: 0.3699 - val_acc: 0.8919\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9498\n",
      "Epoch 00031: val_loss improved from 0.36230 to 0.35581, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/031-0.3558.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1818 - acc: 0.9498 - val_loss: 0.3558 - val_acc: 0.8903\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9522\n",
      "Epoch 00032: val_loss did not improve from 0.35581\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1779 - acc: 0.9521 - val_loss: 0.3634 - val_acc: 0.8912\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9535\n",
      "Epoch 00033: val_loss did not improve from 0.35581\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1674 - acc: 0.9535 - val_loss: 0.3864 - val_acc: 0.8824\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9555\n",
      "Epoch 00034: val_loss improved from 0.35581 to 0.35575, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_4_conv_checkpoint/034-0.3557.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1612 - acc: 0.9555 - val_loss: 0.3557 - val_acc: 0.8919\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9587\n",
      "Epoch 00035: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1541 - acc: 0.9587 - val_loss: 0.3767 - val_acc: 0.8912\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9596\n",
      "Epoch 00036: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1506 - acc: 0.9596 - val_loss: 0.3804 - val_acc: 0.8826\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9616\n",
      "Epoch 00037: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1438 - acc: 0.9615 - val_loss: 0.4087 - val_acc: 0.8756\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9608\n",
      "Epoch 00038: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1444 - acc: 0.9608 - val_loss: 0.3710 - val_acc: 0.8880\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9643\n",
      "Epoch 00039: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1365 - acc: 0.9642 - val_loss: 0.3738 - val_acc: 0.8884\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9657\n",
      "Epoch 00040: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1289 - acc: 0.9657 - val_loss: 0.4016 - val_acc: 0.8866\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9681\n",
      "Epoch 00041: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1252 - acc: 0.9680 - val_loss: 0.4075 - val_acc: 0.8838\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9672\n",
      "Epoch 00042: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1255 - acc: 0.9672 - val_loss: 0.3824 - val_acc: 0.8845\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9699\n",
      "Epoch 00043: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1179 - acc: 0.9699 - val_loss: 0.3946 - val_acc: 0.8789\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9689\n",
      "Epoch 00044: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1163 - acc: 0.9689 - val_loss: 0.3878 - val_acc: 0.8873\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9727\n",
      "Epoch 00045: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1090 - acc: 0.9726 - val_loss: 0.3896 - val_acc: 0.8842\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9723\n",
      "Epoch 00046: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1091 - acc: 0.9723 - val_loss: 0.3906 - val_acc: 0.8884\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9745\n",
      "Epoch 00047: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1019 - acc: 0.9745 - val_loss: 0.3961 - val_acc: 0.8861\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9763\n",
      "Epoch 00048: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0983 - acc: 0.9763 - val_loss: 0.3987 - val_acc: 0.8838\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9768\n",
      "Epoch 00049: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0960 - acc: 0.9767 - val_loss: 0.4142 - val_acc: 0.8796\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9752\n",
      "Epoch 00050: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0963 - acc: 0.9752 - val_loss: 0.3760 - val_acc: 0.8924\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9775\n",
      "Epoch 00051: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0899 - acc: 0.9774 - val_loss: 0.3891 - val_acc: 0.8880\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9767\n",
      "Epoch 00052: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0908 - acc: 0.9767 - val_loss: 0.3903 - val_acc: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9807\n",
      "Epoch 00053: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0812 - acc: 0.9806 - val_loss: 0.3951 - val_acc: 0.8877\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9789\n",
      "Epoch 00054: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0869 - acc: 0.9789 - val_loss: 0.3926 - val_acc: 0.8924\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9824\n",
      "Epoch 00055: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0763 - acc: 0.9824 - val_loss: 0.3941 - val_acc: 0.8847\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9802\n",
      "Epoch 00056: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0796 - acc: 0.9801 - val_loss: 0.5298 - val_acc: 0.8542\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9814\n",
      "Epoch 00057: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0781 - acc: 0.9814 - val_loss: 0.4151 - val_acc: 0.8840\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9837\n",
      "Epoch 00058: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0698 - acc: 0.9837 - val_loss: 0.4095 - val_acc: 0.8828\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9811\n",
      "Epoch 00059: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0785 - acc: 0.9811 - val_loss: 0.4112 - val_acc: 0.8852\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9857\n",
      "Epoch 00060: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0661 - acc: 0.9856 - val_loss: 0.4234 - val_acc: 0.8856\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9823\n",
      "Epoch 00061: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0716 - acc: 0.9823 - val_loss: 0.4319 - val_acc: 0.8807\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9845\n",
      "Epoch 00062: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0658 - acc: 0.9845 - val_loss: 0.4234 - val_acc: 0.8838\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9826\n",
      "Epoch 00063: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0705 - acc: 0.9825 - val_loss: 0.4248 - val_acc: 0.8852\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9854\n",
      "Epoch 00064: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0610 - acc: 0.9854 - val_loss: 0.4124 - val_acc: 0.8866\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9859\n",
      "Epoch 00065: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0601 - acc: 0.9859 - val_loss: 0.4504 - val_acc: 0.8782\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9877\n",
      "Epoch 00066: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0544 - acc: 0.9876 - val_loss: 0.4359 - val_acc: 0.8812\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9862\n",
      "Epoch 00067: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0587 - acc: 0.9863 - val_loss: 0.4468 - val_acc: 0.8821\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9888\n",
      "Epoch 00068: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0524 - acc: 0.9888 - val_loss: 0.4248 - val_acc: 0.8838\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9784\n",
      "Epoch 00069: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0805 - acc: 0.9783 - val_loss: 0.4377 - val_acc: 0.8863\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9846\n",
      "Epoch 00070: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0631 - acc: 0.9846 - val_loss: 0.4093 - val_acc: 0.8910\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9909\n",
      "Epoch 00071: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0445 - acc: 0.9909 - val_loss: 0.4062 - val_acc: 0.8942\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9901\n",
      "Epoch 00072: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0455 - acc: 0.9900 - val_loss: 0.4435 - val_acc: 0.8835\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9881\n",
      "Epoch 00073: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0507 - acc: 0.9881 - val_loss: 0.4619 - val_acc: 0.8765\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9902\n",
      "Epoch 00074: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0464 - acc: 0.9902 - val_loss: 0.4290 - val_acc: 0.8875\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9908\n",
      "Epoch 00075: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0433 - acc: 0.9908 - val_loss: 0.4566 - val_acc: 0.8791\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9912\n",
      "Epoch 00076: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0431 - acc: 0.9912 - val_loss: 0.4839 - val_acc: 0.8784\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9849\n",
      "Epoch 00077: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0579 - acc: 0.9849 - val_loss: 0.4326 - val_acc: 0.8835\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9907\n",
      "Epoch 00078: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0415 - acc: 0.9907 - val_loss: 0.4312 - val_acc: 0.8926\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9921\n",
      "Epoch 00079: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0383 - acc: 0.9921 - val_loss: 0.6176 - val_acc: 0.8526\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9827\n",
      "Epoch 00080: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0658 - acc: 0.9827 - val_loss: 0.4315 - val_acc: 0.8856\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9894\n",
      "Epoch 00081: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0441 - acc: 0.9893 - val_loss: 0.4434 - val_acc: 0.8847\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9877\n",
      "Epoch 00082: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0505 - acc: 0.9877 - val_loss: 0.4392 - val_acc: 0.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9944\n",
      "Epoch 00083: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0325 - acc: 0.9943 - val_loss: 0.4825 - val_acc: 0.8775\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9891\n",
      "Epoch 00084: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0448 - acc: 0.9891 - val_loss: 0.4508 - val_acc: 0.8854\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 603us/sample - loss: 0.4344 - acc: 0.8708\n",
      "Loss: 0.4343552954843111 Accuracy: 0.87082034\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6934 - acc: 0.4755\n",
      "Epoch 00001: val_loss improved from inf to 1.44812, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/001-1.4481.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6934 - acc: 0.4755 - val_loss: 1.4481 - val_acc: 0.5684\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8827 - acc: 0.7431\n",
      "Epoch 00002: val_loss improved from 1.44812 to 0.69073, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/002-0.6907.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8827 - acc: 0.7431 - val_loss: 0.6907 - val_acc: 0.7994\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6180 - acc: 0.8266\n",
      "Epoch 00003: val_loss improved from 0.69073 to 0.53839, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/003-0.5384.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6180 - acc: 0.8267 - val_loss: 0.5384 - val_acc: 0.8460\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8635\n",
      "Epoch 00004: val_loss improved from 0.53839 to 0.44643, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/004-0.4464.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4864 - acc: 0.8635 - val_loss: 0.4464 - val_acc: 0.8696\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8861\n",
      "Epoch 00005: val_loss did not improve from 0.44643\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4048 - acc: 0.8861 - val_loss: 0.4594 - val_acc: 0.8635\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.9025\n",
      "Epoch 00006: val_loss improved from 0.44643 to 0.38783, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/006-0.3878.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3502 - acc: 0.9025 - val_loss: 0.3878 - val_acc: 0.8849\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9140\n",
      "Epoch 00007: val_loss improved from 0.38783 to 0.34646, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/007-0.3465.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3079 - acc: 0.9140 - val_loss: 0.3465 - val_acc: 0.8977\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9215\n",
      "Epoch 00008: val_loss improved from 0.34646 to 0.31562, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/008-0.3156.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2812 - acc: 0.9215 - val_loss: 0.3156 - val_acc: 0.9029\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9290\n",
      "Epoch 00009: val_loss improved from 0.31562 to 0.30099, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/009-0.3010.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2526 - acc: 0.9289 - val_loss: 0.3010 - val_acc: 0.9122\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9354\n",
      "Epoch 00010: val_loss improved from 0.30099 to 0.29230, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/010-0.2923.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2333 - acc: 0.9354 - val_loss: 0.2923 - val_acc: 0.9108\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9415\n",
      "Epoch 00011: val_loss improved from 0.29230 to 0.28739, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/011-0.2874.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2134 - acc: 0.9415 - val_loss: 0.2874 - val_acc: 0.9175\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9452\n",
      "Epoch 00012: val_loss did not improve from 0.28739\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1964 - acc: 0.9453 - val_loss: 0.3164 - val_acc: 0.9040\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9499\n",
      "Epoch 00013: val_loss did not improve from 0.28739\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1830 - acc: 0.9499 - val_loss: 0.3392 - val_acc: 0.9001\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9546\n",
      "Epoch 00014: val_loss improved from 0.28739 to 0.26347, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/014-0.2635.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1655 - acc: 0.9546 - val_loss: 0.2635 - val_acc: 0.9206\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9589\n",
      "Epoch 00015: val_loss did not improve from 0.26347\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1551 - acc: 0.9589 - val_loss: 0.2726 - val_acc: 0.9175\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9630\n",
      "Epoch 00016: val_loss did not improve from 0.26347\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1419 - acc: 0.9630 - val_loss: 0.2673 - val_acc: 0.9208\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9666\n",
      "Epoch 00017: val_loss did not improve from 0.26347\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1307 - acc: 0.9666 - val_loss: 0.3231 - val_acc: 0.8942\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9677\n",
      "Epoch 00018: val_loss improved from 0.26347 to 0.25704, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/018-0.2570.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1245 - acc: 0.9677 - val_loss: 0.2570 - val_acc: 0.9215\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9723\n",
      "Epoch 00019: val_loss did not improve from 0.25704\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1120 - acc: 0.9722 - val_loss: 0.2691 - val_acc: 0.9182\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9750\n",
      "Epoch 00020: val_loss did not improve from 0.25704\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1051 - acc: 0.9750 - val_loss: 0.2762 - val_acc: 0.9159\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9749\n",
      "Epoch 00021: val_loss did not improve from 0.25704\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1012 - acc: 0.9749 - val_loss: 0.2637 - val_acc: 0.9171\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9790\n",
      "Epoch 00022: val_loss improved from 0.25704 to 0.24438, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/022-0.2444.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0888 - acc: 0.9790 - val_loss: 0.2444 - val_acc: 0.9273\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9818\n",
      "Epoch 00023: val_loss did not improve from 0.24438\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0817 - acc: 0.9818 - val_loss: 0.3007 - val_acc: 0.9101\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9807\n",
      "Epoch 00024: val_loss did not improve from 0.24438\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0836 - acc: 0.9807 - val_loss: 0.2543 - val_acc: 0.9264\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9858\n",
      "Epoch 00025: val_loss did not improve from 0.24438\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0702 - acc: 0.9858 - val_loss: 0.2507 - val_acc: 0.9241\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9868\n",
      "Epoch 00026: val_loss improved from 0.24438 to 0.23973, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_5_conv_checkpoint/026-0.2397.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0641 - acc: 0.9868 - val_loss: 0.2397 - val_acc: 0.9287\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9864\n",
      "Epoch 00027: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0635 - acc: 0.9864 - val_loss: 0.2752 - val_acc: 0.9178\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9900\n",
      "Epoch 00028: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0554 - acc: 0.9900 - val_loss: 0.2584 - val_acc: 0.9227\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9907\n",
      "Epoch 00029: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0507 - acc: 0.9907 - val_loss: 0.2649 - val_acc: 0.9192\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9920\n",
      "Epoch 00030: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0466 - acc: 0.9920 - val_loss: 0.2505 - val_acc: 0.9280\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9909\n",
      "Epoch 00031: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0479 - acc: 0.9909 - val_loss: 0.3005 - val_acc: 0.9124\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9938\n",
      "Epoch 00032: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0408 - acc: 0.9938 - val_loss: 0.2542 - val_acc: 0.9215\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9943\n",
      "Epoch 00033: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0374 - acc: 0.9943 - val_loss: 0.2717 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9937\n",
      "Epoch 00034: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0364 - acc: 0.9937 - val_loss: 0.2630 - val_acc: 0.9222\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9959\n",
      "Epoch 00035: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0304 - acc: 0.9958 - val_loss: 0.2781 - val_acc: 0.9194\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9934\n",
      "Epoch 00036: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0363 - acc: 0.9934 - val_loss: 0.2813 - val_acc: 0.9178\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9945\n",
      "Epoch 00037: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0327 - acc: 0.9945 - val_loss: 0.2909 - val_acc: 0.9171\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9962\n",
      "Epoch 00038: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0264 - acc: 0.9962 - val_loss: 0.2726 - val_acc: 0.9250\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9952\n",
      "Epoch 00039: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0300 - acc: 0.9952 - val_loss: 0.2947 - val_acc: 0.9175\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9971\n",
      "Epoch 00040: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0225 - acc: 0.9971 - val_loss: 0.2902 - val_acc: 0.9196\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9946\n",
      "Epoch 00041: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0292 - acc: 0.9946 - val_loss: 0.2729 - val_acc: 0.9273\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9978\n",
      "Epoch 00042: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0199 - acc: 0.9978 - val_loss: 0.2946 - val_acc: 0.9178\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9967\n",
      "Epoch 00043: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0221 - acc: 0.9967 - val_loss: 0.2884 - val_acc: 0.9213\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9953\n",
      "Epoch 00044: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0259 - acc: 0.9953 - val_loss: 0.2703 - val_acc: 0.9234\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9983\n",
      "Epoch 00045: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0180 - acc: 0.9982 - val_loss: 0.2808 - val_acc: 0.9243\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9946\n",
      "Epoch 00046: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0270 - acc: 0.9945 - val_loss: 0.2714 - val_acc: 0.9245\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9956\n",
      "Epoch 00047: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0231 - acc: 0.9956 - val_loss: 0.2548 - val_acc: 0.9308\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9987\n",
      "Epoch 00048: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0139 - acc: 0.9987 - val_loss: 0.2695 - val_acc: 0.9264\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9988\n",
      "Epoch 00049: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0129 - acc: 0.9988 - val_loss: 0.2827 - val_acc: 0.9231\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9936\n",
      "Epoch 00050: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0299 - acc: 0.9935 - val_loss: 0.2989 - val_acc: 0.9248\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9965\n",
      "Epoch 00051: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0198 - acc: 0.9965 - val_loss: 0.2669 - val_acc: 0.9276\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9975\n",
      "Epoch 00052: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0162 - acc: 0.9975 - val_loss: 0.2698 - val_acc: 0.9306\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9991\n",
      "Epoch 00053: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0109 - acc: 0.9991 - val_loss: 0.3123 - val_acc: 0.9171\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9983\n",
      "Epoch 00054: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0140 - acc: 0.9983 - val_loss: 0.2905 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9989\n",
      "Epoch 00055: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0110 - acc: 0.9988 - val_loss: 0.3428 - val_acc: 0.9126\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9902\n",
      "Epoch 00056: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0368 - acc: 0.9902 - val_loss: 0.2742 - val_acc: 0.9269\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9993\n",
      "Epoch 00057: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0095 - acc: 0.9993 - val_loss: 0.2791 - val_acc: 0.9313\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9991\n",
      "Epoch 00058: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0086 - acc: 0.9991 - val_loss: 0.2844 - val_acc: 0.9236\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9992\n",
      "Epoch 00059: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0089 - acc: 0.9992 - val_loss: 0.3144 - val_acc: 0.9133\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9939\n",
      "Epoch 00060: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0245 - acc: 0.9939 - val_loss: 0.2942 - val_acc: 0.9234\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9985\n",
      "Epoch 00061: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0115 - acc: 0.9985 - val_loss: 0.2822 - val_acc: 0.9287\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9982\n",
      "Epoch 00062: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0119 - acc: 0.9982 - val_loss: 0.2905 - val_acc: 0.9299\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9929\n",
      "Epoch 00063: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0285 - acc: 0.9929 - val_loss: 0.2739 - val_acc: 0.9292\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0076 - acc: 0.9993 - val_loss: 0.3022 - val_acc: 0.9252\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9965\n",
      "Epoch 00065: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0169 - acc: 0.9965 - val_loss: 0.3183 - val_acc: 0.9182\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9977\n",
      "Epoch 00066: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0125 - acc: 0.9977 - val_loss: 0.3028 - val_acc: 0.9262\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9979\n",
      "Epoch 00067: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0110 - acc: 0.9979 - val_loss: 0.3340 - val_acc: 0.9171\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0141 - acc: 0.9973 - val_loss: 0.3133 - val_acc: 0.9241\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9971\n",
      "Epoch 00069: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0136 - acc: 0.9971 - val_loss: 0.2918 - val_acc: 0.9255\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9994\n",
      "Epoch 00070: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0071 - acc: 0.9994 - val_loss: 0.2916 - val_acc: 0.9266\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9995\n",
      "Epoch 00071: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0058 - acc: 0.9995 - val_loss: 0.2990 - val_acc: 0.9250\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9979\n",
      "Epoch 00072: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0120 - acc: 0.9978 - val_loss: 0.2951 - val_acc: 0.9259\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0168 - acc: 0.9965 - val_loss: 0.3058 - val_acc: 0.9243\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9992\n",
      "Epoch 00074: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0072 - acc: 0.9992 - val_loss: 0.3153 - val_acc: 0.9264\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9996\n",
      "Epoch 00075: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0058 - acc: 0.9996 - val_loss: 0.3297 - val_acc: 0.9252\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 00076: val_loss did not improve from 0.23973\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0100 - acc: 0.9983 - val_loss: 0.3546 - val_acc: 0.9187\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmclkJslkIwQIS0gCguxbWBQBFbVKlboh+lWrtmK11W/V/myxq61ttdXWft3aqtW6VaVY60bFDcQFKItssi8BAoTsIctkkpl5fn+crJCEBDIkkOf9et1XMnc9987Mec5y7xkjIiillFJH4+joBCillDo5aMBQSinVKhowlFJKtYoGDKWUUq2iAUMppVSraMBQSinVKhowlFJKtYoGDKWUUq2iAUMppVSrRIRrx8aYZ4GLgVwRGd7E8nuAaxukYwiQLCKFxpgsoBQIAgERyWzNMbt37y5paWntkHqllOoaVq1alS8iya1Z14RraBBjzFSgDHihqYBx2LqXAHeJyLk1r7OATBHJb8sxMzMzZeXKlceYYqWU6nqMMataWygPW5OUiCwBClu5+jXAK+FKi1JKqePX4X0Yxpho4ELg9QazBXjfGLPKGHNLx6RMKaVUQ2Hrw2iDS4DPRaRhbeQsEdlnjOkBfGCM2VxTYzlCTUC5BSA1NTX8qVVKqS6qMwSMqzmsOUpE9tX8zTXGvAFMAJoMGCLyFPAU2D6Mw5dXV1eTnZ1NZWVle6e7S/B4PPTt2xeXy9XRSVFKdbAODRjGmHhgGnBdg3kxgENESmv+vwD41bEeIzs7m9jYWNLS0jDGHHeauxIRoaCggOzsbNLT0zs6OUqpDhbO22pfAc4GuhtjsoFfAC4AEflLzWqXAe+LSHmDTXsCb9Rk7hHAP0TkvWNNR2VlpQaLY2SMISkpiby8vI5OilKqEwhbwBCRa1qxzt+Bvx82bycwqj3TosHi2Om1U0rV6vC7pDoDv38/gUBJRydDKaU6NQ0YQFVVDoHAobDsu7i4mCeffPKYtp0xYwbFxcWtXv++++7j4YcfPqZjKaXU0WjAAIxxAqGw7LulgBEIBFrcdsGCBSQkJIQjWUop1WYaMABwIBIMy57nzp3Ljh07GD16NPfccw+LFy9mypQpzJw5k6FDhwJw6aWXMm7cOIYNG8ZTTz1Vt21aWhr5+flkZWUxZMgQ5syZw7Bhw7jgggvw+XwtHnfNmjVMmjSJkSNHctlll1FUVATAo48+ytChQxk5ciRXX301AJ988gmjR49m9OjRjBkzhtLS0rBcC6XUya0zPIdxwmzbdidlZWuOmB8KlQMOHI6oNu/T6x3Naaf9qdnlDz74IBs2bGDNGnvcxYsXs3r1ajZs2FB3q+qzzz5Lt27d8Pl8jB8/niuuuIKkpKTD0r6NV155haeffpqrrrqK119/neuuu+6I49X65je/yWOPPca0adP4+c9/zi9/+Uv+9Kc/8eCDD7Jr1y7cbnddc9fDDz/ME088weTJkykrK8Pj8bT5OiilTn1awwDgxN4JNGHChEbPNTz66KOMGjWKSZMmsXfvXrZt23bENunp6YwePRqAcePGkZWV1ez+S0pKKC4uZtq0aQDccMMNLFlin3scOXIk1157LS+99BIREba8MHnyZO6++24effRRiouL6+YrpVRDXSpnaK4mUFGxBREhJub0E5KOmJiYuv8XL17Mhx9+yNKlS4mOjubss89u8ql0t9td97/T6Txqk1Rz3n33XZYsWcLbb7/Nb37zG9avX8/cuXP5+te/zoIFC5g8eTILFy7k9NNPzLVQSp08tIYB2MsQnj6M2NjYFvsESkpKSExMJDo6ms2bN7Ns2bLjPmZ8fDyJiYl8+umnALz44otMmzaNUCjE3r17Oeecc/jd735HSUkJZWVl7NixgxEjRvCjH/2I8ePHs3nz5uNOg1Lq1NOlahjNMcZJKBSeu6SSkpKYPHkyw4cP56KLLuLrX/96o+UXXnghf/nLXxgyZAiDBw9m0qRJ7XLc559/nltvvZWKigoyMjJ47rnnCAaDXHfddZSUlCAi/O///i8JCQn87Gc/Y9GiRTgcDoYNG8ZFF13ULmlQSp1awvYDSh2hqR9Q2rRpE0OGDGlxO58vi2CwBK+3XR8wP2W05hoqpU5OneIHlE4mxjgQCU8NQymlThUaMLABA4KcSrUtpZRqbxowAHDW/NWAoZRSzdGAQW0NA22WUkqpFmjAAOovgwYMpZRqjgYMtIahlFKtoQEDqO/DCM/De23l9XrbNF8ppU4EDRhoDUMppVpDAwYQzj6MuXPn8sQTT9S9rv2Ro7KyMqZPn87YsWMZMWIEb775Zqv3KSLcc889DB8+nBEjRvDaa68BcODAAaZOncro0aMZPnw4n376KcFgkBtvvLFu3UceeaTdz1Ep1TV0raFB7rwT1hw5vLlTQkSFyu3w5qaNl2T0aPhT88Obz549mzvvvJPvfe97AMybN4+FCxfi8Xh44403iIuLIz8/n0mTJjFz5sxW/Yb2v/71L9asWcPatWvJz89n/PjxTJ06lX/84x987Wtf4yc/+QnBYJCKigrWrFnDvn372LBhA0CbfsFPKaUaClsNwxjzrDEm1xizoZnlZxtjSowxa2qmnzdYdqExZosxZrsxZm640nik9n8OY8yYMeTm5rJ//37Wrl1LYmIi/fr1Q0T48Y9/zMiRIznvvPPYt28fBw8ebNU+P/vsM6655hqcTic9e/Zk2rRprFixgvHjx/Pcc89x3333sX79emJjY8nIyGDnzp3ccccdvPfee8TFxbX7OSqluoZw1jD+DjwOvNDCOp+KyMUNZxj7e6lPAOcD2cAKY8xbIrLxuFPUTE1AQlX4ytfhdqcSGdnjuA9zuFmzZjF//nxycnKYPXs2AC+//DJ5eXmsWrUKl8tFWlpak8Oat8XUqVNZsmQJ7777LjfeeCN333033/zmN1m7di0LFy7kL3/5C/PmzePZZ59tj9NSSnUxYathiMgSoPAYNp0AbBeRnSJSBbwKfKNdE3cYG6PC1+k9e/ZsXn31VebPn8+sWbMAO6x5jx49cLlcLFq0iN27d7d6f1OmTOG1114jGAySl5fHkiVLmDBhArt376Znz57MmTOHm2++mdWrV5Ofn08oFOKKK67g17/+NatXrw7LOSqlTn0d3YdxhjFmLbAf+H8i8hXQB9jbYJ1sYGJ4kxHeB/eGDRtGaWkpffr0ISUlBYBrr72WSy65hBEjRpCZmdmmHyy67LLLWLp0KaNGjcIYw+9//3t69erF888/z0MPPYTL5cLr9fLCCy+wb98+brrpprrh2x944IGwnKNS6tQX1uHNjTFpwDsiMryJZXFASETKjDEzgP8TkdOMMVcCF4rIzTXrXQ9MFJHbmznGLcAtAKmpqeMOL6m3dmju0tJVuFw98Xj6tuUUuwQd3lypU9dJMby5iBwSkbKa/xcALmNMd2Af0K/Bqn1r5jW3n6dEJFNEMpOTk48jRU46y4N7SinVGXVYwDDG9DI195AaYybUpKUAWAGcZoxJN8ZEAlcDb4U/PfqbGEop1ZKw9WEYY14Bzga6G2OygV8ALgAR+QtwJXCbMSYA+ICrxbaPBYwxtwMLscX+Z2v6NsLKPu2tAUMppZoTtoAhItccZfnj2Ntum1q2AFgQjnQ1T2sYSinVEh0aBGDLFlxFQbSGoZRSzdOAAVBRgakSRLTTWymlmqMBA8DhwIQgHDWM4uJinnzyyWPadsaMGTr2k1Kq09CAAeB0goTnSe+WAkYgEGhx2wULFpCQkNDuaVJKqWOhAQPqahjhCBhz585lx44djB49mnvuuYfFixczZcoUZs6cydChQwG49NJLGTduHMOGDeOpp56q2zYtLY38/HyysrIYMmQIc+bMYdiwYVxwwQX4fL4jjvX2228zceJExowZw3nnnVc3mGFZWRk33XQTI0aMYOTIkbz++usAvPfee4wdO5ZRo0Yxffr0dj93pdSppaOHBjmhmhndHCrSgBBBt+B0NrG8BUcZ3ZwHH3yQDRs2sKbmwIsXL2b16tVs2LCB9PR0AJ599lm6deuGz+dj/PjxXHHFFSQlJTXaz7Zt23jllVd4+umnueqqq3j99de57rrrGq1z1llnsWzZMowxPPPMM/z+97/nD3/4A/fffz/x8fGsX78egKKiIvLy8pgzZw5LliwhPT2dwsJjGfZLKdWVdKmA0SxDg+4LqZkRPhMmTKgLFgCPPvoob7zxBgB79+5l27ZtRwSM9PR0Ro8eDcC4cePIyso6Yr/Z2dnMnj2bAwcOUFVVVXeMDz/8kFdffbVuvcTERN5++22mTp1at063bt3a9RyVUqeeLhUwmq0J7DhAqKKU8rQAXu+YutFrwyUmJqbu/8WLF/Phhx+ydOlSoqOjOfvss5sc5tztdtf973Q6m2ySuuOOO7j77ruZOXMmixcv5r777gtL+pVSXZP2YQA4nZiQHYSxvfsxYmNjKS0tbXZ5SUkJiYmJREdHs3nzZpYtW3bMxyopKaFPnz4APP/883Xzzz///EY/E1tUVMSkSZNYsmQJu3btAtAmKaXUUWnAAHA4IFQ7am/7PouRlJTE5MmTGT58OPfcc88Ryy+88EICgQBDhgxh7ty5TJo06ZiPdd999zFr1izGjRtH9+7d6+b/9Kc/paioiOHDhzNq1CgWLVpEcnIyTz31FJdffjmjRo2q+2EnpZRqTliHNz/RMjMzZeXKlY3mtWpo7uxs5GAOZYMgOnooTmd0GFN58tHhzZU6dZ0Uw5t3Kk4nRgjbsxhKKXUq0IABtkkKau6U0oChlFJN0YABdQEjXA/vKaXUqUADBlD7tJ4dT0oHIFRKqaZowIBGTVJaw1BKqaZpwID6GoaA9mEopVTTNGBAp6theL3ejk6CUkodQQMGNOr01hqGUko1TQMGNGiSMu3+q3tz585tNCzHfffdx8MPP0xZWRnTp09n7NixjBgxgjfffPOo+2puGPSmhilvbkhzpZQ6Vl1q8ME737uTNTlNjG8uAmVlhCINuCJwODyt3ufoXqP504XNj28+e/Zs7rzzTr73ve8BMG/ePBYuXIjH4+GNN94gLi6O/Px8Jk2axMyZMzGm+ZFymxoGPRQKNTlMeVNDmiul1PEIW8AwxjwLXAzkisjwJpZfC/wIO5Z4KXCbiKytWZZVMy8IBFr72PpxJNb+ETu4eXsaM2YMubm57N+/n7y8PBITE+nXrx/V1dX8+Mc/ZsmSJTgcDvbt28fBgwfp1atXs/tqahj0vLy8Jocpb2pIc6WUOh7hrGH8HXgceKGZ5buAaSJSZIy5CHgKmNhg+Tkikt+eCWqpJsCqVVR1cxLo6SU6emB7HpZZs2Yxf/58cnJy6gb5e/nll8nLy2PVqlW4XC7S0tKaHNa8VmuHQVdKqXAJWx+GiCwBmh0zW0S+EJHadpJlQN9wpaVVHA5MyBCOB/dmz57Nq6++yvz585k1axZghyLv0aMHLpeLRYsWsXv37hb30dww6M0NU97UkOZKKXU8Okun97eB/zR4LcD7xphVxphbWtrQGHOLMWalMWZlXl7esaegZgDCcNxWO2zYMEpLS+nTpw8pKSkAXHvttaxcuZIRI0bwwgsvcPrpp7e4j+aGQW9umPKmhjRXSqnjEdbhzY0xacA7TfVhNFjnHOBJ4CwRKaiZ10dE9hljegAfAHfU1FhadMzDmwNs2EAgMoi/TwQxMcOOvn4XosObK3XqOmmGNzfGjASeAb5RGywARGRfzd9c4A1gQtgT43Do4INKKdWCDgsYxphU4F/A9SKytcH8GGNMbO3/wAXAhrAnyOnU4c2VUqoF4byt9hXgbKC7MSYb+AXgAhCRvwA/B5KAJ2uePai9fbYn8EbNvAjgHyLy3vGkRURafL4BsDWMgGgN4zCn0i8yKqWOT9gChohcc5TlNwM3NzF/JzCqvdLh8XgoKCggKSmp5aDhcNTVMFoVYLoAEaGgoACPp/UPMiqlTl2n/JPeffv2JTs7m6PeQVVQgFSU4w8IbvcmDRg1PB4Pfft27B3PSqnO4ZQPGC6Xq+4p6BZ9//uE/v40S970MXlyPi5XUvgTp5RSJ5HO8hxGx/N6MeV+EAgGyzs6NUop1elowKjl9WKCIRzVGjCUUqopGjBqxcQA4PBpwFBKqaZowKhV8yt3Th+EQhowlFLqcBowajUIGFrDUEqpI2nAqKUBQymlWqQBo1ZNH4azUgOGUko1RQNGLe3DUEqpFmnAqKVNUkop1SINGLW0SUoppVqkAaNWTQ0jotKlAUMppZqgAaNWTQ0jwu/SPgyllGqCBoxaLhe43UT4IrSGoZRSTTjlR6ttk5gYIvxCMFjR0SlRSqlOR2sYDXm9OCsd2iSllFJN0IDRkNdLhM9ok5RSSjVBA0ZDMTE4KjVgKKVUUzRgNOT14vSJBgyllGpCWAOGMeZZY0yuMWZDM8uNMeZRY8x2Y8w6Y8zYBstuMMZsq5luCGc663i9OH0h7cNQSqkmhLuG8XfgwhaWXwScVjPdAvwZwBjTDfgFMBGYAPzCGJMY1pQCeL04KoJaw1BKqSaE9bZaEVlijElrYZVvAC+IiADLjDEJxpgU4GzgAxEpBDDGfIANPK+EM73ExODwBQkGfWE9jFKtIQKBAEREgDFHX7+6GioqIC7u6OuLQFkZ5OdDMFg/3+GAPn3A7W59Gv1+KC+3+ysrs/MjI+3kctnXgYA9TiAAKSl1Aysc4cAB2LvXpt/hsH8PP5eYGEhPr993Q5WVkJdnt3E67T6iouw1aUppKezaZZcnJ9c9v9vo/GrTLlI/OZ32fXE6m77WwSCUlEBRkZ2qquq3BXt9Y2Pt5PXa40ZEHLmPwkIoKLDXNxisn2rPKyoKoqPt1L170+fYnjr6OYw+wN4Gr7Nr5jU3/wjGmFuwtRNSU1OPLzVeL46KakSqCIUCOBwdfXlUcyoqIDfXfhGrq+snn88u8/lsRhYZCR6PnVwu+wXMzbVTfr7dvuEXsTYjcLns/+XlUFxsp5ISu59u3eonY2wmVTuVl9tMqHYKhew2brf9GwjY+YcO2cmY+n0lJdmM4OBByMmxf/3++szB42mcQURH22X5+fZ8CgvttfF4bKbfpw/06GGP2TB9tefva6Zc5HBAWhoMGgQDBtjtcnNtRpyXZ/fh99v5fr89x7ZwOmHcOJg2zU7BIHz4IXz0EWzc2Lp9RERARoZNY7dukJUFO3fCvn31mXJDyckweLCdevaETZtg7Vq7TUNRUZCY2PizdLTzczrrg1NtkKuoaDodRzun6Gibhqoq+5lr7T6Sk+17FG4nfY4oIk8BTwFkZma28S06jNeLKa8CsUOcOxzx7ZHELq+21Of312feHo/9Um3dWj/t31+fCVVW2i9t7ZfR6bSv9++3mUJx8fGny+u1GXnDY4RC9jiBgJ1iYiAhoX7y+2HHDlixwpb8jKk/H7fb7tPrtSXHlBSbgfj99VNEhM2MY2NtqTYUsiXQggJbug6FbIZ2+unQq5ddz++3mVdlZX0mVlFRX+ocMcIGhh49bHpzciA7216nDRvqg6bbbTPD2kyzZ09bKm1YUq+utpnvli32PVm61O4zOdnuPyPDvq7dn9ttX9eed0yMvSZVVfWTMfWlcafT7vuTT+D//g8eesgeNzoapkyBG2+EIUPsPBF7PUQal+KLi+s/M1u2wJo1tsYxfbpNX0qKXS8UstenrAy2bbPrvv22/SwOHAhjx8JNN8Fpp9lrmZdng29hob1mtZm3x1Nfk3A46vcdCNR/VkKhxpPXa6917eR2N64t+f224FBWZv+Wlzd+byMjbQGie3f71+s98nPq89VPTufxfx9ao6MDxj6gX4PXfWvm7cM2SzWcvzjsqYmJwYjgqLIj1kZEaMCoFQjYL1N+fv0Xy++v/xIYYzO+3bthzx477d9fXyI9GpcLeveu/4J6PDaTqaysL/1HRNgv99ln15ee3W67rctlv2S1JfCoKLusqqo+CFVV2dJocrKdPJ6wXzbVAp8Pli+3mfDEia1vBjtetc18qu06+rK9BdxujHkV28FdIiIHjDELgd826Oi+ALg37KnpYr+JUVZWX2I/cMC+ri3F+nx2/s6dto13z57Gbd3NiYyEfv2gf3+YPLk+c67NoGtrDz6fzSAGDbJBoH9//RJ3NVFRNvifaPo5O3ZhvXTGmFewNYXuxphs7J1PLgAR+QuwAJgBbAcqgJtqlhUaY+4HVtTs6le1HeBhdQoFDBHbBp6VVT/t2dO4BnDoUMv7qG2COOMM+J//sTWA7t1t5p+UZL/wDTsC4+NtM4dDn+5R6pQU7rukrjnKcgG+18yyZ4Fnw5GuZp1kP9Pq89V39u3YUf93xw5bK6isbLx+UhKkptqOzLPPhr59bdNO7952iotr3B5/otpFlVInB62cNdTJfnVPxN5iuH69vasjO7t+2rvXNic1FBNjg8HgwTBjhu0ITEuzU//+zd/KqJRSraEBo6FGTVIndojzigobGNaurZ82bLC3cjZMXr9+tmbwta/Z5qKMDBsYMjJsE1Jr7tdXXZOv2sfukt308vYiwZNwXPsSEUqrSsmvyKdfXD9cziYeimijUn8pu4p34XK4GNx9MA5zfG2bgVAAX7WP6lA1EY4IXA4XLqcLp3Fi9ItyTDRgNHSCmqSCQfjqK3uHSO20cWP9/d5xcTBqFFx7rb1lcsQIGDrU3p7XXkSkxS+NP+AnJCGiXFFt2m9+RT7/2fYfFmxfQGxkLD+c/EMGdhvYaJ1SfylPr36a1QdWk+BJINGTSGJUIiLCzqKd7Czeyc6inVQFq7ho4EVcPuRypvWfVpcphSRE9qFsDpYdpEdMD3p5e+GOsLfYZBVnsWjXIj7O+phNeZu4fMjl3Jp5K92iuh2R1pLKEqJcUUQ6I49YVuovZX/pfqJcUSRHJ7fpOgRCAdYdXMfSvUuJcEQwse9EhvcYTkQTz/X4A36+zPmSZdnLWJq9lMpAJeeln8cFAy5gUNKgRu9RMBSkwFdASWUJxZXFFFcWU1pVij/gxx/04w/4qQpWEZIQghCSEGVVZXyV9xVrc9ayrXAbIbEfsnh3POmJ6aQlpNHb25te3l6kxKaQHJ1MdaiasqoyyqrKKPXboJDvyyevPI+8ijxyy3PJLc+lMmDbPHvG9OSGUTfw7bHfZlDSoLr3aGvBVlYfWE2KN4Wp/afidDRu49yQu4G/rvwrS7OXklWcRYGvoG5ZgieBiX0mckbfM5jafyqTUyc3+T7tLNrJZ3s+Y3P+ZrYUbGFL/hb2lOzBF/ARCAWafH8iHBH0ie1Dv/h+9IvrR//4/ozuNZrM3plkJGZgjCEYCrI+dz2f7/mc9bnrGZY8jOkZ0xnSfUjdexIIBdicv5k1OWso9BVSXlVORXUFFdUV+AI+O1X7qAxUEumMxBvprZtq3/va9y3CEYE30kusOxZvpJdgKEiJ377PJf4SqoPVRLmi8Dg9RLmi6BHTg9G9RjO612h6eXu1+rN5vIy09emSTiwzM1NWrlx57DvYtg0GDWLjjyHhe3+ld+9b2iVdRUWwZAksW2anFSvqbzXt1g0mTLDTmDE2UKSl2ZpCbnkuqw+sZvWB1RwsO8jlQy5nav+pLWb0lYFKHvr8IR5Z9giRzkh6xPSgp7cn3aK6UVJZQk5ZDjllORRXFnNr5q38/vzfH/FFfGfrO1z/xvUUVxYT746nl7cXvby9SIpOopunG4lRiSR6EnEYB5WBSioDlVRUV7DywEqW7l2KIPSM6UmJv4SqYBXXjbyOn075KQmeBB5d/iiPr3ic4spiUuNTKasqo7iyuC4jS/AkMCBxABmJGVSHqnl/x/tUVFeQ6ElkfJ/x7C3Zy86infiD/kZpTopKwh3hZn+pbadLjk4mPTGd/+77L9GuaL41+lvMGTeHnUU7+WjnR3y06yM25W+qO2bPmJ4kRSdR5Csi+1A2pVWljfYf7YomOTqZeE98oy++2+nG5XThcriIcESwrXAby7OXU15dfsT241LG0cvbqy4jKK4sZnfx7rpzSY1PxeVwsaNoBwD94/szrMcwDpYdZH/pfg6WH6y7Tm2RkZjByJ4jGdljJAO7DeRg+UGyirPYVbyLrOIscspyKPQ1f09J7bl3j+5O9+ju9PT2pEe0/VzFueP4z/b/8PaWtwlKkLNSzyLSGcnK/Ss55K+/q6JHTA+uHHIls4bNYn/pfv688s98tucz3E43U/tPJSMxg/SEdNIT0ymvKq8LoBtyNyAIsZGxfG3g17j4tIvp5e3Fe9vf4z/b/8OWgi2ADQIDEgcwuPtg0hPSiXHF2Aw2woPL4SIQClAdqqY6WE1FdQXZpdnsLdnL3kN72Vuyl+pQdd1nYVDSIDbmbaSsyj62HhsZW/d5SPGmcFbqWewv3c+XOV9SUd24JcJgiHZFE+WKIirCHt8T4WkUhGv363a6cUe4cTvdBEIByqrK8AXqn6Z0GAfx7ngSPAlEOCKoDFTiC9gAVLsPgF7eXmT2zuStq986ppqTMWaViGS2al0NGA3s3w99+rDlLoi+64/063fXMe+qoAD+/W+YP98+xVp77/fo0TBpEowaX0Yo9RO+qvyAj3Z+yKb8TURFRBHtiiYmMgZ/wM+BsgN1+3M73fiDfgZ2G8i3x3ybG0bdQEpsSt1yEeHtrW9z18K72Fm0k5mDZ9IzpmddaTC/Ip8ET4ItSXpTKK8u5+X1LzOxz0TmzZpHanwqwVCQX37yS+5fcj9jU8ZyxZArOFh2kANlB8gpy6HAV0Chr5AiX1GjDNvlcBHlimJQ0iC+ftrXuXjQxYxNGUtueS4Pff4Qf175Z/xBP5HOSPwBP5cNuYwfTf4RE/pMAGxp9JD/ECJCYlTjalRFdQUf7PiANza/wbqD60hLSGNgt4EM7DaQXt5e5JXnsb90P/tL91NaVcrEPhM5J/0chiUPwxjDuoPr+OPSP/KP9f+oyxSiXdFMSZ3C1P5TCYaC9hpV1F+jvrF96RvXl5TYFCoDlXUl6/yKfA75D9WXvmtK97UZUXWomn5x/ZjcbzJn9juTM/udSVCCLM9ezvJ9dqoNwgmeBOI98aTGpTKp7yTO6Hd7rSJHAAAgAElEQVQGvWN7A7bU/P6O91m4YyFZxVmkeFPoHdub3rG96RHTg0RPIvGeeOLd8cS6Y/FEeOoyn0hnJA7jqJsinZF4Io7+wIk/4K/7rLgj3I2CYmu2P1B6gBfWvsCL617EHeFmQu8JjO8znnEp49hWuI15X83jna3v1GWIAxIHcGvmrdw4+ka6Rzc/psUh/yEWZy3mna3v8M7Wd+q+E26nm3PSz2HGwBlMz5jOad1OO+ZmsapgFRtyN7Bq/ypW7l/JloItDEsexuRU+z72j+9PVnEWH+2yBY2le5fSL74f41LGkdk7k7EpY+nl7UW0Kxq3031czV3BUJCyqjIcxoE30tvsvoori1mbs5Y1OWtYc3AN5VXlzJs175iO2e4BwxjzfeA5oBR4BhgDzBWR948phWFy3AHj0CGIj2f7bRDxw/tJS/tpm3exYgU8+CC8+aZtekpPh/Nm7SI5cwlVcVvYVbKVrQVb2Zy/mepQNZ4ID1NSp5DZO5OqYBUV1RWUV5djMIzsOZKxKWMZ3Ws0kc5IXt/4Os98+QxLdi8BbGkoNT6VfnH9KK8uZ3HWYoYmD+Wxix7j3PRzj5rW1ze+zk1v3kSkM5I/f/3PPLfmOf6z/T/cNPomnpjxRIvNML5qHyEJ4YnwHNHUcLiDZQd5ZNkjHPIf4o4JdzAkeUjbLmo72F+6n7e2vMXQ5KFM6jupyeYNFV7lVeUs3LGQeHc856Sf0+Y+ipCE+PLAl+RX5DOl/xSiXdFhSmnXEo6AsVZERhljvgZ8B/gZ8KKIjD3KpifUcQeMmseJs250EPrZD8nIeKBVm4nAxx/DAw/Y8XASEuCbcw4Rd8Y/+aT4eT7d8ylgq80ZiRkMShrE8OThTM+YzlmpZ7WqBNfQlvwtvLXlLXaX7GZPyR72HtrLIf8hbh9/O7dPuL1NJa2tBVu5ct6VrM9dj8vh4rGLHuOWcbdop6BSXURbAkZrO71rc48Z2EDxlTkVcxSnEzweIvyCr5W31W7fDrfcAosW2bF/fvX7YrIH/YynN/wN3zofg5MG89tzf8tlQy5jYLeBTXZ8ttXg7oO5p/s9x70fgEFJg1h28zIeWfoI5w84v66ZSCmlDtfa3GuVMeZ9IB241xgTC7S99+1k4PUS4fdTXZ3f4mrBIPzpT/Czn9nhMB57TIg981V+9PFd5K3N46bRNzFn7Bwm9JnQ6Uvr0a5ofjL1Jx2dDKVUJ9fagPFtYDSwU0Qqan7g6KbwJasDeb1E+iOorNzV5OLVB1Zz3byb2ZqbRfDAcNK+NYI5lw7jrQP/5oO3PyCzdyYLrl3A2JRO1VqnlFLHrbUB4wxgjYiUG2OuA8YC/xe+ZHUgrxdXFfh8jQNGdbCaBz57gPs/uZ9gaTKRu69kwLivyJGX+Mnnh4hzx/H4RY9za+atR+0EVkqpk1FrA8afgVHGmFHAD7B3Sr0ATAtXwjpMTAwRlZVUVx8kGCzH6YxhY95GvvnGN1l1YBWerdfSY/mjfP5hN9LS7O2sew/tJc4dd9xPzyqlVGfW2oAREBExxnwDeFxE/maM+XY4E9ZhvF6ch4oAqKzMoigYz9TnpiIhQ/eP5mM2X8FHS+zDdQDGGFLjj/OX/pRS6iTQ2oBRaoy5F7gemGKMcVAzTPkpx+vFedD255eWb+Oad/+Ar7qSHm+sonDbYBYvtr+GppRSXU1rn5yZDfiBb4lIDvYX8B4KW6o6UkwMjgo7Bs1vvniSz/Z8Rt81f+XgxsG8+64dvkMppbqiVgWMmiDxMhBvjLkYqBSRF8Kaso7i9UK5j1XFbp5Y+wGX9P0WW+dfy+9+B2ed1dGJU0qpjtOqgGGMuQr4LzALuApYboy5MpwJ6zBeLzlyiN9sCpARG0vcZ4/h9cINN3R0wpRSqmO1tg/jJ8B4EckFMMYkAx8C88OVsA7j9XLzeT58QQc/GjCQO34czbe+ZYccV0qprqy1fRiO2mBRo6AN255UAtEePhgA16aP4MuFF+L3w223dXSqlFKq47W2hvGeMWYh8ErN69nAgvAkqWNlRfuproQhMRn88c1vc9ZZVYwYoSObKqVUazu97wGeAkbWTE+JyI+Otp0x5kJjzBZjzHZjzNwmlj9ijFlTM201xhQ3WBZssOyt1p/S8dnisj/64tt5Jvv3D+Cmm3afqEMrpVSn1uqhU0XkdeD11q5vjHECTwDnA9nACmPMWyKyscE+72qw/h3Y39mo5ROR0a09XnvZauwvj33yzoUkJuZw/vmrgdNOdDKUUqrTaTFgGGNKgaZ+MMMAIiItdQVPALaLyM6afb0KfAPY2Mz61wC/OGqKw2xrKJ+ECgcfLR7Gddf9mlBIx4VSSik4SpOUiMSKSFwTU+xRggVAH2Bvg9fZNfOOYIzpjx06/eMGsz3GmJXGmGXGmEtbcS7tYkv1AaIL+mAMXHrpG/h8O0/UoZVSqlM7/l/zaR9XA/NFJNhgXn8R2WeMyQA+NsasF5Edh29ojLkFuAUgNfX4x3Ta6sumpOAsvj42h7Q0N5WVGjCUUgrCe2vsPqBfg9d9a+Y15Wrq78ACQET21fzdCSymcf9Gw/WeEpFMEclMTk4+rgSXVZWxrzKX8oJxTOh/EI8no9nfxVBKqa4mnAFjBXCaMSbdGBOJDQpH3O1kjDkdSASWNpiXaIxx1/zfHZhM830f7WZbwTb7T8Eg0mPz8XjSqazcQyhUHe5DK6VUpxe2gCEiAeB2YCGwCZhX81vgvzLGzGyw6tXAqyLSsHN9CLDSGLMWWAQ82PDuqnDZWrDV/pM/mPTog0RFZQAh/P69LW6nlFJdQVj7MERkAYc94CciPz/s9X1NbPcFMCKcaWvKloIt9p/CgaR7FuLxjAPA59tZEzyUUqrr6iyd3p3C1oKtxIZSqQ5AL3Lw1wQJ7cdQSikNGI1sLdhKVPlg+jr3YkqKcbv7YkyE3imllFKcogMIHgsRYUvBFkJ5g0hPKILVqzHGidvdH59PaxhKKaU1jBq55bkc8h/Cs3sw6enA6rVQVkZUVLrWMJRSCq1h1Km9Q6oyexDpY+IhFIL//lefxVBKqRoaMGrU3SFVMIj0Kf3AGPjiCzyedKqr8wkESjs2gUop1cE0YNTYWrAVl3FDSSrpw2Ng2DD4/PO622m1lqGU6uo0YNTYUrCFbgwEcdo+jDPPhKVL8UT2B9BBCJVSXZ4GjBpbC7YS4xtMYiIkJACTJ0NJCVFZAUBrGEoppQEDCIQC7CjcgRQMsrULsDUMIOK/X+F0xuqdUkqpLk8DBpBVnEV1qBrf3sH1AWPAAEhOxnzxBdHRQygt/bJD06iUUh1NAwb1t9QWbG1QwzDG1jK++ILExHMpLV1OIFDWcYlUSqkOpgGD+oBRndMgYIANGNu2kRgYh0iAkpIlHZNApZTqBDRgAFvytxDn6gYV3RsHjMmTAYjbKBjjpqjow45JoFJKdQIaMICthVvp4RgE0DhgjBsHLhfOZauIj59MUdFHHZNApZTqBDRgYGsYXv9gANLSGizweGzQ+PxzEhOnU16+jqqq3A5Jo1JKdbQuHzACoQBDk4fiKZhI7942RjRy5pmwYgWJMdMAKCr6+MQnUimlOoEuHzAiHBG8f/37uNfd1rg5qtbkyeD3E7vd4HTGU1yszVJKqa6pyweMWrt20XTAOOMMAMzS5SQmnqMd30qpLksDBlBdDdnZzQSMlBTIyIAPPyQhYTqVlVk6rpRSqksKa8AwxlxojNlijNlujJnbxPIbjTF5xpg1NdPNDZbdYIzZVjPdEM507tljf/6iyYABMGsWLFxIt6qRAHq3lFKqSwpbwDDGOIEngIuAocA1xpihTaz6moiMrpmeqdm2G/ALYCIwAfiFMSYxXGndWVNhaDZg3HgjBINEvb6cyMje2iyllOqSwlnDmABsF5GdIlIFvAp8o5Xbfg34QEQKRaQI+AC4MEzpZFfNQLQZGc2scPrpMGkS5vnnSUw4l+LijxEJhSs5SinVKYUzYPQB9jZ4nV0z73BXGGPWGWPmG2P6tXHbdrFrF7hc0KelI9x0E3z1Fcl7BlBdnU95+fpwJUcppTqlju70fhtIE5GR2FrE823dgTHmFmPMSmPMyry8vGNKxK5dkJoKTmcLK82eDR4Pif/OAtBmKaVUlxPOgLEP6Nfgdd+aeXVEpEBE/DUvnwHGtXbbBvt4SkQyRSQzOTn5mBLa7C21DcXHw+WX43ztTWKcgygoeOeYjqWUUiercAaMFcBpxph0Y0wkcDXwVsMVjDEpDV7OBDbV/L8QuMAYk1jT2X1BzbywaFXAANv5XVxM2rqxFBcvprx801E3UUqpU0XYAoaIBIDbsRn9JmCeiHxljPmVMWZmzWr/a4z5yhizFvhf4MaabQuB+7FBZwXwq5p57S4YhJkz4ZxzWrHyuedCv34kvZWLMZHs2/d4OJKklFKdkhGRjk5Du8nMzJSVK1eG9yA//Sk88ADbPrqc/KJ3mbDtxzhfmgcjRsDLL4f32Eop1c6MMatEJLNV62rAaKNt22DQIIIDU3Hs3IMJYXvM9+yB5cthwoTwHl8ppdpRWwJGR98ldfI57TSYMQOnL8jBG3qz9rX+yPp1kJgIv/lNR6dOKaXCRgPGsXj3Xdi7F/PAQxT12E1h9VK480546y1Yt66jU6eUUmGhAeNYGUNy8pW4XD1t5/cdd0BsLPz2tx2dMqWUCgsNGMfB4Yikd+/vUFi4AJ+nEG6/HebNgy1bOjppSinV7jRgHKfevb+DMU5by7jrLvuTfQ880NHJUkqpdqcB4zi53b3p0eN/2LfvSSpiCuHWW+Gll+pHNFRKqVOEBox2kJHxO5zOaLZs+Q7yg7vtoFTal6GUOsVowGgHbncvMjIeoqTkE3IcC+G734VnnoFXXunopCmlVLvRgNFOUlK+RXz8VHbs+H9U/eoumDLFDom+fHlHJ00ppdqFBox2YoyDQYP+SjBYwbY9P4R//Qt694ZvfAP27j36DpRSqpPTgNGOYmJOp3//n5KX9xoFZjm88w74fHDJJVBW1tHJU0qdakTgqafg+uvt/2GmAaOdpab+iOjooWzZMgf/gAR47TVYvx6uvhoqKzs6eUp1rH37oDAsA093PQUFcMUV8J3vQE4OVFSE/ZAaMNqZwxHJ0KGvEAgcYsOGywmefzY88YQdTuSCC6CoqH0PKAJ/+xtMnAjbt7fvvpVqTwcPwsiRcP75J6Q0HDZr1nR84W/RIhg1yrZiPPwwLFwIMTFhP6wGjDDwekcyZMiLlJYuZ+vWW5DvfMfeMbV8OZx1Vvv1aezeDV/7Gtx8M/z3v/Dgg8e+r5P5C6w6PxF792BhIaxeDW++2dEpajsR+PGPYcwYGD7cFgJPtHXrYM4cmD4dvF5Ytgx+8ANwnKCsXEROmWncuHHSmeza9StZtAjZs+dhO+Pjj0Xi4kT69BFZt+7YdxwKiTz5pIjXa6cnnxT5zndEIiNF9u9v+/5yc0WGDhV57rljT5NSLXntNREQ+fWvRU47TWTUKJFgsKNTdaSDB0VeeunI71FVlcgNN9hzuPpqkdNPt//PnCmyc2d40+T3i7zyishZZ9ljejwit98uUlbWLrsHVkor89gOz+Tbc+psASMUCsmGDbNk0SKH5OcvsDPXrbMBw+USufZakaVLbQBorfJykauusm/d+eeLZGXZ+Vu3ihgjcu+9bU/o9dfb/XXvLnLoUNu3V0pEJBAQufNOkT/9SaS6un5+bq79bI0fb+e/+KL9vM2f37b9+3wi3/ymyMCBIv/8Z9u+N0ezY4fId79rM2Ow388bbhBZs8ZmzBddZOf/8pf2uH6/yO9+JxITY7e5/36Rysr2SUsoJLJli8hjj4lccoktFILIgAEiDz8sUlDQPsepoQGjEwkEymTFitHyySce2b//OTtz3z6R73/f1jZAZNw4kb/+VWTXrpZ3tm+fSGamDQy///2RX5grrhBJSGhbpv/RR1JXUgKRX/2qLafXenv2iGzYEJ59t0YwKPL5540zMnV0Bw60PmP+wQ/sZwhExo4VWbnSzr/qKlv7Xb/evg4ERAYPFhk+vPW1jPz8+hJ2Rob9e/bZx1dTF7HfqauvFnE4bBq//W2RRYtE7rjDBgMQ6dHDLv/rX4/cfu9ekVmz7Hqnny6yeHHLxwuFbCEvK8vWWmpVV9vj3nln/fnVnuutt4r85z9hq5FpwOhk/P6D8uWX58iiRcjmzXMkEPDZBaWlIn/+s8iwYfUfkP79bcnm6adFliyxVeNQyH75eve2pY233mr6QMuW2X088kjrElZZKTJokP1QVlSIXHqpDWL5+e1x2vUqKmzpKCZGZNOm9t13a/3hD/ba3H13ePYfCtlSanFx08srK0UefFDkxhttKfXFF0U++0ykpKT5/b3wgsicOSJ//KPIhx+K5OWFJ+1NHXvxYpELL7TX7H/+p3Hm1pTnn7fr3n67Lf2npNhM9hvfkLqmqIZeftnOnzfv6OnZts02Y7ndIq++ajPXJ58U6dbNHuPmm0W++KLpwFZRIZKd3fR+ly+36YyOFvnhD23waKiw0NYixo0T+fe/W07jggUiaWn2nG64wZ7XP/9pa1H//KetgVx8sQ0+td91h8N+pydMsOcC9hxnzLDnt3370a9NO9CA0QkFg9WyY8e9smgRsmLFWKmoaNDuGQrZ0vfjj9taQlJS/YcKbEYbGSmSmiqydm3LB5oyxa53tC+4iMh999n9v/eefb1+va29/PCHx36iTfnZz+xxYmNFRoywX+ITaf16e/0SE2063nzz+PcZDIrk5NgM4ZZbbKAHkfh4mzmWltav++mnIkOG2OU9ezZ+b2NjRX7yk8ZBevNmW3qG+uaI2mngQNvX1FxNKRA49nOqrhZ54w2RSZOkrmR9zTX2/xkzbHNoU5YtsxndOefUf+6Ki0Vuu81+nsaMOfLzGAjYazJ06JFpDoVsM9aSJTbjTEqy02efNV6voMAGKLfbpjEtTWTuXHsO994rMnmyfd/BNik13P7FF+126enHX0upVV5ujxsR0fg9q51OP90GkyeesAXCn/9c5KabRM47zzYLz5/f+HNzgmjA6MTy8t6SJUvi5dNPE6Ww8MOmVwoGbenivfdsO+b3v2+/fDk5Rz/AW2/Zt/WllxrPP3SocZV2yxb7ZZo9u/F6119v22QPL2013M/ixbbU+8orzWcihx/n2mtttRpsqbmtCgpsKc/vb9t2lZW2g7VHD9ssNnasDRy1fT+tEQrZ5oKrr7aZX+/eIk5nfUYQFydy2WUijz5a37TXo4dty7/1VqmrOS6o6cfy+WxN6513bHONMfWB47777PVKSLBNILWB6YMPbC1p3Lj6zGfePJvJL11qM6phw2y6Jk+2+/nii9Y1wW3aZAsJKSl23+npNqOuDex//atN41lniRQVNd523z67XXp60zXTjRubrxm98oo93j332PRed50NVrWl7dpp8GDbR9eckhJbw7nwwvr3JSJCZOJEu+9f/ML2oYDItGn2BpHaJq1w1NpycmwBcP16G4zWrDnyunUinSZgABcCW4DtwNwmlt8NbATWAR8B/RssCwJraqa3WnO8kyFgiIhUVGyX5cuHyaJFTsnOflxC7dl5FwzazGTUKBs8vv/9+iavqCg7/6qrREaPthnd4XeD7Nhhv2y33WZfBwK2OWTOHFsiNKbxlzk21jazfPhh0yXF6dNtqbs22M2da7f7xz9af075+SIjR9rt+vWzNTGfr/E6ZWX2bpXDr2Xt8Wqb8bZvt+c9aVLj0vAvfynSq5fNkH/wA5G337Zp/tvf7DUDm5HNmGHbuX/yExsgPv30yNLzF1/U1xAcDpG77mq55Lh+fX3gAFuqP3Cg6XVDIZHXX7cl89r3FGxGec45tg18/Pj6fXXrZmt4h2fmlZW2yeuMM+q3v+QSkX/9q+kgM2+e7QgeOdL2c91+u01zerqtBdX2T7RFIGBrnGDTm5oqcu65tsb2yCO2gLFzZ9va7nNzbU3i8DuIysrsPnv3tsf77ndbVwvvAjpFwACcwA4gA4gE1gJDD1vnHCC65v/bgNcaLCtr6zFPloAhIlJdXSLr1l1S06/xHQkG21hybskzz9Rn6B6PyAUX2C/53XfbDG/AAJtBPP1009vfdpsNGrfeajPR2qaRiy+2GeuCBTYzXbRI5Fvfqu+8HzLEZrS1mXZtO/WTTzY8cVsC9npt7eNoiopsrcDttiXsM8+0++zVS+T//T9bQxo8uD6DHDPGnn95uc04atu4G5o3z657xx22+ai2qeqii2wJtLYZo3YaPtxeq7Y0pYVCNnC0JSPduFFkxYrWrRsI2Az/5ptt8C0sbLw8P9+e56WXSl2z5t13i3z5pQ0gtW3pgwbZGyiaC1ANvf9+/XudmGiv+9SpIgsXtv4cD1dQYGs4hxcAwqWy8ujNul1MZwkYZwALG7y+F7i3hfXHAJ83eH1KBwwRkVAoIDt2zJVFi5BVqyZLaWk7taVWV9tmhI8/bv6L2FKtZv9+m8G43bap5Z//bDmzrKiwwWHQIPuROvdcG0x69rSl3cNrHnv22JJvbKyt6cyYYTO+hx6y7fe1Skpsh2BkZH1zTm3z0PTpUtdufemltknjj3+0mTvYJp2ePW0JuKm7xr773fqAcMklIqtWNT6fRYtEHnjA3kXWnjXAjrBhg23uqW2uMcYG/4UL237nTWVl25sFVafWWQLGlcAzDV5fDzzewvqPAz9t8DoArASWAZe2sN0tNeutTE1Nbf+reQLk5Lwsn36aKIsWOWTz5pulsrIVpb1w27On+Tt+mlNVZZtpajvtHY7GGXFD//2vbXq4+GJbg2jYGTx0qG3yOfNMW9NprpO6qYwrFLKdpbNnN91RWsvnE/ntb206uoodO2xz3o4dHZ0S1Ym0JWAYu377M8ZcCVwoIjfXvL4emCgitzex7nXA7cA0EfHXzOsjIvuMMRnAx8B0EdnR0jEzMzNl5cqV7X0qJ0R1dSG7d9/Pvn1PYEwkqak/ol+/H+B0Rnd00tquuBj+8AdISbHDQbTW3r3w73/DG2/AJ5+AMXbwxiuuCF9alerijDGrRCSzVeuGMWCcAdwnIl+reX0vgIg8cNh65wGPYYNFbjP7+jvwjojMb+mYJ3PAqFVRsZ2dO+eSn/86bnc/MjJ+T48eszHGdHTSTqz8fDsa5+DBHZ0SpU5pbQkY4RyxagVwmjEm3RgTCVwNvNVwBWPMGOCvwMyGwcIYk2iMcdf83x2YjL2b6pQXHT2Q4cPnM3r0YlyuJDZtuoYvv5zCoUMndyBss+7dNVgo1cmELWCISADbzLQQ2ATME5GvjDG/MsbMrFntIcAL/NMYs8YYUxtQhgArjTFrgUXAgyLSJQJGrYSEaYwbt5LBg5/B59vG6tXjWb/+UkpLV3V00pRSXVTYmqQ6wqnQJNWUQOAQ2dmPkJ39JwKBYrp1m0H//j8lPv6Mjk6aUuok11mapFQ7iYiIIy3tF0yatJv09N9y6NByvvzyTFauHEN29uNUV7fzjzIppVQTNGCcRCIi4ujf/14mTcritNMeBwzbt9/BF1+ksHHjtZSVrevoJCqlTmEaME5CERFe+vT5HpmZqxk3bjUpKTdTUPAOK1eOYsOGKykr29DRSVRKnYI0YJzkYmPHMGjQ40yalEX//j+jqOh9Vq4cyVdfXUVBwbsEg76OTqJS6hShnd6nmOrqQvbu/SP79j1GMHgIhyOaxMTz6N59Jt27X47LldjRSVRKdSKd4sG9jqABo14o5Ke4eDH5+W9TUPA2fv8ejHHTvfulpKR8i8TE6Rjj7OhkKqU6mAYM1YiIUFb2JTk5f+fgwZcJBApxu/uSnDyb5OTLiYubhDHaOqlUV6QBQzUrFPKTn/8WOTnPU1T0PiLVREb2pnv3y0hImEJMzEiiok7D4Yjo6KQqpU4ADRiqVQKBEgoK3iEv73UKC98jFLId5Ma4iYkZRmLi+fTseR1e7/AOTqlSKlw0YKg2C4X8VFRspqxsHeXl6ygtXU1x8SdAkJiYUfTseR1JSRcRHX269n0odQrRgKHaRVVVLrm5r3Hw4EuUlv4XAIcjGq93NLGxmURHn47b3Q+Ppx9udz8iIhK73qi6Sp3kNGCodufz7aCk5AtKS1dSWrqKsrIvCYUqGq3j8WTQs+f/0KPHtcTEnN5BKVVKtYUGDBV2IkGqqnLw+7OprNyL37+bwsKFFBV9BITwesfRvfulxMVNIDY2E5erW0cnWSnVBA0YqsP4/QfIzX2Vgwdfoqxsdd18j2cAbncfgsFSAoESgsFDRER0o1evG+jV6wbc7j4dmGqlui4NGKpTqK4upqxsFYcOraC0dAXV1XlERMTjdMYTERFPRcVGiosXAw6SkmbQo8c1eL1j9LZepU6gtgQM/VaqsHG5EkhMnE5i4vRm16mo2E5OzrPk5PydgoJ3ADAmkujoIURFDcThiMSYCIyJwOmMIy5uEvHxZ+Hx9D1Rp6GUqqE1DNUphEIBysvX10wbKC9fT2VlFiKBuqm6urCuo93t7k98/BlERQ0iKmoAUVED8HgyiIzsqU+tK9UGWsNQJx2HI4LY2DHExo5pdh0bVNZSUvJZzbSU3Nx5QKhuHWPcNbf5puJ29yYUqiYUqiAYrECkmujoIcTGZhIXN57o6KE4HK4TcHZKnRq0hqFOaqFQFZWVWfh8O6is3Ell5W4qK/fg9++mqioHYyJxOmNwOKIBKC/fQDBYAtQHl8jI3kRGpuB2p+BwxOBwuGsmDx5PGjExw/F40upqLsFgBRUVmygv34gxTiIje+N298Ht7o3TGdNh1+JkZB8OdZKQcFZHJ6XL0hqG6jIcjkiiowcRHT2oVeuLhPD5dlBaupKystX4/dn4/QcoK1tNYeEBgsEKGtZY6o8TQ0zMEKqri6is3Ak0XdCKjEzB6x1FTMwovN5ROBxRVFXtw+/fT1XVfhyOGOLiJhAXN775BEgAAA0uSURBVJGoqNMwxoHfn0NZ2ZeUla0mFKomMfFc4uIm4XBEtnAe9rbmYNBHVFRGk81wlZV7KS9fh8vVA7e7H5GRPdrUXFdaupoDB/5GVFQG3bp9nejowe32YGYoVMWuXT9h796HAUN6+v2kpt4b9ubE8vLNlJR8Ro8es4iIiA/rsU5FYa1hGGMuBP4PcALPiMiDhy13Ay8A44ACYLaIZNUsuxf4NhAE/ldEFh7teFrDUO0hFAogUkUwWIHPt72uT6WiYiMREYnExAwjJmY40dFDAaiq2o/fvw+/fx8VFZsoK1tLRcUmRKob7NVJZGQvgsESgsEyACIiEmoCyoEG6zmAEA5HDAkJ0/B6RxIIlBIIFBMMllBdnV8X5OxXAyIiEomLO4P4+DNxu/tTUvIZxcUf4/Nta3RexrjwePoTHz+FxMTzSEycTmRkzyPO/9Ch5WRl3U9h4bsY40bED9gHM5OSvk5MzPCaWpWtWblcPdoUSHy+LDZuvJrS0uX07n0bgcAhcnNfJinpGwwZ8nyrMvJQKEB+/htkZ/+RiootJCdfSa9eNxIXd0aTaSkt/ZLdu39Dfv6/AMHl6smAAQ/Ts+e1jdYXEaqqDhAR0Q2n09Pqc2ovIkIg8P/bu/PYOMozjuPfZ++N1/ERx8F2Ajm4mkokXOEKR4M4iiooEhQoRRShopYggUTVEvXmj1at1AKqEAVxlLaUUii0Ka2AEI6KtCQECCEHgRAuBxwHn7G99np3nv4xr82ShHgSkuwLfj7SyDuzs+vf7tj77Lwz876dxGJZ4vEJ++V3enFarYQdDr0OnAG0Ai8Al6jqurJ1rgaOUNVvi8jFwPmqepGIzAbuB+YBzcCTwKGqWtrV77SCYXwRBAUGBl5zvQG3kEpNRiSOaon+/vVs27ac3t7lBMEQudyRVFcfRS43B1Wlu/sZurqW0NW1hHx+E4lEjZtqSSTqSaenuuM0UxFJ0Nu7nJ6eZQwMhP9a8Xg1tbWnUlu7gOrqYykWO0YvsMznX6e7+xmKxS4AJkyYTTI5iVgsjUiKYrGH3t5lJBL1TJt2PS0tCykWu+no+DcdHf+iu/up0U4qR8RiVW4v73Cy2UMBpVDYwvDwFgqFdkQSpFKNJJONJBIT2bz5NkA57LA7aWy8EFVl8+bf8uab15PJzGTWrF+TTNYhknJ7WWERVQ2AgJ6eZbS23szg4FtkMrOYOPFYPvxwMUEwQDZ7KA0N5yGSQrUIlOjvX0Nn52PE4xNpabmGuroFbNq0iG3bXqCm5mRmzPg5g4Nvu/f8SQqF9wFIpZrJZKaTyUx3p4NXEYtVEY9nGR7uYni4nUKhneHhraiWEImPntGXzc4kl5tLLjeXqqo5JJO1H3vPwj3dTfT3r6avbzX5/Aby+Y3k8xspFrsRSVFTM5/6+rOoqzuTXG7ODoUwCAoMDb3H4OA7lErbaGg4b4/+Vn0pGCcAP1XVs9z8IgBV/UXZOo+7df4nIgmgDZgM3FC+bvl6u/qdVjDM542qRv72PjzcxdDQu2MezFct0de3is7OJfT2/pdSqY8gGCIIhgClsfEimpu/QyJRvcNjg6BIodBWtlfVSj6/kYGBDeTzGxgcfAeAZHISyeQUUqkp7gy38MO1WOykuvo4Zs/+M9nszI89d3f3f1i79msMD28Z87VOnHgi06Z9l4aGcxGJUyxuY+vWh2hru4eenueAmPsAj5NI1NHScjXNzQtHP7hVAz744C42bbqBYrETgERiEnV1p1NTcxLFYg+Dg2+56V1KpV5Kpf6yHp2TpFJTSCYbSSYnI5IoO6OvQD7/BoVC22jeWCxDLDaBeHwCsViGoaH3y7rWETKZ6WSzh5DNHkw2O4tCoY3Ozsfp71898gzusVXE4xMIgiG3Z6qj2efP/3DM921nfDmG0QK8VzbfChz3SeuoalFEeoBJbvnz2z3WLgU2487uNPUkk3WRhuAViVNdfTTV1Ufvdp5YLEEmM/UTr4MJi078Ey+8DIJh9y18x9dVW3sK8+atp79/NUFQQLXgfhZHP/whRjo9dYez6RKJapqarqCp6YpIr0MkRnPzt2hoOJ+Ojn+Sy80hl5s75jEU1YAgGCQWy465bYaG2ujvf4W+vlUMD3cQBHlKpQGCYID6+kZ3rOsIqqpm77T5adasXzE09D6dnU+Qz28cPdsvCPpHmxfT6YPIZMJpf/jMH/QWkauAqwAOPPDACqcxZnyLxdJj3L/r05iTyTpqa0/dm5F2KZVqiFxkICw0UY8tpNMHkE4fQH39WXsaj3S6maamb+7x4/e2fXlKwmZgWtn8VLdsp+u4JqkawoPfUR4LgKreoarHqOoxkydP3kvRjTHGbG9fFowXgENEZIaIpICLgcXbrbMYuNzdvgB4SsODKouBi0UkLSIzgEOAFfswqzHGmDHssyYpd0ziGuBxwtNq71bVtSJyI7BSVRcDdwF/FJGNQCdhUcGt91dgHVAEFo51hpQxxph9y670NsaYcWx3zpKyXtqMMcZEYgXDGGNMJFYwjDHGRGIFwxhjTCSfq4PeIrIVeGcPH94A7Nm19fuH7/nAMu4NvucD/zP6ng/8yniQqka6iO1zVTA+DRFZGfVMgUrwPR9Yxr3B93zgf0bf88FnI+POWJOUMcaYSKxgGGOMicQKxkfuqHSAMfieDyzj3uB7PvA/o+/54LORcQd2DMMYY0wktodhjDEmknFfMETkbBHZICIbReSGSucBEJG7RaRdRNaULasXkSUi8ob7OfZIOfsu3zQReVpE1onIWhG51sOMGRFZISKvuIw/c8tniMhyt70fcD0pV4yIxEXkZRF51NN8b4vIqyKySkRWumXebGeXp1ZEHhKR10RkvYic4EtGETnMvXcjU6+IXOdLvt01rguGG3f8VuDLwGzgEjeeeKX9Hjh7u2U3AEtV9RBgqZuvlCJwvarOBo4HFrr3zaeMQ8ACVZ0DzAXOFpHjgV8CN6nqwUAXcGUFMwJcC6wvm/ctH8CXVHVu2WmgPm1ngFuAx1T1cGAO4fvpRUZV3eDeu7nA0cAA8Igv+Xabqo7bCTgBeLxsfhGwqNK5XJbpwJqy+Q1Ak7vdBGyodMaybP8AzvA1IzABeIlwiOAPgcTOtn8Fck0l/LBYADwKiE/5XIa3gYbtlnmznQkHXXsLdzzWx4xlmc4ElvmaL8o0rvcw2Pm4476OHT5FVT9wt9uAKZUMM0JEpgNHAsvxLKNr7lkFtANLgDeBblUtulUqvb1vBr4HBG5+En7lA1DgCRF50Q2HDH5t5xnAVuAe17R3p4hU4VfGERcD97vbPuYb03gvGJ9JGn4tqfjpbSKSA/4GXKeqveX3+ZBRVUsaNgVMBeYBh1cyTzkR+QrQrqovVjrLGOar6lGEzbYLReSU8js92M4J4CjgNlU9Euhnu+YdDzLijkWdCzy4/X0+5ItqvBeMyGOHe2CLiDQBuJ/tlQwjIknCYnGfqj7sFnuVcYSqdgNPEzbx1Lrx46Gy2/sk4FwReRv4C2Gz1C34kw8AVd3sfrYTtr3Pw6/t3Aq0qupyN/8QYQHxKSOEBfclVd3i5n3LF8l4LxhRxh33Rfn455cTHjeoCBERwuF116vqb8ru8injZBGpdbezhMdY1hMWjgvcahXLqKqLVHWqqk4n/Lt7SlUv9SUfgIhUiUj1yG3CNvg1eLSdVbUNeE9EDnOLTicc2tmbjM4lfNQcBf7li6bSB1EqPQHnAK8Ttm//oNJ5XKb7gQ+AYcJvUFcStm8vBd4AngTqK5hvPuEu9GpglZvO8SzjEcDLLuMa4Mdu+UxgBbCRsHkg7cH2Pg141Ld8Lssrblo78v/h03Z2eeYCK922/jtQ51NGoAroAGrKlnmTb3cmu9LbGGNMJOO9ScoYY0xEVjCMMcZEYgXDGGNMJFYwjDHGRGIFwxhjTCRWMIzxgIicNtJjrTG+soJhjDEmEisYxuwGEfmGG2djlYjc7jo47BORm9y4G0tFZLJbd66IPC8iq0XkkZExD0TkYBF50o3V8ZKIzHJPnysb1+E+d0W9Md6wgmFMRCLyBeAi4CQNOzUsAZcSXsm7UlW/CDwL/MQ95A/A91X1CODVsuX3AbdqOFbHiYRX9UPY6+91hGOzzCTsb8oYbyTGXsUY45xOOAjOC+7Lf5aw07gAeMCt8yfgYRGpAWpV9Vm3/F7gQdc3U4uqPgKgqoMA7vlWqGqrm19FOCbKc/v+ZRkTjRUMY6IT4F5VXfSxhSI/2m69Pe1vZ6jsdgn7/zSesSYpY6JbClwgIo0wOrb1QYT/RyM9zH4deE5Ve4AuETnZLb8MeFZVtwGtIvJV9xxpEZmwX1+FMXvIvsEYE5GqrhORHxKOQBcj7E14IeGgPfPcfe2Exzkg7Lb6d64gbAKucMsvA24XkRvdc1y4H1+GMXvMeqs15lMSkT5VzVU6hzH7mjVJGWOMicT2MIwxxkRiexjGGGMisYJhjDEmEisYxhhjIrGCYYwxJhIrGMYYYyKxgmGMMSaS/wPOGU+GhJt5bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 663us/sample - loss: 0.2917 - acc: 0.9146\n",
      "Loss: 0.29169263855814315 Accuracy: 0.91464174\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2814 - acc: 0.6125\n",
      "Epoch 00001: val_loss improved from inf to 0.98755, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/001-0.9876.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.2814 - acc: 0.6125 - val_loss: 0.9876 - val_acc: 0.7249\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.8515\n",
      "Epoch 00002: val_loss improved from 0.98755 to 0.40574, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/002-0.4057.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.5255 - acc: 0.8515 - val_loss: 0.4057 - val_acc: 0.8875\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8990\n",
      "Epoch 00003: val_loss improved from 0.40574 to 0.31245, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/003-0.3125.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3629 - acc: 0.8989 - val_loss: 0.3125 - val_acc: 0.9154\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9214\n",
      "Epoch 00004: val_loss improved from 0.31245 to 0.28290, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/004-0.2829.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2867 - acc: 0.9214 - val_loss: 0.2829 - val_acc: 0.9227\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9340\n",
      "Epoch 00005: val_loss improved from 0.28290 to 0.22816, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/005-0.2282.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2383 - acc: 0.9340 - val_loss: 0.2282 - val_acc: 0.9413\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9445\n",
      "Epoch 00006: val_loss improved from 0.22816 to 0.22044, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/006-0.2204.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2014 - acc: 0.9445 - val_loss: 0.2204 - val_acc: 0.9378\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9519\n",
      "Epoch 00007: val_loss improved from 0.22044 to 0.20678, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/007-0.2068.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1769 - acc: 0.9519 - val_loss: 0.2068 - val_acc: 0.9467\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9584\n",
      "Epoch 00008: val_loss did not improve from 0.20678\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1544 - acc: 0.9584 - val_loss: 0.2658 - val_acc: 0.9241\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9626\n",
      "Epoch 00009: val_loss did not improve from 0.20678\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1399 - acc: 0.9626 - val_loss: 0.2136 - val_acc: 0.9380\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9676\n",
      "Epoch 00010: val_loss improved from 0.20678 to 0.18740, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/010-0.1874.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1209 - acc: 0.9676 - val_loss: 0.1874 - val_acc: 0.9441\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9721\n",
      "Epoch 00011: val_loss improved from 0.18740 to 0.17361, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/011-0.1736.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1083 - acc: 0.9721 - val_loss: 0.1736 - val_acc: 0.9499\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9761\n",
      "Epoch 00012: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0937 - acc: 0.9761 - val_loss: 0.1911 - val_acc: 0.9401\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9785\n",
      "Epoch 00013: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0881 - acc: 0.9785 - val_loss: 0.1736 - val_acc: 0.9471\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9827\n",
      "Epoch 00014: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0743 - acc: 0.9827 - val_loss: 0.2117 - val_acc: 0.9355\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9864\n",
      "Epoch 00015: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0646 - acc: 0.9864 - val_loss: 0.1815 - val_acc: 0.9485\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9864\n",
      "Epoch 00016: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0609 - acc: 0.9864 - val_loss: 0.1745 - val_acc: 0.9467\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9902\n",
      "Epoch 00017: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0506 - acc: 0.9901 - val_loss: 0.2313 - val_acc: 0.9311\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9891\n",
      "Epoch 00018: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0528 - acc: 0.9891 - val_loss: 0.1758 - val_acc: 0.9483\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9926\n",
      "Epoch 00019: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0399 - acc: 0.9926 - val_loss: 0.1952 - val_acc: 0.9411\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9940\n",
      "Epoch 00020: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0367 - acc: 0.9940 - val_loss: 0.1862 - val_acc: 0.9432\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9955\n",
      "Epoch 00021: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0314 - acc: 0.9955 - val_loss: 0.2039 - val_acc: 0.9411\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9959\n",
      "Epoch 00022: val_loss did not improve from 0.17361\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0283 - acc: 0.9959 - val_loss: 0.1949 - val_acc: 0.9429\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9965\n",
      "Epoch 00023: val_loss improved from 0.17361 to 0.16587, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/023-0.1659.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0259 - acc: 0.9965 - val_loss: 0.1659 - val_acc: 0.9520\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9971\n",
      "Epoch 00024: val_loss did not improve from 0.16587\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0231 - acc: 0.9971 - val_loss: 0.1711 - val_acc: 0.9504\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9974\n",
      "Epoch 00025: val_loss did not improve from 0.16587\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0210 - acc: 0.9974 - val_loss: 0.1804 - val_acc: 0.9488\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9967\n",
      "Epoch 00026: val_loss did not improve from 0.16587\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0218 - acc: 0.9967 - val_loss: 0.1757 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9976\n",
      "Epoch 00027: val_loss did not improve from 0.16587\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0174 - acc: 0.9976 - val_loss: 0.2011 - val_acc: 0.9469\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9943\n",
      "Epoch 00028: val_loss improved from 0.16587 to 0.16549, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/028-0.1655.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0282 - acc: 0.9943 - val_loss: 0.1655 - val_acc: 0.9555\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9963\n",
      "Epoch 00029: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0210 - acc: 0.9963 - val_loss: 0.1803 - val_acc: 0.9495\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9970\n",
      "Epoch 00030: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0194 - acc: 0.9970 - val_loss: 0.1962 - val_acc: 0.9439\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9987\n",
      "Epoch 00031: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0130 - acc: 0.9987 - val_loss: 0.1716 - val_acc: 0.9532\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9990\n",
      "Epoch 00032: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0100 - acc: 0.9990 - val_loss: 0.1784 - val_acc: 0.9515\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9987\n",
      "Epoch 00033: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0117 - acc: 0.9987 - val_loss: 0.2250 - val_acc: 0.9394\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9925\n",
      "Epoch 00034: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0299 - acc: 0.9924 - val_loss: 0.1910 - val_acc: 0.9485\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9973\n",
      "Epoch 00035: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0153 - acc: 0.9973 - val_loss: 0.1728 - val_acc: 0.9513\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9996\n",
      "Epoch 00036: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0059 - acc: 0.9996 - val_loss: 0.1805 - val_acc: 0.9515\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9979\n",
      "Epoch 00037: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0130 - acc: 0.9979 - val_loss: 0.1701 - val_acc: 0.9522\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9970\n",
      "Epoch 00038: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0163 - acc: 0.9970 - val_loss: 0.1762 - val_acc: 0.9548\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9994\n",
      "Epoch 00039: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0079 - acc: 0.9994 - val_loss: 0.1875 - val_acc: 0.9488\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9994\n",
      "Epoch 00040: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0067 - acc: 0.9994 - val_loss: 0.1871 - val_acc: 0.9495\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9988\n",
      "Epoch 00041: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0086 - acc: 0.9988 - val_loss: 0.2156 - val_acc: 0.9462\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9961\n",
      "Epoch 00042: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0172 - acc: 0.9961 - val_loss: 0.1967 - val_acc: 0.9483\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 00043: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0062 - acc: 0.9994 - val_loss: 0.2084 - val_acc: 0.9469\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9924\n",
      "Epoch 00044: val_loss did not improve from 0.16549\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0271 - acc: 0.9923 - val_loss: 0.1787 - val_acc: 0.9509\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9979\n",
      "Epoch 00045: val_loss improved from 0.16549 to 0.16386, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_6_conv_checkpoint/045-0.1639.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0115 - acc: 0.9979 - val_loss: 0.1639 - val_acc: 0.9564\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9997\n",
      "Epoch 00046: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0047 - acc: 0.9997 - val_loss: 0.1832 - val_acc: 0.9534\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9968\n",
      "Epoch 00047: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0142 - acc: 0.9968 - val_loss: 0.1857 - val_acc: 0.9543\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 00048: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0046 - acc: 0.9996 - val_loss: 0.1772 - val_acc: 0.9543\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9995\n",
      "Epoch 00049: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0046 - acc: 0.9995 - val_loss: 0.1774 - val_acc: 0.9534\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9987\n",
      "Epoch 00050: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0077 - acc: 0.9988 - val_loss: 0.2328 - val_acc: 0.9376\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9987\n",
      "Epoch 00051: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0080 - acc: 0.9987 - val_loss: 0.2167 - val_acc: 0.9439\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9936\n",
      "Epoch 00052: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0242 - acc: 0.9936 - val_loss: 0.1839 - val_acc: 0.9518\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9994\n",
      "Epoch 00053: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0061 - acc: 0.9994 - val_loss: 0.1784 - val_acc: 0.9550\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0074 - acc: 0.9989 - val_loss: 0.1810 - val_acc: 0.9536\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9998\n",
      "Epoch 00055: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0034 - acc: 0.9998 - val_loss: 0.1746 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9995\n",
      "Epoch 00056: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0050 - acc: 0.9995 - val_loss: 0.1749 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9992\n",
      "Epoch 00057: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0055 - acc: 0.9992 - val_loss: 0.1922 - val_acc: 0.9497\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9981\n",
      "Epoch 00058: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0094 - acc: 0.9981 - val_loss: 0.2138 - val_acc: 0.9469\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9996\n",
      "Epoch 00059: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0044 - acc: 0.9996 - val_loss: 0.2285 - val_acc: 0.9415\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9990\n",
      "Epoch 00060: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0059 - acc: 0.9990 - val_loss: 0.2385 - val_acc: 0.9397\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0189 - acc: 0.9951 - val_loss: 0.1926 - val_acc: 0.9520\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9995\n",
      "Epoch 00062: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0049 - acc: 0.9995 - val_loss: 0.1787 - val_acc: 0.9550\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 00063: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0028 - acc: 0.9998 - val_loss: 0.2168 - val_acc: 0.9474\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9989\n",
      "Epoch 00064: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0064 - acc: 0.9989 - val_loss: 0.2154 - val_acc: 0.9460\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9991\n",
      "Epoch 00065: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 0.2093 - val_acc: 0.9509\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9998\n",
      "Epoch 00066: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1817 - val_acc: 0.9564\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9990\n",
      "Epoch 00067: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0052 - acc: 0.9990 - val_loss: 0.2131 - val_acc: 0.9490\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9979\n",
      "Epoch 00068: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0093 - acc: 0.9979 - val_loss: 0.1888 - val_acc: 0.9550\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9994\n",
      "Epoch 00069: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0042 - acc: 0.9994 - val_loss: 0.1882 - val_acc: 0.9555\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00070: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0042 - acc: 0.9993 - val_loss: 0.2149 - val_acc: 0.9497\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 00071: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0047 - acc: 0.9992 - val_loss: 0.2098 - val_acc: 0.9492\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9957\n",
      "Epoch 00072: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0151 - acc: 0.9957 - val_loss: 0.2110 - val_acc: 0.9490\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00073: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1767 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00074: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.1811 - val_acc: 0.9567\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00075: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1849 - val_acc: 0.9576\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 00076: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0016 - acc: 0.9999 - val_loss: 0.1910 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00077: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.2975 - val_acc: 0.9327\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9991\n",
      "Epoch 00078: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0052 - acc: 0.9991 - val_loss: 0.1970 - val_acc: 0.9515\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9994\n",
      "Epoch 00079: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2642 - val_acc: 0.9401\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967\n",
      "Epoch 00080: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0126 - acc: 0.9967 - val_loss: 0.1944 - val_acc: 0.9527\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 00081: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.1939 - val_acc: 0.9525\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9978\n",
      "Epoch 00082: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0091 - acc: 0.9978 - val_loss: 0.1852 - val_acc: 0.9574\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 00083: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0075 - acc: 0.9981 - val_loss: 0.1816 - val_acc: 0.9567\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9994\n",
      "Epoch 00084: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0041 - acc: 0.9994 - val_loss: 0.1772 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00085: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.2012 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 00086: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.1971 - val_acc: 0.9550\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00087: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2212 - val_acc: 0.9539\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9957\n",
      "Epoch 00088: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.2129 - val_acc: 0.9525\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9979\n",
      "Epoch 00089: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0088 - acc: 0.9979 - val_loss: 0.1802 - val_acc: 0.9567\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00090: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1995 - val_acc: 0.9539\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00091: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.2004 - val_acc: 0.9562\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00092: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.2257 - val_acc: 0.9485\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9982\n",
      "Epoch 00093: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0093 - acc: 0.9982 - val_loss: 0.2001 - val_acc: 0.9522\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00094: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2166 - val_acc: 0.9502\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.16386\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0028 - acc: 0.9997 - val_loss: 0.1945 - val_acc: 0.9562\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.1957 - acc: 0.9464\n",
      "Loss: 0.19572171869833768 Accuracy: 0.94641745\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.6764\n",
      "Epoch 00001: val_loss improved from inf to 0.76089, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/001-0.7609.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 1.0664 - acc: 0.6764 - val_loss: 0.7609 - val_acc: 0.7885\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8852\n",
      "Epoch 00002: val_loss improved from 0.76089 to 0.28294, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/002-0.2829.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4024 - acc: 0.8852 - val_loss: 0.2829 - val_acc: 0.9252\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9209\n",
      "Epoch 00003: val_loss improved from 0.28294 to 0.25148, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/003-0.2515.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2749 - acc: 0.9209 - val_loss: 0.2515 - val_acc: 0.9222\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9373\n",
      "Epoch 00004: val_loss improved from 0.25148 to 0.20170, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/004-0.2017.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2174 - acc: 0.9373 - val_loss: 0.2017 - val_acc: 0.9399\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9495\n",
      "Epoch 00005: val_loss improved from 0.20170 to 0.19187, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/005-0.1919.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1785 - acc: 0.9495 - val_loss: 0.1919 - val_acc: 0.9420\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9574\n",
      "Epoch 00006: val_loss improved from 0.19187 to 0.19110, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/006-0.1911.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1497 - acc: 0.9573 - val_loss: 0.1911 - val_acc: 0.9429\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9621\n",
      "Epoch 00007: val_loss improved from 0.19110 to 0.15796, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/007-0.1580.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1332 - acc: 0.9621 - val_loss: 0.1580 - val_acc: 0.9506\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9703\n",
      "Epoch 00008: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1102 - acc: 0.9703 - val_loss: 0.1776 - val_acc: 0.9455\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9733\n",
      "Epoch 00009: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0981 - acc: 0.9733 - val_loss: 0.1614 - val_acc: 0.9504\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9781\n",
      "Epoch 00010: val_loss improved from 0.15796 to 0.14987, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/010-0.1499.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0844 - acc: 0.9781 - val_loss: 0.1499 - val_acc: 0.9567\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9818\n",
      "Epoch 00011: val_loss did not improve from 0.14987\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0723 - acc: 0.9818 - val_loss: 0.1676 - val_acc: 0.9518\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9824\n",
      "Epoch 00012: val_loss improved from 0.14987 to 0.13638, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/012-0.1364.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0699 - acc: 0.9823 - val_loss: 0.1364 - val_acc: 0.9595\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9865\n",
      "Epoch 00013: val_loss did not improve from 0.13638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0572 - acc: 0.9865 - val_loss: 0.1540 - val_acc: 0.9541\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9904\n",
      "Epoch 00014: val_loss did not improve from 0.13638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0457 - acc: 0.9904 - val_loss: 0.1390 - val_acc: 0.9543\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9905\n",
      "Epoch 00015: val_loss did not improve from 0.13638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0442 - acc: 0.9905 - val_loss: 0.1796 - val_acc: 0.9455\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9912\n",
      "Epoch 00016: val_loss did not improve from 0.13638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0425 - acc: 0.9913 - val_loss: 0.1600 - val_acc: 0.9504\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9962\n",
      "Epoch 00017: val_loss did not improve from 0.13638\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0265 - acc: 0.9962 - val_loss: 0.1373 - val_acc: 0.9611\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9964\n",
      "Epoch 00018: val_loss improved from 0.13638 to 0.13291, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/018-0.1329.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0244 - acc: 0.9964 - val_loss: 0.1329 - val_acc: 0.9592\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9959\n",
      "Epoch 00019: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0256 - acc: 0.9959 - val_loss: 0.1528 - val_acc: 0.9569\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9969\n",
      "Epoch 00020: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0213 - acc: 0.9969 - val_loss: 0.1367 - val_acc: 0.9606\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9906\n",
      "Epoch 00021: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0371 - acc: 0.9906 - val_loss: 0.1341 - val_acc: 0.9590\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9979\n",
      "Epoch 00022: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0154 - acc: 0.9979 - val_loss: 0.1331 - val_acc: 0.9618\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9959\n",
      "Epoch 00023: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0227 - acc: 0.9959 - val_loss: 0.1430 - val_acc: 0.9588\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9989\n",
      "Epoch 00024: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0116 - acc: 0.9988 - val_loss: 0.1520 - val_acc: 0.9560\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9918\n",
      "Epoch 00025: val_loss did not improve from 0.13291\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0316 - acc: 0.9918 - val_loss: 0.1383 - val_acc: 0.9599\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9994\n",
      "Epoch 00026: val_loss improved from 0.13291 to 0.13241, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/026-0.1324.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0090 - acc: 0.9994 - val_loss: 0.1324 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9991\n",
      "Epoch 00027: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9990 - val_loss: 0.1361 - val_acc: 0.9620\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9958\n",
      "Epoch 00028: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0203 - acc: 0.9958 - val_loss: 0.1495 - val_acc: 0.9585\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9992\n",
      "Epoch 00029: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0082 - acc: 0.9992 - val_loss: 0.1497 - val_acc: 0.9592\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9975\n",
      "Epoch 00030: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0145 - acc: 0.9975 - val_loss: 0.1331 - val_acc: 0.9630\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9989\n",
      "Epoch 00031: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0084 - acc: 0.9989 - val_loss: 0.1448 - val_acc: 0.9592\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9988\n",
      "Epoch 00032: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0092 - acc: 0.9988 - val_loss: 0.1558 - val_acc: 0.9578\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9982\n",
      "Epoch 00033: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0108 - acc: 0.9982 - val_loss: 0.2108 - val_acc: 0.9415\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9984\n",
      "Epoch 00034: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0100 - acc: 0.9984 - val_loss: 0.1580 - val_acc: 0.9581\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 00035: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0065 - acc: 0.9992 - val_loss: 0.1514 - val_acc: 0.9592\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9946\n",
      "Epoch 00036: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0210 - acc: 0.9946 - val_loss: 0.1373 - val_acc: 0.9648\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9998\n",
      "Epoch 00037: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0040 - acc: 0.9998 - val_loss: 0.1674 - val_acc: 0.9543\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 00038: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0031 - acc: 0.9999 - val_loss: 0.1429 - val_acc: 0.9602\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9981\n",
      "Epoch 00039: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.1846 - val_acc: 0.9520\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9985\n",
      "Epoch 00040: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0094 - acc: 0.9985 - val_loss: 0.1656 - val_acc: 0.9574\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9981\n",
      "Epoch 00041: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0094 - acc: 0.9981 - val_loss: 0.1588 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 00042: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0036 - acc: 0.9997 - val_loss: 0.1601 - val_acc: 0.9597\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9998\n",
      "Epoch 00043: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0033 - acc: 0.9998 - val_loss: 0.1584 - val_acc: 0.9604\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9988\n",
      "Epoch 00044: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0069 - acc: 0.9988 - val_loss: 0.2764 - val_acc: 0.9294\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0122 - acc: 0.9973 - val_loss: 0.1478 - val_acc: 0.9602\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 00046: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0099 - acc: 0.9980 - val_loss: 0.1567 - val_acc: 0.9599\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9987\n",
      "Epoch 00047: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9987 - val_loss: 0.1840 - val_acc: 0.9536\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 00048: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0030 - acc: 0.9997 - val_loss: 0.1525 - val_acc: 0.9611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9940\n",
      "Epoch 00049: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0216 - acc: 0.9940 - val_loss: 0.1486 - val_acc: 0.9604\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00050: val_loss did not improve from 0.13241\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.1329 - val_acc: 0.9660\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00051: val_loss improved from 0.13241 to 0.13011, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/051-0.1301.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1301 - val_acc: 0.9672\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.13011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0027 - acc: 0.9998 - val_loss: 0.1533 - val_acc: 0.9644\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9975\n",
      "Epoch 00053: val_loss did not improve from 0.13011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0103 - acc: 0.9975 - val_loss: 0.1525 - val_acc: 0.9648\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9991\n",
      "Epoch 00054: val_loss did not improve from 0.13011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0061 - acc: 0.9990 - val_loss: 0.1538 - val_acc: 0.9611\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9958\n",
      "Epoch 00055: val_loss did not improve from 0.13011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0161 - acc: 0.9958 - val_loss: 0.1426 - val_acc: 0.9651\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 00056: val_loss improved from 0.13011 to 0.12031, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_7_conv_checkpoint/056-0.1203.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0030 - acc: 0.9997 - val_loss: 0.1203 - val_acc: 0.9711\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9997\n",
      "Epoch 00057: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0024 - acc: 0.9997 - val_loss: 0.1444 - val_acc: 0.9646\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 00058: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 0.1376 - val_acc: 0.9674\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9982\n",
      "Epoch 00059: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2184 - val_acc: 0.9441\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9945\n",
      "Epoch 00060: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0199 - acc: 0.9945 - val_loss: 0.1437 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 00061: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1313 - val_acc: 0.9676\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00062: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1431 - val_acc: 0.9660\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9992\n",
      "Epoch 00063: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0050 - acc: 0.9992 - val_loss: 0.1387 - val_acc: 0.9676\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0039 - acc: 0.9993 - val_loss: 0.2285 - val_acc: 0.9411\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 00065: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 0.1603 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 00066: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.1510 - val_acc: 0.9604\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0125 - acc: 0.9962 - val_loss: 0.1429 - val_acc: 0.9653\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00068: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1449 - val_acc: 0.9660\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00069: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1791 - val_acc: 0.9588\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00070: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.1416 - val_acc: 0.9662\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00071: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1537 - val_acc: 0.9644\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9960\n",
      "Epoch 00072: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0138 - acc: 0.9960 - val_loss: 0.1516 - val_acc: 0.9613\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00073: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0023 - acc: 0.9997 - val_loss: 0.1518 - val_acc: 0.9618\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9987\n",
      "Epoch 00074: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.1323 - val_acc: 0.9665\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00075: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1460 - val_acc: 0.9634\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00076: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1719 - val_acc: 0.9597\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9948\n",
      "Epoch 00077: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0178 - acc: 0.9948 - val_loss: 0.1301 - val_acc: 0.9672\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00078: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1397 - val_acc: 0.9667\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00079: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1418 - val_acc: 0.9665\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00080: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0035 - acc: 0.9992 - val_loss: 0.1647 - val_acc: 0.9627\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00081: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1547 - val_acc: 0.9637\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00082: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0068 - acc: 0.9985 - val_loss: 0.1632 - val_acc: 0.9623\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9962\n",
      "Epoch 00083: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0126 - acc: 0.9963 - val_loss: 0.1295 - val_acc: 0.9679\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00084: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1282 - val_acc: 0.9693\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00085: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1336 - val_acc: 0.9693\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00086: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1405 - val_acc: 0.9665\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 00087: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0045 - acc: 0.9990 - val_loss: 0.2062 - val_acc: 0.9492\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00088: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0043 - acc: 0.9992 - val_loss: 0.1667 - val_acc: 0.9597\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9971\n",
      "Epoch 00089: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.1516 - val_acc: 0.9644\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9977\n",
      "Epoch 00090: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0094 - acc: 0.9977 - val_loss: 0.1432 - val_acc: 0.9644\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00091: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1340 - val_acc: 0.9665\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00092: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1431 - val_acc: 0.9679\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9966\n",
      "Epoch 00093: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0122 - acc: 0.9966 - val_loss: 0.1377 - val_acc: 0.9655\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00094: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1290 - val_acc: 0.9688\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00095: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1413 - val_acc: 0.9653\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00096: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.2097 - val_acc: 0.9541\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00097: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.1357 - val_acc: 0.9672\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00098: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.1668 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 00099: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0090 - acc: 0.9976 - val_loss: 0.1393 - val_acc: 0.9660\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00100: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1357 - val_acc: 0.9681\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00101: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1513 - val_acc: 0.9653\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00102: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1497 - val_acc: 0.9627\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9976\n",
      "Epoch 00103: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0085 - acc: 0.9975 - val_loss: 0.2012 - val_acc: 0.9555\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 00104: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0060 - acc: 0.9983 - val_loss: 0.1724 - val_acc: 0.9609\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00105: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1429 - val_acc: 0.9658\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 00106: val_loss did not improve from 0.12031\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1425 - val_acc: 0.9676\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 766us/sample - loss: 0.1808 - acc: 0.9529\n",
      "Loss: 0.18081887631695467 Accuracy: 0.95285565\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9957 - acc: 0.6963\n",
      "Epoch 00001: val_loss improved from inf to 0.85651, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/001-0.8565.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9958 - acc: 0.6962 - val_loss: 0.8565 - val_acc: 0.7347\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8880\n",
      "Epoch 00002: val_loss improved from 0.85651 to 0.26201, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/002-0.2620.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3730 - acc: 0.8880 - val_loss: 0.2620 - val_acc: 0.9255\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9243\n",
      "Epoch 00003: val_loss improved from 0.26201 to 0.25439, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/003-0.2544.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2565 - acc: 0.9242 - val_loss: 0.2544 - val_acc: 0.9238\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9413\n",
      "Epoch 00004: val_loss improved from 0.25439 to 0.18556, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/004-0.1856.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2010 - acc: 0.9413 - val_loss: 0.1856 - val_acc: 0.9434\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9521\n",
      "Epoch 00005: val_loss did not improve from 0.18556\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1606 - acc: 0.9521 - val_loss: 0.1911 - val_acc: 0.9418\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9601\n",
      "Epoch 00006: val_loss improved from 0.18556 to 0.17576, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/006-0.1758.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1350 - acc: 0.9600 - val_loss: 0.1758 - val_acc: 0.9462\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9652\n",
      "Epoch 00007: val_loss did not improve from 0.17576\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1190 - acc: 0.9652 - val_loss: 0.1835 - val_acc: 0.9425\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9730\n",
      "Epoch 00008: val_loss improved from 0.17576 to 0.16856, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/008-0.1686.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0953 - acc: 0.9730 - val_loss: 0.1686 - val_acc: 0.9497\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9782\n",
      "Epoch 00009: val_loss improved from 0.16856 to 0.16722, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/009-0.1672.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0817 - acc: 0.9782 - val_loss: 0.1672 - val_acc: 0.9474\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9812\n",
      "Epoch 00010: val_loss did not improve from 0.16722\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0703 - acc: 0.9812 - val_loss: 0.1838 - val_acc: 0.9420\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9837\n",
      "Epoch 00011: val_loss did not improve from 0.16722\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0616 - acc: 0.9837 - val_loss: 0.1723 - val_acc: 0.9453\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9856\n",
      "Epoch 00012: val_loss improved from 0.16722 to 0.15925, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/012-0.1592.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0562 - acc: 0.9856 - val_loss: 0.1592 - val_acc: 0.9457\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9909\n",
      "Epoch 00013: val_loss did not improve from 0.15925\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0400 - acc: 0.9909 - val_loss: 0.1810 - val_acc: 0.9434\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9887\n",
      "Epoch 00014: val_loss improved from 0.15925 to 0.14941, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/014-0.1494.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0444 - acc: 0.9888 - val_loss: 0.1494 - val_acc: 0.9529\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9937\n",
      "Epoch 00015: val_loss did not improve from 0.14941\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0307 - acc: 0.9937 - val_loss: 0.1974 - val_acc: 0.9401\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9899\n",
      "Epoch 00016: val_loss did not improve from 0.14941\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0405 - acc: 0.9899 - val_loss: 0.1497 - val_acc: 0.9560\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9968\n",
      "Epoch 00017: val_loss improved from 0.14941 to 0.14531, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/017-0.1453.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0197 - acc: 0.9968 - val_loss: 0.1453 - val_acc: 0.9588\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9949\n",
      "Epoch 00018: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0247 - acc: 0.9949 - val_loss: 0.1537 - val_acc: 0.9548\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9967\n",
      "Epoch 00019: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0191 - acc: 0.9967 - val_loss: 0.1693 - val_acc: 0.9502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9945\n",
      "Epoch 00020: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0256 - acc: 0.9945 - val_loss: 0.1579 - val_acc: 0.9504\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9981\n",
      "Epoch 00021: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0139 - acc: 0.9981 - val_loss: 0.1496 - val_acc: 0.9588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9974\n",
      "Epoch 00022: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0142 - acc: 0.9974 - val_loss: 0.1889 - val_acc: 0.9485\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9967\n",
      "Epoch 00023: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0177 - acc: 0.9967 - val_loss: 0.2818 - val_acc: 0.9280\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9968\n",
      "Epoch 00024: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0166 - acc: 0.9968 - val_loss: 0.1486 - val_acc: 0.9576\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9981\n",
      "Epoch 00025: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0113 - acc: 0.9981 - val_loss: 0.1727 - val_acc: 0.9522\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9891\n",
      "Epoch 00026: val_loss did not improve from 0.14531\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0372 - acc: 0.9891 - val_loss: 0.1579 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9991\n",
      "Epoch 00027: val_loss improved from 0.14531 to 0.14217, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/027-0.1422.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0074 - acc: 0.9991 - val_loss: 0.1422 - val_acc: 0.9595\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9994\n",
      "Epoch 00028: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0059 - acc: 0.9994 - val_loss: 0.1442 - val_acc: 0.9592\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9984\n",
      "Epoch 00029: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0086 - acc: 0.9984 - val_loss: 0.1574 - val_acc: 0.9576\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9990\n",
      "Epoch 00030: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0082 - acc: 0.9990 - val_loss: 0.1430 - val_acc: 0.9611\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9971\n",
      "Epoch 00031: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0123 - acc: 0.9971 - val_loss: 0.2720 - val_acc: 0.9327\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9969\n",
      "Epoch 00032: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0129 - acc: 0.9969 - val_loss: 0.1965 - val_acc: 0.9485\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9988\n",
      "Epoch 00033: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.1592 - val_acc: 0.9576\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9987\n",
      "Epoch 00034: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0072 - acc: 0.9987 - val_loss: 0.1785 - val_acc: 0.9527\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9979\n",
      "Epoch 00035: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0102 - acc: 0.9979 - val_loss: 0.1763 - val_acc: 0.9525\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9987\n",
      "Epoch 00036: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0075 - acc: 0.9987 - val_loss: 0.1756 - val_acc: 0.9576\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9992\n",
      "Epoch 00037: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0054 - acc: 0.9992 - val_loss: 0.1732 - val_acc: 0.9550\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9985\n",
      "Epoch 00038: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0071 - acc: 0.9985 - val_loss: 0.2556 - val_acc: 0.9311\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9954\n",
      "Epoch 00039: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0162 - acc: 0.9954 - val_loss: 0.2145 - val_acc: 0.9490\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9976\n",
      "Epoch 00040: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0098 - acc: 0.9976 - val_loss: 0.1546 - val_acc: 0.9613\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9987\n",
      "Epoch 00041: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1574 - val_acc: 0.9611\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9992\n",
      "Epoch 00042: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9992 - val_loss: 0.1743 - val_acc: 0.9541\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9987\n",
      "Epoch 00043: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.1495 - val_acc: 0.9639\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
      "Epoch 00044: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.2756 - val_acc: 0.9299\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9983\n",
      "Epoch 00045: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0077 - acc: 0.9983 - val_loss: 0.1539 - val_acc: 0.9609\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 00046: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0060 - acc: 0.9985 - val_loss: 0.2117 - val_acc: 0.9469\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9987\n",
      "Epoch 00047: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.1619 - val_acc: 0.9606\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9984\n",
      "Epoch 00048: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1825 - val_acc: 0.9555\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9941\n",
      "Epoch 00049: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0196 - acc: 0.9941 - val_loss: 0.1463 - val_acc: 0.9644\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 00050: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0033 - acc: 0.9996 - val_loss: 0.1568 - val_acc: 0.9637\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00051: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1519 - val_acc: 0.9651\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1705 - val_acc: 0.9611\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9980\n",
      "Epoch 00053: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0072 - acc: 0.9980 - val_loss: 0.3092 - val_acc: 0.9278\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 00054: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0109 - acc: 0.9966 - val_loss: 0.1772 - val_acc: 0.9569\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9957\n",
      "Epoch 00055: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0161 - acc: 0.9956 - val_loss: 0.1760 - val_acc: 0.9536\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 00056: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0094 - acc: 0.9977 - val_loss: 0.1526 - val_acc: 0.9634\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9980\n",
      "Epoch 00057: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0090 - acc: 0.9980 - val_loss: 0.1584 - val_acc: 0.9620\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00058: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1666 - val_acc: 0.9634\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00059: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1457 - val_acc: 0.9641\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00060: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1563 - val_acc: 0.9623\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978\n",
      "Epoch 00061: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0079 - acc: 0.9977 - val_loss: 0.2289 - val_acc: 0.9490\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00062: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0165 - acc: 0.9953 - val_loss: 0.1586 - val_acc: 0.9609\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00063: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.1534 - val_acc: 0.9651\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 00064: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1555 - val_acc: 0.9639\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00065: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1560 - val_acc: 0.9646\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9960\n",
      "Epoch 00066: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0131 - acc: 0.9960 - val_loss: 0.1448 - val_acc: 0.9632\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00067: val_loss did not improve from 0.14217\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1460 - val_acc: 0.9644\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00068: val_loss improved from 0.14217 to 0.13454, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_BN_8_conv_checkpoint/068-0.1345.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1345 - val_acc: 0.9679\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00069: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.2505 - val_acc: 0.9439\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.1754 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9970\n",
      "Epoch 00071: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0107 - acc: 0.9970 - val_loss: 0.1919 - val_acc: 0.9536\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00072: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1698 - val_acc: 0.9606\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00073: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1545 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00074: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1988 - val_acc: 0.9543\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9980\n",
      "Epoch 00075: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0077 - acc: 0.9980 - val_loss: 0.1738 - val_acc: 0.9578\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9983\n",
      "Epoch 00076: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0072 - acc: 0.9983 - val_loss: 0.1675 - val_acc: 0.9604\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00077: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0033 - acc: 0.9994 - val_loss: 0.1553 - val_acc: 0.9616\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00078: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.1614 - val_acc: 0.9644\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 00079: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0062 - acc: 0.9983 - val_loss: 0.3797 - val_acc: 0.9201\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
      "Epoch 00080: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.2134 - val_acc: 0.9525\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9975\n",
      "Epoch 00081: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0090 - acc: 0.9975 - val_loss: 0.1596 - val_acc: 0.9637\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00082: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1691 - val_acc: 0.9641\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 00083: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1529 - val_acc: 0.9630\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00084: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1502 - val_acc: 0.9644\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00085: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1533 - val_acc: 0.9632\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9975\n",
      "Epoch 00086: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.1625 - val_acc: 0.9627\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00087: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1769 - val_acc: 0.9609\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
      "Epoch 00088: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1590 - val_acc: 0.9630\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 00089: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1783 - val_acc: 0.9613\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9960\n",
      "Epoch 00090: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0130 - acc: 0.9960 - val_loss: 0.1532 - val_acc: 0.9639\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00091: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1503 - val_acc: 0.9665\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9975\n",
      "Epoch 00092: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.1469 - val_acc: 0.9679\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00093: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1542 - val_acc: 0.9651\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9969\n",
      "Epoch 00094: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0113 - acc: 0.9969 - val_loss: 0.1481 - val_acc: 0.9662\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00095: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1480 - val_acc: 0.9627\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9976\n",
      "Epoch 00096: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0093 - acc: 0.9976 - val_loss: 0.1484 - val_acc: 0.9665\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 00097: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1403 - val_acc: 0.9669\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00098: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1553 - val_acc: 0.9644\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00099: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.1623 - val_acc: 0.9632\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9973\n",
      "Epoch 00100: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1498 - val_acc: 0.9634\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00101: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1528 - val_acc: 0.9646\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00102: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1519 - val_acc: 0.9655\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00103: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0072 - acc: 0.9976 - val_loss: 0.1695 - val_acc: 0.9588\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984\n",
      "Epoch 00104: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0055 - acc: 0.9984 - val_loss: 0.1656 - val_acc: 0.9592\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00105: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1658 - val_acc: 0.9620\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1866 - val_acc: 0.9613\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00107: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.1719 - val_acc: 0.9620\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 00108: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0077 - acc: 0.9981 - val_loss: 0.1624 - val_acc: 0.9613\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00109: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1540 - val_acc: 0.9634\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00110: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0033 - acc: 0.9993 - val_loss: 0.1626 - val_acc: 0.9641\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 00111: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1643 - val_acc: 0.9637\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
      "Epoch 00112: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1497 - val_acc: 0.9669\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00113: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1515 - val_acc: 0.9644\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
      "Epoch 00114: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1495 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00115: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1559 - val_acc: 0.9646\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00116: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1551 - val_acc: 0.9634\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 00117: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0074 - acc: 0.9979 - val_loss: 0.1660 - val_acc: 0.9646\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00118: val_loss did not improve from 0.13454\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.1565 - val_acc: 0.9637\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 800us/sample - loss: 0.1825 - acc: 0.9566\n",
      "Loss: 0.1825448181131286 Accuracy: 0.956594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GMP_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 192)          768         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 770us/sample - loss: 0.5715 - acc: 0.8216\n",
      "Loss: 0.5714522497676243 Accuracy: 0.8215992\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 192)          768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 804us/sample - loss: 0.4344 - acc: 0.8708\n",
      "Loss: 0.4343552954843111 Accuracy: 0.87082034\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 256)          1024        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 864us/sample - loss: 0.2917 - acc: 0.9146\n",
      "Loss: 0.29169263855814315 Accuracy: 0.91464174\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 320)          1280        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 874us/sample - loss: 0.1957 - acc: 0.9464\n",
      "Loss: 0.19572171869833768 Accuracy: 0.94641745\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 384)          1536        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 931us/sample - loss: 0.1808 - acc: 0.9529\n",
      "Loss: 0.18081887631695467 Accuracy: 0.95285565\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 384)          1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 959us/sample - loss: 0.1825 - acc: 0.9566\n",
      "Loss: 0.1825448181131286 Accuracy: 0.956594\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GMP_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 192)          768         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 928us/sample - loss: 0.6305 - acc: 0.8183\n",
      "Loss: 0.630510833478791 Accuracy: 0.8182762\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 192)          768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 944us/sample - loss: 0.5381 - acc: 0.8685\n",
      "Loss: 0.5381309267145079 Accuracy: 0.8685358\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 256)          1024        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 982us/sample - loss: 0.3894 - acc: 0.9057\n",
      "Loss: 0.3894147629090435 Accuracy: 0.9057113\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 320)          1280        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2201 - acc: 0.9468\n",
      "Loss: 0.2201064288124891 Accuracy: 0.94683284\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 384)          1536        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1899 - acc: 0.9553\n",
      "Loss: 0.1898834477477588 Accuracy: 0.9553479\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 384)          1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2142 - acc: 0.9526\n",
      "Loss: 0.21417210460036049 Accuracy: 0.952648\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
